---
# Source: bsc/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-bsc
  labels: 
    app.kubernetes.io/name: bsc
    helm.sh/chart: bsc-0.6.43
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.3.13"
    app.kubernetes.io/managed-by: Helm
---
# Source: bsc/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: "my-release-config"
  labels: 
    app.kubernetes.io/name: bsc
    helm.sh/chart: bsc-0.6.43
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.3.13"
    app.kubernetes.io/managed-by: Helm
data:
  config.toml: |-
    [Eth]
    NetworkId = 56
    LightPeers = 100
    NoPruning = false
    NoPrefetch = false
    TrieTimeout = 150000000000
    DisablePeerTxBroadcast = true
    
    [Eth.Miner]
    GasCeil = 140000000
    GasPrice = 3000000000
    Recommit = 10000000000
    
    [Eth.TxPool]
    Locals = []
    NoLocals = true
    Journal = "transactions.rlp"
    Rejournal = 3600000000000
    PriceLimit = 3000000000
    PriceBump = 10
    AccountSlots = 200
    GlobalSlots = 8000
    AccountQueue = 200
    GlobalQueue = 4000
    
    [Eth.GPO]
    Blocks = 20
    Percentile = 60
    OracleThreshold = 1000
    
    [Node]
    IPCPath = "geth.ipc"
    HTTPHost = "0.0.0.0"
    InsecureUnlockAllowed = false
    HTTPPort = 8575
    HTTPVirtualHosts = ["*"]
    HTTPModules = ["eth","net","web3","txpool"]
    WSHost = "0.0.0.0"
    WSPort = 8576
    WSOrigins = ["*"]
    WSModules = ["net","web3","eth"]
    
    [Node.HTTPTimeouts]
    ReadTimeout = 30000000000
    WriteTimeout = 30000000000
    IdleTimeout = 120000000000
    
    [Node.LogConfig]
    FilePath = "bsc.log"
    MaxBytesSize = 10485760
    Level = "info"
    FileRoot = ""
    
    # keep this section the last one, as we may append trusted nodes via config generation
    [Node.P2P]
    EnableMsgEvents = false
    MaxPeers = 50
    NoDiscovery = false
    ListenAddr = ":30311"
    BootstrapNodes = ["enode://1cc4534b14cfe351ab740a1418ab944a234ca2f702915eadb7e558a02010cb7c5a8c295a3b56bcefa7701c07752acd5539cb13df2aab8ae2d98934d712611443@52.71.43.172:30311","enode://28b1d16562dac280dacaaf45d54516b85bc6c994252a9825c5cc4e080d3e53446d05f63ba495ea7d44d6c316b54cd92b245c5c328c37da24605c4a93a0d099c4@34.246.65.14:30311","enode://5a7b996048d1b0a07683a949662c87c09b55247ce774aeee10bb886892e586e3c604564393292e38ef43c023ee9981e1f8b335766ec4f0f256e57f8640b079d5@35.73.137.11:30311"]
    StaticNodes = ["enode://f3cfd69f2808ef64838abd8786342c0b22fdd28268703c8d6812e26e109f9a7cb2b37bd49724ebb46c233289f22da82991c87345eb9a2dadeddb8f37eeb259ac@18.180.28.21:30311","enode://ae74385270d4afeb953561603fcedc4a0e755a241ffdea31c3f751dc8be5bf29c03bf46e3051d1c8d997c45479a92632020c9a84b96dcb63b2259ec09b4fde38@54.178.30.104:30311","enode://d1cabe083d5fc1da9b510889188f06dab891935294e4569df759fc2c4d684b3b4982051b84a9a078512202ad947f9240adc5b6abea5320fb9a736d2f6751c52e@54.238.28.14:30311","enode://f420209bac5324326c116d38d83edfa2256c4101a27cd3e7f9b8287dc8526900f4137e915df6806986b28bc79b1e66679b544a1c515a95ede86f4d809bd65dab@54.178.62.117:30311","enode://c0e8d1abd27c3c13ca879e16f34c12ffee936a7e5d7b7fb6f1af5cc75c6fad704e5667c7bbf7826fcb200d22b9bf86395271b0f76c21e63ad9a388ed548d4c90@54.65.247.12:30311","enode://f1b49b1cf536e36f9a56730f7a0ece899e5efb344eec2fdca3a335465bc4f619b98121f4a5032a1218fa8b69a5488d1ec48afe2abda073280beec296b104db31@13.114.199.41:30311","enode://4924583cfb262b6e333969c86eab8da009b3f7d165cc9ad326914f576c575741e71dc6e64a830e833c25e8c45b906364e58e70cdf043651fd583082ea7db5e3b@18.180.17.171:30311","enode://4d041250eb4f05ab55af184a01aed1a71d241a94a03a5b86f4e32659e1ab1e144be919890682d4afb5e7afd837146ce584d61a38837553d95a7de1f28ea4513a@54.178.99.222:30311","enode://b5772a14fdaeebf4c1924e73c923bdf11c35240a6da7b9e5ec0e6cbb95e78327690b90e8ab0ea5270debc8834454b98eca34cc2a19817f5972498648a6959a3a@54.170.158.102:30311","enode://f329176b187cec87b327f82e78b6ece3102a0f7c89b92a5312e1674062c6e89f785f55fb1b167e369d71c66b0548994c6035c6d85849eccb434d4d9e0c489cdd@34.253.94.130:30311","enode://cbfd1219940d4e312ad94108e7fa3bc34c4c22081d6f334a2e7b36bb28928b56879924cf0353ad85fa5b2f3d5033bbe8ad5371feae9c2088214184be301ed658@54.75.11.3:30311","enode://c64b0a0c619c03c220ea0d7cac754931f967665f9e148b92d2e46761ad9180f5eb5aaef48dfc230d8db8f8c16d2265a3d5407b06bedcd5f0f5a22c2f51c2e69f@54.216.208.163:30311","enode://352a361a9240d4d23bb6fab19cc6dc5a5fc6921abf19de65afe13f1802780aecd67c8c09d8c89043ff86947f171d98ab06906ef616d58e718067e02abea0dda9@79.125.105.65:30311","enode://bb683ef5d03db7d945d6f84b88e5b98920b70aecc22abed8c00d6db621f784e4280e5813d12694c7a091543064456ad9789980766f3f1feb38906cf7255c33d6@54.195.127.237:30311","enode://11dc6fea50630b68a9289055d6b0fb0e22fb5048a3f4e4efd741a7ab09dd79e78d383efc052089e516f0a0f3eacdd5d3ffbe5279b36ecc42ad7cd1f2767fdbdb@46.137.182.25:30311","enode://21530e423b42aed17d7eef67882ebb23357db4f8b10c94d4c71191f52955d97dc13eec03cfeff0fe3a1c89c955e81a6970c09689d21ecbec2142b26b7e759c45@54.216.119.18:30311","enode://d61a31410c365e7fcd50e24d56a77d2d9741d4a57b295cc5070189ad90d0ec749d113b4b0432c6d795eb36597efce88d12ca45e645ec51b3a2144e1c1c41b66a@34.204.129.242:30311","enode://bb91215b1d77c892897048dd58f709f02aacb5355aa8f50f00b67c879c3dffd7eef5b5a152ac46cdfb255295bec4d06701a8032456703c6b604a4686d388ea8f@75.101.197.198:30311","enode://786acbdf5a3cf91b99047a0fd8305e11e54d96ea3a72b1527050d3d6f8c9fc0278ff9ef56f3e56b3b70a283d97c309065506ea2fc3eb9b62477fd014a3ec1a96@107.23.90.162:30311","enode://4653bc7c235c3480968e5e81d91123bc67626f35c207ae4acab89347db675a627784c5982431300c02f547a7d33558718f7795e848d547a327abb111eac73636@54.144.170.236:30311","enode://c6ffd994c4ef130f90f8ee2fc08c1b0f02a6e9b12152092bf5a03dd7af9fd33597d4b2e2000a271cc0648d5e55242aeadd6d5061bb2e596372655ba0722cc704@54.147.151.108:30311","enode://99b07e9dc5f204263b87243146743399b2bd60c98f68d1239a3461d09087e6c417e40f1106fa606ccf54159feabdddb4e7f367559b349a6511e66e525de4906e@54.81.225.170:30311","enode://1479af5ea7bda822e8747d0b967309bced22cad5083b93bc6f4e1d7da7be067cd8495dc4c5a71579f2da8d9068f0c43ad6933d2b335a545b4ae49a846122b261@52.7.247.132:30311","enode://43562d35f274d9e93f5ccac484c7cb185eabc746dbc9f3a56c36dc5a9ef05a3282695de7694a71c0bf4600651f49395b2ee7a6aaef857db2ac896e0fcbe6b518@35.73.15.198:30311","enode://08867e57849456fc9b0b00771f53e87ca6f2dd618c23b34a35d0c851cd484a4b7137905c5b357795025b368e4f8fe4c841b752b0c28cc2dbbf41a03d048e0e24@35.74.39.234:30311"]
---
# Source: bsc/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: "my-release-env"
  labels: 
    app.kubernetes.io/name: bsc
    helm.sh/chart: bsc-0.6.43
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.3.13"
    app.kubernetes.io/managed-by: Helm
data:
  SNAPSHOT_URL_123: "rsync://bsc-bootnode.bsc:1873/bsc"
---
# Source: bsc/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: "my-release-probe-env"
  labels: 
    app.kubernetes.io/name: bsc
    helm.sh/chart: bsc-0.6.43
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.3.13"
    app.kubernetes.io/managed-by: Helm
data:
  env.txt: |-
    ReadinessProbeTimestampDistinct=300
    LivenessProbeTimestampDistinct=300
    StartupProbeTimestampDistinct=300
---
# Source: bsc/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: "my-release-scripts"
  labels: 
    app.kubernetes.io/name: bsc
    helm.sh/chart: bsc-0.6.43
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.3.13"
    app.kubernetes.io/managed-by: Helm
data:
  check_node_health.sh: |-
    #!/usr/bin/env sh
    set -ex # -e exits on error
    
    usage() { echo "Usage: $0 <rpc_endpoint> <max_lag_in_seconds> <last_synced_block_file>]" 1>&2; exit 1; }
    
    rpc_endpoint="$1"
    max_lag_in_seconds="$2"
    last_synced_block_file="$3"
    
    if [ -z "${rpc_endpoint}" ] || [ -z "${max_lag_in_seconds}" ] || [ -z "${last_synced_block_file}" ]; then
        usage
    fi
    
    block_number=$(geth --datadir=/data attach --exec "eth.blockNumber")
    
    if [ -z "${block_number}" ] || [ "${block_number}" == "null" ]; then
        echo "Block number returned by the node is empty or null"
        exit 1
    fi
    
    if [ ! -f ${last_synced_block_file} ]; then
        old_block_number="";
    else
        old_block_number=$(cat ${last_synced_block_file});
    fi;
    
    if [ "${block_number}" != "${old_block_number}" ]; then
      mkdir -p $(dirname "${last_synced_block_file}")
      echo ${block_number} > ${last_synced_block_file}
    fi
    
    file_age=$(($(date +%s) - $(date -r ${last_synced_block_file} +%s)));
    max_age=${max_lag_in_seconds};
    echo "${last_synced_block_file} age is $file_age seconds. Max healthy age is $max_age seconds";
    if [ ${file_age} -lt ${max_age} ]; then exit 0; else exit 1; fi
    
  check_node_readiness.sh: |-
    #!/usr/bin/env sh
    set -ex
    
    public_bsc_node=$3
    allowed_number_of_distinct_between_blocks=$2
    allowed_number_of_time_gap_between_blocks=$2
    local_node_endpoint=${4:-}
    
    # Retrieving latest block timestamp from public bsc node
    function get_public_block {
        geth --datadir=/tmp attach $public_bsc_node --exec "eth.blockNumber" || exit 0
    }
    
    # Retrieving latest local block number
    function get_local_block {
        geth --config=/config/config.toml --datadir=/data attach $local_node_endpoint --exec "eth.blockNumber"
    }
    
    # Retrieving latest block timestamp of a local bsc node
    function get_local_timestamp {
        geth --config=/config/config.toml --datadir=/data attach $local_node_endpoint --exec "eth.getBlock(eth.blockNumber).timestamp"
    }
    
    case "$1" in
            # If public latest block number differs with local latest block for more than 10 blocks => fail, otherwise okay.
            --distinct-blocks)
                if [[ $(expr  $(get_public_block) - $(get_local_block)) -le $allowed_number_of_distinct_between_blocks ]]
                then
                  echo "Current block gap is lower that $allowed_number_of_distinct_between_blocks"
                  exit 0
                else
                  echo "Current block gap is higher that $allowed_number_of_distinct_between_blocks."
                  exit 1
                fi
                ;;
            # If local latest block's timestamp lower than current timestamp for more than 600 seconds (10 minutes)
            --timestamp-distinct)
                if [[ $(expr $(date +%s) - $(get_local_timestamp)) -le $allowed_number_of_time_gap_between_blocks ]]
                then
                  echo  "Current timestamp gap is lower that $allowed_number_of_time_gap_between_blocks"
                  exit 0
                else
                  echo  "Current timestamp gap is higher that $allowed_number_of_time_gap_between_blocks"
                  exit 1
                fi
                ;;
            *)
                echo "Usage: $0 {--distinct-blocks|--timestamp-distinct} {blocks-distinct,|time-range-distinct-seconds} {public-bsc-node-endpoint}
                      Blocks check:
                              $0 --distinct-blocks 10 https://bsc-dataseed1.binance.org
                      Timestamp check:
                              $0 --timestamp-distinct 300"
                exit 1
    esac
    
  get_nodekey.py: |-
    #!/usr/bin/env python
    import os
    import re
    import sys
    
    KEYS = ['']
    
    def get_nodekey():
        node_id = re.search(r'^.*-(\d+)$', os.uname()[1]).group(1)
        return KEYS[int(node_id)]
    
    if __name__ == '__main__':
        nodekey = get_nodekey()
        with open("/data/geth/nodekey", "w") as f:
            sys.stdout.write("Node key: {}".format(nodekey))
            f.write(nodekey)
    
  get_nodekey_ip.py: |-
    #!/usr/bin/env python
    import os
    import sys
    import ipaddress
    import json
    
    nodeKeysFileName = "/generated-config/nodekeys"
    
    def get_nodekey(nodeKeysFileName):
        addr=ipaddress.ip_address(os.environ['MY_POD_IP'])
        net=ipaddress.ip_network("192.168.0.0/20")
        if addr in net:
          node_id=int(addr)-int(net[0])
        else:
          sys.stdout.write("Pod address "+str(addr)+" in not inside network "+str(net))
          sys.exit(1)
        with open(nodeKeysFileName, "r") as f:
          KEYS=json.load(f)
          return KEYS[node_id]
    
    if __name__ == '__main__':
        nodekey = get_nodekey(nodeKeysFileName)
        with open("/data/geth/nodekey", "w") as f:
            sys.stdout.write("Node key: {}".format(nodekey))
            f.write(nodekey)
    
  generate_node_config.sh: |-
    #!/usr/bin/env sh
    
    set -ex # -e exits on error
    SRC_DIR=/config
    DST_DIR=/generated-config
    CONFIG_NAME=config.toml
    TRUSTED_NODES_SRC_URL=gs://bucket/trusted_nodes
    NODEKEYS_SRC_URL=gs://bucket/nodekeys
    NODEKEYS=nodekeys
    TRUSTED_NODES=trusted_nodes
    
    
    
    # check if we really need to generate config
    if [ "${GENERATE_CONFIG}" != "true" ];then
      echo "Config generation disabled, copying instead"
      cp -f "${SRC_DIR}/${CONFIG_NAME}" "${DST_DIR}/${CONFIG_NAME}"
      exit 0
    fi
    
    # config generation
    cd /tmp
    
    gsutil cp "${TRUSTED_NODES_SRC_URL}" "${TRUSTED_NODES}"
    
    # # https://askubuntu.com/a/1175271
    # # replace a matching line with a file content
    # sed  -e "/^TrustedNodes.*/{r${TRUSTED_NODES}" -e "d}" "${SRC_DIR}/${CONFIG_NAME}" > "${DST_DIR}/${CONFIG_NAME}"
    #
    # if [ -s "${DST_DIR}/${CONFIG_NAME}" ];then
    #   echo "Resulting config is empty"
    #   exit 1
    # fi
    
    cp "${SRC_DIR}/${CONFIG_NAME}" "${DST_DIR}/${CONFIG_NAME}"
    echo >> "${DST_DIR}/${CONFIG_NAME}"
    cat "${TRUSTED_NODES}" >> "${DST_DIR}/${CONFIG_NAME}"
    
  init_from_snaphot.sh: |-
    #!/usr/bin/env sh
    set -ex # -e exits on error
    
    DATA_DIR="/data"
    TEST_FILE="${DATA_DIR}/.initialized"
    SNAPSHOT_URL=""
    
    if [ -f ${TEST_FILE} ]; then
        echo "Blockchain already initialized. Exiting..."
        exit 0
    fi
    
    # Cleanup
    rm -rf ${DATA_DIR}/geth
    
    # Download & extract snapshot
    # special handling of zstd
    if [[ "${SNAPSHOT_URL}" =~ "\.zst$" ]]; then
      wget ${SNAPSHOT_URL} -O - | mbuffer -m5% -q -l /tmp/m1.log | zstd -d -T0 | mbuffer -m5% -q -l /tmp/m2.log | tar -b 2048 --overwrite -x -C ${DATA_DIR}
    else
      wget ${SNAPSHOT_URL} -O - | tar --overwrite -x -C ${DATA_DIR}
    fi
    
    
    
    # Mark data dir as initialized
    touch ${TEST_FILE}
    
  init_from_rsync.sh: |-
    #!/usr/bin/env sh
    set -ex # -e exits on error
    
    DATA_DIR="/data"
    TEST_FILE="${DATA_DIR}/.initialized"
    SNAPSHOT_URL="rsync://192.168.8.4/snapshot/geth/node/geth"
    
    # get statefulset pod number from pre-defined env var
    STS_POD_NUMBER=$(echo $MY_POD_NAME|sed -r 's/^.+\-([0-9]+)$/\1/')
    #generate variable name to check for URL override, it should be like SNAPSHOT_URL_123, if present
    VAR_NAME=\$SNAPSHOT_URL_${STS_POD_NUMBER}
    #get the URL, if any
    NEW_URL=$(eval echo $VAR_NAME)
    if [ ! -z "$NEW_URL" ];then
      SNAPSHOT_URL=$NEW_URL
    fi
    
    if [ -f ${TEST_FILE} ]; then
        echo "Blockchain already initialized. Exiting..."
        exit 0
    fi
    
    set +e
    # remove missing files to cleanup
    rsync -av --inplace --delete-before ${SNAPSHOT_URL}/ ${DATA_DIR}/
    # add more times to catch up
    rsync -av --inplace ${SNAPSHOT_URL}/ ${DATA_DIR}/
    rsync -av --inplace ${SNAPSHOT_URL}/ ${DATA_DIR}/
    rsync -av --inplace ${SNAPSHOT_URL}/ ${DATA_DIR}/
    set -e
    
    # Mark data dir as initialized
    touch ${TEST_FILE}
    
  init_from_gcs.sh: |-
    #!/usr/bin/env sh
    set -ex # -e exits on error
    
    # env required
    # S3_ENDPOINT_URL # f.e. "https://storage.googleapis.com"
    # AWS_ACCESS_KEY_ID
    # AWS_SECRET_ACCESS_KEY
    
    DATA_DIR="/data"
    GETH_DIR="${DATA_DIR}/geth"
    CHAINDATA_DIR="${GETH_DIR}/chaindata"
    STATE_TMP_DIR="${GETH_DIR}/state_tmp"
    ANCIENT_TMP_DIR="${GETH_DIR}/ancient_tmp"
    INITIALIZED_FILE="${DATA_DIR}/.initialized"
    OUT_OF_SPACE_FILE="${DATA_DIR}/.out_of_space"
    #without gs:// or s3://, just a bucket name and path
    INDEX_URL="bucket/path/to/file"
    GCS_BASE_URL=""
    S5CMD=/s5cmd
    INDEX="index"
    S_UPDATING="/updating"
    S_TIMESTAMP="/timestamp"
    S_STATE_URL="/state_url"
    S_ANCIENT_URL="/ancient_url"
    S_STATS="/stats"
    MAX_USED_SPACE_PERCENT=93
    
    # allow container interrupt
    trap "{ exit 1; }" INT TERM
    
    if [ -f "${INITIALIZED_FILE}" ]; then
        echo "Blockchain already initialized. Exiting..."
        exit 0
    fi
    
    if [ -f "${OUT_OF_SPACE_FILE}" ]; then
        echo "Seems, we're out of space. Exiting with an error ..."
        cat "${OUT_OF_SPACE_FILE}"
        exit 2
    fi
    
    # we need to create temp files
    cd /tmp
    
    if [ "${GCS_BASE_URL}" == "" ];then
      # get index of source base dirs
      ${S5CMD} cp "s3://${INDEX_URL}" "${INDEX}"
    
      # get the most fresh datadir
      # prune time is ignored here, we assume that all datadirs are pruned frequently enough
      MAX_TIMESTAMP=1
      for _GCS_BASE_URL in $(cat ${INDEX});do
        _TIMESTAMP_URL="${_GCS_BASE_URL}${S_TIMESTAMP}"
        _TIMESTAMP=$(${S5CMD} cat s3://${_TIMESTAMP_URL})
        if [ "${_TIMESTAMP}" -gt "${MAX_TIMESTAMP}" ];then
          GCS_BASE_URL="${_GCS_BASE_URL}"
          MAX_TIMESTAMP=${_TIMESTAMP}
        fi
      done
    else
      echo "Using overridden base URL: ${GCS_BASE_URL}"
    fi
    if [ "${GCS_BASE_URL}" == "" ];then
      echo "Fatal: cannot pick up correct base url, exiting"
      exit 1
    fi
    
    
    UPDATING_URL="${GCS_BASE_URL}${S_UPDATING}"
    TIMESTAMP_URL="${GCS_BASE_URL}${S_TIMESTAMP}"
    STATS_URL="${GCS_BASE_URL}${S_STATS}"
    
    # get state and ancient sources
    STATE_URL="${GCS_BASE_URL}${S_STATE_URL}"
    ANCIENT_URL="${GCS_BASE_URL}${S_ANCIENT_URL}"
    
    STATE_SRC="$(${S5CMD} cat s3://${STATE_URL})"
    ANCIENT_SRC="$(${S5CMD} cat s3://${ANCIENT_URL})"
    REMOTE_STATS="$(${S5CMD} cat s3://${STATS_URL})"
    
    # save sync source
    echo "${GCS_BASE_URL}" > "${DATA_DIR}/source"
    
    set +e
    
    # some background monitoring for humans
    set +x
    while true;do
      INODES=$(df -Phi ${DATA_DIR} | tail -n 1 | awk '{print $3}')
      SIZE=$(df -P -BG ${DATA_DIR} | tail -n 1 | awk '{print $3}')G
      echo -e "$(date -Iseconds) | SOURCE TOTAL ${REMOTE_STATS} | DST USED Inodes:\t${INODES} Size:\t${SIZE}"
      sleep 2
    done &
    MON_PID=$!
    set -x
    
    # get start and stop timestamps from the cloud
    UPDATING_0="$(${S5CMD} cat s3://${UPDATING_URL})"
    TIMESTAMP_0="$(${S5CMD} cat s3://${TIMESTAMP_URL})"
    
    
    # we're ready to perform actual data sync
    
    # we're done when all are true
    # 1) start and stop timestamps did not changed during data sync - no process started or finished updating the cloud
    # 2) start timestamp is before stop timestamp - no process is in progress updating the cloud
    # 3) 0 objects copied
    SYNC=2
    CLEANUP=1
    while [ "${SYNC}" -gt 0 ] ; do
    
        # Cleanup
        if [ ${CLEANUP} -eq 1 ];then
          echo "$(date -Iseconds) Cleaning up local dir ${GETH_DIR} ..."
          mkdir -p "${GETH_DIR}"
          mv "${GETH_DIR}" "${GETH_DIR}.old" && rm -rf "${GETH_DIR}.old" &
          CLEANUP=0
        fi
    
        # sync from cloud to local disk, with removing existing [missing in the cloud] files
        # run multiple syncs in background
    
        time ${S5CMD} sync --delete s3://${STATE_SRC}/* ${STATE_TMP_DIR}/ > cplist_state.txt &
        STATE_CP_PID=$!
        time nice ${S5CMD} sync --delete --part-size 200 --concurrency 2 s3://${ANCIENT_SRC}/* ${ANCIENT_TMP_DIR}/ > cplist_ancient.txt &
        ANCIENT_CP_PID=$!
    
        # wait for all syncs to complete
        # shell tracks all sub-processes and stores exit codes internally
        # it's not required to stay in wait state for all background processes at the same time
        # we'll handle these processes sequentially
        wait ${STATE_CP_PID}
        STATE_CP_EXIT_CODE=$?
        wait ${ANCIENT_CP_PID}
        ANCIENT_CP_EXIT_CODE=$?
    
        # let's handle out of disk space specially, thus we don't re-try, just stuck here if disk usage is high
        VOLUME_USAGE_PERCENT=$(df "${DATA_DIR}" | tail -n 1 | awk '{print $5}'|tr -d %)
        if [ "${VOLUME_USAGE_PERCENT}" -gt "${MAX_USED_SPACE_PERCENT}" ];then
          set +x
          # stop monitoring
          if [ ${MON_PID} -ne 0 ];then kill ${MON_PID};MON_PID=0; fi
          # out of inodes error is "handled" by "set -e"
          echo "We're out of disk space. Marking ${DATA_DIR} as out-of-space and exiting. Check the source snapshot size" | tee -a "${OUT_OF_SPACE_FILE}"
          echo "Source snapshot size ${REMOTE_STATS}" | tee -a "${OUT_OF_SPACE_FILE}"
          echo "Disk usage is ${VOLUME_USAGE_PERCENT}%" | tee -a "${OUT_OF_SPACE_FILE}"
          df -P -BG "${DATA_DIR}" | tee -a "${OUT_OF_SPACE_FILE}"
          exit 2
        fi
        # s5cmd uses 0 for success and 1 for any errors
        # no errors - we're good to go
        # any errors - retry the download
        # all the exit codes have to be 0
        if [ "${STATE_CP_EXIT_CODE}" -ne "0" ] || [ "${ANCIENT_CP_EXIT_CODE}" -ne "0" ];then
          echo "s5cmd sync returned non-zero, retrying sync after the short sleep"
          # wait some time to not spam with billable requests too frequently
          sleep 60
          SYNC=2
          continue
        fi
        # get start and stop timestamps from the cloud after sync
        UPDATING_1="$(${S5CMD} cat s3://${UPDATING_URL})"
        TIMESTAMP_1="$(${S5CMD} cat s3://${TIMESTAMP_URL})"
    
        # compare timestamps before and after sync
        # ensuring start timestamp is earlier than stop timestamp
        if [ "${UPDATING_0}" -eq "${UPDATING_1}" ] && [ "${TIMESTAMP_0}" -eq "${TIMESTAMP_1}" ] && [ "${TIMESTAMP_1}" -gt "${UPDATING_1}" ] ;then
          echo "Timestamps did not changed and start timestamp is before stop timestamp"
          echo -e "U_0=${UPDATING_0}\tU_1=${UPDATING_1},\tT_0=${TIMESTAMP_0}\tT_1=${TIMESTAMP_1}"
          let SYNC=SYNC-1
        else
          echo "Source timestamps changed or start timestamp is after stop timestamp, running sync again ..."
          echo -e "U_0=${UPDATING_0}\tU_1=${UPDATING_1},\tT_0=${TIMESTAMP_0}\tT_1=${TIMESTAMP_1}"
          # end  timestamps -> begin timestamps
          UPDATING_0=${UPDATING_1}
          TIMESTAMP_0=${TIMESTAMP_1}
          SYNC=2
          continue
        fi
    
        # stop monitoring, we don't expect massive data copying
        if [ ${MON_PID} -ne 0 ];then
          kill ${MON_PID}
          MON_PID=0
        fi
    
        # get number of objects copied
        CP_OBJ_NUMBER_STATE=$(wc -l <  cplist_state.txt )
        CP_OBJ_NUMBER_ANCIENT=$(wc -l < cplist_ancient.txt )
        #  0 objects copied ?
        if [ "${CP_OBJ_NUMBER_STATE}" -eq 0 ] && [ "${CP_OBJ_NUMBER_ANCIENT}" -eq 0 ];then
          echo -e "State objects copied:\t${CP_OBJ_NUMBER_STATE}, ancient objects copied:\t${CP_OBJ_NUMBER_ANCIENT}"
          let SYNC=SYNC-1
        else
          echo -e "State objects copied:\t${CP_OBJ_NUMBER_STATE}, ancient objects copied:\t${CP_OBJ_NUMBER_ANCIENT}, running sync again ... "
          SYNC=2
          continue
        fi
    done
    
    set -e
    # prepare geth datadir from tmp dirs
    mv "${STATE_TMP_DIR}" "${CHAINDATA_DIR}"
    rm -rf "${CHAINDATA_DIR}/ancient"
    mv "${ANCIENT_TMP_DIR}" "${CHAINDATA_DIR}/ancient"
    
    # Mark data dir as initialized
    touch ${INITIALIZED_FILE}
    
  sync_to_gcs.sh: |-
    #!/usr/bin/env sh
    set -ex # -e exits on error
    
    # env required
    # S3_ENDPOINT_URL # f.e. "https://storage.googleapis.com"
    # AWS_ACCESS_KEY_ID
    # AWS_SECRET_ACCESS_KEY
    # SYNC_TO_GCS True or any other value
    
    # enable sync via env
    if [ "${SYNC_TO_GCS}" != "True" ];then
      exit 0
    fi
    
    DATA_DIR="/data"
    INITIALIZED_FILE="${DATA_DIR}/.initialized"
    CHAINDATA_DIR="${DATA_DIR}/geth/chaindata"
    #without gs:// or s3://, just a bucket name and path
    GCS_BASE_URL="bucket/path/to/dir"
    # GSUTIL=$(which gsutil)
    
    S5CMD=/s5cmd
    CPLOG="${DATA_DIR}/cplog.txt"
    CPLIST="${DATA_DIR}/cplist.txt"
    RMLOG="${DATA_DIR}/rmlog.txt"
    RMLIST="${DATA_DIR}/rmlist.txt"
    
    # s5cmd excludes just by file extension, not by file path
    EXCLUDE_ANCIENT="--exclude *.cidx --exclude *.ridx --exclude *.cdat --exclude *.rdat"
    EXCLUDE_STATE="--exclude *.ldb --exclude *.sst"
    
    S_UPDATING="/updating"
    S_TIMESTAMP="/timestamp"
    S_STATE_URL="/state_url"
    S_ANCIENT_URL="/ancient_url"
    S_STATS="/stats"
    
    if [ "${GCS_BASE_URL}" == "" ];then
      echo "Fatal: cannot use empty base url, exiting"
      exit 1
    fi
    
    # find a file older than 30 minutes.
    # 0 - file not found, it means that file is fresh or missing in the filesystem or some find error
    # other value - file is found and is older than 30 minutes
    
    IS_INITIALIZED_FILE_OLD=$(find "${INITIALIZED_FILE}" -type f -mmin +30|wc -l)
    
    if [ "${IS_INITIALIZED_FILE_OLD}" == "0" ]; then
        echo "Blockchain initialized recently, skipping the upload. Exiting..."
        exit 0
    fi
    
    # we need to create temp files
    cd /tmp
    
    # get timestamp, state and ancient DSTs
    UPDATING_URL="${GCS_BASE_URL}${S_UPDATING}"
    TIMESTAMP_URL="${GCS_BASE_URL}${S_TIMESTAMP}"
    STATS_URL="${GCS_BASE_URL}${S_STATS}"
    
    STATE_URL="${GCS_BASE_URL}${S_STATE_URL}"
    ANCIENT_URL="${GCS_BASE_URL}${S_ANCIENT_URL}"
    
    STATE_DST="$(${S5CMD} cat s3://${STATE_URL})"
    ANCIENT_DST="$(${S5CMD} cat s3://${ANCIENT_URL})"
    
    # mark begin of sync in the cloud
    date +%s > updating
    ${S5CMD} cp updating "s3://${UPDATING_URL}"
    
    # we're ready to perform actual data copy
    
    # sync from local disk to cloud with removing existing [missing on local disk] files
    # run multiple syncs in background
    # sync is recursive by default, thus we need to exclude ancient data here
    time ${S5CMD} --stat --log error sync --delete ${EXCLUDE_ANCIENT} "${CHAINDATA_DIR}/" "s3://${STATE_DST}/" &
    STATE_CP_PID=$!
    time nice ${S5CMD} --stat --log error sync --delete --part-size 200 --concurrency 2 ${EXCLUDE_STATE} "${CHAINDATA_DIR}/ancient/" "s3://${ANCIENT_DST}/" &
    ANCIENT_CP_PID=$!
    # Wait for each specified child process and return its termination status
    # errors are "handled" by "set -e"
    wait ${ANCIENT_CP_PID}
    wait ${STATE_CP_PID}
    
    
    # update timestamp
    # TODO store timestamp inside readinnes check and use it instead of now()
    date +%s > timestamp
    ${S5CMD} cp timestamp "s3://${TIMESTAMP_URL}"
    
    # update stats
    INODES=$(df -Phi "${DATA_DIR}" | tail -n 1 | awk '{print $3}')
    # force GB output
    SIZE=$(df -P -BG "${DATA_DIR}" | tail -n 1 | awk '{print $3}')G
    echo -ne "Inodes:\t${INODES} Size:\t${SIZE}" > stats
    ${S5CMD} cp stats "s3://${STATS_URL}"
    
  gcs_cleanup.sh: |-
    #!/usr/bin/env sh
    set -ex
    
    # env required
    # AWS_ACCESS_KEY_ID
    # AWS_SECRET_ACCESS_KEY
    
    DATA_DIR="/data"
    RMLIST="${DATA_DIR}/rmlist.txt"
    RMLOG="${DATA_DIR}/rmlog.txt"
    GSUTIL="/google-cloud-sdk/bin/gsutil"
    GCLOUD="/google-cloud-sdk/bin/gcloud"
    BOTOCONFIG="${HOME}/.boto"
    
    # allow container interrupt
    trap "{ exit 1; }" INT TERM
    
    # disable gcloud auth
    ${GCLOUD} config set pass_credentials_to_gsutil false
    # using env vars to auth to GCS, as we use these env vars for s5cmd already
    set +x
    echo -e "[Credentials]\ngs_access_key_id = ${AWS_ACCESS_KEY_ID}\ngs_secret_access_key = ${AWS_SECRET_ACCESS_KEY}" > "${BOTOCONFIG}"
    set -x
    
    # creating empty list if missing, f.e .no sync-to-gcs were running
    touch "${RMLIST}"
    
    OBJ_TO_REMOVE=$(wc -l < "${RMLIST}")
    set +e
    if [ "${OBJ_TO_REMOVE}" -gt "10000" ];then
      split -l 2000 "${RMLIST}" /tmp/rmlist-
      find /tmp -name "rmlist-*" -print0| xargs -0 -P 50 -n1 -I{} bash -c "nice ${GSUTIL} rm -f -I < {}"
      echo "$(date -Iseconds) Removed objects: ${OBJ_TO_REMOVE}" | tee -a "${RMLOG}"
    else
      if [ "${OBJ_TO_REMOVE}" -gt "0" ];then
        time ${GSUTIL} -m rm -I < "${RMLIST}"
        echo "$(date -Iseconds) Removed objects: ${OBJ_TO_REMOVE}" | tee -a "${RMLOG}"
      else
        echo "No objects to remove"
      fi
    fi
    
    # cleanup removal list
    true > "${RMLIST}"
    
    # sleep in an endless cycle to allow container interrupt
    set +x
    while true; do sleep 10;done
    
  prune.sh: |-
    #!/usr/bin/env sh
    
    set -x
    # env required
    # BSC_PRUNE = True or any other value
    
    DATA_DIR="/data"
    
    # TODO
    # mark cloud timestamp as outdated, as we're not going to provide a fresh snapshot soon
    # it should be done by operator - just keep pod up & running for some time and cloud timestamp will lag on it's own
    
    GETH=/usr/local/bin/geth
    
    ret=0
    # we need to check env var to start pruning
    if [ "${BSC_PRUNE}" == "True" ] ; then
      # background logging
      tail -F "${DATA_DIR}/bsc.log" &
      $GETH --config=/config/config.toml --datadir=${DATA_DIR} --cache 8192 snapshot prune-state
      # prune-block will turn our full node into light one actually
      # $GETH --config=/config/config.toml --datadir=${DATA_DIR} --datadir.ancient=${DATA_DIR}/geth/chaindata/ancient --cache 8192 snapshot prune-block
      ret=$?
      if [ "${ret}" -eq "0" ];then
        # update timestamp
        date +%s > "${DATA_DIR}/prune_timestamp"
      fi
    fi
    
    exit $ret
    
  prune_block.sh: |-
    #!/usr/bin/env sh
    
    # script performs BSC block prune from ancientDB, removing [unneeded in some cases] historical blocks
    # good use case is a bootnode w/o RPC
    # block prune should speed up node in runtime as well as allow to use less disk space
    # after block prune the node will not be able to serve RPC on pruned blocks
    # Around last 90000 blocks are kept in the node state before moving on to ancientDB
    
    set -x
    
    DATA_DIR="/data"
    
    GETH=/usr/local/bin/geth
    # how much recent blocks do we need to keep. Default 0 means we clean up ancientDB completely
    BLOCKS_RESERVED=${1:-0}
    ANCIENT=${DATA_DIR}/geth/chaindata/ancient/
    ret=0
      # background logging
      tail -F "${DATA_DIR}/bsc.log" &
      # prune-block will turn our full node into light one actually
      $GETH --config=/config/config.toml --datadir=${DATA_DIR} --datadir.ancient=${ANCIENT} --cache 8192 snapshot prune-block --block-amount-reserved=${BLOCKS_RESERVED}
      ret=$?
      if [ "${ret}" -eq "0" ];then
        # update timestamp
        date +%s > "${DATA_DIR}/block_prune_timestamp"
      fi
    
    exit $ret
    
  cron.sh: |-
    #!/usr/bin/env sh
    
    # possible values are
    # "sync" to trigger sync-to-gcs
    # "prune" to trigger prune
    MODE="${1}"
    POD_NAME=my-release-bsc-0
    CONFIGMAP_NAME="my-release-env"
    KUBECTL=$(which kubectl)
    # wait timeout, f.e "30s"
    WAIT_TIMEOUT="${2}"
    PATCH_DATA=""
    
    check_ret(){
            ret="${1}"
            msg="${2}"
            # allow to override exit code, default value is ret
            exit_code=${3:-${ret}}
            if [ ! "${ret}" -eq 0 ];then
                    echo "${msg}"
                    echo "return code ${ret}, exit code ${exit_code}"
                    exit "${exit_code}"
            fi
    }
    
    # input data checking
    [ "${MODE}" = "prune" ] || [ "${MODE}" = "sync" ]
    check_ret $? "$(date -Iseconds) Mode value \"${MODE}\" is incorrect, abort"
    
    # wait for pod to become ready
    echo "$(date -Iseconds) Waiting ${WAIT_TIMEOUT} for pod ${POD_NAME} to become ready ..."
    ${KUBECTL} wait --timeout="${WAIT_TIMEOUT}" --for=condition=Ready pod "${POD_NAME}"
    ret=$?
    # exit code override to "success"
    check_ret "${ret}" "$(date -Iseconds) Pod ${POD_NAME} is not ready, nothing to do, exiting" 0
    
    # ensuring pod is not terminating now
    # https://github.com/kubernetes/kubernetes/issues/22839
    echo "$(date -Iseconds) Checking for pod ${POD_NAME} to not terminate ..."
    DELETION_TIMESTAMP=$(${KUBECTL} get -o jsonpath='{.metadata.deletionTimestamp}' pod "${POD_NAME}")
    ret=$?
    check_ret "${ret}" "$(date -Iseconds) Cannot get pod ${POD_NAME}, abort"
    
    # empty timestamp means that pod is not terminating now
    if [ "${DELETION_TIMESTAMP}" = "" ];then
      # we're good to go
      echo "$(date -Iseconds) Pod ${POD_NAME} is ready, continuing"
    
      case "${MODE}" in
      "sync")
        echo "$(date -Iseconds) Patching configmap ${CONFIGMAP_NAME} to enable sync and disable prune"
        # disable prune, enable sync-to-gcs
        PATCH_DATA='{"data":{"BSC_PRUNE":"False","SYNC_TO_GCS":"True"}}'
        ;;
      "prune")
        echo "$(date -Iseconds) Patching configmap ${CONFIGMAP_NAME} to enable prune and disable sync"
        # disable sync-to-gcs, enable prune
        PATCH_DATA='{"data":{"BSC_PRUNE":"True","SYNC_TO_GCS":"False"}}'
        ;;
      "*")
         check_ret 1 "$(date -Iseconds) Mode value \"${MODE}\" is incorrect, abort"
         ;;
      esac
      ${KUBECTL} patch configmap "${CONFIGMAP_NAME}"  --type merge --patch ${PATCH_DATA}
      ret=$?
      check_ret "${ret}" "$(date -Iseconds) Fatal: cannot patch configmap ${CONFIGMAP_NAME}, abort"
    
      echo "$(date -Iseconds) Deleting pod ${POD_NAME} to trigger action inside init container ..."
      # delete the pod to trigger action inside init container
      ${KUBECTL} delete pod "${POD_NAME}" --wait=false
      ret=$?
      check_ret "${ret}" "$(date -Iseconds) Fatal: cannot delete pod ${POD_NAME}, abort"
      echo "$(date -Iseconds) Pod ${POD_NAME} deleted successfully, exiting. Check pod logs after respawn."
    else
      # pod is terminating now, try later on next iteration
      echo "$(date -Iseconds) Pod ${POD_NAME} is terminating now, nothing to do, exiting"
      exit 0
    fi
    
  update-pod-deletion-cost.sh: |-
    #!/usr/bin/env sh
    
    # this script updates "pod-deletion-cost" pod annotation based on disk usage
    
    # required env
    # MY_POD_NAME - pod name to annotate
    
    # get used space from this dir's mount point
    DATA_DIR="${1}"
    # sleep between annotate iterations, in 10*second
    INTERVAL="${2}"
    
    KUBECTL=$(which kubectl)
    ANNOTATION=""
    
    # allow container interrupts
    trap "{ exit 1; }" INT TERM
    
    while [ true ]; do
      # get dir's mount point usage in MB
      SIZE=$(df -P -BM "${DATA_DIR}" | tail -n 1 | awk '{print $3}'|sed 's/M//g')
      # use negative values, the bigger is the disk usage the lower is the pod deletion cost
      # thus the most "heavy" pod will be removed first
      ANNOTATION="controller.kubernetes.io/pod-deletion-cost=-${SIZE}"
      ${KUBECTL} annotate --overwrite=true pod "${MY_POD_NAME}" "${ANNOTATION}"
      ret=$?
      if [ ${ret} -eq 0 ];then
        echo "$(date -Iseconds) Annotated pod ${MY_POD_NAME} with ${ANNOTATION}"
      else
        echo "$(date -Iseconds) Error annotating pod ${MY_POD_NAME} with ${ANNOTATION}"
      fi
      # we need to sleep <short-delay> inside cycle to handle pod termination w/o delays
      for i in $(seq 2 "${INTERVAL}");do sleep 10;done
    done
    
  controller-hook.sh: |-
    #!/usr/bin/env bash
    
    # This is needed to work-around GKE local SSD scale up issue https://github.com/kubernetes/autoscaler/issues/2145
    
    # watch for scale-related changes in the specified controller
    # in scale up case -> scale up secondary controller to the same replica value
    # in scale down case -> scale down secondary controller to 0
    
    controller=StatefulSet
    apiVersion="apps/v1"
    
    watchName="OnModified${controller}"
    debugLog="/tmp/debug.log"
    
    if [[ $1 == "--config" ]] ; then
      cat <<EOF
    configVersion: v1
    kubernetes:
    - name: "${watchName}"
      apiVersion: ${apiVersion}
      kind: $controller
      executeHookOnEvent: ["Modified"]
      labelSelector:
        matchLabels:
          app.kubernetes.io/name: bsc
          app.kubernetes.io/instance: my-release
      namespace:
        nameSelector:
          matchNames: ["bsc-0.6.43.tgz"]
      jqFilter: ".spec.replicas"
    EOF
    else
    	# ignore Synchronization for simplicity
    	type=$(jq -r '.[0].type' "${BINDING_CONTEXT_PATH}")
    	if [[ "${type}" == "Synchronization" ]] ; then
    	  echo Got Synchronization event
    	  exit 0
    	fi
      	ARRAY_COUNT=$(jq -r '. | length-1' "${BINDING_CONTEXT_PATH}")
    	for i in $(seq 0 "${ARRAY_COUNT}");do
    		bindingName=$(jq -r ".[$i].binding" "${BINDING_CONTEXT_PATH}")
    		resourceEvent=$(jq -r ".[$i].watchEvent" "${BINDING_CONTEXT_PATH}")
    		resourceName=$(jq -r ".[$i].object.metadata.name" "${BINDING_CONTEXT_PATH}")
    		if [[ "${bindingName}" == "${watchName}" && "${resourceEvent}" == "Modified" ]] ; then
    		  	echo "${controller} ${resourceName} was scaled"
    		  	newReplicas=$(jq -r ".[$i].object.spec.replicas" "${BINDING_CONTEXT_PATH}")
            oldReplicas=$(jq -r ".[$i].object.status.replicas" "${BINDING_CONTEXT_PATH}")
            addController=$(jq -r ".[$i].object.metadata.annotations.additionalControllerName" "${BINDING_CONTEXT_PATH}")
            namespace=$(jq -r ".[$i].object.metadata.namespace" "${BINDING_CONTEXT_PATH}")
            if [[ "${newReplicas}" -gt "${oldReplicas}" ]];then
              echo "$(date -Iseconds) scale UP ${controller} ${addController}, new=${newReplicas}, old=${oldReplicas}" | tee -a "${debugLog}"
              kubectl --namespace "${namespace}" scale "${controller}" "${addController}" --replicas="${newReplicas}"
            fi
            if [[ "${newReplicas}" -lt "${oldReplicas}" ]];then
              echo "$(date -Iseconds) scale DOWN ${controller} ${addController} to 0, new=${newReplicas}, old=${oldReplicas}" | tee -a "${debugLog}"
              kubectl --namespace "${namespace}" scale "${controller}" "${addController}" --replicas=0
            fi
    		fi
    	done
    #	echo  "$(date -Iseconds)"  >> "${debugLog}"
    #	cat "${BINDING_CONTEXT_PATH}" | jq . >> "${debugLog}"
    fi
    
  rotate.sh: |-
    #!/usr/bin/env bash
    
    # this script rotates bootnodes - replaces old node with a new one
    # scale up a new controller
    # wait a couple hours for bootnode to grab some p2p peers
    # ensure node is up & synced using JSON RPC via k8s service
    # scale down an old controller
    
    # We have to use 1 controller per bootnode due to external static IP and static node ID
    
    # f.e statefulset/bootnode-0 or cloneset/bootnode-1
    NEW_CONTROLLER="${1}"
    #
    OLD_CONTROLLER="${2}"
    # bsc rpc endpoint to check, f.e. bsc-bootnode-0:8575, where bsc-bootnode-0 is a k8s service name pointing to a single node
    RPC_ENDPOINT="${3}"
    # node is allowed to lag for seconds
    MAX_NODE_LAG=120
    # wait for the new node to spin up and sync up
    # ensure that cronjob's activeDeadlineSeconds is greater than this value
    WAIT_TIME=7200
    
    KUBECTL=$(which kubectl)
    CURL=$(which curl)
    JQ=$(which jq)
    
    check_ret(){
            ret="${1}"
            msg="${2}"
            # allow to override exit code, default value is ret
            exit_code=${3:-${ret}}
            if [ ! "${ret}" -eq 0 ];then
                    echo "${msg}"
                    echo "return code ${ret}, exit code ${exit_code}"
                    exit "${exit_code}"
            fi
    }
    
    # spinning up a new node
    echo "$(date -Iseconds) scaling ${NEW_CONTROLLER} up to 1"
    ${KUBECTL} scale "${NEW_CONTROLLER}" --replicas=1
    check_ret $? "$(date -Iseconds) FATAL: cannot scale ${NEW_CONTROLLER} to 1 replica" 1
    
    # wait for a new node to sync up
    echo "$(date -Iseconds) sleeping ${WAIT_TIME} seconds"
    sleep ${WAIT_TIME}
    echo "$(date -Iseconds) check node readinnes"
    
    # health check
    # cannot use existing check_node_readiness.sh as there is no bsc binary in kubectl docker image
    # get latest block and parse it's timestamp via jq
    JSON_RPC_REQUEST='{"jsonrpc":"2.0","method":"eth_getBlockByNumber","params":["latest",false],"id":1}'
    LATEST_BLOCK_TIMESTAMP_HEX=$(${CURL} -s --data-binary ${JSON_RPC_REQUEST} -H 'Content-Type: application/json' "${RPC_ENDPOINT}"|${JQ} -r .result.timestamp)
    [ -n "${LATEST_BLOCK_TIMESTAMP_HEX}" ]
    check_ret $? "$(date -Iseconds) FATAL: node ${NEW_CONTROLLER} did not pass the health check - empty LATEST_BLOCK_TIMESTAMP_HEX" 2
    echo "$(date -Iseconds) Latest block timestamp hex ${LATEST_BLOCK_TIMESTAMP_HEX}"
    
    # convert timestamp from hex to dec
    LATEST_BLOCK_TIMESTAMP=$(printf '%d' "${LATEST_BLOCK_TIMESTAMP_HEX}")
    [ -n "${LATEST_BLOCK_TIMESTAMP}" ]
    check_ret $? "$(date -Iseconds) FATAL: node ${NEW_CONTROLLER} did not pass the health check - empty LATEST_BLOCK_TIMESTAMP" 3
    echo "$(date -Iseconds) Latest block timestamp ${LATEST_BLOCK_TIMESTAMP}"
    
    # is node synced up ?
    [[ $(($(date +%s) - ${LATEST_BLOCK_TIMESTAMP})) -le ${MAX_NODE_LAG} ]]
    check_ret $? "$(date -Iseconds) FATAL: node ${NEW_CONTROLLER} timestamp lag is greater than ${MAX_NODE_LAG}, ts=${LATEST_BLOCK_TIMESTAMP}, now=$(date +%s)" 4
    echo "$(date -Iseconds) node ${NEW_CONTROLLER} timestamp ${LATEST_BLOCK_TIMESTAMP} is fresh, now=$(date +%s)"
    
    # scaling down an old node
    echo "$(date -Iseconds) scaling ${OLD_CONTROLLER} down to 0"
    ${KUBECTL} scale "${OLD_CONTROLLER}" --replicas=0
    check_ret $? "$(date -Iseconds) FATAL: cannot scale ${OLD_CONTROLLER} to 0 replica" 5
    exit 0
---
# Source: bsc/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release
  labels: 
    app.kubernetes.io/name: bsc
    helm.sh/chart: bsc-0.6.43
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.3.13"
    app.kubernetes.io/managed-by: Helm
spec:
  type:  ClusterIP
  ports:
  - port: 8575
    name: "jsonrpc"
    protocol: TCP
  - port: 8576
    name: "web-socket"
    protocol: TCP
  - port: 8577
    name: "qraphql"
    protocol: TCP
  - port: 30311
    name: "p2p"
    protocol: TCP
  - port: 30311
    name: "p2p-discovery"
    protocol: UDP
  - port: 9368
    name: "metrics"
    protocol: TCP
  - port: 6060
    name: bsc-metrics
    protocol: "TCP"
    targetPort: 6060
  selector:
    app.kubernetes.io/name: bsc
    app.kubernetes.io/instance: my-release
    manualstatus: "in-service"
---
# Source: bsc/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-bsc
  labels: 
    app.kubernetes.io/name: bsc
    helm.sh/chart: bsc-0.6.43
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.3.13"
    app.kubernetes.io/managed-by: Helm
  annotations:
spec:
  replicas: 1 # by default is 1
  updateStrategy:
    type: RollingUpdate
  serviceName: "my-release-service"
  podManagementPolicy: OrderedReady
  selector:
    matchLabels:
      app.kubernetes.io/name: bsc
      app.kubernetes.io/instance: my-release
  template:
    metadata:
      labels:
        app.kubernetes.io/name: bsc
        app.kubernetes.io/instance: my-release
        bsc/chain: mainnet
        bsc/role: rpc
        manualstatus: in-service
      annotations:
        checksum/config: 49ab0bd70c861c615b839b3c6eb800fb5f53a27a3fbe4f1388e9928eb8a3a94f
    spec:
      enableServiceLinks: false
      serviceAccountName: my-release-bsc
      securityContext:
        fsGroup: 101
        runAsGroup: 101
        runAsUser: 101
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchLabels:
                  app.kubernetes.io/name: bsc
                  bsc/chain: mainnet
              topologyKey: failure-domain.beta.kubernetes.io/zone
            weight: 100
      terminationGracePeriodSeconds: 180
      containers:
      - name: bsc
        image: "ghcr.io/bnb-chain/bsc:1.3.13"
        imagePullPolicy: Always
        command:
          - geth
        args:
          - --config=/config/config.toml
          - --datadir=/data
          - --syncmode=full
          - --gcmode=full
          - --maxpeers=50
          - --cache=8192
          - --snapshot=false
          - --persistdiff=false
          - --diffblock=86400
          - --port=30311
          - --rpc.allow-unprotected-txs
          - --history.transactions=0
          - --cache.preimages
          - --tries-verify-mode=local
          - --metrics
          - --pprof
          - --pprof.addr=0.0.0.0
          - --pprof.port=6060
        envFrom:
          - configMapRef:
              name: "my-release-env"
        workingDir: "/data"
        resources:
          {}
        ports:
        - containerPort: 8575
          name: "jsonrpc"
          protocol: TCP
        - containerPort: 8576
          name: "web-socket"
          protocol: TCP
        - containerPort: 8577
          name: "qraphql"
          protocol: TCP
        - containerPort: 30311
          hostPort: 30311
          name: "p2p"
          protocol: TCP
        - containerPort: 30311
          hostPort: 30311
          name: "p2p-discovery"
          protocol: UDP
        - containerPort: 9368
          name: "metrics"
          protocol: TCP
        volumeMounts:
        - name: generated-bsc-config
          mountPath: /config
        - name: scripts
          mountPath: /scripts
        - name: probe-env
          mountPath: /env
        - name: bsc-pvc
          mountPath: /data
        startupProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - source /env/env.txt && /bin/sh /scripts/check_node_health.sh http://127.0.0.1:8575 $StartupProbeTimestampDistinct last_synced_block.txt
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 10
          failureThreshold: 12
      - name: logger
        image: "krallin/ubuntu-tini:latest"
        imagePullPolicy: IfNotPresent
        args: [ "tail","-F","/data/bsc.log" ]
        lifecycle:
          preStop:
            exec:
              # just sleep a bit to tell k8s "kill the container a bit later", thus we'll be able to grab more logs
              command:
                - /bin/sh
                - -c
                - "sleep 30"
        resources:
          limits:
            cpu: 300m
            memory: 128Mi
          requests:
            cpu: 0m
            memory: 128Mi
        workingDir: "/data"
        volumeMounts:
          - name: bsc-pvc
            mountPath: /data
            readOnly: true
      initContainers:
      - name: remove-lock
        command:
          - rm
          - -f
          - /data/geth/LOCK
        image: busybox
        imagePullPolicy: IfNotPresent
        resources:
          {}
        volumeMounts:
          - name: bsc-pvc
            mountPath: /data
      - name: generate-bsc-config
        command:
          - sh
          - /scripts/generate_node_config.sh
        env:
          - name: GENERATE_CONFIG
            value: "false"
          - name: HOME
            value: "/tmp"
        image: google/cloud-sdk:alpine
        imagePullPolicy: IfNotPresent
        resources:
          {}
        volumeMounts:
          - name: bsc-config
            mountPath: /config
          - name: generated-bsc-config
            mountPath: /generated-config
          - name: scripts
            mountPath: /scripts
      volumes:
        - name: bsc-config
          configMap:
            name: "my-release-config"
        - name: generated-bsc-config
          emptyDir: {}
        - name: scripts
          configMap:
            name: "my-release-scripts"
        - name: probe-env
          configMap:
            name: "my-release-probe-env"
  volumeClaimTemplates:
    - metadata:
        name: bsc-pvc
        labels:
          app: bsc
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 1000Gi
        volumeMode: Filesystem
---
# Source: bsc/templates/cronjob.yaml
---
