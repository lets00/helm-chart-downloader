---
# Source: nvidia-gpu-exporter/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-nvidia-gpu-exporter
  labels:
    helm.sh/chart: nvidia-gpu-exporter-0.3.1
    app.kubernetes.io/name: nvidia-gpu-exporter
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "0.3.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: nvidia-gpu-exporter/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-nvidia-gpu-exporter
  labels:
    helm.sh/chart: nvidia-gpu-exporter-0.3.1
    app.kubernetes.io/name: nvidia-gpu-exporter
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "0.3.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 9835
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: nvidia-gpu-exporter
    app.kubernetes.io/instance: my-release
---
# Source: nvidia-gpu-exporter/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: my-release-nvidia-gpu-exporter
  labels:
    helm.sh/chart: nvidia-gpu-exporter-0.3.1
    app.kubernetes.io/name: nvidia-gpu-exporter
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "0.3.0"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: nvidia-gpu-exporter
      app.kubernetes.io/instance: my-release
  template:
    metadata:
      labels:
        app.kubernetes.io/name: nvidia-gpu-exporter
        app.kubernetes.io/instance: my-release
    spec:
      serviceAccountName: my-release-nvidia-gpu-exporter
      securityContext:
        {}
      containers:
        - name: nvidia-gpu-exporter
          securityContext:
            privileged: true
          image: "docker.io/utkuozdemir/nvidia_gpu_exporter:0.3.0"
          imagePullPolicy: IfNotPresent
          args:
            - --web.listen-address
            - :9835
            - --web.telemetry-path
            - /metrics
            - --nvidia-smi-command
            - nvidia-smi
            - --query-field-names
            - AUTO
            - --log.level
            - info
            - --log.format
            - logfmt
          ports:
            - name: http
              containerPort: 9835
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          volumeMounts:
            - mountPath: /dev/nvidiactl
              name: nvidiactl
            - mountPath: /dev/nvidia0
              name: nvidia0
            - mountPath: /usr/bin/nvidia-smi
              name: nvidia-smi
            - mountPath: /usr/lib/x86_64-linux-gnu/libnvidia-ml.so
              name: libnvidia-ml-so
            - mountPath: /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1
              name: libnvidia-ml-so-1
          resources:
            {}
      volumes:
        - hostPath:
            path: /dev/nvidiactl
          name: nvidiactl
        - hostPath:
            path: /dev/nvidia0
          name: nvidia0
        - hostPath:
            path: /usr/bin/nvidia-smi
          name: nvidia-smi
        - hostPath:
            path: /usr/lib/x86_64-linux-gnu/libnvidia-ml.so
          name: libnvidia-ml-so
        - hostPath:
            path: /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1
          name: libnvidia-ml-so-1
---
# Source: nvidia-gpu-exporter/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "my-release-nvidia-gpu-exporter-test-connection"
  labels:
    helm.sh/chart: nvidia-gpu-exporter-0.3.1
    app.kubernetes.io/name: nvidia-gpu-exporter
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "0.3.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['my-release-nvidia-gpu-exporter:9835']
  restartPolicy: Never
