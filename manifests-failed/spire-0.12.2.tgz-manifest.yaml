---
# Source: spire/charts/spiffe-csi-driver/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-spiffe-csi-driver
  namespace: spire-0.12.2.tgz
  labels:
    helm.sh/chart: spiffe-csi-driver-0.1.0
    app.kubernetes.io/name: spiffe-csi-driver
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "0.2.3"
    app.kubernetes.io/managed-by: Helm
---
# Source: spire/charts/spire-agent/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-agent
  namespace: spire-0.12.2.tgz
  labels:
    helm.sh/chart: spire-agent-0.1.0
    app.kubernetes.io/name: agent
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: spire/charts/spire-server/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-server
  labels:
    helm.sh/chart: spire-server-0.1.0
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: spire/charts/spire-agent/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-agent
  namespace: spire-0.12.2.tgz
data:
  agent.conf: |
    agent {
      data_dir = "/run/spire"
      log_level = "info"
      server_address = "my-release-server"
      server_port = "8081"
      socket_path = "/run/spire/agent-sockets/spire-agent.sock"
      trust_bundle_path = "/run/spire/bundle/bundle.crt"
      trust_domain = "example.org"
    }

    plugins {
      NodeAttestor "k8s_psat" {
        plugin_data {
          cluster = "example-cluster"
        }
      }

      KeyManager "memory" {
        plugin_data {
        }
      }

      WorkloadAttestor "k8s" {
        plugin_data {
          # Defaults to the secure kubelet port by default.
          # Minikube does not have a cert in the cluster CA bundle that
          # can authenticate the kubelet cert, so skip validation.
          skip_kubelet_verification = true
        }
      }

      WorkloadAttestor "unix" {
          plugin_data {
          }
      }
    }

    health_checks {
      listener_enabled = true
      bind_address = "0.0.0.0"
      bind_port = "8080"
      live_path = "/live"
      ready_path = "/ready"
    }
---
# Source: spire/charts/spire-server/templates/bundle-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: spire-bundle
  namespace: spire-0.12.2.tgz
---
# Source: spire/charts/spire-server/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-server
  namespace: spire-0.12.2.tgz
data:
  server.conf: |
    server {
      bind_address = "0.0.0.0"
      bind_port = "8081"
      socket_path = "/run/spire/server-sockets/spire-server.sock"
      trust_domain = "example.org"
      data_dir = "/run/spire/data"
      log_level = "info"
      # AWS requires the use of RSA.  EC cryptography is not supported
      ca_key_type = "rsa-2048"

      jwt_issuer = "oidc-discovery.example.org"

      default_x509_svid_ttl = "1h"
      default_jwt_svid_ttl = "1h"
      ca_subject = {
        country = ["NL"],
        organization = ["Example"],
        common_name = "example.org",
      }
    }

    plugins {
      DataStore "sql" {
        plugin_data {
          database_type = "sqlite3"
          connection_string = "/run/spire/data/datastore.sqlite3"
        }
      }

      NodeAttestor "k8s_psat" {
        plugin_data {
          clusters = {
            "example-cluster" = {
              service_account_allow_list = ["spire-0.12.2.tgz:my-release-agent"]
            }
          }
        }
      }

      KeyManager "disk" {
        plugin_data {
          keys_path = "/run/spire/data/keys.json"
        }
      }

      Notifier "k8sbundle" {
        plugin_data {
          namespace = "spire-0.12.2.tgz"
          config_map = "spire-bundle"
        }
      }
    }

    health_checks {
      listener_enabled = true
      bind_address = "0.0.0.0"
      bind_port = "8080"
      live_path = "/live"
      ready_path = "/ready"
    }
---
# Source: spire/charts/spire-server/templates/controller-manager-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-controller-manager
  namespace: spire-0.12.2.tgz
data:
  controller-manager-config.yaml: |
    apiVersion: spire.spiffe.io/v1alpha1
    kind: ControllerManagerConfig
    metadata:
      name: my-release-controller-manager
      namespace: spire-0.12.2.tgz
      labels:
        helm.sh/chart: spire-server-0.1.0
        app.kubernetes.io/name: server
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/version: "1.6.0"
        app.kubernetes.io/managed-by: Helm
    metrics:
      bindAddress: 0.0.0.0:8082
    health:
      healthProbeBindAddress: 0.0.0.0:8083
    leaderElection:
      leaderElect: true
      resourceName: 7e49158a.spiffe.io
      resourceNamespace: spire-0.12.2.tgz
    validatingWebhookConfigurationName: my-release-controller-manager-webhook
    clusterName: example-cluster
    trustDomain: example.org
    ignoreNamespaces:
      - kube-system
      - kube-public
      - local-path-storage
    spireServerSocketPath: "/run/spire/server-sockets/spire-server.sock"
---
# Source: spire/charts/spire-agent/templates/roles.yaml
# Required cluster role to allow spire-agent to query k8s API server
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: my-release-agent
rules:
  - apiGroups: [""]
    resources:
      - pods
      - nodes
      - nodes/proxy
    verbs: ["get"]
---
# Source: spire/charts/spire-server/templates/controller-manager-roles.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: my-release-controller-manager
rules:
  - apiGroups: [""]
    resources: ["namespaces"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["admissionregistration.k8s.io"]
    resources: ["validatingwebhookconfigurations"]
    verbs: ["get", "list", "patch", "watch"]
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["spire.spiffe.io"]
    resources: ["clusterfederatedtrustdomains"]
    verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
  - apiGroups: ["spire.spiffe.io"]
    resources: ["clusterfederatedtrustdomains/finalizers"]
    verbs: ["update"]
  - apiGroups: ["spire.spiffe.io"]
    resources: ["clusterfederatedtrustdomains/status"]
    verbs: ["get", "patch", "update"]
  - apiGroups: ["spire.spiffe.io"]
    resources: ["clusterspiffeids"]
    verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
  - apiGroups: ["spire.spiffe.io"]
    resources: ["clusterspiffeids/finalizers"]
    verbs: ["update"]
  - apiGroups: ["spire.spiffe.io"]
    resources: ["clusterspiffeids/status"]
    verbs: ["get", "patch", "update"]
---
# Source: spire/charts/spire-server/templates/roles.yaml
# ClusterRole to allow spire-server node attestor to query Token Review API
# and to be able to push certificate bundles to a configmap
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: my-release-server
rules:
    # allow TokenReview requests (to verify service account tokens for PSAT
    # attestation)
  - apiGroups: [authentication.k8s.io]
    resources: [tokenreviews]
    verbs:
      - get
      - watch
      - list
      - create
  - apiGroups: [""]
    resources: [nodes, pods]
    verbs:
      - get
      - list
---
# Source: spire/charts/spire-agent/templates/roles.yaml
# Binds above cluster role to spire-agent service account
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: my-release-agent
  namespace: spire-0.12.2.tgz
subjects:
  - kind: ServiceAccount
    name: my-release-agent
    namespace: spire-0.12.2.tgz
roleRef:
  kind: ClusterRole
  name: my-release-agent
  apiGroup: rbac.authorization.k8s.io
---
# Source: spire/charts/spire-server/templates/controller-manager-roles.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: my-release-controller-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: my-release-controller-manager
subjects:
- kind: ServiceAccount
  name: my-release-server
  namespace: spire-0.12.2.tgz
---
# Source: spire/charts/spire-server/templates/roles.yaml
# Binds above cluster role to spire-server service account
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: my-release-server
  namespace: spire-0.12.2.tgz
subjects:
  - kind: ServiceAccount
    name: my-release-server
    namespace: spire-0.12.2.tgz
roleRef:
  kind: ClusterRole
  name: my-release-server
  apiGroup: rbac.authorization.k8s.io
---
# Source: spire/charts/spire-server/templates/controller-manager-roles.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: my-release-controller-manager-leader-election
  namespace: spire-0.12.2.tgz
rules:
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
  - apiGroups: ["coordination.k8s.io"]
    resources: ["leases"]
    verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "patch"]
---
# Source: spire/charts/spire-server/templates/roles.yaml
# ClusterRole to allow spire-server node attestor to query Token Review API
# and to be able to push certificate bundles to a configmap
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: my-release-server
  namespace: spire-0.12.2.tgz
rules:
  # allow "get" access to pods (to resolve selectors for PSAT attestation)
  - apiGroups: [""]
    resources: [pods]
    verbs: [get]
    # allow access to "get" and "patch" the spire-bundle ConfigMap (for SPIRE
    # agent bootstrapping, see the spire-bundle ConfigMap below)
  - apiGroups: [""]
    resources: [configmaps]
    resourceNames: [spire-bundle]
    verbs:
      - get
      - patch
---
# Source: spire/charts/spire-server/templates/controller-manager-roles.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: my-release-controller-manager-leader-election
  namespace: spire-0.12.2.tgz
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: my-release-controller-manager-leader-election
subjects:
- kind: ServiceAccount
  name: my-release-server
  namespace: spire-0.12.2.tgz
---
# Source: spire/charts/spire-server/templates/roles.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: my-release-server
  namespace: spire-0.12.2.tgz
subjects:
  - kind: ServiceAccount
    name: my-release-server
    namespace: spire-0.12.2.tgz
roleRef:
  kind: Role
  name: my-release-server
  apiGroup: rbac.authorization.k8s.io
---
# Source: spire/charts/spire-server/templates/controller-manager-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-controller-manager-webhook
  namespace: spire-0.12.2.tgz
  labels:
    helm.sh/chart: spire-server-0.1.0
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: https
      port: 443
      targetPort: https
      protocol: TCP
  selector:
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: my-release
---
# Source: spire/charts/spire-server/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-server
  namespace: spire-0.12.2.tgz
  labels:
    helm.sh/chart: spire-server-0.1.0
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: grpc
      port: 8081
      targetPort: grpc
      protocol: TCP
  selector:
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: my-release
---
# Source: spire/charts/spiffe-csi-driver/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: my-release-spiffe-csi-driver
  namespace: spire-0.12.2.tgz
  labels:
    helm.sh/chart: spiffe-csi-driver-0.1.0
    app.kubernetes.io/name: spiffe-csi-driver
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "0.2.3"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: spiffe-csi-driver
      app.kubernetes.io/instance: my-release
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: spiffe-csi-driver
        app.kubernetes.io/instance: my-release
    spec:
      serviceAccountName: my-release-spiffe-csi-driver
      containers:
        # This is the container which runs the SPIFFE CSI driver.
        - name: spiffe-csi-driver
          image: ghcr.io/spiffe/spiffe-csi-driver:0.2.3
          imagePullPolicy: IfNotPresent
          args: [
            "-workload-api-socket-dir", "/spire-agent-socket",
            "-csi-socket-path", "/spiffe-csi/csi.sock",
          ]
          env:
            # The CSI driver needs a unique node ID. The node name can be
            # used for this purpose.
            - name: MY_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          volumeMounts:
            # The volume containing the SPIRE agent socket. The SPIFFE CSI
            # driver will mount this directory into containers.
            - mountPath: /spire-agent-socket
              name: spire-agent-socket-dir
              readOnly: true
            # The volume that will contain the CSI driver socket shared
            # with the kubelet and the driver registrar.
            - mountPath: /spiffe-csi
              name: spiffe-csi-socket-dir
            # The volume containing mount points for containers.
            - mountPath: /var/lib/kubelet/pods
              mountPropagation: Bidirectional
              name: mountpoint-dir
          securityContext:
            readOnlyRootFilesystem: true
            capabilities:
              drop:
                - all
            privileged: true
          resources:
            {}
        # This container runs the CSI Node Driver Registrar which takes care
        # of all the little details required to register a CSI driver with
        # the kubelet.
        - name: node-driver-registrar
          image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.6.2
          imagePullPolicy: IfNotPresent
          args: [
            "-csi-address", "/spiffe-csi/csi.sock",
            "-kubelet-registration-path", "/var/lib/kubelet/plugins/csi.spiffe.io/csi.sock",
            "-health-port", "9809"
          ]
          volumeMounts:
            # The registrar needs access to the SPIFFE CSI driver socket
            - mountPath: /spiffe-csi
              name: spiffe-csi-socket-dir
            # The registrar needs access to the Kubelet plugin registration
            # directory
            - name: kubelet-plugin-registration-dir
              mountPath: /registration
          ports:
            - containerPort: 9809
              name: healthz
          livenessProbe:
            httpGet:
              path: /healthz
              port: healthz
            initialDelaySeconds: 5
            timeoutSeconds: 5
          resources:
            {}
      volumes:
        - name: spire-agent-socket-dir
          hostPath:
            path: /run/spire/agent-sockets
            type: DirectoryOrCreate
        # This volume is where the socket for kubelet->driver communication lives
        - name: spiffe-csi-socket-dir
          hostPath:
            path: /var/lib/kubelet/plugins/csi.spiffe.io
            type: DirectoryOrCreate
        # This volume is where the SPIFFE CSI driver mounts volumes
        - name: mountpoint-dir
          hostPath:
            path: /var/lib/kubelet/pods
            type: Directory
        # This volume is where the node-driver-registrar registers the plugin
        # with kubelet
        - name: kubelet-plugin-registration-dir
          hostPath:
            path: /var/lib/kubelet/plugins_registry
            type: Directory
---
# Source: spire/charts/spire-agent/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: my-release-agent
  namespace: spire-0.12.2.tgz
  labels:
    helm.sh/chart: spire-agent-0.1.0
    app.kubernetes.io/name: agent
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: agent
      app.kubernetes.io/instance: my-release
  template:
    metadata:
      labels:
        app.kubernetes.io/name: agent
        app.kubernetes.io/instance: my-release
    spec:
      hostPID: true
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      serviceAccountName: my-release-agent
      securityContext:
        {}
      initContainers:
        - name: init
          # This is a small image with wait-for-it, choose whatever image
          # you prefer that waits for a service to be up. This image is built
          # from https://github.com/vishnubob/wait-for-it
          image: cgr.dev/chainguard/wait-for-it:latest-20230113
          imagePullPolicy: IfNotPresent
          args: ["-t", "30", "-h", "my-release-server", "-p", "8081"]
          resources:
            {}
      containers:
        - name: spire-agent
          image: ghcr.io/spiffe/spire-agent:1.6.0
          imagePullPolicy: IfNotPresent
          args: ["-config", "/run/spire/config/agent.conf"]
          volumeMounts:
            - name: spire-config
              mountPath: /run/spire/config
              readOnly: true
            - name: spire-bundle
              mountPath: /run/spire/bundle
              readOnly: true
            - name: spire-agent-socket-dir
              mountPath: /run/spire/agent-sockets
              readOnly: false
            - name: spire-token
              mountPath: /var/run/secrets/tokens
          livenessProbe:
            httpGet:
              path: /live
              port: 8080
            initialDelaySeconds: 15
            periodSeconds: 60
          readinessProbe:
            httpGet:
              path: /ready
              port: 8080
            initialDelaySeconds: 15
            periodSeconds: 60
          resources:
            {}
      volumes:
        - name: spire-config
          configMap:
            name: my-release-agent
        - name: spire-bundle
          configMap:
            name: spire-bundle
        - name: spire-token
          projected:
            sources:
            - serviceAccountToken:
                path: spire-agent
                expirationSeconds: 7200
                audience: spire-server
        - name: spire-agent-socket-dir
          hostPath:
            path: /run/spire/agent-sockets
            type: DirectoryOrCreate
---
# Source: spire/charts/spire-server/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-server
  labels:
    helm.sh/chart: spire-server-0.1.0
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  serviceName: my-release-server
  selector:
    matchLabels:
      app.kubernetes.io/name: server
      app.kubernetes.io/instance: my-release
  template:
    metadata:
      labels:
        app.kubernetes.io/name: server
        app.kubernetes.io/instance: my-release
    spec:
      serviceAccountName: my-release-server
      shareProcessNamespace: true
      securityContext:
        {}
      containers:
        - name: spire-server
          securityContext:
            {}
          image: ghcr.io/spiffe/spire-server:1.6.0
          imagePullPolicy: IfNotPresent
          args:
            - -config
            - /run/spire/config/server.conf
          ports:
            - name: grpc
              containerPort: 8081
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /live
              port: 8080
            failureThreshold: 2
            initialDelaySeconds: 15
            periodSeconds: 60
            timeoutSeconds: 3
          readinessProbe:
            httpGet:
              path: /ready
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 5
          resources:
            {}
          volumeMounts:
            - name: spire-server-socket
              mountPath: /run/spire/server-sockets
              readOnly: false
            - name: spire-config
              mountPath: /run/spire/config
              readOnly: true
            - name: spire-data
              mountPath: /run/spire/data
              readOnly: false
        - name: spire-controller-manager
          securityContext:
            {}
          image: ghcr.io/spiffe/spire-controller-manager:0.2.2
          imagePullPolicy: IfNotPresent
          args:
            - --config=controller-manager-config.yaml
          ports:
            - name: https
              containerPort: 9443
              protocol: TCP
            - containerPort: 8083
              name: healthz
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /healthz
              port: healthz
          readinessProbe:
            httpGet:
              path: /readyz
              port: healthz
          resources:
            {}
          volumeMounts:
            - name: spire-server-socket
              mountPath: /run/spire/server-sockets
              readOnly: true
            - name: controller-manager-config
              mountPath: /controller-manager-config.yaml
              subPath: controller-manager-config.yaml
              readOnly: true
      volumes:
        - name: spire-config
          configMap:
            name: my-release-server
        - name: spire-server-socket
          emptyDir: {}
        - name: controller-manager-config
          configMap:
            name: my-release-controller-manager
  volumeClaimTemplates:
    - metadata:
        name: spire-data
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 1Gi
---
# Source: spire/charts/spiffe-csi-driver/templates/spiffe-csi-driver.yaml
apiVersion: storage.k8s.io/v1
kind: CSIDriver
metadata:
  name: "csi.spiffe.io"
spec:
  # Only ephemeral, inline volumes are supported. There is no need for a
  # controller to provision and attach volumes.
  attachRequired: false

  # Request the pod information which the CSI driver uses to verify that an
  # ephemeral mount was requested.
  podInfoOnMount: true

  # Don't change ownership on the contents of the mount since the Workload API
  # Unix Domain Socket is typically open to all (i.e. 0777).
  fsGroupPolicy: None

  # Declare support for ephemeral volumes only.
  volumeLifecycleModes:
    - Ephemeral
---
# Source: spire/charts/spire-server/templates/controller-manager-cluster-ids.yaml
apiVersion: spire.spiffe.io/v1alpha1
kind: ClusterSPIFFEID
metadata:
  name: my-release-controller-manager-service-account-based
spec:
  spiffeIDTemplate: "spiffe://{{ .TrustDomain }}/ns/{{ .PodMeta.Namespace }}/sa/{{ .PodSpec.ServiceAccountName }}"
---
# Source: spire/charts/spire-server/templates/controller-manager-webhook.yaml
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  name: my-release-controller-manager-webhook
webhooks:
  - admissionReviewVersions: ["v1"]
    clientConfig:
      service:
        name: my-release-controller-manager-webhook
        namespace: spire-0.12.2.tgz
        path: /validate-spire-spiffe-io-v1alpha1-clusterfederatedtrustdomain
    failurePolicy: Fail
    name: vclusterfederatedtrustdomain.kb.io
    rules:
      - apiGroups: ["spire.spiffe.io"]
        apiVersions: ["v1alpha1"]
        operations: ["CREATE", "UPDATE"]
        resources: ["clusterfederatedtrustdomains"]
    sideEffects: None
  - admissionReviewVersions: ["v1"]
    clientConfig:
      service:
        name: my-release-controller-manager-webhook
        namespace: spire-0.12.2.tgz
        path: /validate-spire-spiffe-io-v1alpha1-clusterspiffeid
    failurePolicy: Fail
    name: vclusterspiffeid.kb.io
    rules:
      - apiGroups: ["spire.spiffe.io"]
        apiVersions: ["v1alpha1"]
        operations: ["CREATE", "UPDATE"]
        resources: ["clusterspiffeids"]
    sideEffects: None
---
# Source: spire/charts/spire-server/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "my-release-server-test-connection"
  labels:
    helm.sh/chart: spire-server-0.1.0
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['nc']
      args: ['-zvw3', 'my-release-server', '8081']
  restartPolicy: Never
