---
# Source: signoz/charts/clickhouse/templates/clickhouse-instance/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-clickhouse
  namespace: signoz-0.43.0.tgz
  labels:
    helm.sh/chart: clickhouse-24.1.0
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: clickhouse
    app.kubernetes.io/version: "24.1.2"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
---
# Source: signoz/charts/clickhouse/templates/clickhouse-operator/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-clickhouse-operator
  namespace: signoz-0.43.0.tgz
  labels:
    helm.sh/chart: clickhouse-24.1.0
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: operator
    app.kubernetes.io/version: "24.1.2"
    app.kubernetes.io/managed-by: Helm
    clickhouse.altinity.com/chop: 0.21.2
---
# Source: signoz/charts/k8s-infra/templates/otel-agent/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-k8s-infra-otel-agent
  namespace: signoz-0.43.0.tgz
  labels:
    helm.sh/chart: k8s-infra-0.11.5
    app.kubernetes.io/version: "0.88.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: k8s-infra
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: otel-agent
---
# Source: signoz/charts/k8s-infra/templates/otel-deployment/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-k8s-infra-otel-deployment
  namespace: signoz-0.43.0.tgz
  labels:
    helm.sh/chart: k8s-infra-0.11.5
    app.kubernetes.io/version: "0.88.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: k8s-infra
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: otel-deployment
---
# Source: signoz/templates/alertmanager/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-signoz-alertmanager
  labels:
    helm.sh/chart: signoz-0.43.0
    app.kubernetes.io/name: signoz
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: alertmanager
    app.kubernetes.io/version: "0.47.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: signoz/templates/frontend/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-signoz-frontend
  labels:
    helm.sh/chart: signoz-0.43.0
    app.kubernetes.io/name: signoz
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: frontend
    app.kubernetes.io/version: "0.47.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: signoz/templates/otel-collector-metrics/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-signoz-otel-collector-metrics
  labels:
    helm.sh/chart: signoz-0.43.0
    app.kubernetes.io/name: signoz
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: otel-collector-metrics
    app.kubernetes.io/version: "0.47.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: signoz/templates/otel-collector/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-signoz-otel-collector
  labels:
    helm.sh/chart: signoz-0.43.0
    app.kubernetes.io/name: signoz
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: otel-collector
    app.kubernetes.io/version: "0.47.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: signoz/templates/query-service/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-signoz-query-service
  labels:
    helm.sh/chart: signoz-0.43.0
    app.kubernetes.io/name: signoz
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: query-service
    app.kubernetes.io/version: "0.47.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: signoz/charts/clickhouse/templates/clickhouse-operator/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-release-clickhouse-operator
  namespace: signoz-0.43.0.tgz
  labels: 
    helm.sh/chart: clickhouse-24.1.0
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: operator
    app.kubernetes.io/version: "24.1.2"
    app.kubernetes.io/managed-by: Helm
    clickhouse.altinity.com/chop: 0.21.2
type: Opaque
data:
  username: Y2xpY2tob3VzZV9vcGVyYXRvcg==
  password: Y2xpY2tob3VzZV9vcGVyYXRvcl9wYXNzd29yZA==
---
# Source: signoz/charts/clickhouse/charts/zookeeper/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-zookeeper-scripts
  namespace: signoz-0.43.0.tgz
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-11.4.2
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
data:
  init-certs.sh: |-
    #!/bin/bash
  setup.sh: |-
    #!/bin/bash

    # Execute entrypoint as usual after obtaining ZOO_SERVER_ID
    # check ZOO_SERVER_ID in persistent volume via myid
    # if not present, set based on POD hostname
    if [[ -f "/bitnami/zookeeper/data/myid" ]]; then
        export ZOO_SERVER_ID="$(cat /bitnami/zookeeper/data/myid)"
    else
        HOSTNAME="$(hostname -s)"
        if [[ $HOSTNAME =~ (.*)-([0-9]+)$ ]]; then
            ORD=${BASH_REMATCH[2]}
            export ZOO_SERVER_ID="$((ORD + 1 ))"
        else
            echo "Failed to get index from hostname $HOSTNAME"
            exit 1
        fi
    fi
    exec /entrypoint.sh /run.sh
---
# Source: signoz/charts/clickhouse/templates/clickhouse-instance/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-clickhouse-custom-functions
  namespace: signoz-0.43.0.tgz
  labels:
    helm.sh/chart: clickhouse-24.1.0
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: clickhouse
    app.kubernetes.io/version: "24.1.2"
    app.kubernetes.io/managed-by: Helm
data:
  custom-functions.xml: |
    <functions>
        <function>
            <type>executable</type>
            <name>histogramQuantile</name>
            <return_type>Float64</return_type>
            <argument>
                <type>Array(Float64)</type>
                <name>buckets</name>
            </argument>
            <argument>
                <type>Array(Float64)</type>
                <name>counts</name>
            </argument>
            <argument>
                <type>Float64</type>
                <name>quantile</name>
            </argument>
            <format>CSV</format>
            <command>./histogramQuantile</command>
        </function>
    </functions>
---
# Source: signoz/charts/clickhouse/templates/clickhouse-operator/configmaps/etc-confd-files.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-clickhouse-operator-etc-confd-files
  namespace: signoz-0.43.0.tgz
  labels:
    helm.sh/chart: clickhouse-24.1.0
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: operator
    app.kubernetes.io/version: "24.1.2"
    app.kubernetes.io/managed-by: Helm
    clickhouse.altinity.com/chop: 0.21.2
data: 
  null
---
# Source: signoz/charts/clickhouse/templates/clickhouse-operator/configmaps/etc-configd-files.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-clickhouse-operator-etc-configd-files
  namespace: signoz-0.43.0.tgz
  labels:
    helm.sh/chart: clickhouse-24.1.0
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: operator
    app.kubernetes.io/version: "24.1.2"
    app.kubernetes.io/managed-by: Helm
    clickhouse.altinity.com/chop: 0.21.2
data:
  01-clickhouse-01-listen.xml: |
    <yandex>
        <!-- Listen wildcard address to allow accepting connections from other containers and host network. -->
        <listen_host>::</listen_host>
        <listen_host>0.0.0.0</listen_host>
        <listen_try>1</listen_try>
    </yandex>
  01-clickhouse-02-logger.xml: |
    <yandex>
        <logger>
            <!-- Possible levels: https://github.com/pocoproject/poco/blob/devel/Foundation/include/Poco/Logger.h#L105 -->
            <level>information</level>
            <log>/var/log/clickhouse-server/clickhouse-server.log</log>
            <errorlog>/var/log/clickhouse-server/clickhouse-server.err.log</errorlog>
            <size>1000M</size>
            <count>10</count>
            <!-- Default behavior is autodetection (log to console if not daemon mode and is tty) -->
            <console>1</console>
        </logger>
    </yandex>
  01-clickhouse-03-query_log.xml: |
    <yandex>
        <query_log replace="1">
            <database>system</database>
            <table>query_log</table>
            <engine>Engine = MergeTree PARTITION BY event_date ORDER BY event_time TTL event_date + interval 30 day</engine>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </query_log>
    </yandex>
  01-clickhouse-04-part_log.xml: |
    <yandex>
        <part_log replace="1">
            <database>system</database>
            <table>part_log</table>
            <engine>Engine = MergeTree PARTITION BY event_date ORDER BY event_time TTL event_date + interval 30 day</engine>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </part_log>
    </yandex>
  01-clickhouse-05-trace_log.xml: |-
    <yandex>
        <trace_log replace="1">
            <database>system</database>
            <table>trace_log</table>
            <engine>Engine = MergeTree PARTITION BY event_date ORDER BY event_time TTL event_date + interval 7 day</engine>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </trace_log>
    </yandex>

  01-clickhouse-06-asynchronous_insert_log.xml: |
    <yandex>
        <asynchronous_insert_log replace="1">
            <database>system</database>
            <table>asynchronous_insert_log</table>
            <engine>Engine = MergeTree PARTITION BY event_date ORDER BY event_time TTL event_date + interval 7 day</engine>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </asynchronous_insert_log>
    </yandex>
  01-clickhouse-07-asynchronous_metric_log.xml: |
    <yandex>
        <asynchronous_metric_log replace="1">
            <database>system</database>
            <table>asynchronous_metric_log</table>
            <engine>Engine = MergeTree PARTITION BY event_date ORDER BY event_time TTL event_date + interval 30 day</engine>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </asynchronous_metric_log>
    </yandex>
  01-clickhouse-08-backup_log.xml: |
    <yandex>
        <backup_log replace="1">
            <database>system</database>
            <table>backup_log</table>
            <engine>Engine = MergeTree PARTITION BY event_date ORDER BY event_time TTL event_date + interval 7 day</engine>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </backup_log>
    </yandex>
  01-clickhouse-09-blob_storage_log.xml: |
    <yandex>
        <blob_storage_log replace="1">
            <database>system</database>
            <table>blob_storage_log</table>
            <engine>Engine = MergeTree PARTITION BY event_date ORDER BY event_time TTL event_date + interval 30 day</engine>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </blob_storage_log>
    </yandex>
  01-clickhouse-10-crash_log.xml: |
    <yandex>
        <crash_log replace="1">
            <database>system</database>
            <table>crash_log</table>
            <engine>Engine = MergeTree PARTITION BY event_date ORDER BY event_time TTL event_date + interval 30 day</engine>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </crash_log>
    </yandex>
  01-clickhouse-11-metric_log.xml: |
    <yandex>
        <metric_log replace="1">
            <database>system</database>
            <table>metric_log</table>
            <engine>Engine = MergeTree PARTITION BY event_date ORDER BY event_time TTL event_date + interval 30 day</engine>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </metric_log>
    </yandex>
  01-clickhouse-12-query_thread_log.xml: |
    <yandex>
        <query_thread_log replace="1">
            <database>system</database>
            <table>query_thread_log</table>
            <engine>Engine = MergeTree PARTITION BY event_date ORDER BY event_time TTL event_date + interval 7 day</engine>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </query_thread_log>
    </yandex>
  01-clickhouse-13-query_views_log.xml: |
    <yandex>
        <query_views_log replace="1">
            <database>system</database>
            <table>query_views_log</table>
            <engine>Engine = MergeTree PARTITION BY event_date ORDER BY event_time TTL event_date + interval 15 day</engine>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </query_views_log>
    </yandex>
  01-clickhouse-14-session_log.xml: |
    <yandex>
        <session_log replace="1">
            <database>system</database>
            <table>session_log</table>
            <engine>Engine = MergeTree PARTITION BY event_date ORDER BY event_time TTL event_date + interval 30 day</engine>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </session_log>
    </yandex>
  01-clickhouse-15-zookeeper_log.xml: |
    <yandex>
        <zookeeper_log replace="1">
            <database>system</database>
            <table>zookeeper_log</table>
            <engine>Engine = MergeTree PARTITION BY event_date ORDER BY event_time TTL event_date + interval 30 day</engine>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </zookeeper_log>
    </yandex>
  01-clickhouse-16-processors_profile_log.xml: |
    <yandex>
        <processors_profile_log replace="1">
            <database>system</database>
            <table>processors_profile_log</table>
            <engine>Engine = MergeTree PARTITION BY event_date ORDER BY event_time TTL event_date + interval 7 day</engine>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </processors_profile_log>
    </yandex>
---
# Source: signoz/charts/clickhouse/templates/clickhouse-operator/configmaps/etc-files.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-clickhouse-operator-etc-files
  namespace: signoz-0.43.0.tgz
  labels:
    helm.sh/chart: clickhouse-24.1.0
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: operator
    app.kubernetes.io/version: "24.1.2"
    app.kubernetes.io/managed-by: Helm
    clickhouse.altinity.com/chop: 0.21.2
data:
  config.yaml: |
      #
      # Template parameters available:
      #   WATCH_NAMESPACES=
      #   CH_USERNAME_PLAIN=
      #   CH_PASSWORD_PLAIN=
      #   CH_CREDENTIALS_SECRET_NAMESPACE=
      #   CH_CREDENTIALS_SECRET_NAME=clickhouse-operator

      ################################################
      ##
      ## Watch Section
      ##
      ################################################
      watch:
        # List of namespaces where clickhouse-operator watches for events.
        # Concurrently running operators should watch on different namespaces.
        #namespaces: ["dev", "test"]
        namespaces: []

      clickhouse:
        configuration:
          ################################################
          ##
          ## Configuration Files Section
          ##
          ################################################
          file:
            path:
              # Path to the folder where ClickHouse configuration files common for all instances within a CHI are located.
              common: config.d
              # Path to the folder where ClickHouse configuration files unique for each instance (host) within a CHI are located.
              host: conf.d
              # Path to the folder where ClickHouse configuration files with users' settings are located.
              # Files are common for all instances within a CHI.
              user: users.d
          ################################################
          ##
          ## Configuration Users Section
          ##
          ################################################
          user:
            # Default settings for user accounts, created by the operator.
            # NB. These are not access credentials or settings for 'default' user account,
            # it is a template for filling out missing fields for all user accounts to be created by the operator, except:
            # 1. 'default' account, which DOES NOT use provided password, but uses all the rest of the fields
            # Password for 'default' account as to be provided explicitly, if to be used
            # 2. CHOP account, which DOES NOT use profile, quota, password and regexp, but uses network IPs - extending them
            # Password for CHOP account is used from `clickhouse.access.password` section
            default:
              # Default values for ClickHouse user configuration
              # 1. user/profile - string
              # 2. user/quota - string
              # 3. user/networks/ip - multiple strings
              # 4. user/password - string
              profile: "default"
              quota: "default"
              networksIP:
                - "::1"
                - "127.0.0.1"
              password: "default"
          ################################################
          ##
          ## Configuration Network Section
          ##
          ################################################
          network:
            # Default host_regexp to limit network connectivity from outside
            hostRegexpTemplate: "(chi-{chi}-[^.]+\\d+-\\d+|clickhouse\\-{chi})\\.{namespace}\\.svc\\.cluster\\.local$"

        ################################################
        ##
        ## Configuration Restart Policy Section
        ## Configuration restart policy describes what configuration changes require ClickHouse restart
        ##
        ################################################
        configurationRestartPolicy:
          rules:
            - version: "*"
              rules:
                - settings/*: "yes"
                - settings/dictionaries_config: "no"
                - settings/logger: "no"
                - settings/macros/*: "no"
                - settings/max_server_memory_*: "no"
                - settings/max_*_to_drop: "no"
                - settings/max_concurrent_queries: "no"
                - settings/models_config: "no"
                - settings/user_defined_executable_functions_config: "no"

                - zookeeper/*: "yes"

                - files/config.d/*.xml: "yes"
                - files/config.d/*dict*.xml: "no"

                - profiles/default/background_*_pool_size: "yes"
                - profiles/default/max_*_for_server: "yes"
            - version: "21.*"
              rules:
                - settings/logger: "yes"

        #################################################
        ##
        ## Access to ClickHouse instances
        ##
        ################################################
        access:
          # Possible values for `scheme` are:
          # 1. http
          # 2. https
          scheme: ""
          # ClickHouse credentials (username, password and port) to be used by the operator to connect to ClickHouse instances.
          # Used for:
          # 1. Metrics requests
          # 2. Schema maintenance
          # 3. DROP DNS CACHE
          # User with these credentials can be specified in additional ClickHouse .xml config files,
          # located in `clickhouse.configuration.file.path.user` folder
          username: ""
          password: ""
          rootCA: ""

          # Location of the k8s Secret with username and password to be used by the operator to connect to ClickHouse instances.
          # Can be used instead of explicitly specified username and password which are:
          # clickhouse.access.username
          # clickhouse.access.password
          # Secret should have two keys:
          # 1. username
          # 2. password
          secret:
            # Empty `namespace` means that k8s secret would be looked in the same namespace where operator's pod is running.
            namespace: ""
            # Empty `name` means no k8s Secret would be looked for
            name: "my-release-clickhouse-operator"
          # Port where to connect to ClickHouse instances to
          port: 8123

          # Timeouts used to limit connection and queries from the operator to ClickHouse instances
          # Specified in seconds.
          timeouts:
            connect: 2
            query: 5

        metrics:
          timeouts:
            collect: 9

      ################################################
      ##
      ## Templates Section
      ##
      ################################################
      template:
        chi:
          # Path to the folder where ClickHouseInstallation .yaml manifests are located.
          # Manifests are applied in sorted alpha-numeric order.
          path: templates.d

      ################################################
      ##
      ## Reconcile Section
      ##
      ################################################
      reconcile:
        runtime:
          # Max number of concurrent CHI reconciles in progress
          reconcileCHIsThreadsNumber: 10
          # Max number of concurrent shard reconciles in progress
          reconcileShardsThreadsNumber: 1
          # The maximum percentage of cluster shards that may be reconciled in parallel
          reconcileShardsMaxConcurrencyPercent: 50

        statefulSet:
          create:
            # What to do in case created StatefulSet is not in 'Ready' after `reconcile.statefulSet.update.timeout` seconds
            # Possible options:
            # 1. abort - do nothing, just break the process and wait for an admin to assist
            # 2. delete - delete newly created problematic StatefulSet
            # 3. ignore - ignore an error, pretend nothing happened and move on to the next StatefulSet
            onFailure: ignore

          update:
            # How many seconds to wait for created/updated StatefulSet to be 'Ready'
            timeout: 300
            # How many seconds to wait between checks/polls for created/updated StatefulSet status
            pollInterval: 5
            # What to do in case updated StatefulSet is not in 'Ready' after `reconcile.statefulSet.update.timeout` seconds
            # Possible options:
            # 1. abort - do nothing, just break the process and wait for an admin to assist
            # 2. rollback - delete Pod and rollback StatefulSet to previous Generation.
            # Pod would be recreated by StatefulSet based on rollback-ed configuration
            # 3. ignore - ignore an error, pretend nothing happened and move on to the next StatefulSet
            onFailure: rollback

        host:
          # Whether reconciler should wait for a host:
          # - to be excluded from a cluster
          # OR
          # - to be included into a cluster
          # respectfully
          wait:
            exclude: true
            include: false

      ################################################
      ##
      ## Annotations management
      ##
      ################################################
      annotation:
        # Applied when:
        #  1. Propagating annotations from the CHI's `metadata.annotations` to child objects' `metadata.annotations`,
        #  2. Propagating annotations from the CHI Template's `metadata.annotations` to CHI's `metadata.annotations`,
        # Include annotations from the following list:
        # Applied only when not empty. Empty list means "include all, no selection"
        include: []
        # Exclude annotations from the following list:
        exclude: []

      ################################################
      ##
      ## Labels management
      ##
      ################################################
      label:
        # Applied when:
        #  1. Propagating labels from the CHI's `metadata.labels` to child objects' `metadata.labels`,
        #  2. Propagating labels from the CHI Template's `metadata.labels` to CHI's `metadata.labels`,
        # Include labels from the following list:
        # Applied only when not empty. Empty list means "include all, no selection"
        include: []
        # Exclude labels from the following list:
        # Applied only when not empty. Empty list means "nothing to exclude, no selection"
        exclude: []
        # Whether to append *Scope* labels to StatefulSet and Pod.
        # Full list of available *scope* labels check in 'labeler.go'
        #  LabelShardScopeIndex
        #  LabelReplicaScopeIndex
        #  LabelCHIScopeIndex
        #  LabelCHIScopeCycleSize
        #  LabelCHIScopeCycleIndex
        #  LabelCHIScopeCycleOffset
        #  LabelClusterScopeIndex
        #  LabelClusterScopeCycleSize
        #  LabelClusterScopeCycleIndex
        #  LabelClusterScopeCycleOffset
        appendScope: "no"

      ################################################
      ##
      ## StatefulSet management
      ##
      ################################################
      statefulSet:
        revisionHistoryLimit: 0

      ################################################
      ##
      ## Pod management
      ##
      ################################################
      pod:
        # Grace period for Pod termination.
        # How many seconds to wait between sending
        # SIGTERM and SIGKILL during Pod termination process.
        # Increase this number is case of slow shutdown.
        terminationGracePeriod: 30

      ################################################
      ##
      ## Log parameters
      ##
      ################################################
      logger:
        logtostderr: "true"
        alsologtostderr: "false"
        v: "1"
        stderrthreshold: ""
        vmodule: ""
        log_backtrace_at: ""
---
# Source: signoz/charts/clickhouse/templates/clickhouse-operator/configmaps/etc-templatesd-files.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-clickhouse-operator-etc-templatesd-files
  namespace: signoz-0.43.0.tgz
  labels:
    helm.sh/chart: clickhouse-24.1.0
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: operator
    app.kubernetes.io/version: "24.1.2"
    app.kubernetes.io/managed-by: Helm
    clickhouse.altinity.com/chop: 0.21.2
data:
  001-templates.json.example: |
    {
      "apiVersion": "clickhouse.altinity.com/v1",
      "kind": "ClickHouseInstallationTemplate",
      "metadata": {
        "name": "01-default-volumeclaimtemplate"
      },
      "spec": {
        "templates": {
          "volumeClaimTemplates": [
            {
              "name": "chi-default-volume-claim-template",
              "spec": {
                "accessModes": [
                  "ReadWriteOnce"
                ],
                "resources": {
                  "requests": {
                    "storage": "2Gi"
                  }
                }
              }
            }
          ],
          "podTemplates": [
            {
              "name": "chi-default-oneperhost-pod-template",
              "distribution": "OnePerHost",
              "spec": {
                "containers" : [
                  {
                    "name": "clickhouse",
                    "image": "clickhouse/clickhouse-server:22.3",
                    "ports": [
                      {
                        "name": "http",
                        "containerPort": 8123
                      },
                      {
                        "name": "client",
                        "containerPort": 9000
                      },
                      {
                        "name": "interserver",
                        "containerPort": 9009
                      }
                    ]
                  }
                ]
              }
            }
          ]
        }
      }
    }
  default-pod-template.yaml.example: |
    apiVersion: "clickhouse.altinity.com/v1"
    kind: "ClickHouseInstallationTemplate"
    metadata:
      name: "default-oneperhost-pod-template"
    spec:
      templates:
        podTemplates:
          - name: default-oneperhost-pod-template
            distribution: "OnePerHost"
  default-storage-template.yaml.example: |
    apiVersion: "clickhouse.altinity.com/v1"
    kind: "ClickHouseInstallationTemplate"
    metadata:
      name: "default-storage-template-2Gi"
    spec:
      templates:
        volumeClaimTemplates:
          - name: default-storage-template-2Gi
            spec:
              accessModes:
                - ReadWriteOnce
              resources:
                requests:
                  storage: 2Gi
  readme: |-
    Templates in this folder are packaged with an operator and available via 'useTemplate'
---
# Source: signoz/charts/clickhouse/templates/clickhouse-operator/configmaps/etc-usersd-files.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-clickhouse-operator-etc-usersd-files
  namespace: signoz-0.43.0.tgz
  labels:
    helm.sh/chart: clickhouse-24.1.0
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: operator
    app.kubernetes.io/version: "24.1.2"
    app.kubernetes.io/managed-by: Helm
    clickhouse.altinity.com/chop: 0.21.2
data:
  01-clickhouse-user.xml: |
    <yandex>
        <users>
            <clickhouse_operator>
                <networks>
                    <ip>127.0.0.1</ip>
                </networks>
                <password_sha256_hex>716b36073a90c6fe1d445ac1af85f4777c5b7a155cea359961826a030513e448</password_sha256_hex>
                <profile>clickhouse_operator</profile>
                <quota>default</quota>
            </clickhouse_operator>
        </users>
        <profiles>
            <clickhouse_operator>
                <log_queries>0</log_queries>
                <skip_unavailable_shards>1</skip_unavailable_shards>
                <http_connection_timeout>10</http_connection_timeout>
            </clickhouse_operator>
        </profiles>
    </yandex>
  02-clickhouse-default-profile.xml: |
    <yandex>
      <profiles>
        <default>
          <log_queries>1</log_queries>
          <connect_timeout_with_failover_ms>1000</connect_timeout_with_failover_ms>
          <distributed_aggregation_memory_efficient>1</distributed_aggregation_memory_efficient>
          <parallel_view_processing>1</parallel_view_processing>
        </default>
      </profiles>
    </yandex>
  03-database-ordinary.xml: |
    <yandex>
        <profiles>
            <default>
                <default_database_engine>Ordinary</default_database_engine>
            </default>
        </profiles>
    </yandex>
---
# Source: signoz/charts/k8s-infra/templates/otel-agent/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-k8s-infra-otel-agent
  namespace: signoz-0.43.0.tgz
  labels:
    helm.sh/chart: k8s-infra-0.11.5
    app.kubernetes.io/version: "0.88.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: k8s-infra
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: otel-agent
data:
  otel-agent-config.yaml: |-
    
    exporters:
      otlp:
        endpoint: ${OTEL_EXPORTER_OTLP_ENDPOINT}
        headers:
          signoz-access-token: Bearer ${SIGNOZ_API_KEY}
        tls:
          insecure: ${OTEL_EXPORTER_OTLP_INSECURE}
          insecure_skip_verify: ${OTEL_EXPORTER_OTLP_INSECURE_SKIP_VERIFY}
    extensions:
      health_check:
        endpoint: 0.0.0.0:13133
      pprof:
        endpoint: localhost:1777
      zpages:
        endpoint: localhost:55679
    processors:
      batch:
        send_batch_size: 10000
        timeout: 200ms
      k8sattributes:
        extract:
          metadata:
          - k8s.namespace.name
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.pod.start_time
          - k8s.deployment.name
          - k8s.node.name
        filter:
          node_from_env_var: K8S_NODE_NAME
        passthrough: false
        pod_association:
        - sources:
          - from: resource_attribute
            name: k8s.pod.ip
        - sources:
          - from: resource_attribute
            name: k8s.pod.uid
        - sources:
          - from: connection
      resourcedetection:
        detectors:
        - system
        override: true
        system:
          hostname_sources:
          - dns
          - os
        timeout: 2s
      resourcedetection/internal:
        detectors:
        - env
        override: true
        timeout: 2s
    receivers:
      filelog/k8s:
        exclude:
        - /var/log/pods/kube-system_*/*/*.log
        - /var/log/pods/*_hotrod*_*/*/*.log
        - /var/log/pods/*_locust*_*/*/*.log
        include:
        - /var/log/pods/*/*/*.log
        include_file_name: false
        include_file_path: true
        operators:
        - id: get-format
          routes:
          - expr: body matches "^\\{"
            output: parser-docker
          - expr: body matches "^[^ Z]+ "
            output: parser-crio
          - expr: body matches "^[^ Z]+Z"
            output: parser-containerd
          type: router
        - id: parser-crio
          output: extract_metadata_from_filepath
          regex: ^(?P<time>[^ Z]+) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$
          timestamp:
            layout: "2006-01-02T15:04:05.000000000-07:00"
            layout_type: gotime
            parse_from: attributes.time
          type: regex_parser
        - id: parser-containerd
          output: extract_metadata_from_filepath
          regex: ^(?P<time>[^ ^Z]+Z) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$
          timestamp:
            layout: '%Y-%m-%dT%H:%M:%S.%LZ'
            parse_from: attributes.time
          type: regex_parser
        - id: parser-docker
          output: extract_metadata_from_filepath
          timestamp:
            layout: '%Y-%m-%dT%H:%M:%S.%LZ'
            parse_from: attributes.time
          type: json_parser
        - id: extract_metadata_from_filepath
          output: add_cluster_name
          parse_from: attributes["log.file.path"]
          regex: ^.*\/(?P<namespace>[^_]+)_(?P<pod_name>[^_]+)_(?P<uid>[a-f0-9\-]+)\/(?P<container_name>[^\._]+)\/(?P<restart_count>\d+)\.log$
          type: regex_parser
        - field: resource["k8s.cluster.name"]
          id: add_cluster_name
          output: move_stream
          type: add
          value: EXPR(env("K8S_CLUSTER_NAME"))
        - from: attributes.stream
          id: move_stream
          output: move_container_name
          to: attributes["log.iostream"]
          type: move
        - from: attributes.container_name
          id: move_container_name
          output: move_namespace
          to: resource["k8s.container.name"]
          type: move
        - from: attributes.namespace
          id: move_namespace
          output: move_pod_name
          to: resource["k8s.namespace.name"]
          type: move
        - from: attributes.pod_name
          id: move_pod_name
          output: move_restart_count
          to: resource["k8s.pod.name"]
          type: move
        - from: attributes.restart_count
          id: move_restart_count
          output: move_uid
          to: resource["k8s.container.restart_count"]
          type: move
        - from: attributes.uid
          id: move_uid
          output: move_log
          to: resource["k8s.pod.uid"]
          type: move
        - from: attributes.log
          id: move_log
          to: body
          type: move
        start_at: beginning
      hostmetrics:
        collection_interval: 30s
        scrapers:
          cpu: {}
          disk: {}
          filesystem: {}
          load: {}
          memory: {}
          network: {}
      kubeletstats:
        auth_type: serviceAccount
        collection_interval: 30s
        endpoint: ${K8S_HOST_IP}:10250
        extra_metadata_labels:
        - container.id
        - k8s.volume.type
        insecure_skip_verify: true
        metric_groups:
        - container
        - pod
        - node
        - volume
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
            max_recv_msg_size_mib: 4
          http:
            endpoint: 0.0.0.0:4318
    service:
      extensions:
      - health_check
      - zpages
      - pprof
      pipelines:
        logs:
          exporters:
          - otlp
          processors:
          - k8sattributes
          - batch
          receivers:
          - otlp
          - filelog/k8s
        metrics:
          exporters:
          - otlp
          processors:
          - k8sattributes
          - batch
          receivers:
          - otlp
        metrics/internal:
          exporters:
          - otlp
          processors:
          - resourcedetection/internal
          - resourcedetection
          - k8sattributes
          - batch
          receivers:
          - hostmetrics
          - kubeletstats
        traces:
          exporters:
          - otlp
          processors:
          - k8sattributes
          - batch
          receivers:
          - otlp
      telemetry:
        metrics:
          address: 0.0.0.0:8888
---
# Source: signoz/charts/k8s-infra/templates/otel-deployment/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-k8s-infra-otel-deployment
  namespace: signoz-0.43.0.tgz
  labels:
    helm.sh/chart: k8s-infra-0.11.5
    app.kubernetes.io/version: "0.88.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: k8s-infra
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: otel-deployment
data:
  otel-deployment-config.yaml: |-
    exporters:
      otlp:
        endpoint: ${OTEL_EXPORTER_OTLP_ENDPOINT}
        headers:
          signoz-access-token: Bearer ${SIGNOZ_API_KEY}
        tls:
          insecure: ${OTEL_EXPORTER_OTLP_INSECURE}
          insecure_skip_verify: ${OTEL_EXPORTER_OTLP_INSECURE_SKIP_VERIFY}
    extensions:
      health_check:
        endpoint: 0.0.0.0:13133
      pprof:
        endpoint: localhost:1777
      zpages:
        endpoint: localhost:55679
    processors:
      batch:
        send_batch_size: 10000
        timeout: 1s
      resourcedetection/internal:
        detectors:
        - env
        override: true
        timeout: 2s
    receivers:
      k8s_cluster:
        allocatable_types_to_report:
        - cpu
        - memory
        collection_interval: 30s
        node_conditions_to_report:
        - Ready
        - MemoryPressure
    service:
      extensions:
      - health_check
      - zpages
      - pprof
      pipelines:
        metrics/internal:
          exporters:
          - otlp
          processors:
          - resourcedetection/internal
          - batch
          receivers:
          - k8s_cluster
      telemetry:
        metrics:
          address: 0.0.0.0:8888
---
# Source: signoz/templates/frontend/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-signoz-frontend
  labels:
    helm.sh/chart: signoz-0.43.0
    app.kubernetes.io/name: signoz
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: frontend
    app.kubernetes.io/version: "0.47.0"
    app.kubernetes.io/managed-by: Helm
data:
  default.conf: |-
    server {
      listen       3301;
      server_name  _;
      
      client_max_body_size 24M;
      large_client_header_buffers 8 16k;
      gzip on;
      gzip_static on;
      gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/xml+rss text/javascript;
      gzip_proxied  any;
      gzip_vary on;
      gzip_comp_level 6;
      gzip_buffers 16 8k;
      gzip_http_version 1.1;

      location / {
          if ( $uri = '/index.html' ) {
              add_header Cache-Control no-store always;
          }
          root   /usr/share/nginx/html;
          index  index.html index.htm;
          try_files $uri $uri/ /index.html;
      }

      location  ~ ^/api/(v1|v3)/logs/(tail|livetail){
        proxy_pass http://my-release-signoz-query-service:8080;
        proxy_http_version 1.1;

        # connection will be closed if no data is read for 600s between successive read operations
        proxy_read_timeout 600s;

        # dont buffer the data send it directly to client.
        proxy_buffering off;
        proxy_cache off;
      }

      location /api {
        proxy_pass http://my-release-signoz-query-service:8080/api;
        proxy_http_version 1.1;

        # connection will be closed if no data is read for 600s between successive read operations
        proxy_read_timeout 600s;
      }

      # redirect server error pages to the static page /50x.html
      #
      error_page   500 502 503 504  /50x.html;
      location = /50x.html {
          root   /usr/share/nginx/html;
      }
    }
---
# Source: signoz/templates/otel-collector-metrics/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-signoz-otel-collector-metrics
  labels:
    helm.sh/chart: signoz-0.43.0
    app.kubernetes.io/name: signoz
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: otel-collector-metrics
    app.kubernetes.io/version: "0.47.0"
    app.kubernetes.io/managed-by: Helm
data:
  otel-collector-metrics-config.yaml: |-
    exporters:
      clickhousemetricswrite:
        endpoint: tcp://${CLICKHOUSE_USER}:${CLICKHOUSE_PASSWORD}@${CLICKHOUSE_HOST}:${CLICKHOUSE_PORT}/${CLICKHOUSE_DATABASE}
        timeout: 15s
      clickhousemetricswrite/hostmetrics:
        endpoint: tcp://${CLICKHOUSE_USER}:${CLICKHOUSE_PASSWORD}@${CLICKHOUSE_HOST}:${CLICKHOUSE_PORT}/${CLICKHOUSE_DATABASE}
        resource_to_telemetry_conversion:
          enabled: true
    extensions:
      health_check:
        endpoint: 0.0.0.0:13133
      pprof:
        endpoint: localhost:1777
      zpages:
        endpoint: localhost:55679
    processors:
      batch:
        send_batch_size: 10000
        timeout: 1s
      k8sattributes/hostmetrics:
        extract:
          metadata:
          - k8s.namespace.name
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.pod.start_time
          - k8s.deployment.name
          - k8s.node.name
        filter:
          node_from_env_var: K8S_NODE_NAME
        passthrough: false
        pod_association:
        - sources:
          - from: resource_attribute
            name: k8s.pod.ip
        - sources:
          - from: resource_attribute
            name: k8s.pod.uid
        - sources:
          - from: connection
      memory_limiter: null
      resourcedetection:
        detectors:
        - env
        - system
        system:
          hostname_sources:
          - dns
          - os
        timeout: 2s
    receivers:
      hostmetrics:
        collection_interval: 30s
        scrapers:
          cpu: {}
          disk: {}
          filesystem: {}
          load: {}
          memory: {}
          network: {}
      prometheus:
        config:
          scrape_configs:
          - job_name: generic-collector
            kubernetes_sd_configs:
            - role: pod
            relabel_configs:
            - action: keep
              regex: true
              source_labels:
              - __meta_kubernetes_pod_annotation_signoz_io_scrape
            - action: replace
              regex: (.+)
              source_labels:
              - __meta_kubernetes_pod_annotation_signoz_io_path
              target_label: __metrics_path__
            - action: replace
              separator: ':'
              source_labels:
              - __meta_kubernetes_pod_ip
              - __meta_kubernetes_pod_annotation_signoz_io_port
              target_label: __address__
            - replacement: generic-collector
              target_label: job_name
            - action: replace
              source_labels:
              - __meta_kubernetes_pod_label_app_kubernetes_io_name
              target_label: signoz_k8s_name
            - action: replace
              source_labels:
              - __meta_kubernetes_pod_label_app_kubernetes_io_instance
              target_label: signoz_k8s_instance
            - action: replace
              source_labels:
              - __meta_kubernetes_pod_label_app_kubernetes_io_component
              target_label: signoz_k8s_component
            - action: replace
              source_labels:
              - __meta_kubernetes_namespace
              target_label: k8s_namespace_name
            - action: replace
              source_labels:
              - __meta_kubernetes_pod_name
              target_label: k8s_pod_name
            - action: replace
              source_labels:
              - __meta_kubernetes_pod_uid
              target_label: k8s_pod_uid
            - action: replace
              source_labels:
              - __meta_kubernetes_pod_container_name
              target_label: k8s_container_name
            - action: drop
              regex: (.+)-init
              source_labels:
              - __meta_kubernetes_pod_container_name
            - action: replace
              source_labels:
              - __meta_kubernetes_pod_node_name
              target_label: k8s_node_name
            - action: replace
              source_labels:
              - __meta_kubernetes_pod_ready
              target_label: k8s_pod_ready
            - action: replace
              source_labels:
              - __meta_kubernetes_pod_phase
              target_label: k8s_pod_phase
            scrape_interval: 60s
    service:
      extensions:
      - health_check
      - zpages
      - pprof
      pipelines:
        metrics:
          exporters:
          - clickhousemetricswrite
          processors:
          - batch
          receivers:
          - prometheus
        metrics/hostmetrics:
          exporters:
          - clickhousemetricswrite/hostmetrics
          processors:
          - resourcedetection
          - k8sattributes/hostmetrics
          - batch
          receivers:
          - hostmetrics
      telemetry:
        metrics:
          address: 0.0.0.0:8888
---
# Source: signoz/templates/otel-collector/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-signoz-otel-collector
  labels:
    helm.sh/chart: signoz-0.43.0
    app.kubernetes.io/name: signoz
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: otel-collector
    app.kubernetes.io/version: "0.47.0"
    app.kubernetes.io/managed-by: Helm
data:
  otel-collector-config.yaml: |-
    exporters:
      clickhouselogsexporter:
        dsn: tcp://${CLICKHOUSE_USER}:${CLICKHOUSE_PASSWORD}@${CLICKHOUSE_HOST}:${CLICKHOUSE_PORT}/${CLICKHOUSE_LOG_DATABASE}
        timeout: 10s
      clickhousemetricswrite:
        endpoint: tcp://${CLICKHOUSE_USER}:${CLICKHOUSE_PASSWORD}@${CLICKHOUSE_HOST}:${CLICKHOUSE_PORT}/${CLICKHOUSE_DATABASE}
        resource_to_telemetry_conversion:
          enabled: true
        timeout: 15s
      clickhousetraces:
        datasource: tcp://${CLICKHOUSE_USER}:${CLICKHOUSE_PASSWORD}@${CLICKHOUSE_HOST}:${CLICKHOUSE_PORT}/${CLICKHOUSE_TRACE_DATABASE}
        low_cardinal_exception_grouping: ${LOW_CARDINAL_EXCEPTION_GROUPING}
      prometheus:
        endpoint: 0.0.0.0:8889
    extensions:
      health_check:
        endpoint: 0.0.0.0:13133
      pprof:
        endpoint: localhost:1777
      zpages:
        endpoint: localhost:55679
    processors:
      batch:
        send_batch_size: 50000
        timeout: 1s
      k8sattributes:
        extract:
          metadata:
          - k8s.namespace.name
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.pod.start_time
          - k8s.deployment.name
          - k8s.node.name
        filter:
          node_from_env_var: K8S_NODE_NAME
        passthrough: false
        pod_association:
        - sources:
          - from: resource_attribute
            name: k8s.pod.ip
        - sources:
          - from: resource_attribute
            name: k8s.pod.uid
        - sources:
          - from: connection
      memory_limiter: null
      resourcedetection:
        detectors:
        - env
        - system
        system:
          hostname_sources:
          - dns
          - os
        timeout: 2s
      signozspanmetrics/cumulative:
        dimensions:
        - default: default
          name: service.namespace
        - default: default
          name: deployment.environment
        - name: signoz.collector.id
        dimensions_cache_size: 100000
        latency_histogram_buckets:
        - 100us
        - 1ms
        - 2ms
        - 6ms
        - 10ms
        - 50ms
        - 100ms
        - 250ms
        - 500ms
        - 1000ms
        - 1400ms
        - 2000ms
        - 5s
        - 10s
        - 20s
        - 40s
        - 60s
        metrics_exporter: clickhousemetricswrite
      signozspanmetrics/delta:
        aggregation_temporality: AGGREGATION_TEMPORALITY_DELTA
        dimensions:
        - default: default
          name: service.namespace
        - default: default
          name: deployment.environment
        - name: signoz.collector.id
        dimensions_cache_size: 100000
        latency_histogram_buckets:
        - 100us
        - 1ms
        - 2ms
        - 6ms
        - 10ms
        - 50ms
        - 100ms
        - 250ms
        - 500ms
        - 1000ms
        - 1400ms
        - 2000ms
        - 5s
        - 10s
        - 20s
        - 40s
        - 60s
        metrics_exporter: clickhousemetricswrite
    receivers:
      hostmetrics:
        collection_interval: 30s
        scrapers:
          cpu: {}
          disk: {}
          filesystem: {}
          load: {}
          memory: {}
          network: {}
      httplogreceiver/heroku:
        endpoint: 0.0.0.0:8081
        source: heroku
      httplogreceiver/json:
        endpoint: 0.0.0.0:8082
        source: json
      jaeger:
        protocols:
          grpc:
            endpoint: 0.0.0.0:14250
          thrift_http:
            endpoint: 0.0.0.0:14268
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
            max_recv_msg_size_mib: 16
          http:
            endpoint: 0.0.0.0:4318
      otlp/spanmetrics:
        protocols:
          grpc:
            endpoint: localhost:12345
    service:
      extensions:
      - health_check
      - zpages
      - pprof
      pipelines:
        logs:
          exporters:
          - clickhouselogsexporter
          processors:
          - batch
          receivers:
          - otlp
          - httplogreceiver/heroku
          - httplogreceiver/json
        metrics:
          exporters:
          - clickhousemetricswrite
          processors:
          - batch
          receivers:
          - otlp
        metrics/internal:
          exporters:
          - clickhousemetricswrite
          processors:
          - resourcedetection
          - k8sattributes
          - batch
          receivers:
          - hostmetrics
        traces:
          exporters:
          - clickhousetraces
          processors:
          - signozspanmetrics/cumulative
          - signozspanmetrics/delta
          - batch
          receivers:
          - otlp
          - jaeger
      telemetry:
        metrics:
          address: 0.0.0.0:8888
  otel-collector-opamp-config.yaml: |-
    server_endpoint: "ws://my-release-signoz-query-service:4320/v1/opamp"
---
# Source: signoz/templates/query-service/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-signoz-query-service
data:
  prometheus.yml: |
    # my global config
    global:
      scrape_interval:     5s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
      evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
      # scrape_timeout is set to the global default (10s).

    # Alertmanager configuration
    alerting:
      alertmanagers:
      - static_configs:
        - targets:
          - my-release-signoz-alertmanager:9093

    # Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
    rule_files:
      # - "first_rules.yml"
      # - "second_rules.yml"
      - 'alerts.yml'

    # A scrape configuration containing exactly one endpoint to scrape:
    # Here it's Prometheus itself.
    scrape_configs: []

    remote_read:
      - url: tcp://admin:27ff0399-0d3a-4bd8-919d-17c2181e6fb9@my-release-clickhouse:9000/signoz_metrics

  
  cache.yaml: |
    inmemory:
      ttl: 24h
    name: cache
    provider: inmemory
---
# Source: signoz/charts/k8s-infra/templates/otel-agent/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: my-release-k8s-infra-otel-agent-signoz-0.43.0.tgz
  namespace: signoz-0.43.0.tgz
rules:
  
  - apiGroups:
    - ""
    resources:
    - pods
    - namespaces
    - nodes
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - apps
    resources:
    - replicasets
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - extensions
    resources:
    - replicasets
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - nodes
    - endpoints
    verbs:
    - list
    - watch
  - apiGroups:
    - batch
    resources:
    - jobs
    verbs:
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - nodes/proxy
    verbs:
    - get
  - apiGroups:
    - ""
    resources:
    - nodes/stats
    - configmaps
    - events
    verbs:
    - create
    - get
  - apiGroups:
    - ""
    resourceNames:
    - otel-container-insight-clusterleader
    resources:
    - configmaps
    verbs:
    - get
    - update
---
# Source: signoz/charts/k8s-infra/templates/otel-deployment/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: my-release-k8s-infra-otel-deployment-signoz-0.43.0.tgz
  namespace: signoz-0.43.0.tgz
rules:
  - apiGroups:
    - ""
    resources:
    - events
    - namespaces
    - namespaces/status
    - nodes
    - nodes/spec
    - pods
    - pods/status
    - replicationcontrollers
    - replicationcontrollers/status
    - resourcequotas
    - services
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - apps
    resources:
    - daemonsets
    - deployments
    - replicasets
    - statefulsets
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - extensions
    resources:
    - daemonsets
    - deployments
    - replicasets
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - batch
    resources:
    - jobs
    - cronjobs
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - autoscaling
    resources:
    - horizontalpodautoscalers
    verbs:
    - get
    - list
    - watch
---
# Source: signoz/templates/otel-collector-metrics/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: my-release-signoz-otel-collector-metrics-signoz-0.43.0.tgz
  labels:
    helm.sh/chart: signoz-0.43.0
    app.kubernetes.io/name: signoz
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: otel-collector-metrics
    app.kubernetes.io/version: "0.47.0"
    app.kubernetes.io/managed-by: Helm
rules: 
  - apiGroups:
    - ""
    resources:
    - pods
    - namespaces
    - nodes
    verbs:
    - get
    - watch
    - list
  - apiGroups:
    - batch
    resources:
    - jobs
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - apps
    resources:
    - replicasets
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - extensions
    resources:
    - replicasets
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - nodes
    - nodes/proxy
    - services
    - endpoints
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - extensions
    resources:
    - ingresses
    verbs:
    - get
    - list
    - watch
  - nonResourceURLs:
    - /metrics
    verbs:
    - get
---
# Source: signoz/templates/otel-collector/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: my-release-signoz-otel-collector-signoz-0.43.0.tgz
  namespace: signoz-0.43.0.tgz
rules: 
  - apiGroups:
    - ""
    resources:
    - pods
    - namespaces
    - nodes
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - apps
    resources:
    - replicasets
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - extensions
    resources:
    - replicasets
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - batch
    resources:
    - jobs
    verbs:
    - get
    - list
    - watch
---
# Source: signoz/charts/k8s-infra/templates/otel-agent/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: my-release-k8s-infra-otel-agent-signoz-0.43.0.tgz
  namespace: signoz-0.43.0.tgz
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: my-release-k8s-infra-otel-agent-signoz-0.43.0.tgz
subjects:
  - name: my-release-k8s-infra-otel-agent
    kind: ServiceAccount
    namespace: signoz-0.43.0.tgz
---
# Source: signoz/charts/k8s-infra/templates/otel-deployment/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: my-release-k8s-infra-otel-deployment-signoz-0.43.0.tgz
  namespace: signoz-0.43.0.tgz
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: my-release-k8s-infra-otel-deployment-signoz-0.43.0.tgz
subjects:
  - name: my-release-k8s-infra-otel-deployment
    kind: ServiceAccount
    namespace: signoz-0.43.0.tgz
---
# Source: signoz/templates/otel-collector-metrics/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: my-release-signoz-otel-collector-metrics-signoz-0.43.0.tgz
  labels:
    helm.sh/chart: signoz-0.43.0
    app.kubernetes.io/name: signoz
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: otel-collector-metrics
    app.kubernetes.io/version: "0.47.0"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: my-release-signoz-otel-collector-metrics-signoz-0.43.0.tgz
subjects:
  - name: my-release-signoz-otel-collector-metrics
    kind: ServiceAccount
    namespace: signoz-0.43.0.tgz
---
# Source: signoz/templates/otel-collector/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: my-release-signoz-otel-collector-signoz-0.43.0.tgz
  namespace: signoz-0.43.0.tgz
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: my-release-signoz-otel-collector-signoz-0.43.0.tgz
subjects:
  - name: my-release-signoz-otel-collector
    kind: ServiceAccount
    namespace: signoz-0.43.0.tgz
---
# Source: signoz/charts/clickhouse/templates/clickhouse-operator/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: my-release-clickhouse-operator
  namespace: signoz-0.43.0.tgz
  labels:
    helm.sh/chart: clickhouse-24.1.0
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: operator
    app.kubernetes.io/version: "24.1.2"
    app.kubernetes.io/managed-by: Helm
    clickhouse.altinity.com/chop: 0.21.2
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - services
      - persistentvolumeclaims
      - secrets
    verbs:
      - get
      - list
      - patch
      - update
      - watch
      - create
      - delete
  - apiGroups:
      - ""
    resources:
      - endpoints
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
  - apiGroups:
      - ""
    resources:
      - persistentvolumes
      - pods
    verbs:
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - apps
    resources:
      - statefulsets
    verbs:
      - get
      - list
      - patch
      - update
      - watch
      - create
      - delete
  - apiGroups:
      - apps
    resources:
      - replicasets
    verbs:
      - get
      - patch
      - update
      - delete
  - apiGroups:
      - apps
    resourceNames:
      - my-release-clickhouse-operator
    resources:
      - deployments
    verbs:
      - get
      - patch
      - update
      - delete
  - apiGroups:
      - policy
    resources:
      - poddisruptionbudgets
    verbs:
      - get
      - list
      - patch
      - update
      - watch
      - create
      - delete
  - apiGroups:
      - clickhouse.altinity.com
    resources:
      - clickhouseinstallations
    verbs:
      - get
      - patch
      - update
      - delete
  - apiGroups:
      - clickhouse.altinity.com
    resources:
      - clickhouseinstallations
      - clickhouseinstallationtemplates
      - clickhouseoperatorconfigurations
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - clickhouse.altinity.com
    resources:
      - clickhouseinstallations/finalizers
      - clickhouseinstallationtemplates/finalizers
      - clickhouseoperatorconfigurations/finalizers
    verbs:
      - update
  - apiGroups:
      - clickhouse.altinity.com
    resources:
      - clickhouseinstallations/status
      - clickhouseinstallationtemplates/status
      - clickhouseoperatorconfigurations/status
    verbs:
      - get
      - update
      - patch
      - create
      - delete
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - get
      - list
---
# Source: signoz/charts/clickhouse/templates/clickhouse-operator/rolebinding.yaml
# Setup RoleBinding between Role and ServiceAccount.
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: my-release-clickhouse-operator
  namespace: signoz-0.43.0.tgz
  labels:
    helm.sh/chart: clickhouse-24.1.0
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: operator
    app.kubernetes.io/version: "24.1.2"
    app.kubernetes.io/managed-by: Helm
    clickhouse.altinity.com/chop: 0.21.2
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: my-release-clickhouse-operator
subjects:
- kind: ServiceAccount
  name: my-release-clickhouse-operator
  namespace: signoz-0.43.0.tgz
---
# Source: signoz/charts/clickhouse/charts/zookeeper/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-zookeeper-headless
  namespace: signoz-0.43.0.tgz
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-11.4.2
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: tcp-client
      port: 2181
      targetPort: client
    - name: tcp-follower
      port: 2888
      targetPort: follower
    - name: tcp-election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: zookeeper
---
# Source: signoz/charts/clickhouse/charts/zookeeper/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-zookeeper
  namespace: signoz-0.43.0.tgz
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-11.4.2
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-client
      port: 2181
      targetPort: client
      nodePort: null
    - name: tcp-follower
      port: 2888
      targetPort: follower
    - name: tcp-election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: zookeeper
---
# Source: signoz/charts/clickhouse/templates/clickhouse-operator/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: my-release-clickhouse-operator-metrics
  namespace: signoz-0.43.0.tgz
  labels:
    helm.sh/chart: clickhouse-24.1.0
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: operator
    app.kubernetes.io/version: "24.1.2"
    app.kubernetes.io/managed-by: Helm
    clickhouse.altinity.com/chop: 0.21.2
spec:
  type: ClusterIP
  ports:
    - port: 8888
      name: my-release-clickhouse-operator-metrics
  selector:
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: operator
---
# Source: signoz/charts/k8s-infra/templates/otel-agent/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-k8s-infra-otel-agent
  namespace: signoz-0.43.0.tgz
  labels:
    helm.sh/chart: k8s-infra-0.11.5
    app.kubernetes.io/version: "0.88.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: k8s-infra
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: otel-agent
spec:
  type: ClusterIP
  ports:
    
    - name: health-check
      port: 13133
      targetPort: health-check
      protocol: TCP
      nodePort: null
    - name: metrics
      port: 8888
      targetPort: metrics
      protocol: TCP
      nodePort: null
    - name: otlp
      port: 4317
      targetPort: otlp
      protocol: TCP
      nodePort: null
    - name: otlp-http
      port: 4318
      targetPort: otlp-http
      protocol: TCP
      nodePort: null
  selector:
    app.kubernetes.io/name: k8s-infra
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: otel-agent
  internalTrafficPolicy: Local
---
# Source: signoz/charts/k8s-infra/templates/otel-deployment/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-k8s-infra-otel-deployment
  namespace: signoz-0.43.0.tgz
  labels:
    helm.sh/chart: k8s-infra-0.11.5
    app.kubernetes.io/version: "0.88.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: k8s-infra
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: otel-deployment
spec:
  type: ClusterIP
  ports:
    
    - name: health-check
      port: 13133
      targetPort: health-check
      protocol: TCP
      nodePort: null
  selector:
    app.kubernetes.io/name: k8s-infra
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: otel-deployment
---
# Source: signoz/templates/alertmanager/services.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-signoz-alertmanager
  labels:
    helm.sh/chart: signoz-0.43.0
    app.kubernetes.io/name: signoz
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: alertmanager
    app.kubernetes.io/version: "0.47.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 9093
      
      nodePort: null
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: signoz
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: alertmanager
---
# Source: signoz/templates/alertmanager/services.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-signoz-alertmanager-headless
  labels:
    helm.sh/chart: signoz-0.43.0
    app.kubernetes.io/name: signoz
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: alertmanager
    app.kubernetes.io/version: "0.47.0"
    app.kubernetes.io/managed-by: Helm
spec:
  clusterIP: None
  ports:
    - port: 9093
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: signoz
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: alertmanager
---
# Source: signoz/templates/frontend/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-signoz-frontend
  labels:
    helm.sh/chart: signoz-0.43.0
    app.kubernetes.io/name: signoz
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: frontend
    app.kubernetes.io/version: "0.47.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 3301
      
      nodePort: null
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: signoz
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: frontend
---
# Source: signoz/templates/otel-collector-metrics/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-signoz-otel-collector-metrics
  labels:
    helm.sh/chart: signoz-0.43.0
    app.kubernetes.io/name: signoz
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: otel-collector-metrics
    app.kubernetes.io/version: "0.47.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:    
    - name: health-check
      port: 13133
      targetPort: health-check
      protocol: TCP
      nodePort: null
  selector:
    app.kubernetes.io/name: signoz
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: otel-collector-metrics
---
# Source: signoz/templates/otel-collector/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-signoz-otel-collector
  labels:
    helm.sh/chart: signoz-0.43.0
    app.kubernetes.io/name: signoz
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: otel-collector
    app.kubernetes.io/version: "0.47.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:    
    - name: jaeger-grpc
      port: 14250
      targetPort: jaeger-grpc
      protocol: TCP
      nodePort: null
    - name: jaeger-thrift
      port: 14268
      targetPort: jaeger-thrift
      protocol: TCP
      nodePort: null
    - name: logsheroku
      port: 8081
      targetPort: logsheroku
      protocol: TCP
      nodePort: null
    - name: logsjson
      port: 8082
      targetPort: logsjson
      protocol: TCP
      nodePort: null
    - name: metrics
      port: 8888
      targetPort: metrics
      protocol: TCP
      nodePort: null
    - name: otlp
      port: 4317
      targetPort: otlp
      protocol: TCP
      nodePort: null
    - name: otlp-http
      port: 4318
      targetPort: otlp-http
      protocol: TCP
      nodePort: null
  selector:
    app.kubernetes.io/name: signoz
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: otel-collector
---
# Source: signoz/templates/query-service/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-signoz-query-service
  labels:
    helm.sh/chart: signoz-0.43.0
    app.kubernetes.io/name: signoz
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: query-service
    app.kubernetes.io/version: "0.47.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8080      
      nodePort: null
      protocol: TCP
      targetPort: http
    - name: http-internal
      port: 8085      
      nodePort: null
      protocol: TCP
      targetPort: http-internal
    - name: opamp-internal
      port: 4320      
      nodePort: null
      protocol: TCP
      targetPort: opamp-internal
  selector:
    app.kubernetes.io/name: signoz
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: query-service
---
# Source: signoz/charts/k8s-infra/templates/otel-agent/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: my-release-k8s-infra-otel-agent
  namespace: signoz-0.43.0.tgz
  labels:
    helm.sh/chart: k8s-infra-0.11.5
    app.kubernetes.io/version: "0.88.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: k8s-infra
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: otel-agent
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: k8s-infra
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: otel-agent
  minReadySeconds: 5
  template:
    metadata:
      annotations:
        signoz.io/path: /metrics
        signoz.io/port: "8888"
        signoz.io/scrape: "true"
        checksum/config: 26fbe6cfba0b348c3f9a3fe2c200ea5eaade55ddb130193d6caaaac2b0e6fa7e
      labels:
        app.kubernetes.io/name: k8s-infra
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/component: otel-agent
    spec:      
      serviceAccountName: my-release-k8s-infra-otel-agent
      securityContext:
        {}
      priorityClassName: ""
      tolerations:
        - operator: Exists
      volumes:
        - name: otel-agent-config-vol
          configMap:
            name: my-release-k8s-infra-otel-agent
        - name: varlog
          hostPath:
            path: /var/log
        - name: varlibdockercontainers
          hostPath:
            path: /var/lib/docker/containers
      containers:
        - name: my-release-k8s-infra-otel-agent
          image: docker.io/otel/opentelemetry-collector-contrib:0.88.0
          imagePullPolicy: IfNotPresent
          ports:
            - name: health-check
              containerPort: 13133
              protocol: TCP
              hostPort: 13133
            - name: metrics
              containerPort: 8888
              protocol: TCP
              hostPort: 8888
            - name: otlp
              containerPort: 4317
              protocol: TCP
              hostPort: 4317
            - name: otlp-http
              containerPort: 4318
              protocol: TCP
              hostPort: 4318
          command:
            - "/otelcol-contrib"
          args:
            - "--config=/conf/otel-agent-config.yaml"
          env:
            
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: my-release-signoz-otel-collector:4317
            - name: OTEL_EXPORTER_OTLP_INSECURE
              value: "true"
            - name: OTEL_EXPORTER_OTLP_INSECURE_SKIP_VERIFY
              value: "false"
            - name: OTEL_SECRETS_PATH
              value: /secrets
            
            - name: K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: K8S_POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
            - name: K8S_HOST_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: K8S_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: K8S_POD_UID
              valueFrom:
                fieldRef:
                  fieldPath: metadata.uid
            - name: K8S_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: K8S_CLUSTER_NAME
              value: 
            - name: SIGNOZ_COMPONENT
              value: otel-agent
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: "signoz.component=$(SIGNOZ_COMPONENT),k8s.cluster.name=$(K8S_CLUSTER_NAME),k8s.pod.uid=$(K8S_POD_UID),k8s.pod.ip=$(K8S_POD_IP)"
          securityContext:
            {}
          volumeMounts:
            - name: otel-agent-config-vol
              mountPath: /conf
            - name: varlog
              mountPath: /var/log
              readOnly: true
            - name: varlibdockercontainers
              mountPath: /var/lib/docker/containers
              readOnly: true
          livenessProbe:
            httpGet:
              port: 13133
              path: /
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            httpGet:
              port: 13133
              path: /
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
---
# Source: signoz/charts/clickhouse/templates/clickhouse-operator/deployment.yaml
kind: Deployment
apiVersion: apps/v1
metadata:
  name: my-release-clickhouse-operator
  namespace: signoz-0.43.0.tgz
  labels:
    helm.sh/chart: clickhouse-24.1.0
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: operator
    app.kubernetes.io/version: "24.1.2"
    app.kubernetes.io/managed-by: Helm
    clickhouse.altinity.com/chop: 0.21.2
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: clickhouse
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: operator
  template:
    metadata:
      annotations:
        signoz.io/port: "8888"
        signoz.io/scrape: "true"
        checksum/files: d3a908ea2b1c8725514dc73d7b2b79900ad928bee8bb91534f8ec1a96a258c42
        checksum/confd-files: 1b475047d6f79deb3ee0ef735c4802f22cad77dc6c77ab3650d54db8e48f2291
        checksum/configd-files: ec8b8800c18f7969b82f03c647e503721b879ee373286d52e2f9eaced0cafab2
        checksum/templatesd-files: 7b8aff0006843c8069bf07f06531c7a931d8331b07f89f400de73699145d4363
        checksum/usersd-files: 69702954a032f74ac2c62414891e765496b92b315c827a4fa2ea21e43271cb3d
      labels:
        app.kubernetes.io/name: clickhouse
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/component: operator
    spec:      
      serviceAccountName: my-release-clickhouse-operator
      priorityClassName: ""
      securityContext:
        {}
      volumes:
        - name: etc-clickhouse-operator-folder
          configMap:
            name: my-release-clickhouse-operator-etc-files
        - name: etc-clickhouse-operator-confd-folder
          configMap:
            name: my-release-clickhouse-operator-etc-confd-files
        - name: etc-clickhouse-operator-configd-folder
          configMap:
            name: my-release-clickhouse-operator-etc-configd-files
        - name: etc-clickhouse-operator-templatesd-folder
          configMap:
            name: my-release-clickhouse-operator-etc-templatesd-files
        - name: etc-clickhouse-operator-usersd-folder
          configMap:
            name: my-release-clickhouse-operator-etc-usersd-files
      containers:
        - name: my-release-clickhouse-operator
          image: docker.io/altinity/clickhouse-operator:0.21.2
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: etc-clickhouse-operator-folder
              mountPath: /etc/clickhouse-operator
            - name: etc-clickhouse-operator-confd-folder
              mountPath: /etc/clickhouse-operator/conf.d
            - name: etc-clickhouse-operator-configd-folder
              mountPath: /etc/clickhouse-operator/config.d
            - name: etc-clickhouse-operator-templatesd-folder
              mountPath: /etc/clickhouse-operator/templates.d
            - name: etc-clickhouse-operator-usersd-folder
              mountPath: /etc/clickhouse-operator/users.d
          env:
            # Pod-specific
            # spec.nodeName: ip-172-20-52-62.ec2.internal
            - name: OPERATOR_POD_NODE_NAME
              valueFrom:
                fieldRef:
                    fieldPath: spec.nodeName
            # metadata.name: clickhouse-operator-6f87589dbb-ftcsf
            - name: OPERATOR_POD_NAME
              valueFrom:
                fieldRef:
                    fieldPath: metadata.name
            # metadata.namespace: kube-system
            - name: OPERATOR_POD_NAMESPACE
              valueFrom:
                fieldRef:
                    fieldPath: metadata.namespace
            # status.podIP: 100.96.3.2
            - name: OPERATOR_POD_IP
              valueFrom:
                fieldRef:
                    fieldPath: status.podIP
            # spec.serviceAccount: my-release-clickhouse-operator
            # spec.serviceAccountName: my-release-clickhouse-operator
            - name: OPERATOR_POD_SERVICE_ACCOUNT
              valueFrom:
                fieldRef:
                    fieldPath: spec.serviceAccountName
            
            # Container-specific
            - name: OPERATOR_CONTAINER_CPU_REQUEST
              valueFrom:
                resourceFieldRef:
                    containerName: my-release-clickhouse-operator
                    resource: requests.cpu
            - name: OPERATOR_CONTAINER_CPU_LIMIT
              valueFrom:
                resourceFieldRef:
                    containerName: my-release-clickhouse-operator
                    resource: limits.cpu
            - name: OPERATOR_CONTAINER_MEM_REQUEST
              valueFrom:
                resourceFieldRef:
                    containerName: my-release-clickhouse-operator
                    resource: requests.memory
            - name: OPERATOR_CONTAINER_MEM_LIMIT
              valueFrom:
                resourceFieldRef:
                    containerName: my-release-clickhouse-operator
                    resource: limits.memory
          resources:
            null

        - name: my-release-clickhouse-metrics-exporter
          image: docker.io/altinity/metrics-exporter:0.21.2
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: etc-clickhouse-operator-folder
              mountPath: /etc/clickhouse-operator
            - name: etc-clickhouse-operator-confd-folder
              mountPath: /etc/clickhouse-operator/conf.d
            - name: etc-clickhouse-operator-configd-folder
              mountPath: /etc/clickhouse-operator/config.d
            - name: etc-clickhouse-operator-templatesd-folder
              mountPath: /etc/clickhouse-operator/templates.d
            - name: etc-clickhouse-operator-usersd-folder
              mountPath: /etc/clickhouse-operator/users.d
          env:
            # Pod-specific
            # spec.nodeName: ip-172-20-52-62.ec2.internal
            - name: OPERATOR_POD_NODE_NAME
              valueFrom:
                fieldRef:
                    fieldPath: spec.nodeName
            # metadata.name: clickhouse-operator-6f87589dbb-ftcsf
            - name: OPERATOR_POD_NAME
              valueFrom:
                fieldRef:
                    fieldPath: metadata.name
            # metadata.namespace: kube-system
            - name: OPERATOR_POD_NAMESPACE
              valueFrom:
                fieldRef:
                    fieldPath: metadata.namespace
            # status.podIP: 100.96.3.2
            - name: OPERATOR_POD_IP
              valueFrom:
                fieldRef:
                    fieldPath: status.podIP
            # spec.serviceAccount: my-release-clickhouse-operator
            # spec.serviceAccountName: my-release-clickhouse-operator
            - name: OPERATOR_POD_SERVICE_ACCOUNT
              valueFrom:
                fieldRef:
                    fieldPath: spec.serviceAccountName
            
            # Container-specific
            - name: OPERATOR_CONTAINER_CPU_REQUEST
              valueFrom:
                resourceFieldRef:
                    containerName: my-release-clickhouse-operator
                    resource: requests.cpu
            - name: OPERATOR_CONTAINER_CPU_LIMIT
              valueFrom:
                resourceFieldRef:
                    containerName: my-release-clickhouse-operator
                    resource: limits.cpu
            - name: OPERATOR_CONTAINER_MEM_REQUEST
              valueFrom:
                resourceFieldRef:
                    containerName: my-release-clickhouse-operator
                    resource: requests.memory
            - name: OPERATOR_CONTAINER_MEM_LIMIT
              valueFrom:
                resourceFieldRef:
                    containerName: my-release-clickhouse-operator
                    resource: limits.memory
          ports:
            - containerPort: 8888
              name: metrics
          resources:
            null
---
# Source: signoz/charts/k8s-infra/templates/otel-deployment/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-k8s-infra-otel-deployment
  namespace: signoz-0.43.0.tgz
  labels:
    helm.sh/chart: k8s-infra-0.11.5
    app.kubernetes.io/version: "0.88.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: k8s-infra
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: otel-deployment
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: k8s-infra
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: otel-deployment
  minReadySeconds: 5
  progressDeadlineSeconds: 120
  replicas: 
  template:
    metadata:
      annotations:
        signoz.io/path: /metrics
        signoz.io/port: "8888"
        signoz.io/scrape: "true"
        checksum/config: c3962a605698609588baca83b47561c02632deca04319f8642a208cc79210d45
      labels:
        app.kubernetes.io/name: k8s-infra
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/component: otel-deployment
    spec:      
      serviceAccountName: my-release-k8s-infra-otel-deployment
      securityContext:
        {}
      priorityClassName: ""
      volumes:
        - name: otel-deployment-config-vol
          configMap:
            name: my-release-k8s-infra-otel-deployment
      containers:
        - name: my-release-k8s-infra-otel-deployment
          image: docker.io/otel/opentelemetry-collector-contrib:0.88.0
          imagePullPolicy: IfNotPresent
          ports:
            - name: health-check
              containerPort: 13133
              protocol: TCP
          command:
            - "/otelcol-contrib"
          args:
            - "--config=/conf/otel-deployment-config.yaml"
          securityContext:
            {}
          env:
            
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: my-release-signoz-otel-collector:4317
            - name: OTEL_EXPORTER_OTLP_INSECURE
              value: "true"
            - name: OTEL_EXPORTER_OTLP_INSECURE_SKIP_VERIFY
              value: "false"
            - name: OTEL_SECRETS_PATH
              value: /secrets
            
            - name: K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: K8S_POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
            - name: K8S_HOST_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: K8S_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: K8S_POD_UID
              valueFrom:
                fieldRef:
                  fieldPath: metadata.uid
            - name: K8S_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: K8S_CLUSTER_NAME
              value: 
            - name: SIGNOZ_COMPONENT
              value: otel-deployment
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: signoz.component=$(SIGNOZ_COMPONENT),k8s.cluster.name=$(K8S_CLUSTER_NAME)
          volumeMounts:
            - name: otel-deployment-config-vol
              mountPath: /conf
          livenessProbe:
            httpGet:
              port: 13133
              path: /
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            httpGet:
              port: 13133
              path: /
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
---
# Source: signoz/templates/frontend/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-signoz-frontend
  labels:
    helm.sh/chart: signoz-0.43.0
    app.kubernetes.io/name: signoz
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: frontend
    app.kubernetes.io/version: "0.47.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    helm.sh/hook-weight: "5"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: signoz
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: frontend
  template:
    metadata:
      annotations:
        checksum/config: 481c1ebcaf0dd0ec894eb78ff7b9e2d59945ec7e01f902466dce20fba4fdbc26
      labels:
        app.kubernetes.io/name: signoz
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/component: frontend
    spec:
      serviceAccountName: my-release-signoz-frontend
      priorityClassName: ""
      securityContext:
        {}
      volumes:
        - name: nginx-config
          configMap:
            name: my-release-signoz-frontend
      initContainers:
        - name: my-release-signoz-frontend-init
          image: docker.io/busybox:1.35
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - until wget --spider -q my-release-signoz-query-service:8080/api/v1/health?live=1; do echo -e "waiting for query-service"; sleep 5; done; echo -e "query-service ready, starting frontend now";
          resources:
            {}
      containers:
        - name: my-release-signoz-frontend
          securityContext:
            {}
          image: docker.io/signoz/frontend:0.47.0
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort:  3301
              protocol: TCP
          env:
          - name: REACT_APP_QUERY_SERVICE_URL
            value: ":8080"
          volumeMounts:
            - name: nginx-config
              mountPath: /etc/nginx/conf.d
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
---
# Source: signoz/templates/otel-collector-metrics/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-signoz-otel-collector-metrics
  labels:
    helm.sh/chart: signoz-0.43.0
    app.kubernetes.io/name: signoz
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: otel-collector-metrics
    app.kubernetes.io/version: "0.47.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    
    helm.sh/hook-weight: "3"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: signoz
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: otel-collector-metrics
  minReadySeconds: 5
  progressDeadlineSeconds: 120
  replicas: 1 # adjust replica count to your own requirements
  template:
    metadata:
      annotations:
        signoz.io/port: "8888"
        signoz.io/scrape: "true"
        checksum/config: e1bb97ca7262f6748b1560558bb3e3fafb50f5ee0d9df00995eb7fc64a6ba4cb
      labels:
        app.kubernetes.io/name: signoz
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/component: otel-collector-metrics
    spec:
      serviceAccountName: my-release-signoz-otel-collector-metrics
      priorityClassName: ""
      securityContext:
        {}
      initContainers:
        - name: "my-release-signoz-otel-collector-metrics-migrate-init"
          image: docker.io/groundnuty/k8s-wait-for:v2.0
          imagePullPolicy: IfNotPresent
          args:
          - "job"
          - "my-release-signoz-schema-migrator-init"
      containers:
        - name: my-release-signoz-otel-collector-metrics
          securityContext:
            {}
          image: docker.io/signoz/signoz-otel-collector:0.88.26
          imagePullPolicy: IfNotPresent
          ports:
            - name: health-check
              containerPort: 13133
              protocol: TCP
          command:
            - "/signoz-collector"
          args:
            - "--config=/conf/otel-collector-metrics-config.yaml"
            - "--feature-gates=-pkg.translator.prometheus.NormalizeName"
          env:
            - name: CLICKHOUSE_HOST
              value: my-release-clickhouse
            - name: CLICKHOUSE_PORT
              value: "9000"
            - name: CLICKHOUSE_HTTP_PORT
              value: "8123"
            - name: CLICKHOUSE_CLUSTER
              value: "cluster"
            - name: CLICKHOUSE_DATABASE
              value: "signoz_metrics"
            - name: CLICKHOUSE_TRACE_DATABASE
              value: "signoz_traces"
            - name: CLICKHOUSE_LOG_DATABASE
              value: "signoz_logs"
            - name: CLICKHOUSE_USER
              value: "admin"
            - name: CLICKHOUSE_PASSWORD
              value: "27ff0399-0d3a-4bd8-919d-17c2181e6fb9"
            - name: CLICKHOUSE_SECURE
              value: "false"
            - name: CLICKHOUSE_VERIFY
              value: "false"
            
            - name: K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: K8S_POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
            - name: K8S_HOST_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: K8S_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: K8S_POD_UID
              valueFrom:
                fieldRef:
                  fieldPath: metadata.uid
            - name: K8S_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: K8S_CLUSTER_NAME
              value: 
            - name: SIGNOZ_COMPONENT
              value: otel-collector-metrics
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: signoz.component=$(SIGNOZ_COMPONENT),k8s.cluster.name=$(K8S_CLUSTER_NAME),k8s.pod.uid=$(K8S_POD_UID),k8s.pod.ip=$(K8S_POD_IP)
          volumeMounts:
            - name: otel-collector-metrics-config-vol
              mountPath: /conf
            # - name: otel-collector-secrets
            #   mountPath: /secrets
          livenessProbe:
            httpGet:
              port: 13133
              path: /
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            httpGet:
              port: 13133
              path: /
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
      volumes:
        - name: otel-collector-metrics-config-vol
          configMap:
            name: my-release-signoz-otel-collector-metrics
#        - secret:
#            name: otel-collector-secrets
#            items:
#              - key: cert.pem
#                path: cert.pem
#              - key: key.pem
#                path: key.pem
---
# Source: signoz/templates/otel-collector/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-signoz-otel-collector
  labels:
    helm.sh/chart: signoz-0.43.0
    app.kubernetes.io/name: signoz
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: otel-collector
    app.kubernetes.io/version: "0.47.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    
    helm.sh/hook-weight: "3"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: signoz
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: otel-collector
  minReadySeconds: 5
  progressDeadlineSeconds: 120
  replicas: 1
  template:
    metadata:
      annotations:
        signoz.io/port: "8888"
        signoz.io/scrape: "true"
        checksum/config: 2a09051ebfc73dc7bac87d0b798d0a2e57e4fe994d0a607cc5835e643a316e15
      labels:
        app.kubernetes.io/name: signoz
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/component: otel-collector
    spec:
      serviceAccountName: my-release-signoz-otel-collector
      priorityClassName: ""
      topologySpreadConstraints: 
        - labelSelector:
            matchLabels:
              app.kubernetes.io/component: otel-collector
          maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
      securityContext:
        {}
      # todo: add k8s-wait-for initContainer here
      # this initContainer waits for the schema migrator job to finish
      initContainers:
        - name: "my-release-signoz-otel-collector-migrate-init"
          image: docker.io/groundnuty/k8s-wait-for:v2.0
          imagePullPolicy: IfNotPresent
          args:
          - "job"
          - "my-release-signoz-schema-migrator-init"
      containers:
        - name: my-release-signoz-otel-collector
          image: docker.io/signoz/signoz-otel-collector:0.88.26
          imagePullPolicy: IfNotPresent
          ports:
            - name: jaeger-grpc
              containerPort: 14250
              protocol: TCP
            - name: jaeger-thrift
              containerPort: 14268
              protocol: TCP
            - name: logsheroku
              containerPort: 8081
              protocol: TCP
            - name: logsjson
              containerPort: 8082
              protocol: TCP
            - name: metrics
              containerPort: 8888
              protocol: TCP
            - name: otlp
              containerPort: 4317
              protocol: TCP
            - name: otlp-http
              containerPort: 4318
              protocol: TCP
          command:
            - "/signoz-collector"
          args:
            - "--config=/conf/otel-collector-config.yaml"
            - "--manager-config=/conf/otel-collector-opamp-config.yaml"
            - "--copy-path=/var/tmp/collector-config.yaml"
            - "--feature-gates=-pkg.translator.prometheus.NormalizeName"
          env:
            - name: CLICKHOUSE_HOST
              value: my-release-clickhouse
            - name: CLICKHOUSE_PORT
              value: "9000"
            - name: CLICKHOUSE_HTTP_PORT
              value: "8123"
            - name: CLICKHOUSE_CLUSTER
              value: "cluster"
            - name: CLICKHOUSE_DATABASE
              value: "signoz_metrics"
            - name: CLICKHOUSE_TRACE_DATABASE
              value: "signoz_traces"
            - name: CLICKHOUSE_LOG_DATABASE
              value: "signoz_logs"
            - name: CLICKHOUSE_USER
              value: "admin"
            - name: CLICKHOUSE_PASSWORD
              value: "27ff0399-0d3a-4bd8-919d-17c2181e6fb9"
            - name: CLICKHOUSE_SECURE
              value: "false"
            - name: CLICKHOUSE_VERIFY
              value: "false"
            
            - name: K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: K8S_POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
            - name: K8S_HOST_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: K8S_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: K8S_POD_UID
              valueFrom:
                fieldRef:
                  fieldPath: metadata.uid
            - name: K8S_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: K8S_CLUSTER_NAME
              value: 
            - name: SIGNOZ_COMPONENT
              value: otel-collector
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: signoz.component=$(SIGNOZ_COMPONENT),k8s.cluster.name=$(K8S_CLUSTER_NAME),k8s.pod.uid=$(K8S_POD_UID),k8s.pod.ip=$(K8S_POD_IP)
            - name: LOW_CARDINAL_EXCEPTION_GROUPING
              value: "false"
          volumeMounts:
            - name: otel-collector-config-vol
              mountPath: /conf
            # - name: otel-collector-secrets
            #   mountPath: /secrets
          livenessProbe:
            httpGet:
              port: 13133
              path: /
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            httpGet:
              port: 13133
              path: /
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          resources:
            requests:
              cpu: 100m
              memory: 200Mi
      volumes:
        - name: otel-collector-config-vol
          configMap:
            name: my-release-signoz-otel-collector
#        - secret:
#            name: otel-collector-secrets
#            items:
#              - key: cert.pem
#                path: cert.pem
#              - key: key.pem
#                path: key.pem
---
# Source: signoz/charts/clickhouse/charts/zookeeper/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-zookeeper
  namespace: signoz-0.43.0.tgz
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-11.4.2
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
    role: zookeeper
spec:
  replicas: 1
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app.kubernetes.io/name: zookeeper
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: zookeeper
  serviceName: my-release-zookeeper-headless
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  template:
    metadata:
      annotations:
      labels:
        app.kubernetes.io/name: zookeeper
        helm.sh/chart: zookeeper-11.4.2
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: zookeeper
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: zookeeper
                    app.kubernetes.io/instance: my-release
                    app.kubernetes.io/component: zookeeper
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      initContainers:
      containers:
        - name: zookeeper
          image: docker.io/bitnami/zookeeper:3.7.1
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 1001
          command:
            - /scripts/setup.sh
          resources:
            limits: {}
            requests:
              cpu: 100m
              memory: 256Mi
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: ZOO_DATA_LOG_DIR
              value: ""
            - name: ZOO_PORT_NUMBER
              value: "2181"
            - name: ZOO_TICK_TIME
              value: "2000"
            - name: ZOO_INIT_LIMIT
              value: "10"
            - name: ZOO_SYNC_LIMIT
              value: "5"
            - name: ZOO_PRE_ALLOC_SIZE
              value: "65536"
            - name: ZOO_SNAPCOUNT
              value: "100000"
            - name: ZOO_MAX_CLIENT_CNXNS
              value: "60"
            - name: ZOO_4LW_COMMANDS_WHITELIST
              value: "srvr, mntr, ruok"
            - name: ZOO_LISTEN_ALLIPS_ENABLED
              value: "no"
            - name: ZOO_AUTOPURGE_INTERVAL
              value: "1"
            - name: ZOO_AUTOPURGE_RETAIN_COUNT
              value: "3"
            - name: ZOO_MAX_SESSION_TIMEOUT
              value: "40000"
            - name: ZOO_SERVERS
              value: my-release-zookeeper-0.my-release-zookeeper-headless.signoz-0.43.0.tgz.svc.cluster.local:2888:3888::1 
            - name: ZOO_ENABLE_AUTH
              value: "no"
            - name: ZOO_ENABLE_QUORUM_AUTH
              value: "no"
            - name: ZOO_HEAP_SIZE
              value: "1024"
            - name: ZOO_LOG_LEVEL
              value: "ERROR"
            - name: ALLOW_ANONYMOUS_LOGIN
              value: "yes"
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
          ports:
            - name: client
              containerPort: 2181
            - name: follower
              containerPort: 2888
            - name: election
              containerPort: 3888
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command: ['/bin/bash', '-c', 'echo "ruok" | timeout 2 nc -w 2 localhost 2181 | grep imok']
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command: ['/bin/bash', '-c', 'echo "ruok" | timeout 2 nc -w 2 localhost 2181 | grep imok']
          volumeMounts:
            - name: scripts
              mountPath: /scripts/setup.sh
              subPath: setup.sh
            - name: data
              mountPath: /bitnami/zookeeper
      volumes:
        - name: scripts
          configMap:
            name: my-release-zookeeper-scripts
            defaultMode: 0755
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: signoz/templates/alertmanager/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-signoz-alertmanager
  labels:
    helm.sh/chart: signoz-0.43.0
    app.kubernetes.io/name: signoz
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: alertmanager
    app.kubernetes.io/version: "0.47.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    
    helm.sh/hook-weight: "4"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: signoz
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: alertmanager
  serviceName: my-release-signoz-alertmanager-headless
  template:
    metadata:
      labels:
        app.kubernetes.io/name: signoz
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/component: alertmanager
      annotations:
        checksum/config: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
    spec:
      serviceAccountName: my-release-signoz-alertmanager
      priorityClassName: ""
      securityContext:
        fsGroup: 65534
      initContainers:
        - name: my-release-signoz-alertmanager-init
          image: docker.io/busybox:1.35
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - until wget --spider -q my-release-signoz-query-service:8080/api/v1/health?live=1; do echo -e "waiting for query-service"; sleep 5; done; echo -e "query-service ready, starting alertmanager now";
          resources:
            {}
      containers:
        - name: my-release-signoz-alertmanager
          securityContext:
            runAsGroup: 65534
            runAsNonRoot: true
            runAsUser: 65534
          image: docker.io/signoz/alertmanager:0.23.5
          imagePullPolicy: IfNotPresent
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
          args:
            - --storage.path=/alertmanager
            - --queryService.url=http://my-release-signoz-query-service:8085
            - --cluster.advertise-address=$(POD_IP):9094
          ports:
            - name: http
              containerPort: 9093
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          volumeMounts:
            - name: storage
              mountPath: /alertmanager
  volumeClaimTemplates:
    - metadata:
        name: storage
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 100Mi
---
# Source: signoz/templates/query-service/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-signoz-query-service
  labels:
    helm.sh/chart: signoz-0.43.0
    app.kubernetes.io/name: signoz
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: query-service
    app.kubernetes.io/version: "0.47.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    helm.sh/hook-weight: "2"
spec:
  serviceName: my-release-signoz-query-service
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: signoz
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: query-service
  template:
    metadata:
      annotations:
        checksum/config: 42d00c4ca57adfdc254a2b84437593f4e98d7912c1808bec4d8a4e12ea7bd18a
      labels:
        app.kubernetes.io/name: signoz
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/component: query-service
    spec:
      serviceAccountName: my-release-signoz-query-service
      priorityClassName: ""
      securityContext:
        {}
      initContainers:
        - name: my-release-signoz-query-service-init
          image: docker.io/busybox:1.35
          imagePullPolicy: IfNotPresent
          env:
            
            - name: CLICKHOUSE_HOST
              value: my-release-clickhouse
            - name: CLICKHOUSE_PORT
              value: "9000"
            - name: CLICKHOUSE_HTTP_PORT
              value: "8123"
            - name: CLICKHOUSE_CLUSTER
              value: "cluster"
            - name: CLICKHOUSE_USER
              value: "admin"
            - name: CLICKHOUSE_PASSWORD
              value: "27ff0399-0d3a-4bd8-919d-17c2181e6fb9"
            - name: CLICKHOUSE_SECURE
              value: "false"
          command:
            - sh
            - -c
            - until wget --user "${CLICKHOUSE_USER}:${CLICKHOUSE_PASSWORD}" --spider -q my-release-clickhouse:8123/ping; do echo -e "waiting for clickhouseDB"; sleep 5; done; echo -e "clickhouse ready, starting query service now";
          resources:
            {}
      containers:
        - name: my-release-signoz-query-service
          securityContext:
            {}
          image: docker.io/signoz/query-service:0.47.0
          imagePullPolicy: IfNotPresent
          args:
            - --config=/root/config/prometheus.yml
            - --experimental.cache-config
            - /root/config/cache.yaml
            - --flux-interval
            - 30m
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
            - name: http-internal
              containerPort: 8085
              protocol: TCP
            - name: opamp-internal
              containerPort: 4320
              protocol: TCP
          env:
            
            - name: CLICKHOUSE_HOST
              value: my-release-clickhouse
            - name: CLICKHOUSE_PORT
              value: "9000"
            - name: CLICKHOUSE_HTTP_PORT
              value: "8123"
            - name: CLICKHOUSE_CLUSTER
              value: "cluster"
            - name: CLICKHOUSE_USER
              value: "admin"
            - name: CLICKHOUSE_PASSWORD
              value: "27ff0399-0d3a-4bd8-919d-17c2181e6fb9"
            - name: CLICKHOUSE_SECURE
              value: "false"
            - name: STORAGE
              value: clickhouse
            - name: ClickHouseUrl
              value: tcp://my-release-clickhouse:9000/?username=admin&password=27ff0399-0d3a-4bd8-919d-17c2181e6fb9
            - name: ALERTMANAGER_API_PREFIX
              value: http://my-release-signoz-alertmanager:9093/api/
            - name: GODEBUG
              value: netdns=go
            - name: TELEMETRY_ENABLED
              value: "true"
            - name: DEPLOYMENT_TYPE
              value: kubernetes-helm
          livenessProbe:
            httpGet:
              port: http
              path: /api/v1/health
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            httpGet:
              port: http
              path: /api/v1/health?live=1
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          volumeMounts:
            - name: prometheus
              mountPath: /root/config
            - name: dashboards
              mountPath: /root/config/dashboards
            - name: signoz-db
              mountPath: /var/lib/signoz/
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
      volumes:
        - name: prometheus
          configMap:
            name: my-release-signoz-query-service
        - name: dashboards
          emptyDir: {}
  volumeClaimTemplates:
    - metadata:
        name: signoz-db
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 1Gi
---
# Source: signoz/templates/schema-migrator/migrations-init.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: my-release-signoz-schema-migrator-init
  labels:
    app.kubernetes.io/name: signoz
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: schema-migrator-init
  annotations:
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "1"
spec:
  template:
    metadata:
      labels:
        app.kubernetes.io/name: signoz
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/component: schema-migrator-init
    spec:
      initContainers:
        - name: my-release-signoz-schema-migrator-init
          # todo: use schema migrator variables here
          image: docker.io/busybox:1.35
          imagePullPolicy: IfNotPresent
          env:
            
            - name: CLICKHOUSE_HOST
              value: my-release-clickhouse
            - name: CLICKHOUSE_PORT
              value: "9000"
            - name: CLICKHOUSE_HTTP_PORT
              value: "8123"
            - name: CLICKHOUSE_CLUSTER
              value: "cluster"
            - name: CLICKHOUSE_USER
              value: "admin"
            - name: CLICKHOUSE_PASSWORD
              value: "27ff0399-0d3a-4bd8-919d-17c2181e6fb9"
            - name: CLICKHOUSE_SECURE
              value: "false"
          command:
            - sh
            - -c
            - until wget --user "$(CLICKHOUSE_USER):$(CLICKHOUSE_PASSWORD)" --spider -q my-release-clickhouse:8123/ping; do echo -e "waiting for clickhouseDB"; sleep 5; done; echo -e "clickhouse ready, starting schema migrator now";
          resources:
            {}
        # ClickHouse ready check
        - name: my-release-signoz-schema-migrator-ch-ready
          image: docker.io/clickhouse/clickhouse-server:24.1.2-alpine
          imagePullPolicy: IfNotPresent
          env:
            
            - name: CLICKHOUSE_HOST
              value: my-release-clickhouse
            - name: CLICKHOUSE_PORT
              value: "9000"
            - name: CLICKHOUSE_HTTP_PORT
              value: "8123"
            - name: CLICKHOUSE_CLUSTER
              value: "cluster"
            - name: CLICKHOUSE_USER
              value: "admin"
            - name: CLICKHOUSE_PASSWORD
              value: "27ff0399-0d3a-4bd8-919d-17c2181e6fb9"
            - name: CLICKHOUSE_SECURE
              value: "false"
            - name: CLICKHOUSE_VERSION
              value: 24.1.2
            - name: CLICKHOUSE_SHARDS
              value: "1"
            - name: CLICKHOUSE_REPLICAS
              value: "1"
          command: 
            - sh
            - -c
            - |
              echo "Running clickhouse ready check"
              while true
              do
                version="$(CLICKHOUSE_VERSION)"
                shards="$(CLICKHOUSE_SHARDS)"
                replicas="$(CLICKHOUSE_REPLICAS)"
                current_version="$(clickhouse client --host ${CLICKHOUSE_HOST} --port ${CLICKHOUSE_PORT} --user "${CLICKHOUSE_USER}" --password "${CLICKHOUSE_PASSWORD}" -q "SELECT version()")"
                if [ -z "$current_version" ]; then
                  echo "waiting for clickhouse to be ready"
                  sleep 5
                  continue
                fi
                if [ -z "$(echo "$current_version" | grep "$version")" ]; then
                  echo "expected version: $version, current version: $current_version"
                  echo "waiting for clickhouse with correct version"
                  sleep 5
                  continue
                fi
                current_shards="$(clickhouse client --host ${CLICKHOUSE_HOST} --port ${CLICKHOUSE_PORT} --user "${CLICKHOUSE_USER}" --password "${CLICKHOUSE_PASSWORD}" -q "SELECT count(DISTINCT(shard_num)) FROM system.clusters WHERE cluster = '${CLICKHOUSE_CLUSTER}'")"
                if [ -z "$current_shards" ]; then
                  echo "waiting for clickhouse to be ready"
                  sleep 5
                  continue
                fi
                if [ "$current_shards" -ne "$shards" ]; then
                  echo "expected shard count: $shards, current shard count: $current_shards"
                  echo "waiting for clickhouse with correct shard count"
                  sleep 5
                  continue
                fi
                current_replicas="$(clickhouse client --host ${CLICKHOUSE_HOST} --port ${CLICKHOUSE_PORT} --user "${CLICKHOUSE_USER}" --password "${CLICKHOUSE_PASSWORD}" -q "SELECT count(DISTINCT(replica_num)) FROM system.clusters WHERE cluster = '${CLICKHOUSE_CLUSTER}'")"
                if [ -z "$current_replicas" ]; then
                  echo "waiting for clickhouse to be ready"
                  sleep 5
                  continue
                fi
                if [ "$current_replicas" -ne "$replicas" ]; then
                  echo "expected replica count: $replicas, current replica count: $current_replicas"
                  echo "waiting for clickhouse with correct replica count"
                  sleep 5
                  continue
                fi
                break
              done
              echo "clickhouse ready, starting schema migrator now"
          resources:
            {}
      containers:
        - name: schema-migrator
          image: docker.io/signoz/signoz-schema-migrator:0.88.26
          imagePullPolicy: IfNotPresent
          env:
            
            - name: CLICKHOUSE_HOST
              value: my-release-clickhouse
            - name: CLICKHOUSE_PORT
              value: "9000"
            - name: CLICKHOUSE_HTTP_PORT
              value: "8123"
            - name: CLICKHOUSE_CLUSTER
              value: "cluster"
            - name: CLICKHOUSE_USER
              value: "admin"
            - name: CLICKHOUSE_PASSWORD
              value: "27ff0399-0d3a-4bd8-919d-17c2181e6fb9"
            - name: CLICKHOUSE_SECURE
              value: "false"
          args:
            - "--dsn"
            - "tcp://$(CLICKHOUSE_USER):$(CLICKHOUSE_PASSWORD)@my-release-clickhouse:9000"
      restartPolicy: OnFailure
---
# Source: signoz/charts/clickhouse/templates/clickhouse-instance/clickhouse-instance.yaml
apiVersion: clickhouse.altinity.com/v1
kind: ClickHouseInstallation
metadata:
  name: my-release-clickhouse
  namespace: signoz-0.43.0.tgz
  labels:
    helm.sh/chart: clickhouse-24.1.0
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: clickhouse
    app.kubernetes.io/version: "24.1.2"
    app.kubernetes.io/managed-by: Helm
spec:
  defaults:
    templates:
      dataVolumeClaimTemplate: data-volumeclaim-template
      # logVolumeClaimTemplate: log-volumeclaim-template
      serviceTemplate: service-template

  configuration:
    users:
      admin/password: 27ff0399-0d3a-4bd8-919d-17c2181e6fb9
      admin/networks/ip:
        - "10.0.0.0/8"
        - "100.64.0.0/10"
        - "172.16.0.0/12"
        - "192.0.0.0/24"
        - "198.18.0.0/15"
        - "192.168.0.0/16"
      admin/profile: default
      admin/quota: default

    profiles:
      default/allow_experimental_window_functions: "1"
      default/allow_nondeterministic_mutations: "1"

    clusters:
      - name: "cluster"
        templates:
          podTemplate: pod-template
        layout:
          replicasCount: 1
          shardsCount: 1

    settings:
      format_schema_path: /etc/clickhouse-server/config.d/
      prometheus/endpoint: /metrics
      prometheus/port: 9363
      user_defined_executable_functions_config: /etc/clickhouse-server/functions/custom-functions.xml
      user_scripts_path: /var/lib/clickhouse/user_scripts/

    files:
      events.proto: |
        syntax = "proto3";
        message Event {
          string uuid = 1;
          string event = 2;
          string properties = 3;
          string timestamp = 4;
          uint64 team_id = 5;
          string distinct_id = 6;
          string created_at = 7;
          string elements_chain = 8;
        }
            

    zookeeper:
      nodes:
        - host: my-release-zookeeper-0.my-release-zookeeper-headless
          port: 2181

  templates:
    podTemplates:
      - name: pod-template
        metadata:
          labels:
            helm.sh/chart: clickhouse-24.1.0
            app.kubernetes.io/name: clickhouse
            app.kubernetes.io/instance: my-release
            app.kubernetes.io/component: clickhouse
            app.kubernetes.io/version: "24.1.2"
            app.kubernetes.io/managed-by: Helm
          annotations:
            signoz.io/path: /metrics
            signoz.io/port: "9363"
            signoz.io/scrape: "true"
        spec:          
          serviceAccountName: my-release-clickhouse
          priorityClassName: ""
          securityContext:
            fsGroup: 101
            fsGroupChangePolicy: OnRootMismatch
            runAsGroup: 101
            runAsUser: 101

          volumes:
            - name: shared-binary-volume
              emptyDir: {}
            - name: custom-functions-volume
              configMap:
                name: my-release-clickhouse-custom-functions
          initContainers:
            - name: my-release-clickhouse-udf-init
              image: docker.io/alpine:3.18.2
              imagePullPolicy: IfNotPresent
              command:
                - sh
                - -c
                - |
                  set -x
                  wget -O /tmp/histogramQuantile https://github.com/SigNoz/signoz/raw/develop/deploy/docker/clickhouse-setup/user_scripts/histogramQuantile
                  mv /tmp/histogramQuantile  /var/lib/clickhouse/user_scripts/histogramQuantile
                  chmod +x /var/lib/clickhouse/user_scripts/histogramQuantile
              volumeMounts:
                - name: shared-binary-volume
                  mountPath: /var/lib/clickhouse/user_scripts
          containers:
            - name: clickhouse
              # KEEP CLICKHOUSE-SERVER VERSION IN SYNC WITH
              # https://github.com/SigNoz/signoz/blob/develop/deploy/docker/clickhouse-setup/docker-compose.yaml#L5
              image: docker.io/clickhouse/clickhouse-server:24.1.2-alpine
              imagePullPolicy: IfNotPresent
              command:
                - /bin/bash
                - -c
                - /usr/bin/clickhouse-server --config-file=/etc/clickhouse-server/config.xml

              ports:
                - name: http
                  containerPort: 8123
                - name: client
                  containerPort: 9000
                - name: interserver
                  containerPort: 9009

              volumeMounts:
                - name: data-volumeclaim-template
                  mountPath: /var/lib/clickhouse
                - name: shared-binary-volume
                  mountPath: /var/lib/clickhouse/user_scripts
                - name: custom-functions-volume
                  mountPath: /etc/clickhouse-server/functions
                # - name: log-volumeclaim-template
                #   mountPath: /var/log/clickhouse-server
              resources: 
                requests:
                  cpu: 100m
                  memory: 200Mi

    serviceTemplates:
      - name: service-template
        generateName: my-release-clickhouse
        metadata:
          labels:
            helm.sh/chart: clickhouse-24.1.0
            app.kubernetes.io/name: clickhouse
            app.kubernetes.io/instance: my-release
            app.kubernetes.io/component: clickhouse
            app.kubernetes.io/version: "24.1.2"
            app.kubernetes.io/managed-by: Helm
        spec:
          type: ClusterIP
          ports:
            - name: http
              port: 8123
              nodePort: null
            - name: tcp
              port: 9000
              nodePort: null
    volumeClaimTemplates:
      - name: data-volumeclaim-template
        reclaimPolicy: Retain
        spec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 20Gi
---
# Source: signoz/charts/k8s-infra/templates/otel-agent/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "my-release-k8s-infra-otel-agent-test-connection"
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: docker.io/busybox:1.35
      command: ['wget']
      args: ['my-release-k8s-infra-otel-agent:13133']
  restartPolicy: Never
---
# Source: signoz/charts/k8s-infra/templates/otel-deployment/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "my-release-k8s-infra-otel-deployment-test-connection"
  labels:
    helm.sh/chart: k8s-infra-0.11.5
    app.kubernetes.io/version: "0.88.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: k8s-infra
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: otel-deployment
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: docker.io/busybox:1.35
      command: ['wget']
      args: ['my-release-k8s-infra-otel-deployment:13133']
  restartPolicy: Never
---
# Source: signoz/templates/frontend/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "my-release-signoz-frontend-test-connection"
  labels:
    helm.sh/chart: signoz-0.43.0
    app.kubernetes.io/name: signoz
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: frontend
    app.kubernetes.io/version: "0.47.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test-success
spec:
  containers:
    - name: wget
      image: docker.io/busybox:1.35
      command: ['wget']
      args: ['my-release-signoz-frontend:3301']
  restartPolicy: Never
---
# Source: signoz/templates/query-service/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "my-release-signoz-query-service-test-connection"
  labels:
    helm.sh/chart: signoz-0.43.0
    app.kubernetes.io/name: signoz
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: query-service
    app.kubernetes.io/version: "0.47.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test-success
spec:
  containers:
    - name: wget
      image: docker.io/busybox:1.35
      command: ['wget']
      args: ['http://my-release-signoz-query-service:8080/api/v1/health?live=1']
      # NOTE: replace the path with version or healthcheck URL after the implementation
  restartPolicy: Never
