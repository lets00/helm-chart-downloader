---
# Source: lgtm-distributed/charts/mimir/templates/alertmanager/alertmanager-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-release-mimir-alertmanager
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: alertmanager
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: alertmanager
  maxUnavailable: 1
---
# Source: lgtm-distributed/charts/mimir/templates/compactor/compactor-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-release-mimir-compactor
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: compactor
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: compactor
  maxUnavailable: 1
---
# Source: lgtm-distributed/charts/mimir/templates/distributor/distributor-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-release-mimir-distributor
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: distributor
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: distributor
  maxUnavailable: 1
---
# Source: lgtm-distributed/charts/mimir/templates/ingester/ingester-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-release-mimir-ingester
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: ingester
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: ingester
  maxUnavailable: 1
---
# Source: lgtm-distributed/charts/mimir/templates/nginx/nginx-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-release-mimir-nginx
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: nginx
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: nginx
  maxUnavailable: 1
---
# Source: lgtm-distributed/charts/mimir/templates/overrides-exporter/overrides-exporter-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-release-mimir-overrides-exporter
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: overrides-exporter
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: overrides-exporter
  maxUnavailable: 1
---
# Source: lgtm-distributed/charts/mimir/templates/querier/querier-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-release-mimir-querier
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: querier
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: querier
  maxUnavailable: 1
---
# Source: lgtm-distributed/charts/mimir/templates/query-frontend/query-frontend-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-release-mimir-query-frontend
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: query-frontend
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: query-frontend
  maxUnavailable: 1
---
# Source: lgtm-distributed/charts/mimir/templates/query-scheduler/query-scheduler-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-release-mimir-query-scheduler
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: query-scheduler
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: query-scheduler
  maxUnavailable: 1
---
# Source: lgtm-distributed/charts/mimir/templates/ruler/ruler-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-release-mimir-ruler
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: ruler
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: ruler
  maxUnavailable: 1
---
# Source: lgtm-distributed/charts/mimir/templates/store-gateway/store-gateway-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-release-mimir-store-gateway
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: store-gateway
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: store-gateway
  maxUnavailable: 1
---
# Source: lgtm-distributed/charts/grafana/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: false
metadata:
  labels:
    helm.sh/chart: grafana-7.3.12
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "10.4.3"
    app.kubernetes.io/managed-by: Helm
  name: my-release-grafana
  namespace: lgtm-distributed-2.0.0.tgz
---
# Source: lgtm-distributed/charts/loki/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-loki
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: loki-0.79.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "2.9.6"
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
---
# Source: lgtm-distributed/charts/mimir/charts/minio/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: "minio-sa"
---
# Source: lgtm-distributed/charts/mimir/charts/rollout_operator/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-rollout-operator
  labels:
    helm.sh/chart: rollout-operator-0.14.0
    app.kubernetes.io/name: rollout-operator
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "v0.13.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: lgtm-distributed/charts/mimir/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-mimir
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "lgtm-distributed-2.0.0.tgz"
---
# Source: lgtm-distributed/charts/tempo/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-tempo
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: tempo-1.10.0
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "2.5.0"
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: false
---
# Source: lgtm-distributed/charts/grafana/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-release-grafana
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: grafana-7.3.12
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "10.4.3"
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  
  admin-user: "YWRtaW4="
  admin-password: "dDRtaTR1ZGhYVFJFeVJTdmJSaURYMG15MXlEUzhLb0lFVXc4YWlYMA=="
  ldap-toml: ""
---
# Source: lgtm-distributed/charts/mimir/charts/minio/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-release-minio
  labels:
    app: minio
    chart: minio-5.0.14
    release: my-release
    heritage: Helm
type: Opaque
data:
  rootUser: "Z3JhZmFuYS1taW1pcg=="
  rootPassword: "c3VwZXJzZWNyZXQ="
---
# Source: lgtm-distributed/charts/grafana/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-grafana
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: grafana-7.3.12
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "10.4.3"
    app.kubernetes.io/managed-by: Helm
data:
  
  grafana.ini: |
    [analytics]
    check_for_updates = true
    [grafana_net]
    url = https://grafana.net
    [log]
    mode = console
    [paths]
    data = /var/lib/grafana/
    logs = /var/log/grafana
    plugins = /var/lib/grafana/plugins
    provisioning = /etc/grafana/provisioning
    [server]
    domain = ''
  datasources.yaml: |
    apiVersion: 1
    datasources:
    - isDefault: false
      name: Loki
      type: loki
      uid: loki
      url: http://my-release-loki-gateway
    - isDefault: true
      name: Mimir
      type: prometheus
      uid: prom
      url: http://my-release-mimir-nginx/prometheus
    - isDefault: false
      jsonData:
        lokiSearch:
          datasourceUid: loki
        serviceMap:
          datasourceUid: prom
        tracesToLogsV2:
          datasourceUid: loki
        tracesToMetrics:
          datasourceUid: prom
      name: Tempo
      type: tempo
      uid: tempo
      url: http://my-release-tempo-query-frontend:3100
---
# Source: lgtm-distributed/charts/loki/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-loki
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: loki-0.79.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "2.9.6"
    app.kubernetes.io/managed-by: Helm
data:
  config.yaml: |
    auth_enabled: false
    chunk_store_config:
      max_look_back_period: 0s
    common:
      compactor_address: http://my-release-loki-compactor:3100
    compactor:
      shared_store: filesystem
      working_directory: /var/loki/compactor
    distributor:
      ring:
        kvstore:
          store: memberlist
    frontend:
      compress_responses: true
      log_queries_longer_than: 5s
      tail_proxy_url: http://my-release-loki-querier:3100
    frontend_worker:
      frontend_address: my-release-loki-query-frontend-headless:9095
    ingester:
      chunk_block_size: 262144
      chunk_encoding: snappy
      chunk_idle_period: 30m
      chunk_retain_period: 1m
      lifecycler:
        ring:
          kvstore:
            store: memberlist
          replication_factor: 1
      max_transfer_retries: 0
      wal:
        dir: /var/loki/wal
    ingester_client:
      grpc_client_config:
        grpc_compression: gzip
    limits_config:
      enforce_metric_name: false
      max_cache_freshness_per_query: 10m
      reject_old_samples: true
      reject_old_samples_max_age: 168h
      split_queries_by_interval: 15m
    memberlist:
      join_members:
      - my-release-loki-memberlist
    query_range:
      align_queries_with_step: true
      cache_results: true
      max_retries: 5
      results_cache:
        cache:
          embedded_cache:
            enabled: true
            ttl: 24h
    ruler:
      alertmanager_url: https://alertmanager.xx
      external_url: https://alertmanager.xx
      ring:
        kvstore:
          store: memberlist
      rule_path: /tmp/loki/scratch
      storage:
        local:
          directory: /etc/loki/rules
        type: local
    runtime_config:
      file: /var/loki-runtime/runtime.yaml
    schema_config:
      configs:
      - from: "2020-09-07"
        index:
          period: 24h
          prefix: loki_index_
        object_store: filesystem
        schema: v11
        store: boltdb-shipper
    server:
      http_listen_port: 3100
    storage_config:
      boltdb_shipper:
        active_index_directory: /var/loki/index
        cache_location: /var/loki/cache
        cache_ttl: 168h
        shared_store: filesystem
      filesystem:
        directory: /var/loki/chunks
    table_manager:
      retention_deletes_enabled: false
      retention_period: 0s
---
# Source: lgtm-distributed/charts/loki/templates/gateway/configmap-gateway.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-loki-gateway
  labels:
    helm.sh/chart: loki-0.79.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "2.9.6"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: gateway
data:
  nginx.conf: |
    worker_processes  5;  ## Default: 1
    error_log  /dev/stderr;
    pid        /tmp/nginx.pid;
    worker_rlimit_nofile 8192;
    
    events {
      worker_connections  4096;  ## Default: 1024
    }
    
    http {
      client_body_temp_path /tmp/client_temp;
      proxy_temp_path       /tmp/proxy_temp_path;
      fastcgi_temp_path     /tmp/fastcgi_temp;
      uwsgi_temp_path       /tmp/uwsgi_temp;
      scgi_temp_path        /tmp/scgi_temp;
    
      proxy_http_version    1.1;
    
      default_type application/octet-stream;
      log_format   main '$remote_addr - $remote_user [$time_local]  $status '
            '"$request" $body_bytes_sent "$http_referer" '
            '"$http_user_agent" "$http_x_forwarded_for"';
      access_log   /dev/stderr  main;
    
      sendfile     on;
      tcp_nopush   on;
      resolver kube-dns.kube-system.svc.cluster.local;
    
      server {
        listen             8080;
    
        location = / {
          return 200 'OK';
          auth_basic off;
          access_log off;
        }
    
        location = /api/prom/push {
          set $api_prom_push_backend http://my-release-loki-distributor.lgtm-distributed-2.0.0.tgz.svc.cluster.local;
          proxy_pass       $api_prom_push_backend:3100$request_uri;
          proxy_http_version 1.1;
        }
    
        location = /api/prom/tail {
          set $api_prom_tail_backend http://my-release-loki-querier.lgtm-distributed-2.0.0.tgz.svc.cluster.local;
          proxy_pass       $api_prom_tail_backend:3100$request_uri;
          proxy_set_header Upgrade $http_upgrade;
          proxy_set_header Connection "upgrade";
          proxy_http_version 1.1;
        }
    
        # Ruler
        location ~ /prometheus/api/v1/alerts.* {
          proxy_pass       http://my-release-loki-ruler.lgtm-distributed-2.0.0.tgz.svc.cluster.local:3100$request_uri;
        }
        location ~ /prometheus/api/v1/rules.* {
          proxy_pass       http://my-release-loki-ruler.lgtm-distributed-2.0.0.tgz.svc.cluster.local:3100$request_uri;
        }
        location ~ /api/prom/rules.* {
          proxy_pass       http://my-release-loki-ruler.lgtm-distributed-2.0.0.tgz.svc.cluster.local:3100$request_uri;
        }
        location ~ /api/prom/alerts.* {
          proxy_pass       http://my-release-loki-ruler.lgtm-distributed-2.0.0.tgz.svc.cluster.local:3100$request_uri;
        }
    
        location ~ /api/prom/.* {
          set $api_prom_backend http://my-release-loki-query-frontend-headless.lgtm-distributed-2.0.0.tgz.svc.cluster.local;
          proxy_pass       $api_prom_backend:3100$request_uri;
          proxy_http_version 1.1;
        }
    
        location = /loki/api/v1/push {
          set $loki_api_v1_push_backend http://my-release-loki-distributor.lgtm-distributed-2.0.0.tgz.svc.cluster.local;
          proxy_pass       $loki_api_v1_push_backend:3100$request_uri;
          proxy_http_version 1.1;
        }
    
        location = /loki/api/v1/tail {
          set $loki_api_v1_tail_backend http://my-release-loki-querier.lgtm-distributed-2.0.0.tgz.svc.cluster.local;
          proxy_pass       $loki_api_v1_tail_backend:3100$request_uri;
          proxy_set_header Upgrade $http_upgrade;
          proxy_set_header Connection "upgrade";
          proxy_http_version 1.1;
        }
    
        location ~ /loki/api/.* {
          set $loki_api_backend http://my-release-loki-query-frontend-headless.lgtm-distributed-2.0.0.tgz.svc.cluster.local;
          proxy_pass       $loki_api_backend:3100$request_uri;
          proxy_http_version 1.1;
        }
      }
    }
---
# Source: lgtm-distributed/charts/loki/templates/runtime-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-loki-runtime
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: loki-0.79.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "2.9.6"
    app.kubernetes.io/managed-by: Helm
data:
  runtime.yaml: |
    
    {}
---
# Source: lgtm-distributed/charts/mimir/charts/minio/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-minio
  labels:
    app: minio
    chart: minio-5.0.14
    release: my-release
    heritage: Helm
data:
  initialize: |-
    #!/bin/sh
    set -e ; # Have script exit in the event of a failed command.
    MC_CONFIG_DIR="/tmp/minio/mc/"
    MC="/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}"
    
    # connectToMinio
    # Use a check-sleep-check loop to wait for MinIO service to be available
    connectToMinio() {
      SCHEME=$1
      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
      set -e ; # fail if we can't read the keys.
      ACCESS=$(cat /config/rootUser) ; SECRET=$(cat /config/rootPassword) ;
      set +e ; # The connections to minio are allowed to fail.
      echo "Connecting to MinIO server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
      MC_COMMAND="${MC} alias set myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
      $MC_COMMAND ;
      STATUS=$? ;
      until [ $STATUS = 0 ]
      do
        ATTEMPTS=`expr $ATTEMPTS + 1` ;
        echo \"Failed attempts: $ATTEMPTS\" ;
        if [ $ATTEMPTS -gt $LIMIT ]; then
          exit 1 ;
        fi ;
        sleep 2 ; # 1 second intervals between attempts
        $MC_COMMAND ;
        STATUS=$? ;
      done ;
      set -e ; # reset `e` as active
      return 0
    }
    
    # checkBucketExists ($bucket)
    # Check if the bucket exists, by using the exit code of `mc ls`
    checkBucketExists() {
      BUCKET=$1
      CMD=$(${MC} stat myminio/$BUCKET > /dev/null 2>&1)
      return $?
    }
    
    # createBucket ($bucket, $policy, $purge)
    # Ensure bucket exists, purging if asked to
    createBucket() {
      BUCKET=$1
      POLICY=$2
      PURGE=$3
      VERSIONING=$4
      OBJECTLOCKING=$5
    
      # Purge the bucket, if set & exists
      # Since PURGE is user input, check explicitly for `true`
      if [ $PURGE = true ]; then
        if checkBucketExists $BUCKET ; then
          echo "Purging bucket '$BUCKET'."
          set +e ; # don't exit if this fails
          ${MC} rm -r --force myminio/$BUCKET
          set -e ; # reset `e` as active
        else
          echo "Bucket '$BUCKET' does not exist, skipping purge."
        fi
      fi
    
    # Create the bucket if it does not exist and set objectlocking if enabled (NOTE: versioning will be not changed if OBJECTLOCKING is set because it enables versioning to the Buckets created)
    if ! checkBucketExists $BUCKET ; then
        if [ ! -z $OBJECTLOCKING ] ; then
          if [ $OBJECTLOCKING = true ] ; then
              echo "Creating bucket with OBJECTLOCKING '$BUCKET'"
              ${MC} mb --with-lock myminio/$BUCKET
          elif [ $OBJECTLOCKING = false ] ; then
                echo "Creating bucket '$BUCKET'"
                ${MC} mb myminio/$BUCKET
          fi
      elif [ -z $OBJECTLOCKING ] ; then
            echo "Creating bucket '$BUCKET'"
            ${MC} mb myminio/$BUCKET
      else
        echo "Bucket '$BUCKET' already exists."  
      fi
      fi
    
    
      # set versioning for bucket if objectlocking is disabled or not set
      if [ $OBJECTLOCKING = false ] ; then
      if [ ! -z $VERSIONING ] ; then
        if [ $VERSIONING = true ] ; then
            echo "Enabling versioning for '$BUCKET'"
            ${MC} version enable myminio/$BUCKET
        elif [ $VERSIONING = false ] ; then
            echo "Suspending versioning for '$BUCKET'"
            ${MC} version suspend myminio/$BUCKET
        fi
        fi
      else
          echo "Bucket '$BUCKET' versioning unchanged."
      fi
    
    
      # At this point, the bucket should exist, skip checking for existence
      # Set policy on the bucket
      echo "Setting policy of bucket '$BUCKET' to '$POLICY'."
      ${MC} anonymous set $POLICY myminio/$BUCKET
    }
    
    # Try connecting to MinIO instance
    scheme=http
    connectToMinio $scheme
    
    
    
    # Create the buckets
    createBucket mimir-tsdb "none" false false false
    createBucket mimir-ruler "none" false false false
    createBucket enterprise-metrics-tsdb "none" false false false
    createBucket enterprise-metrics-admin "none" false false false
    createBucket enterprise-metrics-ruler "none" false false false
    
  add-user: |-
    #!/bin/sh
    set -e ; # Have script exit in the event of a failed command.
    MC_CONFIG_DIR="/tmp/minio/mc/"
    MC="/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}"
    
    # AccessKey and secretkey credentials file are added to prevent shell execution errors caused by special characters.
    # Special characters for example : ',",<,>,{,}
    MINIO_ACCESSKEY_SECRETKEY_TMP="/tmp/accessKey_and_secretKey_tmp"
    
    # connectToMinio
    # Use a check-sleep-check loop to wait for MinIO service to be available
    connectToMinio() {
      SCHEME=$1
      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
      set -e ; # fail if we can't read the keys.
      ACCESS=$(cat /config/rootUser) ; SECRET=$(cat /config/rootPassword) ;
      set +e ; # The connections to minio are allowed to fail.
      echo "Connecting to MinIO server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
      MC_COMMAND="${MC} alias set myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
      $MC_COMMAND ;
      STATUS=$? ;
      until [ $STATUS = 0 ]
      do
        ATTEMPTS=`expr $ATTEMPTS + 1` ;
        echo \"Failed attempts: $ATTEMPTS\" ;
        if [ $ATTEMPTS -gt $LIMIT ]; then
          exit 1 ;
        fi ;
        sleep 2 ; # 1 second intervals between attempts
        $MC_COMMAND ;
        STATUS=$? ;
      done ;
      set -e ; # reset `e` as active
      return 0
    }
    
    # checkUserExists ()
    # Check if the user exists, by using the exit code of `mc admin user info`
    checkUserExists() {
      CMD=$(${MC} admin user info myminio $(head -1 $MINIO_ACCESSKEY_SECRETKEY_TMP) > /dev/null 2>&1)
      return $?
    }
    
    # createUser ($policy)
    createUser() {
      POLICY=$1
      #check accessKey_and_secretKey_tmp file
      if [[ ! -f $MINIO_ACCESSKEY_SECRETKEY_TMP ]];then
        echo "credentials file does not exist"
        return 1
      fi
      if [[ $(cat $MINIO_ACCESSKEY_SECRETKEY_TMP|wc -l) -ne 2 ]];then
        echo "credentials file is invalid"
        rm -f $MINIO_ACCESSKEY_SECRETKEY_TMP
        return 1
      fi
      USER=$(head -1 $MINIO_ACCESSKEY_SECRETKEY_TMP)
      # Create the user if it does not exist
      if ! checkUserExists ; then
        echo "Creating user '$USER'"
        cat $MINIO_ACCESSKEY_SECRETKEY_TMP | ${MC} admin user add myminio
      else
        echo "User '$USER' already exists."
      fi
      #clean up credentials files.
      rm -f $MINIO_ACCESSKEY_SECRETKEY_TMP
    
      # set policy for user
      if [ ! -z $POLICY -a $POLICY != " " ] ; then
          echo "Adding policy '$POLICY' for '$USER'"
          set +e ; # policy already attach errors out, allow it.
          ${MC} admin policy attach myminio $POLICY --user=$USER
          set -e
      else
          echo "User '$USER' has no policy attached."
      fi
    }
    
    # Try connecting to MinIO instance
    scheme=http
    connectToMinio $scheme
    
    
    
    # Create the users
    echo console > $MINIO_ACCESSKEY_SECRETKEY_TMP
    echo console123 >> $MINIO_ACCESSKEY_SECRETKEY_TMP
    createUser consoleAdmin
    
  add-policy: |-
    #!/bin/sh
    set -e ; # Have script exit in the event of a failed command.
    MC_CONFIG_DIR="/tmp/minio/mc/"
    MC="/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}"
    
    # connectToMinio
    # Use a check-sleep-check loop to wait for MinIO service to be available
    connectToMinio() {
      SCHEME=$1
      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
      set -e ; # fail if we can't read the keys.
      ACCESS=$(cat /config/rootUser) ; SECRET=$(cat /config/rootPassword) ;
      set +e ; # The connections to minio are allowed to fail.
      echo "Connecting to MinIO server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
      MC_COMMAND="${MC} alias set myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
      $MC_COMMAND ;
      STATUS=$? ;
      until [ $STATUS = 0 ]
      do
        ATTEMPTS=`expr $ATTEMPTS + 1` ;
        echo \"Failed attempts: $ATTEMPTS\" ;
        if [ $ATTEMPTS -gt $LIMIT ]; then
          exit 1 ;
        fi ;
        sleep 2 ; # 1 second intervals between attempts
        $MC_COMMAND ;
        STATUS=$? ;
      done ;
      set -e ; # reset `e` as active
      return 0
    }
    
    # checkPolicyExists ($policy)
    # Check if the policy exists, by using the exit code of `mc admin policy info`
    checkPolicyExists() {
      POLICY=$1
      CMD=$(${MC} admin policy info myminio $POLICY > /dev/null 2>&1)
      return $?
    }
    
    # createPolicy($name, $filename)
    createPolicy () {
      NAME=$1
      FILENAME=$2
    
      # Create the name if it does not exist
      echo "Checking policy: $NAME (in /config/$FILENAME.json)"
      if ! checkPolicyExists $NAME ; then
        echo "Creating policy '$NAME'"
      else
        echo "Policy '$NAME' already exists."
      fi
      ${MC} admin policy create myminio $NAME /config/$FILENAME.json
    
    }
    
    # Try connecting to MinIO instance
    scheme=http
    connectToMinio $scheme
    
    
    
  add-svcacct: |-
    #!/bin/sh
    set -e ; # Have script exit in the event of a failed command.
    MC_CONFIG_DIR="/tmp/minio/mc/"
    MC="/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}"
    
    # AccessKey and secretkey credentials file are added to prevent shell execution errors caused by special characters.
    # Special characters for example : ',",<,>,{,}
    MINIO_ACCESSKEY_SECRETKEY_TMP="/tmp/accessKey_and_secretKey_svcacct_tmp"
    
    # connectToMinio
    # Use a check-sleep-check loop to wait for MinIO service to be available
    connectToMinio() {
      SCHEME=$1
      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
      set -e ; # fail if we can't read the keys.
      ACCESS=$(cat /config/rootUser) ; SECRET=$(cat /config/rootPassword) ;
      set +e ; # The connections to minio are allowed to fail.
      echo "Connecting to MinIO server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
      MC_COMMAND="${MC} alias set myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
      $MC_COMMAND ;
      STATUS=$? ;
      until [ $STATUS = 0 ]
      do
        ATTEMPTS=`expr $ATTEMPTS + 1` ;
        echo \"Failed attempts: $ATTEMPTS\" ;
        if [ $ATTEMPTS -gt $LIMIT ]; then
          exit 1 ;
        fi ;
        sleep 2 ; # 2 second intervals between attempts
        $MC_COMMAND ;
        STATUS=$? ;
      done ;
      set -e ; # reset `e` as active
      return 0
    }
    
    # checkSvcacctExists ()
    # Check if the svcacct exists, by using the exit code of `mc admin user svcacct info`
    checkSvcacctExists() {
      CMD=$(${MC} admin user svcacct info myminio $(head -1 $MINIO_ACCESSKEY_SECRETKEY_TMP) > /dev/null 2>&1)
      return $?
    }
    
    # createSvcacct ($user)
    createSvcacct () {
      USER=$1
      FILENAME=$2
      #check accessKey_and_secretKey_tmp file
      if [[ ! -f $MINIO_ACCESSKEY_SECRETKEY_TMP ]];then
        echo "credentials file does not exist"
        return 1
      fi
      if [[ $(cat $MINIO_ACCESSKEY_SECRETKEY_TMP|wc -l) -ne 2 ]];then
        echo "credentials file is invalid"
        rm -f $MINIO_ACCESSKEY_SECRETKEY_TMP
        return 1
      fi
      SVCACCT=$(head -1 $MINIO_ACCESSKEY_SECRETKEY_TMP)
      # Create the svcacct if it does not exist
      if ! checkSvcacctExists ; then
        echo "Creating svcacct '$SVCACCT'"
        # Check if policy file is define
        if [ -z $FILENAME ]; then
          ${MC} admin user svcacct add --access-key $(head -1 $MINIO_ACCESSKEY_SECRETKEY_TMP) --secret-key $(tail -n1 $MINIO_ACCESSKEY_SECRETKEY_TMP) myminio $USER
        else
          ${MC} admin user svcacct add --access-key $(head -1 $MINIO_ACCESSKEY_SECRETKEY_TMP) --secret-key $(tail -n1 $MINIO_ACCESSKEY_SECRETKEY_TMP) --policy /config/$FILENAME.json myminio $USER
        fi
      else
        echo "Svcacct '$SVCACCT' already exists."
      fi
      #clean up credentials files.
      rm -f $MINIO_ACCESSKEY_SECRETKEY_TMP
    }
    
    # Try connecting to MinIO instance
    scheme=http
    connectToMinio $scheme
    
    
    
  custom-command: |-
    #!/bin/sh
    set -e ; # Have script exit in the event of a failed command.
    MC_CONFIG_DIR="/tmp/minio/mc/"
    MC="/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}"
    
    # connectToMinio
    # Use a check-sleep-check loop to wait for MinIO service to be available
    connectToMinio() {
      SCHEME=$1
      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
      set -e ; # fail if we can't read the keys.
      ACCESS=$(cat /config/rootUser) ; SECRET=$(cat /config/rootPassword) ;
      set +e ; # The connections to minio are allowed to fail.
      echo "Connecting to MinIO server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
      MC_COMMAND="${MC} alias set myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
      $MC_COMMAND ;
      STATUS=$? ;
      until [ $STATUS = 0 ]
      do
        ATTEMPTS=`expr $ATTEMPTS + 1` ;
        echo \"Failed attempts: $ATTEMPTS\" ;
        if [ $ATTEMPTS -gt $LIMIT ]; then
          exit 1 ;
        fi ;
        sleep 2 ; # 1 second intervals between attempts
        $MC_COMMAND ;
        STATUS=$? ;
      done ;
      set -e ; # reset `e` as active
      return 0
    }
    
    # runCommand ($@)
    # Run custom mc command
    runCommand() {
      ${MC} "$@"
      return $?
    }
    
    # Try connecting to MinIO instance
    scheme=http
    connectToMinio $scheme
---
# Source: lgtm-distributed/charts/mimir/templates/alertmanager/alertmanager-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-mimir-alertmanager-fallback-config
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: alertmanager
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {} 
  namespace: "lgtm-distributed-2.0.0.tgz"
data:
  alertmanager_fallback_config.yaml: |
    receivers:
        - name: default-receiver
    route:
        receiver: default-receiver
---
# Source: lgtm-distributed/charts/mimir/templates/mimir-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-mimir-config
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  namespace: "lgtm-distributed-2.0.0.tgz"
data:
  mimir.yaml: |
    
    activity_tracker:
      filepath: /active-query-tracker/activity.log
    alertmanager:
      data_dir: /data
      enable_api: true
      external_url: /alertmanager
      fallback_config_file: /configs/alertmanager_fallback_config.yaml
    alertmanager_storage:
      backend: s3
      s3:
        access_key_id: grafana-mimir
        bucket_name: mimir-ruler
        endpoint: my-release-minio.lgtm-distributed-2.0.0.tgz.svc:9000
        insecure: true
        secret_access_key: supersecret
    blocks_storage:
      backend: s3
      bucket_store:
        sync_dir: /data/tsdb-sync
      s3:
        access_key_id: grafana-mimir
        bucket_name: mimir-tsdb
        endpoint: my-release-minio.lgtm-distributed-2.0.0.tgz.svc:9000
        insecure: true
        secret_access_key: supersecret
      tsdb:
        dir: /data/tsdb
        head_compaction_interval: 15m
        wal_replay_concurrency: 3
    compactor:
      compaction_interval: 30m
      data_dir: /data
      deletion_delay: 2h
      first_level_compaction_wait_period: 25m
      max_closing_blocks_concurrency: 2
      max_opening_blocks_concurrency: 4
      sharding_ring:
        heartbeat_period: 1m
        heartbeat_timeout: 4m
        wait_stability_min_duration: 1m
      symbols_flushers_concurrency: 4
    distributor:
      ring:
        heartbeat_period: 1m
        heartbeat_timeout: 4m
    frontend:
      parallelize_shardable_queries: true
      scheduler_address: my-release-mimir-query-scheduler-headless.lgtm-distributed-2.0.0.tgz.svc:9095
    frontend_worker:
      grpc_client_config:
        max_send_msg_size: 419430400
      scheduler_address: my-release-mimir-query-scheduler-headless.lgtm-distributed-2.0.0.tgz.svc:9095
    ingester:
      ring:
        final_sleep: 0s
        heartbeat_period: 2m
        heartbeat_timeout: 10m
        num_tokens: 512
        tokens_file_path: /data/tokens
        unregister_on_shutdown: false
    ingester_client:
      grpc_client_config:
        max_recv_msg_size: 104857600
        max_send_msg_size: 104857600
    limits:
      max_cache_freshness: 10m
      max_query_parallelism: 240
      max_total_query_length: 12000h
    memberlist:
      abort_if_cluster_join_fails: false
      compression_enabled: false
      join_members:
      - dns+my-release-mimir-gossip-ring.lgtm-distributed-2.0.0.tgz.svc.cluster.local.:7946
    querier:
      max_concurrent: 16
    query_scheduler:
      max_outstanding_requests_per_tenant: 800
    ruler:
      alertmanager_url: dnssrvnoa+http://_http-metrics._tcp.my-release-mimir-alertmanager-headless.lgtm-distributed-2.0.0.tgz.svc.cluster.local./alertmanager
      enable_api: true
      rule_path: /data
    ruler_storage:
      backend: s3
      s3:
        access_key_id: grafana-mimir
        bucket_name: mimir-ruler
        endpoint: my-release-minio.lgtm-distributed-2.0.0.tgz.svc:9000
        insecure: true
        secret_access_key: supersecret
    runtime_config:
      file: /var/mimir/runtime.yaml
    store_gateway:
      sharding_ring:
        heartbeat_period: 1m
        heartbeat_timeout: 4m
        tokens_file_path: /data/tokens
        unregister_on_shutdown: false
        wait_stability_min_duration: 1m
    usage_stats:
      installation_mode: helm
---
# Source: lgtm-distributed/charts/mimir/templates/nginx/nginx-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-mimir-nginx
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: nginx
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  namespace: "lgtm-distributed-2.0.0.tgz"
data:
  nginx.conf: |
    worker_processes  5;  ## Default: 1
    error_log  /dev/stderr error;
    pid        /tmp/nginx.pid;
    worker_rlimit_nofile 8192;
    
    events {
      worker_connections  4096;  ## Default: 1024
    }
    
    http {
      client_body_temp_path /tmp/client_temp;
      proxy_temp_path       /tmp/proxy_temp_path;
      fastcgi_temp_path     /tmp/fastcgi_temp;
      uwsgi_temp_path       /tmp/uwsgi_temp;
      scgi_temp_path        /tmp/scgi_temp;
    
      default_type application/octet-stream;
      log_format   main '$remote_addr - $remote_user [$time_local]  $status '
            '"$request" $body_bytes_sent "$http_referer" '
            '"$http_user_agent" "$http_x_forwarded_for"';
      access_log   /dev/stderr  main;
    
      sendfile           on;
      tcp_nopush         on;
      proxy_http_version 1.1;
      resolver kube-dns.kube-system.svc.cluster.local.;
    
      # Ensure that X-Scope-OrgID is always present, default to the no_auth_tenant for backwards compatibility when multi-tenancy was turned off.
      map $http_x_scope_orgid $ensured_x_scope_orgid {
        default $http_x_scope_orgid;
        "" "anonymous";
      }
    
      map $http_x_scope_orgid $has_multiple_orgid_headers {
        default 0;
        "~^.+,.+$" 1;
      }
    
      proxy_read_timeout 300;
      server {
        listen 8080;
        listen [::]:8080;
    
        if ($has_multiple_orgid_headers = 1) {
            return 400 'Sending multiple X-Scope-OrgID headers is not allowed. Use a single header with | as separator instead.';
        }
    
        location = / {
          return 200 'OK';
          auth_basic off;
        }
    
        proxy_set_header X-Scope-OrgID $ensured_x_scope_orgid;
    
        # Distributor endpoints
        location /distributor {
          set $distributor my-release-mimir-distributor-headless.lgtm-distributed-2.0.0.tgz.svc.cluster.local.;
          proxy_pass      http://$distributor:8080$request_uri;
        }
        location = /api/v1/push {
          set $distributor my-release-mimir-distributor-headless.lgtm-distributed-2.0.0.tgz.svc.cluster.local.;
          proxy_pass      http://$distributor:8080$request_uri;
        }
        location /otlp/v1/metrics {
          set $distributor my-release-mimir-distributor-headless.lgtm-distributed-2.0.0.tgz.svc.cluster.local.;
          proxy_pass      http://$distributor:8080$request_uri;
        }
    
        # Alertmanager endpoints
        location /alertmanager {
          set $alertmanager my-release-mimir-alertmanager-headless.lgtm-distributed-2.0.0.tgz.svc.cluster.local.;
          proxy_pass      http://$alertmanager:8080$request_uri;
        }
        location = /multitenant_alertmanager/status {
          set $alertmanager my-release-mimir-alertmanager-headless.lgtm-distributed-2.0.0.tgz.svc.cluster.local.;
          proxy_pass      http://$alertmanager:8080$request_uri;
        }
        location = /api/v1/alerts {
          set $alertmanager my-release-mimir-alertmanager-headless.lgtm-distributed-2.0.0.tgz.svc.cluster.local.;
          proxy_pass      http://$alertmanager:8080$request_uri;
        }
    
        # Ruler endpoints
        location /prometheus/config/v1/rules {
          set $ruler my-release-mimir-ruler.lgtm-distributed-2.0.0.tgz.svc.cluster.local.;
          proxy_pass      http://$ruler:8080$request_uri;
        }
        location /prometheus/api/v1/rules {
          set $ruler my-release-mimir-ruler.lgtm-distributed-2.0.0.tgz.svc.cluster.local.;
          proxy_pass      http://$ruler:8080$request_uri;
        }
    
        location /prometheus/api/v1/alerts {
          set $ruler my-release-mimir-ruler.lgtm-distributed-2.0.0.tgz.svc.cluster.local.;
          proxy_pass      http://$ruler:8080$request_uri;
        }
        location = /ruler/ring {
          set $ruler my-release-mimir-ruler.lgtm-distributed-2.0.0.tgz.svc.cluster.local.;
          proxy_pass      http://$ruler:8080$request_uri;
        }
    
        # Rest of /prometheus goes to the query frontend
        location /prometheus {
          set $query_frontend my-release-mimir-query-frontend.lgtm-distributed-2.0.0.tgz.svc.cluster.local.;
          proxy_pass      http://$query_frontend:8080$request_uri;
        }
    
        # Buildinfo endpoint can go to any component
        location = /api/v1/status/buildinfo {
          set $query_frontend my-release-mimir-query-frontend.lgtm-distributed-2.0.0.tgz.svc.cluster.local.;
          proxy_pass      http://$query_frontend:8080$request_uri;
        }
    
        # Compactor endpoint for uploading blocks
        location /api/v1/upload/block/ {
          set $compactor my-release-mimir-compactor.lgtm-distributed-2.0.0.tgz.svc.cluster.local.;
          proxy_pass      http://$compactor:8080$request_uri;
        }
      }
    }
---
# Source: lgtm-distributed/charts/mimir/templates/runtime-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-mimir-runtime
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  namespace: "lgtm-distributed-2.0.0.tgz"
data:
  runtime.yaml: |
    
    {}
---
# Source: lgtm-distributed/charts/tempo/templates/configmap-runtime.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-tempo-runtime
  labels:
    helm.sh/chart: tempo-1.10.0
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "2.5.0"
    app.kubernetes.io/managed-by: Helm
  namespace: "lgtm-distributed-2.0.0.tgz"
data:
  overrides.yaml: |
    
    overrides: {}
---
# Source: lgtm-distributed/charts/tempo/templates/configmap-tempo.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-tempo-config
  labels:
    helm.sh/chart: tempo-1.10.0
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "2.5.0"
    app.kubernetes.io/managed-by: Helm
  namespace: "lgtm-distributed-2.0.0.tgz"
data:
  tempo-query.yaml: |
    backend: 127.0.0.1:3100
    
  tempo.yaml: |
    
    cache:
      caches:
      - memcached:
          consistent_hash: true
          host: 'my-release-tempo-memcached'
          service: memcached-client
          timeout: 500ms
        roles:
        - parquet-footer
        - bloom
        - frontend-search
    compactor:
      compaction:
        block_retention: 48h
        compacted_block_retention: 1h
        compaction_cycle: 30s
        compaction_window: 1h
        max_block_bytes: 107374182400
        max_compaction_objects: 6000000
        max_time_per_tenant: 5m
        retention_concurrency: 10
        v2_in_buffer_bytes: 5242880
        v2_out_buffer_bytes: 20971520
        v2_prefetch_traces_count: 1000
      ring:
        kvstore:
          store: memberlist
    distributor:
      receivers: null
      ring:
        kvstore:
          store: memberlist
    ingester:
      lifecycler:
        ring:
          kvstore:
            store: memberlist
          replication_factor: 3
        tokens_file_path: /var/tempo/tokens.json
    memberlist:
      abort_if_cluster_join_fails: false
      bind_addr: []
      bind_port: 7946
      gossip_interval: 1s
      gossip_nodes: 2
      gossip_to_dead_nodes_time: 30s
      join_members:
      - dns+my-release-tempo-gossip-ring:7946
      leave_timeout: 5s
      left_ingesters_timeout: 5m
      max_join_backoff: 1m
      max_join_retries: 10
      min_join_backoff: 1s
      node_name: ""
      packet_dial_timeout: 5s
      packet_write_timeout: 5s
      pull_push_interval: 30s
      randomize_node_name: true
      rejoin_interval: 0s
      retransmit_factor: 2
      stream_timeout: 10s
    multitenancy_enabled: false
    overrides:
      per_tenant_override_config: /runtime-config/overrides.yaml
    querier:
      frontend_worker:
        frontend_address: my-release-tempo-query-frontend-discovery:9095
      max_concurrent_queries: 20
      search:
        external_backend: null
        external_endpoints: []
        external_hedge_requests_at: 8s
        external_hedge_requests_up_to: 2
        prefer_self: 10
        query_timeout: 30s
      trace_by_id:
        query_timeout: 10s
    query_frontend:
      max_outstanding_per_tenant: 2000
      max_retries: 2
      search:
        concurrent_jobs: 1000
        target_bytes_per_job: 104857600
      trace_by_id:
        query_shards: 50
    server:
      grpc_server_max_recv_msg_size: 4194304
      grpc_server_max_send_msg_size: 4194304
      http_listen_port: 3100
      http_server_read_timeout: 30s
      http_server_write_timeout: 30s
      log_format: logfmt
      log_level: info
    storage:
      trace:
        backend: local
        blocklist_poll: 5m
        local:
          path: /var/tempo/traces
        pool:
          max_workers: 400
          queue_depth: 20000
        wal:
          path: /var/tempo/wal
    usage_report:
      reporting_enabled: true
---
# Source: lgtm-distributed/charts/mimir/charts/minio/templates/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-release-minio
  labels:
    app: minio
    chart: minio-5.0.14
    release: my-release
    heritage: Helm
spec:
  accessModes:
    - "ReadWriteOnce"
  resources:
    requests:
      storage: "5Gi"
---
# Source: lgtm-distributed/charts/grafana/templates/clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels:
    helm.sh/chart: grafana-7.3.12
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "10.4.3"
    app.kubernetes.io/managed-by: Helm
  name: my-release-grafana-clusterrole
rules: []
---
# Source: lgtm-distributed/charts/grafana/templates/clusterrolebinding.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: my-release-grafana-clusterrolebinding
  labels:
    helm.sh/chart: grafana-7.3.12
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "10.4.3"
    app.kubernetes.io/managed-by: Helm
subjects:
  - kind: ServiceAccount
    name: my-release-grafana
    namespace: lgtm-distributed-2.0.0.tgz
roleRef:
  kind: ClusterRole
  name: my-release-grafana-clusterrole
  apiGroup: rbac.authorization.k8s.io
---
# Source: lgtm-distributed/charts/grafana/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: my-release-grafana
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: grafana-7.3.12
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "10.4.3"
    app.kubernetes.io/managed-by: Helm
rules: []
---
# Source: lgtm-distributed/charts/mimir/charts/rollout_operator/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: my-release-rollout-operator
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - list
  - get
  - watch
  - delete
- apiGroups:
  - apps
  resources:
  - statefulsets
  verbs:
  - list
  - get
  - watch
- apiGroups:
  - apps
  resources:
  - statefulsets/status
  verbs:
  - update
---
# Source: lgtm-distributed/charts/grafana/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: my-release-grafana
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: grafana-7.3.12
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "10.4.3"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: my-release-grafana
subjects:
- kind: ServiceAccount
  name: my-release-grafana
  namespace: lgtm-distributed-2.0.0.tgz
---
# Source: lgtm-distributed/charts/mimir/charts/rollout_operator/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: my-release-rollout-operator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: my-release-rollout-operator
subjects:
- kind: ServiceAccount
  name: my-release-rollout-operator
---
# Source: lgtm-distributed/charts/grafana/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-grafana
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: grafana-7.3.12
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "10.4.3"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: service
      port: 80
      protocol: TCP
      targetPort: 3000
  selector:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: my-release
---
# Source: lgtm-distributed/charts/loki/templates/distributor/service-distributor.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-loki-distributor
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: loki-0.79.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "2.9.6"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: distributor
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 3100
      targetPort: http
      protocol: TCP
    - name: grpc
      port: 9095
      targetPort: grpc
      protocol: TCP
  selector:
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: distributor
---
# Source: lgtm-distributed/charts/loki/templates/gateway/service-gateway.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-loki-gateway
  labels:
    helm.sh/chart: loki-0.79.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "2.9.6"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: gateway
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 80
      targetPort: http
      protocol: TCP
  selector:
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: gateway
---
# Source: lgtm-distributed/charts/loki/templates/ingester/service-ingester-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-loki-ingester-headless
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: ingester
    prometheus.io/service-monitor: "false"
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: http
      port: 3100
      targetPort: http
      protocol: TCP
    - name: grpc
      port: 9095
      targetPort: grpc
      protocol: TCP
  selector:
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: ingester
---
# Source: lgtm-distributed/charts/loki/templates/ingester/service-ingester.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-loki-ingester
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: loki-0.79.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "2.9.6"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: ingester
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 3100
      targetPort: http
      protocol: TCP
    - name: grpc
      port: 9095
      targetPort: grpc
      protocol: TCP
  selector:
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: ingester
---
# Source: lgtm-distributed/charts/loki/templates/querier/service-querier-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-loki-querier-headless
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: querier
    prometheus.io/service-monitor: "false"
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: http
      port: 3100
      targetPort: http
      protocol: TCP
    - name: grpc
      port: 9095
      targetPort: grpc
      protocol: TCP
  selector:
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: querier
---
# Source: lgtm-distributed/charts/loki/templates/querier/service-querier.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-loki-querier
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: loki-0.79.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "2.9.6"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: querier
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 3100
      targetPort: http
      protocol: TCP
    - name: grpc
      port: 9095
      targetPort: grpc
      protocol: TCP
  selector:
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: querier
---
# Source: lgtm-distributed/charts/loki/templates/query-frontend/service-query-frontend-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-loki-query-frontend-headless
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: loki-0.79.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "2.9.6"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: query-frontend
    prometheus.io/service-monitor: "false"
spec:
  clusterIP: None
  type: ClusterIP
  publishNotReadyAddresses: true
  ports:
    - name: http
      port: 3100
      targetPort: http
      protocol: TCP
    - name: grpc
      port: 9095
      targetPort: grpc
      protocol: TCP
    - name: grpclb
      port: 9096
      targetPort: grpc
      protocol: TCP
  selector:
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: query-frontend
---
# Source: lgtm-distributed/charts/loki/templates/query-frontend/service-query-frontend.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-loki-query-frontend
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: loki-0.79.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "2.9.6"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: query-frontend
spec:
  type: ClusterIP
  publishNotReadyAddresses: true
  ports:
    - name: http
      port: 3100
      targetPort: http
      protocol: TCP
    - name: grpc
      port: 9095
      targetPort: grpc
      protocol: TCP
    - name: grpclb
      port: 9096
      targetPort: grpc
      protocol: TCP
  selector:
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: query-frontend
---
# Source: lgtm-distributed/charts/loki/templates/service-memberlist.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-loki-memberlist
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: loki-0.79.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "2.9.6"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp
      port: 7946
      targetPort: http-memberlist
      protocol: TCP
  selector:
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/part-of: memberlist
---
# Source: lgtm-distributed/charts/mimir/charts/minio/templates/console-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-minio-console
  labels:
    app: minio
    chart: minio-5.0.14
    release: my-release
    heritage: Helm
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 9001
      protocol: TCP
      targetPort: 9001
  selector:
    app: minio
    release: my-release
---
# Source: lgtm-distributed/charts/mimir/charts/minio/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-minio
  labels:
    app: minio
    chart: minio-5.0.14
    release: my-release
    heritage: Helm
    monitoring: "true"
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 9000
      protocol: TCP
      targetPort: 9000
  selector:
    app: minio
    release: my-release
---
# Source: lgtm-distributed/charts/mimir/templates/alertmanager/alertmanager-svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-mimir-alertmanager-headless
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: alertmanager
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
    prometheus.io/service-monitor: "false"
  annotations:
    {}
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - port: 8080
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
    - port: 9095
      protocol: TCP
      name: grpc
      targetPort: grpc
    - port: 9094
      protocol: TCP
      name: cluster
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: alertmanager
---
# Source: lgtm-distributed/charts/mimir/templates/alertmanager/alertmanager-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-mimir-alertmanager
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: alertmanager
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  type: ClusterIP
  ports:
    - port: 8080
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
    - port: 9095
      protocol: TCP
      name: grpc
      targetPort: grpc
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: alertmanager
---
# Source: lgtm-distributed/charts/mimir/templates/compactor/compactor-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-mimir-compactor
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: compactor
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  type: ClusterIP
  ports:
    - port: 8080
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
    - port: 9095
      protocol: TCP
      name: grpc
      targetPort: grpc
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: compactor
---
# Source: lgtm-distributed/charts/mimir/templates/distributor/distributor-svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-mimir-distributor-headless
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: distributor
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
    prometheus.io/service-monitor: "false"
  annotations:
    {}
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - port: 8080
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
    - port: 9095
      protocol: TCP
      name: grpc
      targetPort: grpc
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: distributor
---
# Source: lgtm-distributed/charts/mimir/templates/distributor/distributor-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-mimir-distributor
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: distributor
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  type: ClusterIP
  ports:
    - port: 8080
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
    - port: 9095
      protocol: TCP
      name: grpc
      targetPort: grpc
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: distributor
---
# Source: lgtm-distributed/charts/mimir/templates/gossip-ring/gossip-ring-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-mimir-gossip-ring
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: gossip-ring
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: gossip-ring
      port: 7946
      appProtocol: tcp
      protocol: TCP
      targetPort: 7946
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/part-of: memberlist
---
# Source: lgtm-distributed/charts/mimir/templates/ingester/ingester-svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-mimir-ingester-headless
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: ingester
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
    prometheus.io/service-monitor: "false"
  annotations:
    {}
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - port: 8080
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
    - port: 9095
      protocol: TCP
      name: grpc
      targetPort: grpc
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: ingester
---
# Source: lgtm-distributed/charts/mimir/templates/ingester/ingester-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-mimir-ingester
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: ingester
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  type: ClusterIP
  ports:
    - port: 8080
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
    - port: 9095
      protocol: TCP
      name: grpc
      targetPort: grpc
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: ingester
---
# Source: lgtm-distributed/charts/mimir/templates/nginx/nginx-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-mimir-nginx
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: nginx
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  type: ClusterIP
  ports:
    - name: http-metric
      port: 80
      targetPort: http-metric
      protocol: TCP
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: nginx
---
# Source: lgtm-distributed/charts/mimir/templates/overrides-exporter/overrides-exporter-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-mimir-overrides-exporter
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: overrides-exporter
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  type: ClusterIP
  ports:
    - port: 8080
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
    - port: 9095
      protocol: TCP
      name: grpc
      targetPort: grpc
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: overrides-exporter
---
# Source: lgtm-distributed/charts/mimir/templates/querier/querier-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-mimir-querier
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: querier
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  type: ClusterIP
  ports:
    - port: 8080
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
    - port: 9095
      protocol: TCP
      name: grpc
      targetPort: grpc
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: querier
---
# Source: lgtm-distributed/charts/mimir/templates/query-frontend/query-frontend-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-mimir-query-frontend
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: query-frontend
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  type: ClusterIP
  ports:
    - port: 8080
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
    - port: 9095
      protocol: TCP
      name: grpc
      targetPort: grpc
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: query-frontend
---
# Source: lgtm-distributed/charts/mimir/templates/query-scheduler/query-scheduler-svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-mimir-query-scheduler-headless
  namespace: "lgtm-distributed-2.0.0.tgz"
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: query-scheduler
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
    prometheus.io/service-monitor: "false"
  annotations:
    {}
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - port: 8080
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
    - port: 9095
      protocol: TCP
      name: grpc
      targetPort: grpc
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: query-scheduler
---
# Source: lgtm-distributed/charts/mimir/templates/query-scheduler/query-scheduler-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-mimir-query-scheduler
  namespace: "lgtm-distributed-2.0.0.tgz"
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: query-scheduler
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
spec:
  type: ClusterIP
  ports:
    - port: 8080
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
    - port: 9095
      protocol: TCP
      name: grpc
      targetPort: grpc
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: query-scheduler
---
# Source: lgtm-distributed/charts/mimir/templates/ruler/ruler-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-mimir-ruler
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: ruler
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  type: ClusterIP
  ports:
    - port: 8080
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: ruler
---
# Source: lgtm-distributed/charts/mimir/templates/store-gateway/store-gateway-svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-mimir-store-gateway-headless
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: store-gateway
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
    prometheus.io/service-monitor: "false"
  annotations:
    {}
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - port: 8080
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
    - port: 9095
      protocol: TCP
      name: grpc
      targetPort: grpc
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: store-gateway
---
# Source: lgtm-distributed/charts/mimir/templates/store-gateway/store-gateway-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-mimir-store-gateway
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: store-gateway
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  type: ClusterIP
  ports:
    - port: 8080
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
    - port: 9095
      protocol: TCP
      name: grpc
      targetPort: grpc
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: store-gateway
---
# Source: lgtm-distributed/charts/tempo/templates/compactor/service-compactor.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-tempo-compactor
  labels:
    helm.sh/chart: tempo-1.10.0
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: compactor
    app.kubernetes.io/version: "2.5.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: http-metrics
      port: 3100
      targetPort: 3100
      protocol: TCP
  selector:
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: compactor
---
# Source: lgtm-distributed/charts/tempo/templates/distributor/service-distributor-discovery.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-tempo-distributor-discovery
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: tempo-1.10.0
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: distributor
    app.kubernetes.io/version: "2.5.0"
    app.kubernetes.io/managed-by: Helm
    prometheus.io/service-monitor: "false"
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: http-metrics
      port: 3100
      targetPort: http-metrics
  selector:
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: distributor
---
# Source: lgtm-distributed/charts/tempo/templates/distributor/service-distributor.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-tempo-distributor
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: tempo-1.10.0
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: distributor
    app.kubernetes.io/version: "2.5.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: http-metrics
      port: 3100
      targetPort: http-metrics
    - name: grpc
      port: 9095
      protocol: TCP
      targetPort: 9095
  selector:
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: distributor
---
# Source: lgtm-distributed/charts/tempo/templates/gossip-ring/service-gossip-ring.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-tempo-gossip-ring
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: tempo-1.10.0
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: gossip-ring
    app.kubernetes.io/version: "2.5.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: gossip-ring
      port: 7946
      protocol: TCP
      targetPort: http-memberlist
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/part-of: memberlist
---
# Source: lgtm-distributed/charts/tempo/templates/ingester/service-ingester-discovery.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-tempo-ingester-discovery
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: tempo-1.10.0
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: ingester
    app.kubernetes.io/version: "2.5.0"
    app.kubernetes.io/managed-by: Helm
    prometheus.io/service-monitor: "false"
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: http-metrics
      port: 3100
      protocol: TCP
      targetPort: 3100
    - name: grpc
      port: 9095
      protocol: TCP
      targetPort: 9095
  selector:
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: ingester
---
# Source: lgtm-distributed/charts/tempo/templates/ingester/service-ingester.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-tempo-ingester
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: tempo-1.10.0
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: ingester
    app.kubernetes.io/version: "2.5.0"
    app.kubernetes.io/managed-by: Helm
spec:
  ports:
    - name: http-metrics
      port: 3100
      protocol: TCP
      targetPort: 3100
    - name: grpc
      port: 9095
      protocol: TCP
      targetPort: 9095
  selector:
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: ingester
---
# Source: lgtm-distributed/charts/tempo/templates/memcached/service-memcached.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-tempo-memcached
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: tempo-1.10.0
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: memcached
    app.kubernetes.io/version: "2.5.0"
    app.kubernetes.io/managed-by: Helm
spec:
  ports:
  - name: memcached-client
    port: 11211
    targetPort: 11211
  - name: http-metrics
    port: 9150
    targetPort: http-metrics
  selector:
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: memcached
---
# Source: lgtm-distributed/charts/tempo/templates/querier/service-querier.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-tempo-querier
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: tempo-1.10.0
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: querier
    app.kubernetes.io/version: "2.5.0"
    app.kubernetes.io/managed-by: Helm
spec:
  ports:
    - name: http-metrics
      port: 3100
      protocol: TCP
      targetPort: 3100
    - name: grpc
      port: 9095
      protocol: TCP
      targetPort: 9095
  selector:
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: querier
---
# Source: lgtm-distributed/charts/tempo/templates/query-frontend/service-query-frontend-discovery.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-tempo-query-frontend-discovery
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: tempo-1.10.0
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: query-frontend
    app.kubernetes.io/version: "2.5.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: http
      port: 3100
      targetPort: 3100
    - name: grpc
      port: 9095
      protocol: TCP
      targetPort: 9095
    - name: grpclb
      port: 9096
      protocol: TCP
      targetPort: grpc
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: query-frontend
---
# Source: lgtm-distributed/charts/tempo/templates/query-frontend/service-query-frontend.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-tempo-query-frontend
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: tempo-1.10.0
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: query-frontend
    app.kubernetes.io/version: "2.5.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: http-metrics
      port: 3100
      targetPort: 3100
    - name: grpc
      port: 9095
      protocol: TCP
      targetPort: 9095
  selector:
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: query-frontend
---
# Source: lgtm-distributed/charts/grafana/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-grafana
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: grafana-7.3.12
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "10.4.3"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: grafana
      app.kubernetes.io/instance: my-release
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: grafana
        app.kubernetes.io/instance: my-release
      annotations:
        checksum/config: 950a3085e73d37c5eabd13df0944feb78a92c2daf521a572c6879530a3f6cc06
        checksum/sc-dashboard-provider-config: 593c0a8778b83f11fe80ccb21dfb20bc46705e2be3178df1dc4c89d164c8cd9c
        checksum/secret: dbe86923d736bb77c3156a6444b6fe7fc181bb63e4f29a9f9c5da301e3fbd6b9
        kubectl.kubernetes.io/default-container: grafana
    spec:
      
      serviceAccountName: my-release-grafana
      automountServiceAccountToken: true
      securityContext:
        fsGroup: 472
        runAsGroup: 472
        runAsNonRoot: true
        runAsUser: 472
      enableServiceLinks: true
      containers:
        - name: grafana
          image: "docker.io/grafana/grafana:10.4.3"
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - name: config
              mountPath: "/etc/grafana/grafana.ini"
              subPath: grafana.ini
            - name: storage
              mountPath: "/var/lib/grafana"
            - name: config
              mountPath: "/etc/grafana/provisioning/datasources/datasources.yaml"
              subPath: "datasources.yaml"
          ports:
            - name: grafana
              containerPort: 3000
              protocol: TCP
            - name: gossip-tcp
              containerPort: 9094
              protocol: TCP
            - name: gossip-udp
              containerPort: 9094
              protocol: UDP
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: GF_SECURITY_ADMIN_USER
              valueFrom:
                secretKeyRef:
                  name: my-release-grafana
                  key: admin-user
            - name: GF_SECURITY_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-grafana
                  key: admin-password
            - name: GF_PATHS_DATA
              value: /var/lib/grafana/
            - name: GF_PATHS_LOGS
              value: /var/log/grafana
            - name: GF_PATHS_PLUGINS
              value: /var/lib/grafana/plugins
            - name: GF_PATHS_PROVISIONING
              value: /etc/grafana/provisioning
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
            initialDelaySeconds: 60
            timeoutSeconds: 30
          readinessProbe:
            httpGet:
              path: /api/health
              port: 3000
      volumes:
        - name: config
          configMap:
            name: my-release-grafana
        - name: storage
          emptyDir: {}
---
# Source: lgtm-distributed/charts/loki/templates/distributor/deployment-distributor.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-loki-distributor
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: loki-0.79.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "2.9.6"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: distributor
    app.kubernetes.io/part-of: memberlist
spec:
  replicas: 1
  strategy:
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: loki
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: distributor
  template:
    metadata:
      annotations:
        checksum/config: a3ab99f0b70b9af56cbd2e876aac942086dec7a7110a9665b0ff76c81f8209cd
      labels:
        app.kubernetes.io/name: loki
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/component: distributor
        app.kubernetes.io/part-of: memberlist
    spec:
      serviceAccountName: my-release-loki
      
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
      terminationGracePeriodSeconds: 30
      containers:
        - name: distributor
          image: docker.io/grafana/loki:2.9.6
          imagePullPolicy: IfNotPresent
          args:
            - -config.file=/etc/loki/config/config.yaml
            - -target=distributor
          ports:
            - name: http
              containerPort: 3100
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
            - name: http-memberlist
              containerPort: 7946
              protocol: TCP
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          readinessProbe:
            httpGet:
              path: /ready
              port: http
            initialDelaySeconds: 30
            timeoutSeconds: 1
          livenessProbe:
            httpGet:
              path: /ready
              port: http
            initialDelaySeconds: 300
          volumeMounts:
            - name: config
              mountPath: /etc/loki/config
            - name: runtime-config
              mountPath: /var/loki-runtime
          resources:
            {}
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/name: loki
                  app.kubernetes.io/instance: my-release
                  app.kubernetes.io/component: distributor
              topologyKey: kubernetes.io/hostname
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: loki
                    app.kubernetes.io/instance: my-release
                    app.kubernetes.io/component: distributor
                topologyKey: failure-domain.beta.kubernetes.io/zone
        
      volumes:
        - name: config
          configMap:
            name: my-release-loki
        - name: runtime-config
          configMap:
            name: my-release-loki-runtime
---
# Source: lgtm-distributed/charts/loki/templates/gateway/deployment-gateway.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-loki-gateway
  labels:
    helm.sh/chart: loki-0.79.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "2.9.6"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: gateway
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: loki
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: gateway
  template:
    metadata:
      annotations:
        checksum/config: d13b68c28b45037966d2354263072caeca35ab1f245b23f06c9206f8b444e96c
      labels:
        app.kubernetes.io/name: loki
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/component: gateway
    spec:
      serviceAccountName: my-release-loki
      
      securityContext:
        fsGroup: 101
        runAsGroup: 101
        runAsNonRoot: true
        runAsUser: 101
      terminationGracePeriodSeconds: 30
      containers:
        - name: nginx
          image: docker.io/nginxinc/nginx-unprivileged:1.20.2-alpine
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          readinessProbe:
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 15
            timeoutSeconds: 1
          livenessProbe:
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 30
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          volumeMounts:
            - name: config
              mountPath: /etc/nginx
            - name: tmp
              mountPath: /tmp
            - name: docker-entrypoint-d-override
              mountPath: /docker-entrypoint.d
          resources:
            {}
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/name: loki
                  app.kubernetes.io/instance: my-release
                  app.kubernetes.io/component: gateway
              topologyKey: kubernetes.io/hostname
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: loki
                    app.kubernetes.io/instance: my-release
                    app.kubernetes.io/component: gateway
                topologyKey: failure-domain.beta.kubernetes.io/zone
        
      volumes:
        - name: config
          configMap:
            name: my-release-loki-gateway
        - name: tmp
          emptyDir: {}
        - name: docker-entrypoint-d-override
          emptyDir: {}
---
# Source: lgtm-distributed/charts/loki/templates/query-frontend/deployment-query-frontend.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-loki-query-frontend
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: loki-0.79.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "2.9.6"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: query-frontend
spec:
  replicas: 1
  strategy:
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: loki
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: query-frontend
  template:
    metadata:
      annotations:
        checksum/config: a3ab99f0b70b9af56cbd2e876aac942086dec7a7110a9665b0ff76c81f8209cd
      labels:
        app.kubernetes.io/name: loki
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/component: query-frontend
        app.kubernetes.io/part-of: memberlist
    spec:
      serviceAccountName: my-release-loki
      
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
      terminationGracePeriodSeconds: 30
      containers:
        - name: query-frontend
          image: docker.io/grafana/loki:2.9.6
          imagePullPolicy: IfNotPresent
          args:
            - -config.file=/etc/loki/config/config.yaml
            - -target=query-frontend
          ports:
            - name: http
              containerPort: 3100
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
            - name: http-memberlist
              containerPort: 7946
              protocol: TCP
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          readinessProbe:
            httpGet:
              path: /ready
              port: http
            initialDelaySeconds: 30
            timeoutSeconds: 1
          livenessProbe:
            httpGet:
              path: /ready
              port: http
            initialDelaySeconds: 300
          volumeMounts:
            - name: config
              mountPath: /etc/loki/config
            - name: runtime-config
              mountPath: /var/loki-runtime
          resources:
            {}
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/name: loki
                  app.kubernetes.io/instance: my-release
                  app.kubernetes.io/component: query-frontend
              topologyKey: kubernetes.io/hostname
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: loki
                    app.kubernetes.io/instance: my-release
                    app.kubernetes.io/component: query-frontend
                topologyKey: failure-domain.beta.kubernetes.io/zone
        
      volumes:
        - name: config
          configMap:
            name: my-release-loki
        - name: runtime-config
          configMap:
            name: my-release-loki-runtime
---
# Source: lgtm-distributed/charts/mimir/charts/minio/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-minio
  labels:
    app: minio
    chart: minio-5.0.14
    release: my-release
    heritage: Helm
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 100%
      maxUnavailable: 0
  replicas: 1
  selector:
    matchLabels:
      app: minio
      release: my-release
  template:
    metadata:
      name: my-release-minio
      labels:
        app: minio
        release: my-release
      annotations:
        checksum/secrets: 6267a0d4e2c6da5aa19ee7c83305f99fe30532dcc46562512e353766842be80f
        checksum/config: 4ed109c6d9d7b3c580e3aa012a782574ac3ea7501abdfd0a1a6cc10719c201f2
    spec:
      securityContext:
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
        fsGroupChangePolicy: OnRootMismatch
      
      serviceAccountName: minio-sa
      containers:
        - name: minio
          image: "quay.io/minio/minio:RELEASE.2023-09-30T07-02-29Z"
          imagePullPolicy: IfNotPresent
          command:
            - "/bin/sh"
            - "-ce"
            - "/usr/bin/docker-entrypoint.sh minio server /export -S /etc/minio/certs/ --address :9000 --console-address :9001"
          volumeMounts:
            - name: minio-user
              mountPath: "/tmp/credentials"
              readOnly: true
            - name: export
              mountPath: /export            
          ports:
            - name: http
              containerPort: 9000
            - name: http-console
              containerPort: 9001
          env:
            - name: MINIO_ROOT_USER
              valueFrom:
                secretKeyRef:
                  name: my-release-minio
                  key: rootUser
            - name: MINIO_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-minio
                  key: rootPassword
            - name: MINIO_PROMETHEUS_AUTH_TYPE
              value: "public"
          resources:
            requests:
              cpu: 20m
              memory: 128Mi      
      volumes:
        - name: export
          persistentVolumeClaim:
            claimName: my-release-minio
        - name: minio-user
          secret:
            secretName: my-release-minio
---
# Source: lgtm-distributed/charts/mimir/charts/rollout_operator/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-rollout-operator
  labels:
    helm.sh/chart: rollout-operator-0.14.0
    app.kubernetes.io/name: rollout-operator
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "v0.13.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  minReadySeconds: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: rollout-operator
      app.kubernetes.io/instance: my-release
  strategy:
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
  template:
    metadata:
      labels:
        app.kubernetes.io/name: rollout-operator
        app.kubernetes.io/instance: my-release
    spec:
      serviceAccountName: my-release-rollout-operator
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      containers:
        - name: rollout-operator
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          image: "grafana/rollout-operator:v0.13.0"
          imagePullPolicy: IfNotPresent
          args:
          - -kubernetes.namespace=lgtm-distributed-2.0.0.tgz
          ports:
            - name: http-metrics
              containerPort: 8001
              protocol: TCP
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 5
            timeoutSeconds: 1
          resources:
            limits:
              memory: 200Mi
            requests:
              cpu: 20m
              memory: 100Mi
---
# Source: lgtm-distributed/charts/mimir/templates/distributor/distributor-dep.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-mimir-distributor
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: distributor
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  # If replicas is not number (when using values file it's float64, when using --set arg it's int64) and is false (i.e. null) don't set it
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: distributor
  strategy:
    rollingUpdate:
      maxSurge: 15%
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      labels:
        helm.sh/chart: mimir-5.3.0
        app.kubernetes.io/name: mimir
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/version: "2.12.0"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: distributor
        app.kubernetes.io/part-of: memberlist
      annotations:
        checksum/config: 8b3218baa3d6a8d16faf0299cf63569d55f4896d2c3d409aeea7267c729864e4
      namespace: "lgtm-distributed-2.0.0.tgz"
    spec:
      serviceAccountName: my-release-mimir
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      containers:
        - name: distributor
          image: "grafana/mimir:2.12.0"
          imagePullPolicy: IfNotPresent
          args:
            - "-target=distributor"
            - "-config.expand-env=true"
            - "-config.file=/etc/mimir/mimir.yaml"
            # When write requests go through distributors via gRPC, we want gRPC clients to re-resolve the distributors DNS
            # endpoint before the distributor process is terminated, in order to avoid any failures during graceful shutdown.
            # To achieve it, we set a shutdown delay greater than the gRPC max connection age.
            - "-server.grpc.keepalive.max-connection-age=60s"
            - "-server.grpc.keepalive.max-connection-age-grace=5m"
            - "-server.grpc.keepalive.max-connection-idle=1m"
            - "-shutdown-delay=90s"
          volumeMounts:
            - name: config
              mountPath: /etc/mimir
            - name: runtime-config
              mountPath: /var/mimir
            - name: storage
              mountPath: "/data"
              subPath: 
            - name: active-queries
              mountPath: /active-query-tracker
          ports:
            - name: http-metrics
              containerPort: 8080
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
            - name: memberlist
              containerPort: 7946
              protocol: TCP
          livenessProbe:
            null
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 45
          resources:
            requests:
              cpu: 20m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          env:
            - name: "GOMAXPROCS"
              value: "8"
            - name: "JAEGER_REPORTER_MAX_QUEUE_SIZE"
              value: "1000"
          envFrom:
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app.kubernetes.io/name: mimir
            app.kubernetes.io/instance: my-release
            app.kubernetes.io/component: distributor
      terminationGracePeriodSeconds: 100
      volumes:
        - name: config
          configMap:
            name: my-release-mimir-config
            items:
              - key: "mimir.yaml"
                path: "mimir.yaml"
        - name: runtime-config
          configMap:
            name: my-release-mimir-runtime
        - name: storage
          emptyDir: {}
        - name: active-queries
          emptyDir: {}
---
# Source: lgtm-distributed/charts/mimir/templates/nginx/nginx-dep.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-mimir-nginx
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: nginx
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  replicas: 1
  strategy:
    rollingUpdate:
      maxSurge: 15%
      maxUnavailable: 0
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: nginx
  template:
    metadata:
      annotations:
        checksum/config: 213f65aae28f2b587bcc3e4bb9226dbdf1e181df3b0c53f7296c155552da37d4
      labels:
        helm.sh/chart: mimir-5.3.0
        app.kubernetes.io/name: mimir
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/version: "2.12.0"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: nginx
      namespace: "lgtm-distributed-2.0.0.tgz"
    spec:
      serviceAccountName: my-release-mimir
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      terminationGracePeriodSeconds: 30
      containers:
        - name: nginx
          image: docker.io/nginxinc/nginx-unprivileged:1.25-alpine
          imagePullPolicy: IfNotPresent
          ports:
            - name: http-metric
              containerPort: 8080
              protocol: TCP
          env:
          envFrom:
          readinessProbe:
            httpGet:
              path: /
              port: http-metric
            initialDelaySeconds: 15
            timeoutSeconds: 1
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          volumeMounts:
            - name: config
              mountPath: /etc/nginx/nginx.conf
              subPath: nginx.conf
            - name: tmp
              mountPath: /tmp
            - name: docker-entrypoint-d-override
              mountPath: /docker-entrypoint.d
          resources:
            {}
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app.kubernetes.io/name: mimir
            app.kubernetes.io/instance: my-release
            app.kubernetes.io/component: nginx
      volumes:
        - name: config
          configMap:
            name: my-release-mimir-nginx
        - name: tmp
          emptyDir: {}
        - name: docker-entrypoint-d-override
          emptyDir: {}
---
# Source: lgtm-distributed/charts/mimir/templates/overrides-exporter/overrides-exporter-dep.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    {}
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: overrides-exporter
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  name: my-release-mimir-overrides-exporter
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: overrides-exporter
  strategy:
    rollingUpdate:
      maxSurge: 15%
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      labels:
        helm.sh/chart: mimir-5.3.0
        app.kubernetes.io/name: mimir
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/version: "2.12.0"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: overrides-exporter
      annotations:
        checksum/config: 8b3218baa3d6a8d16faf0299cf63569d55f4896d2c3d409aeea7267c729864e4
      namespace: "lgtm-distributed-2.0.0.tgz"
    spec:
      serviceAccountName: my-release-mimir
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      containers:
        - name: overrides-exporter
          image: "grafana/mimir:2.12.0"
          imagePullPolicy: IfNotPresent
          args:
            - "-target=overrides-exporter"
            - "-config.expand-env=true"
            - "-config.file=/etc/mimir/mimir.yaml"
          volumeMounts:
            - name: config
              mountPath: /etc/mimir
            - name: runtime-config
              mountPath: /var/mimir
            - name: storage
              mountPath: "/data"
              subPath: 
            - name: active-queries
              mountPath: /active-query-tracker
          ports:
            - name: http-metrics
              containerPort: 8080
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 45
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 45
          resources:
            requests:
              cpu: 20m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          env:
          envFrom:
      
      terminationGracePeriodSeconds: 60
      volumes:
        - name: config
          configMap:
            name: my-release-mimir-config
            items:
              - key: "mimir.yaml"
                path: "mimir.yaml"
        - name: runtime-config
          configMap:
            name: my-release-mimir-runtime
        - name: storage
          emptyDir: {}
        - name: active-queries
          emptyDir: {}
---
# Source: lgtm-distributed/charts/mimir/templates/querier/querier-dep.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-mimir-querier
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: querier
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  # If replicas is not number (when using values file it's float64, when using --set arg it's int64) and is false (i.e. null) don't set it
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: querier
  strategy:
    rollingUpdate:
      maxSurge: 15%
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      labels:
        helm.sh/chart: mimir-5.3.0
        app.kubernetes.io/name: mimir
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/version: "2.12.0"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: querier
        app.kubernetes.io/part-of: memberlist
      annotations:
        checksum/config: 8b3218baa3d6a8d16faf0299cf63569d55f4896d2c3d409aeea7267c729864e4
    spec:
      serviceAccountName: my-release-mimir
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      containers:
        - name: querier
          image: "grafana/mimir:2.12.0"
          imagePullPolicy: IfNotPresent
          args:
            - "-target=querier"
            - "-config.expand-env=true"
            - "-config.file=/etc/mimir/mimir.yaml"
          volumeMounts:
            - name: config
              mountPath: /etc/mimir
            - name: runtime-config
              mountPath: /var/mimir
            - name: storage
              mountPath: "/data"
              subPath: 
            - name: active-queries
              mountPath: /active-query-tracker
          ports:
            - name: http-metrics
              containerPort: 8080
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
            - name: memberlist
              containerPort: 7946
              protocol: TCP
          livenessProbe:
            null
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 45
          resources:
            requests:
              cpu: 20m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          env:
            - name: "GOMAXPROCS"
              value: "5"
            - name: "JAEGER_REPORTER_MAX_QUEUE_SIZE"
              value: "5000"
          envFrom:
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app.kubernetes.io/name: mimir
            app.kubernetes.io/instance: my-release
            app.kubernetes.io/component: querier
      terminationGracePeriodSeconds: 180
      volumes:
        - name: config
          configMap:
            name: my-release-mimir-config
            items:
              - key: "mimir.yaml"
                path: "mimir.yaml"
        - name: runtime-config
          configMap:
            name: my-release-mimir-runtime
        - name: storage
          emptyDir: {}
        - name: active-queries
          emptyDir: {}
---
# Source: lgtm-distributed/charts/mimir/templates/query-frontend/query-frontend-dep.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-mimir-query-frontend
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: query-frontend
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  # If replicas is not number (when using values file it's float64, when using --set arg it's int64) and is false (i.e. null) don't set it
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: query-frontend
  strategy:
    rollingUpdate:
      maxSurge: 15%
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      labels:
        helm.sh/chart: mimir-5.3.0
        app.kubernetes.io/name: mimir
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/version: "2.12.0"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: query-frontend
      annotations:
        checksum/config: 8b3218baa3d6a8d16faf0299cf63569d55f4896d2c3d409aeea7267c729864e4
      namespace: "lgtm-distributed-2.0.0.tgz"
    spec:
      serviceAccountName: my-release-mimir
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      containers:
        - name: query-frontend
          image: "grafana/mimir:2.12.0"
          imagePullPolicy: IfNotPresent
          args:
            - "-target=query-frontend"
            - "-config.expand-env=true"
            - "-config.file=/etc/mimir/mimir.yaml"
            # Reduce the likelihood of queries hitting terminated query-frontends.
            - "-server.grpc.keepalive.max-connection-age=30s"
            - "-shutdown-delay=90s"
          volumeMounts:
            - name: runtime-config
              mountPath: /var/mimir
            - name: config
              mountPath: /etc/mimir
            - name: storage
              mountPath: /data
            - name: active-queries
              mountPath: /active-query-tracker
          ports:
            - name: http-metrics
              containerPort: 8080
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
          livenessProbe:
            null
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 45
          resources:
            requests:
              cpu: 20m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          env:
            - name: "JAEGER_REPORTER_MAX_QUEUE_SIZE"
              value: "5000"
          envFrom:
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app.kubernetes.io/name: mimir
            app.kubernetes.io/instance: my-release
            app.kubernetes.io/component: query-frontend
      terminationGracePeriodSeconds: 390
      volumes:
        - name: config
          configMap:
            name: my-release-mimir-config
            items:
              - key: "mimir.yaml"
                path: "mimir.yaml"
        - name: runtime-config
          configMap:
            name: my-release-mimir-runtime
        - name: storage
          emptyDir: {}
        - name: active-queries
          emptyDir: {}
---
# Source: lgtm-distributed/charts/mimir/templates/query-scheduler/query-scheduler-dep.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-mimir-query-scheduler
  namespace: "lgtm-distributed-2.0.0.tgz"
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: query-scheduler
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: query-scheduler
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      labels:
        helm.sh/chart: mimir-5.3.0
        app.kubernetes.io/name: mimir
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/version: "2.12.0"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: query-scheduler
      annotations:
        checksum/config: 8b3218baa3d6a8d16faf0299cf63569d55f4896d2c3d409aeea7267c729864e4
    spec:
      serviceAccountName: my-release-mimir
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      containers:
        - name: query-scheduler
          image: "grafana/mimir:2.12.0"
          imagePullPolicy: IfNotPresent
          args:
            - "-target=query-scheduler"
            - "-config.expand-env=true"
            - "-config.file=/etc/mimir/mimir.yaml"
          volumeMounts:
            - name: runtime-config
              mountPath: /var/mimir
            - name: config
              mountPath: /etc/mimir
            - name: storage
              mountPath: /data
            - name: active-queries
              mountPath: /active-query-tracker
          ports:
            - name: http-metrics
              containerPort: 8080
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
          livenessProbe:
            null
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 45
          resources:
            requests:
              cpu: 20m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          env:
          envFrom:
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app.kubernetes.io/name: mimir
            app.kubernetes.io/instance: my-release
            app.kubernetes.io/component: query-scheduler
      terminationGracePeriodSeconds: 180
      volumes:
        - name: config
          configMap:
            name: my-release-mimir-config
            items:
              - key: "mimir.yaml"
                path: "mimir.yaml"
        - name: runtime-config
          configMap:
            name: my-release-mimir-runtime
        - name: storage
          emptyDir: {}
        - name: active-queries
          emptyDir: {}
---
# Source: lgtm-distributed/charts/mimir/templates/ruler/ruler-dep.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-mimir-ruler
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: ruler
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: ruler
  strategy:
    rollingUpdate:
      maxSurge: 50%
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      labels:
        helm.sh/chart: mimir-5.3.0
        app.kubernetes.io/name: mimir
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/version: "2.12.0"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: ruler
        app.kubernetes.io/part-of: memberlist
      annotations:
        checksum/config: 8b3218baa3d6a8d16faf0299cf63569d55f4896d2c3d409aeea7267c729864e4
      namespace: "lgtm-distributed-2.0.0.tgz"
    spec:
      serviceAccountName: my-release-mimir
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      containers:
        - name: ruler
          image: "grafana/mimir:2.12.0"
          imagePullPolicy: IfNotPresent
          args:
            - "-target=ruler"
            - "-config.expand-env=true"
            - "-config.file=/etc/mimir/mimir.yaml"
            - "-distributor.remote-timeout=10s"
          volumeMounts:
            - name: config
              mountPath: /etc/mimir
            - name: runtime-config
              mountPath: /var/mimir
            - name: storage
              mountPath: "/data"
              subPath: 
            - name: active-queries
              mountPath: /active-query-tracker
          ports:
            - name: http-metrics
              containerPort: 8080
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
            - name: memberlist
              containerPort: 7946
              protocol: TCP
          livenessProbe:
            null
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 45
          resources:
            requests:
              cpu: 20m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          env:
            - name: "JAEGER_REPORTER_MAX_QUEUE_SIZE"
              value: "1000"
          envFrom:
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app.kubernetes.io/name: mimir
            app.kubernetes.io/instance: my-release
            app.kubernetes.io/component: ruler
      terminationGracePeriodSeconds: 180
      volumes:
        - name: config
          configMap:
            name: my-release-mimir-config
            items:
              - key: "mimir.yaml"
                path: "mimir.yaml"
        - name: runtime-config
          configMap:
            name: my-release-mimir-runtime
        - name: storage
          emptyDir: {}
        - name: active-queries
          emptyDir: {}
---
# Source: lgtm-distributed/charts/tempo/templates/compactor/deployment-compactor.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-tempo-compactor
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: tempo-1.10.0
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: compactor
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.5.0"
    app.kubernetes.io/managed-by: Helm
spec:
  minReadySeconds: 10
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: tempo
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: compactor
  strategy:
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
  template:
    metadata:
      labels:
        helm.sh/chart: tempo-1.10.0
        app.kubernetes.io/name: tempo
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/version: "2.5.0"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: compactor
        app.kubernetes.io/part-of: memberlist
      annotations:
        checksum/config: 17b78309633b96f68ff8649acfb63a193a9c400d28822f5e25d5e742989fe4bc
    spec:
      serviceAccountName: my-release-tempo
      securityContext:
        fsGroup: 1000
      enableServiceLinks: false
      
      containers:
        - args:
            - -target=compactor
            - -config.file=/conf/tempo.yaml
            - -mem-ballast-size-mbs=1024
          image: docker.io/grafana/tempo:2.5.0
          imagePullPolicy: IfNotPresent
          name: compactor
          ports:
            - containerPort: 3100
              name: http-metrics
            - containerPort: 7946
              name: http-memberlist
          resources:
            {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1000
            runAsNonRoot: true
            runAsUser: 1000
          volumeMounts:
            - mountPath: /conf
              name: config
            - mountPath: /runtime-config
              name: runtime-config
            - mountPath: /var/tempo
              name: tempo-compactor-store
      terminationGracePeriodSeconds: 30
      volumes:
        - name: config
          configMap:
            name: my-release-tempo-config
            items:
              - key: "tempo.yaml"
                path: "tempo.yaml"
        - name: runtime-config
          configMap:
            name: my-release-tempo-runtime
            items:
              - key: "overrides.yaml"
                path: "overrides.yaml"
        - name: tempo-compactor-store
          emptyDir: {}
---
# Source: lgtm-distributed/charts/tempo/templates/distributor/deployment-distributor.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-tempo-distributor
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: tempo-1.10.0
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: distributor
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.5.0"
    app.kubernetes.io/managed-by: Helm
spec:
  minReadySeconds: 10
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: tempo
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: distributor
  template:
    metadata:
      labels:
        helm.sh/chart: tempo-1.10.0
        app.kubernetes.io/name: tempo
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/version: "2.5.0"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: distributor
        app.kubernetes.io/part-of: memberlist
      annotations:
        checksum/config: 17b78309633b96f68ff8649acfb63a193a9c400d28822f5e25d5e742989fe4bc
    spec:
      serviceAccountName: my-release-tempo
      securityContext:
        fsGroup: 1000
      enableServiceLinks: false
      
      containers:
        - args:
            - -target=distributor
            - -config.file=/conf/tempo.yaml
            - -mem-ballast-size-mbs=1024
          image: docker.io/grafana/tempo:2.5.0
          imagePullPolicy: IfNotPresent
          name: distributor
          ports:
            - containerPort: 7946
              name: http-memberlist
              protocol: TCP
            - containerPort: 3100
              name: http-metrics
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 30
            timeoutSeconds: 1
          resources:
            {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1000
            runAsNonRoot: true
            runAsUser: 1000
          volumeMounts:
            - mountPath: /conf
              name: config
            - mountPath: /runtime-config
              name: runtime-config
            - mountPath: /var/tempo
              name: tempo-distributor-store
      terminationGracePeriodSeconds: 30
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: tempo
              app.kubernetes.io/instance: my-release
              app.kubernetes.io/component: distributor
        
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/name: tempo
                  app.kubernetes.io/instance: my-release
                  app.kubernetes.io/component: distributor
              topologyKey: kubernetes.io/hostname
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: tempo
                    app.kubernetes.io/instance: my-release
                    app.kubernetes.io/component: distributor
                topologyKey: topology.kubernetes.io/zone
        
      volumes:
        - name: config
          configMap:
            name: my-release-tempo-config
            items:
              - key: "tempo.yaml"
                path: "tempo.yaml"
        - name: runtime-config
          configMap:
            name: my-release-tempo-runtime
            items:
              - key: "overrides.yaml"
                path: "overrides.yaml"
        - name: tempo-distributor-store
          emptyDir: {}
---
# Source: lgtm-distributed/charts/tempo/templates/querier/deployment-querier.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-tempo-querier
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: tempo-1.10.0
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: querier
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.5.0"
    app.kubernetes.io/managed-by: Helm
spec:
  minReadySeconds: 10
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: tempo
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: querier
  strategy:
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
  template:
    metadata:
      labels:
        helm.sh/chart: tempo-1.10.0
        app.kubernetes.io/name: tempo
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/version: "2.5.0"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: querier
        app.kubernetes.io/part-of: memberlist
      annotations:
        checksum/config: 17b78309633b96f68ff8649acfb63a193a9c400d28822f5e25d5e742989fe4bc
    spec:
      serviceAccountName: my-release-tempo
      securityContext:
        fsGroup: 1000
      enableServiceLinks: false
      
      containers:
        - args:
            - -target=querier
            - -config.file=/conf/tempo.yaml
            - -mem-ballast-size-mbs=1024
          image: docker.io/grafana/tempo:2.5.0
          imagePullPolicy: IfNotPresent
          name: querier
          ports:
            - containerPort: 7946
              name: http-memberlist
              protocol: TCP
            - containerPort: 3100
              name: http-metrics
          resources:
            {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1000
            runAsNonRoot: true
            runAsUser: 1000
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 30
            timeoutSeconds: 1
          volumeMounts:
            - mountPath: /conf
              name: config
            - mountPath: /runtime-config
              name: runtime-config
            - mountPath: /var/tempo
              name: tempo-querier-store
      terminationGracePeriodSeconds: 30
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: tempo
              app.kubernetes.io/instance: my-release
              app.kubernetes.io/component: querier
        
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/name: tempo
                  app.kubernetes.io/instance: my-release
                  app.kubernetes.io/component: querier
              topologyKey: kubernetes.io/hostname
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: tempo
                    app.kubernetes.io/instance: my-release
                    app.kubernetes.io/component: querier
                topologyKey: topology.kubernetes.io/zone
        
      volumes:
        - name: config
          configMap:
            name: my-release-tempo-config
            items:
              - key: "tempo.yaml"
                path: "tempo.yaml"
        - name: runtime-config
          configMap:
            name: my-release-tempo-runtime
            items:
              - key: "overrides.yaml"
                path: "overrides.yaml"
        - name: tempo-querier-store
          emptyDir: {}
---
# Source: lgtm-distributed/charts/tempo/templates/query-frontend/deployment-query-frontend.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-tempo-query-frontend
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: tempo-1.10.0
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: query-frontend
    app.kubernetes.io/version: "2.5.0"
    app.kubernetes.io/managed-by: Helm
spec:
  minReadySeconds: 10
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: tempo
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: query-frontend
  strategy:
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
  template:
    metadata:
      labels:
        helm.sh/chart: tempo-1.10.0
        app.kubernetes.io/name: tempo
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/version: "2.5.0"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: query-frontend
      annotations:
        checksum/config: 17b78309633b96f68ff8649acfb63a193a9c400d28822f5e25d5e742989fe4bc
    spec:
      serviceAccountName: my-release-tempo
      securityContext:
        fsGroup: 1000
      enableServiceLinks: false
      
      containers:
        - args:
            - -target=query-frontend
            - -config.file=/conf/tempo.yaml
            - -mem-ballast-size-mbs=1024
          image: docker.io/grafana/tempo:2.5.0
          imagePullPolicy: IfNotPresent
          name: query-frontend
          ports:
            - containerPort: 3100
              name: http-metrics
            - containerPort: 9095
              name: grpc
          resources:
            {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1000
            runAsNonRoot: true
            runAsUser: 1000
          volumeMounts:
            - mountPath: /conf
              name: config
            - mountPath: /runtime-config
              name: runtime-config
            - mountPath: /var/tempo
              name: tempo-queryfrontend-store
      terminationGracePeriodSeconds: 30
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: tempo
              app.kubernetes.io/instance: my-release
              app.kubernetes.io/component: query-frontend
        
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/name: tempo
                  app.kubernetes.io/instance: my-release
                  app.kubernetes.io/component: query-frontend
              topologyKey: kubernetes.io/hostname
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: tempo
                    app.kubernetes.io/instance: my-release
                    app.kubernetes.io/component: query-frontend
                topologyKey: topology.kubernetes.io/zone
        
      volumes:
        - name: config
          configMap:
            name: my-release-tempo-config
            items:
              - key: "tempo.yaml"
                path: "tempo.yaml"
        - name: runtime-config
          configMap:
            name: my-release-tempo-runtime
            items:
              - key: "overrides.yaml"
                path: "overrides.yaml"
        - name: tempo-queryfrontend-store
          emptyDir: {}
---
# Source: lgtm-distributed/charts/loki/templates/ingester/statefulset-ingester.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-loki-ingester
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: loki-0.79.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "2.9.6"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: ingester
    app.kubernetes.io/part-of: memberlist
spec:
  replicas: 1
  podManagementPolicy: Parallel
  updateStrategy:
    rollingUpdate:
      partition: 0
  serviceName: my-release-loki-ingester-headless
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: loki
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: ingester
  template:
    metadata:
      annotations:
        checksum/config: a3ab99f0b70b9af56cbd2e876aac942086dec7a7110a9665b0ff76c81f8209cd
      labels:
        app.kubernetes.io/name: loki
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/component: ingester
        app.kubernetes.io/part-of: memberlist
    spec:
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: loki
              app.kubernetes.io/instance: my-release
              app.kubernetes.io/component: ingester
        
      serviceAccountName: my-release-loki
      
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
      terminationGracePeriodSeconds: 300
      containers:
        - name: ingester
          image: docker.io/grafana/loki:2.9.6
          imagePullPolicy: IfNotPresent
          args:
            - -config.file=/etc/loki/config/config.yaml
            - -target=ingester
          ports:
            - name: http
              containerPort: 3100
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
            - name: http-memberlist
              containerPort: 7946
              protocol: TCP
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          
          readinessProbe:
            httpGet:
              path: /ready
              port: http
            initialDelaySeconds: 30
            timeoutSeconds: 1
          
          livenessProbe:
            httpGet:
              path: /ready
              port: http
            initialDelaySeconds: 300
          volumeMounts:
            - name: config
              mountPath: /etc/loki/config
            - name: runtime-config
              mountPath: /var/loki-runtime
            - name: data
              mountPath: /var/loki
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/name: loki
                  app.kubernetes.io/instance: my-release
                  app.kubernetes.io/component: ingester
              topologyKey: kubernetes.io/hostname
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: loki
                    app.kubernetes.io/instance: my-release
                    app.kubernetes.io/component: ingester
                topologyKey: failure-domain.beta.kubernetes.io/zone
        
      volumes:
        - name: config
          configMap:
            name: my-release-loki
        - name: runtime-config
          configMap:
            name: my-release-loki-runtime
        - name: data
          emptyDir: {}
---
# Source: lgtm-distributed/charts/loki/templates/querier/statefulset-querier.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-loki-querier
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: loki-0.79.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "2.9.6"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: querier
    app.kubernetes.io/part-of: memberlist
spec:
  replicas: 1
  podManagementPolicy: Parallel
  updateStrategy:
    rollingUpdate:
      partition: 0
  serviceName: my-release-loki-querier-headless
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: loki
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: querier
  template:
    metadata:
      annotations:
        checksum/config: a3ab99f0b70b9af56cbd2e876aac942086dec7a7110a9665b0ff76c81f8209cd
      labels:
        app.kubernetes.io/name: loki
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/component: querier
        app.kubernetes.io/part-of: memberlist
    spec:
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: loki
              app.kubernetes.io/instance: my-release
              app.kubernetes.io/component: querier
        
      serviceAccountName: my-release-loki
      
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
      terminationGracePeriodSeconds: 30
      containers:
        - name: querier
          image: docker.io/grafana/loki:2.9.6
          imagePullPolicy: IfNotPresent
          args:
            - -config.file=/etc/loki/config/config.yaml
            - -target=querier
          ports:
            - name: http
              containerPort: 3100
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
            - name: http-memberlist
              containerPort: 7946
              protocol: TCP
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          readinessProbe:
            httpGet:
              path: /ready
              port: http
            initialDelaySeconds: 30
            timeoutSeconds: 1
          livenessProbe:
            httpGet:
              path: /ready
              port: http
            initialDelaySeconds: 300
          volumeMounts:
            - name: config
              mountPath: /etc/loki/config
            - name: runtime-config
              mountPath: /var/loki-runtime
            - name: data
              mountPath: /var/loki
          resources:
            {}
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/name: loki
                  app.kubernetes.io/instance: my-release
                  app.kubernetes.io/component: querier
              topologyKey: kubernetes.io/hostname
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: loki
                    app.kubernetes.io/instance: my-release
                    app.kubernetes.io/component: querier
                topologyKey: failure-domain.beta.kubernetes.io/zone
        
      volumes:
        - name: config
          configMap:
            name: my-release-loki
        - name: runtime-config
          configMap:
            name: my-release-loki-runtime
        - name: data
          emptyDir: {}
---
# Source: lgtm-distributed/charts/mimir/templates/alertmanager/alertmanager-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-mimir-alertmanager
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: alertmanager
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: alertmanager
  updateStrategy:
    type: RollingUpdate
  serviceName: my-release-mimir-alertmanager
  volumeClaimTemplates:
    - metadata:
        name: storage
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: "1Gi"
  template:
    metadata:
      labels:
        helm.sh/chart: mimir-5.3.0
        app.kubernetes.io/name: mimir
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/version: "2.12.0"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: alertmanager
        app.kubernetes.io/part-of: memberlist
      annotations:
        checksum/config: 8b3218baa3d6a8d16faf0299cf63569d55f4896d2c3d409aeea7267c729864e4
        checksum/alertmanager-fallback-config: c14e002f44cb493f701beab1f7f729e3135ce8970938a8423f287474a4701a73
      namespace: "lgtm-distributed-2.0.0.tgz"
    spec:
      serviceAccountName: my-release-mimir
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app.kubernetes.io/name: mimir
            app.kubernetes.io/instance: my-release
            app.kubernetes.io/component: alertmanager
      terminationGracePeriodSeconds: 60
      volumes:
        - name: config
          configMap:
            name: my-release-mimir-config
            items:
              - key: "mimir.yaml"
                path: "mimir.yaml"
        - name: runtime-config
          configMap:
            name: my-release-mimir-runtime
        - name: tmp
          emptyDir: {}
        - name: active-queries
          emptyDir: {}
        - name: alertmanager-fallback-config
          configMap:
            name: my-release-mimir-alertmanager-fallback-config
      containers:
        - name: alertmanager
          image: "grafana/mimir:2.12.0"
          imagePullPolicy: IfNotPresent
          args:
            - "-target=alertmanager"
            - "-config.expand-env=true"
            - "-config.file=/etc/mimir/mimir.yaml"
          volumeMounts:
            - name: config
              mountPath: /etc/mimir
            - name: runtime-config
              mountPath: /var/mimir
            - name: storage
              mountPath: "/data"
            - name: alertmanager-fallback-config
              mountPath: /configs/
            - name: tmp
              mountPath: /tmp
            - name: active-queries
              mountPath: /active-query-tracker
          ports:
            - name: http-metrics
              containerPort: 8080
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
            - name: memberlist
              containerPort: 7946
              protocol: TCP
          livenessProbe:
            null
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 45
          resources:
            requests:
              cpu: 20m
              memory: 32Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          env:
          envFrom:
---
# Source: lgtm-distributed/charts/mimir/templates/compactor/compactor-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-mimir-compactor
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: compactor
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  podManagementPolicy: OrderedReady
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: compactor
  updateStrategy:
    type: RollingUpdate
  serviceName: my-release-mimir-compactor
  volumeClaimTemplates:
    - metadata:
        name: storage
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: "2Gi"
  template:
    metadata:
      labels:
        helm.sh/chart: mimir-5.3.0
        app.kubernetes.io/name: mimir
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/version: "2.12.0"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: compactor
        app.kubernetes.io/part-of: memberlist
      annotations:
        checksum/config: 8b3218baa3d6a8d16faf0299cf63569d55f4896d2c3d409aeea7267c729864e4
      namespace: "lgtm-distributed-2.0.0.tgz"
    spec:
      serviceAccountName: my-release-mimir
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app.kubernetes.io/name: mimir
            app.kubernetes.io/instance: my-release
            app.kubernetes.io/component: compactor
      terminationGracePeriodSeconds: 240
      volumes:
        - name: config
          configMap:
            name: my-release-mimir-config
            items:
              - key: "mimir.yaml"
                path: "mimir.yaml"
        - name: runtime-config
          configMap:
            name: my-release-mimir-runtime
        - name: active-queries
          emptyDir: {}
      containers:
        - name: compactor
          image: "grafana/mimir:2.12.0"
          imagePullPolicy: IfNotPresent
          args:
            - "-target=compactor"
            - "-config.expand-env=true"
            - "-config.file=/etc/mimir/mimir.yaml"
          volumeMounts:
            - name: config
              mountPath: /etc/mimir
            - name: runtime-config
              mountPath: /var/mimir
            - name: storage
              mountPath: "/data"
            - name: active-queries
              mountPath: /active-query-tracker
          ports:
            - name: http-metrics
              containerPort: 8080
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
            - name: memberlist
              containerPort: 7946
              protocol: TCP
          livenessProbe:
            null
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 60
          resources:
            requests:
              cpu: 20m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          env:
          envFrom:
---
# Source: lgtm-distributed/charts/mimir/templates/ingester/ingester-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-mimir-ingester
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: ingester
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  podManagementPolicy: Parallel
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: ingester
  updateStrategy:
    type: RollingUpdate
  serviceName: my-release-mimir-ingester-headless
  volumeClaimTemplates:
    - metadata:
        name: storage
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: "2Gi"
  template:
    metadata:
      labels:
        helm.sh/chart: mimir-5.3.0
        app.kubernetes.io/name: mimir
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/version: "2.12.0"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: ingester
        app.kubernetes.io/part-of: memberlist
      annotations:
        checksum/config: 8b3218baa3d6a8d16faf0299cf63569d55f4896d2c3d409aeea7267c729864e4
      namespace: "lgtm-distributed-2.0.0.tgz"
    spec:
      serviceAccountName: my-release-mimir
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app.kubernetes.io/name: mimir
            app.kubernetes.io/instance: my-release
            app.kubernetes.io/component: ingester
      terminationGracePeriodSeconds: 240
      volumes:
        - name: config
          configMap:
            name: my-release-mimir-config
            items:
              - key: "mimir.yaml"
                path: "mimir.yaml"
        - name: runtime-config
          configMap:
            name: my-release-mimir-runtime
        - name: active-queries
          emptyDir: {}
      containers:
        - name: ingester
          image: "grafana/mimir:2.12.0"
          imagePullPolicy: IfNotPresent
          args:
            - "-target=ingester"
            - "-config.expand-env=true"
            - "-config.file=/etc/mimir/mimir.yaml"
            - "-ingester.ring.instance-availability-zone=zone-default"
            - "-server.grpc-max-concurrent-streams=500"
          volumeMounts:
            - name: config
              mountPath: /etc/mimir
            - name: runtime-config
              mountPath: /var/mimir
            - name: storage
              mountPath: "/data"
            - name: active-queries
              mountPath: /active-query-tracker
          ports:
            - name: http-metrics
              containerPort: 8080
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
            - name: memberlist
              containerPort: 7946
              protocol: TCP
          livenessProbe:
            null
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 60
          resources:
            requests:
              cpu: 20m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          env:
            - name: "JAEGER_REPORTER_MAX_QUEUE_SIZE"
              value: "1000"
          envFrom:
---
# Source: lgtm-distributed/charts/mimir/templates/store-gateway/store-gateway-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-mimir-store-gateway
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: store-gateway
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  podManagementPolicy: OrderedReady
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: store-gateway
  updateStrategy:
    type: RollingUpdate
  serviceName: my-release-mimir-store-gateway-headless
  volumeClaimTemplates:
    - metadata:
        name: storage
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: "2Gi"
  template:
    metadata:
      labels:
        helm.sh/chart: mimir-5.3.0
        app.kubernetes.io/name: mimir
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/version: "2.12.0"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: store-gateway
        app.kubernetes.io/part-of: memberlist
      annotations:
        checksum/config: 8b3218baa3d6a8d16faf0299cf63569d55f4896d2c3d409aeea7267c729864e4
      namespace: "lgtm-distributed-2.0.0.tgz"
    spec:
      serviceAccountName: my-release-mimir
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app.kubernetes.io/name: mimir
            app.kubernetes.io/instance: my-release
            app.kubernetes.io/component: store-gateway
      terminationGracePeriodSeconds: 240
      volumes:
        - name: config
          configMap:
            name: my-release-mimir-config
            items:
              - key: "mimir.yaml"
                path: "mimir.yaml"
        - name: runtime-config
          configMap:
            name: my-release-mimir-runtime
        - name: active-queries
          emptyDir: {}
      containers:
        - name: store-gateway
          image: "grafana/mimir:2.12.0"
          imagePullPolicy: IfNotPresent
          args:
            - "-target=store-gateway"
            - "-config.expand-env=true"
            - "-config.file=/etc/mimir/mimir.yaml"
          volumeMounts:
            - name: config
              mountPath: /etc/mimir
            - name: runtime-config
              mountPath: /var/mimir
            - name: storage
              mountPath: "/data"
            - name: active-queries
              mountPath: /active-query-tracker
          ports:
            - name: http-metrics
              containerPort: 8080
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
            - name: memberlist
              containerPort: 7946
              protocol: TCP
          livenessProbe:
            null
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 60
          resources:
            requests:
              cpu: 20m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          env:
            - name: "GOMAXPROCS"
              value: "5"
            - name: "GOMEMLIMIT"
              value: "536870912"
            - name: "JAEGER_REPORTER_MAX_QUEUE_SIZE"
              value: "1000"
          envFrom:
---
# Source: lgtm-distributed/charts/tempo/templates/ingester/statefulset-ingester.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-tempo-ingester
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: tempo-1.10.0
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: ingester
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.5.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: tempo
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: ingester
  serviceName: ingester
  podManagementPolicy: Parallel
  updateStrategy:
    rollingUpdate:
      partition: 0
  template:
    metadata:
      labels:
        helm.sh/chart: tempo-1.10.0
        app.kubernetes.io/name: tempo
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/version: "2.5.0"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: ingester
        app.kubernetes.io/part-of: memberlist
      annotations:
        checksum/config: 17b78309633b96f68ff8649acfb63a193a9c400d28822f5e25d5e742989fe4bc
    spec:
      serviceAccountName: my-release-tempo
      securityContext:
        fsGroup: 1000
      enableServiceLinks: false
      
      initContainers:
        []
      containers:
        - args:
            - -target=ingester
            - -config.file=/conf/tempo.yaml
            - -mem-ballast-size-mbs=1024
          image: docker.io/grafana/tempo:2.5.0
          imagePullPolicy: IfNotPresent
          name: ingester
          ports:
            - name: grpc
              containerPort: 9095
            - name: http-memberlist
              containerPort: 7946
            - name: http-metrics
              containerPort: 3100
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 30
            timeoutSeconds: 1
          resources:
            {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1000
            runAsNonRoot: true
            runAsUser: 1000
          volumeMounts:
            - mountPath: /conf
              name: config
            - mountPath: /runtime-config
              name: runtime-config
            - mountPath: /var/tempo
              name: data
      terminationGracePeriodSeconds: 300
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: tempo
              app.kubernetes.io/instance: my-release
              app.kubernetes.io/component: ingester
        
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: tempo
                    app.kubernetes.io/instance: my-release
                    app.kubernetes.io/component: ingester
                topologyKey: kubernetes.io/hostname
            - weight: 75
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: tempo
                    app.kubernetes.io/instance: my-release
                    app.kubernetes.io/component: ingester
                topologyKey: topology.kubernetes.io/zone
        
      volumes:
        - name: config
          configMap:
            name: my-release-tempo-config
            items:
              - key: "tempo.yaml"
                path: "tempo.yaml"
        - name: runtime-config
          configMap:
            name: my-release-tempo-runtime
            items:
              - key: "overrides.yaml"
                path: "overrides.yaml"
        - name: data
          emptyDir: {}
---
# Source: lgtm-distributed/charts/tempo/templates/memcached/statefulset-memcached.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-tempo-memcached
  namespace: lgtm-distributed-2.0.0.tgz
  labels:
    helm.sh/chart: tempo-1.10.0
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: memcached
    app.kubernetes.io/version: "2.5.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: tempo
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: memcached
  serviceName: memcached
  template:
    metadata:
      labels:
        helm.sh/chart: tempo-1.10.0
        app.kubernetes.io/name: tempo
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/component: memcached
        app.kubernetes.io/version: "2.5.0"
        app.kubernetes.io/managed-by: Helm
    spec:
      serviceAccountName: my-release-tempo
      securityContext:
        fsGroup: 1000
      enableServiceLinks: false
      
      containers:
        - image: docker.io/memcached:1.6.23-alpine
          imagePullPolicy: IfNotPresent
          name: memcached
          ports:
            - containerPort: 11211
              name: client
          resources:
            {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1000
            runAsNonRoot: true
            runAsUser: 1000
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: tempo
              app.kubernetes.io/instance: my-release
              app.kubernetes.io/component: memcached
        
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/name: tempo
                  app.kubernetes.io/instance: my-release
                  app.kubernetes.io/component: memcached
              topologyKey: kubernetes.io/hostname
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: tempo
                    app.kubernetes.io/instance: my-release
                    app.kubernetes.io/component: memcached
                topologyKey: topology.kubernetes.io/zone
        
  updateStrategy:
    type: RollingUpdate
---
# Source: lgtm-distributed/charts/mimir/templates/minio/create-bucket-job.yaml
# Minio provides post-install hook to create bucket
# however the hook won't be executed if helm install is run
# with --wait flag. Hence this job is a workaround for that.
# See https://github.com/grafana/mimir/issues/2464
apiVersion: batch/v1
kind: Job
metadata:
  name: my-release-mimir-make-minio-buckets-5.0.14
  namespace: "lgtm-distributed-2.0.0.tgz"
  labels:
    app: mimir-make-bucket-job
    chart: mimir-5.3.0
    release: my-release
    heritage: Helm
spec:
  template:
    metadata:
      labels:
        app: mimir-job
        release: my-release
    spec:
      restartPolicy: OnFailure      
      volumes:
        - name: minio-configuration
          projected:
            sources:
            - configMap:
                name: my-release-minio
            - secret:
                name: my-release-minio
      containers:
      - name: minio-mc
        image: "quay.io/minio/mc:RELEASE.2023-09-29T16-41-22Z"
        imagePullPolicy: IfNotPresent
        command: ["/bin/sh", "/config/initialize"]
        env:
          - name: MINIO_ENDPOINT
            value: my-release-minio
          - name: MINIO_PORT
            value: "9000"
        volumeMounts:
          - name: minio-configuration
            mountPath: /config
        resources:
          requests:
            memory: 128Mi
---
# Source: lgtm-distributed/charts/grafana/templates/tests/test-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    helm.sh/chart: grafana-7.3.12
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "10.4.3"
    app.kubernetes.io/managed-by: Helm
  name: my-release-grafana-test
  namespace: lgtm-distributed-2.0.0.tgz
  annotations:
    "helm.sh/hook": test-success
    "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"
---
# Source: lgtm-distributed/charts/grafana/templates/tests/test-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-grafana-test
  namespace: lgtm-distributed-2.0.0.tgz
  annotations:
    "helm.sh/hook": test-success
    "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"
  labels:
    helm.sh/chart: grafana-7.3.12
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "10.4.3"
    app.kubernetes.io/managed-by: Helm
data:
  run.sh: |-
    @test "Test Health" {
      url="http://my-release-grafana/api/health"

      code=$(wget --server-response --spider --timeout 90 --tries 10 ${url} 2>&1 | awk '/^  HTTP/{print $2}')
      [ "$code" == "200" ]
    }
---
# Source: lgtm-distributed/charts/grafana/templates/tests/test.yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-release-grafana-test
  labels:
    helm.sh/chart: grafana-7.3.12
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "10.4.3"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test-success
    "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"
  namespace: lgtm-distributed-2.0.0.tgz
spec:
  serviceAccountName: my-release-grafana-test
  containers:
    - name: my-release-test
      image: "docker.io/bats/bats:v1.4.1"
      imagePullPolicy: "IfNotPresent"
      command: ["/opt/bats/bin/bats", "-t", "/tests/run.sh"]
      volumeMounts:
        - mountPath: /tests
          name: tests
          readOnly: true
  volumes:
    - name: tests
      configMap:
        name: my-release-grafana-test
  restartPolicy: Never
---
# Source: lgtm-distributed/charts/mimir/charts/minio/templates/post-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: my-release-minio-post-job
  labels:
    app: minio-post-job
    chart: minio-5.0.14
    release: my-release
    heritage: Helm
  annotations:
    "helm.sh/hook": post-install,post-upgrade
    "helm.sh/hook-delete-policy": hook-succeeded,before-hook-creation
spec:
  template:
    metadata:
      labels:
        app: minio-job
        release: my-release
    spec:
      restartPolicy: OnFailure      
      volumes:
        - name: etc-path
          emptyDir: {}
        - name: tmp
          emptyDir: {}
        - name: minio-configuration
          projected:
            sources:
              - configMap:
                  name: my-release-minio
              - secret:
                  name: my-release-minio
      serviceAccountName: minio-sa
      containers:
        - name: minio-make-bucket
          image: "quay.io/minio/mc:RELEASE.2023-09-29T16-41-22Z"
          imagePullPolicy: IfNotPresent
          command: [ "/bin/sh", "/config/initialize" ]
          env:
            - name: MINIO_ENDPOINT
              value: my-release-minio
            - name: MINIO_PORT
              value: "9000"
          volumeMounts:
            - name: etc-path
              mountPath: /etc/minio/mc
            - name: tmp
              mountPath: /tmp
            - name: minio-configuration
              mountPath: /config
          resources:
            requests:
              memory: 128Mi
        - name: minio-make-user
          image: "quay.io/minio/mc:RELEASE.2023-09-29T16-41-22Z"
          imagePullPolicy: IfNotPresent
          command: [ "/bin/sh", "/config/add-user" ]
          env:
            - name: MINIO_ENDPOINT
              value: my-release-minio
            - name: MINIO_PORT
              value: "9000"
          volumeMounts:
            - name: etc-path
              mountPath: /etc/minio/mc
            - name: tmp
              mountPath: /tmp
            - name: minio-configuration
              mountPath: /config
          resources:
            requests:
              memory: 128Mi
---
# Source: lgtm-distributed/charts/mimir/templates/smoke-test/smoke-test-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: my-release-mimir-smoke-test
  labels:
    helm.sh/chart: mimir-5.3.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: smoke-test
    app.kubernetes.io/version: "2.12.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
  namespace: "lgtm-distributed-2.0.0.tgz"
spec:
  backoffLimit: 5
  completions: 1
  parallelism: 1
  selector:
  template:
    metadata:
      labels:
        helm.sh/chart: mimir-5.3.0
        app.kubernetes.io/name: mimir
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/version: "2.12.0"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: smoke-test
    spec:
      serviceAccountName: my-release-mimir
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      initContainers:
        []
      containers:
        - name: smoke-test
          image: "grafana/mimir-continuous-test:2.12.0"
          imagePullPolicy: IfNotPresent
          args:
            - "-tests.smoke-test"
            - "-tests.write-endpoint=http://my-release-mimir-nginx.lgtm-distributed-2.0.0.tgz.svc:80"
            - "-tests.read-endpoint=http://my-release-mimir-nginx.lgtm-distributed-2.0.0.tgz.svc:80/prometheus"
            - "-tests.tenant-id="
            - "-tests.write-read-series-test.num-series=1000"
            - "-tests.write-read-series-test.max-query-age=48h"
            - "-server.metrics-port=8080"
          volumeMounts:
          env:
          envFrom:
      restartPolicy: OnFailure
      volumes:
