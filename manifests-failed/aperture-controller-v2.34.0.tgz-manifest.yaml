---
# Source: aperture-controller/charts/prometheus/templates/server/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    component: "server"
    app: prometheus
    release: my-release
    chart: prometheus-15.18.0
    heritage: Helm
  name: my-release-prometheus-server
  namespace: aperture-controller-v2.34.0.tgz
  annotations:
    {}
---
# Source: aperture-controller/templates/operator-rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-aperture-controller-operator
  namespace: aperture-controller-v2.34.0.tgz
  labels:
    app.kubernetes.io/name: aperture-controller
    helm.sh/chart: aperture-controller-v2.34.0
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: aperture-controller-manager
automountServiceAccountToken: true
---
# Source: aperture-controller/charts/prometheus/templates/server/cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    component: "server"
    app: prometheus
    release: my-release
    chart: prometheus-15.18.0
    heritage: Helm
  name: my-release-prometheus-server
  namespace: aperture-controller-v2.34.0.tgz
data:
  allow-snippet-annotations: "false"
  alerting_rules.yml: |
    {}
  alerts: |
    {}
  prometheus.yml: |
    global:
      evaluation_interval: 1m
      scrape_interval: 1m
      scrape_timeout: 10s
    rule_files:
    - /etc/config/recording_rules.yml
    - /etc/config/alerting_rules.yml
    - /etc/config/rules
    - /etc/config/alerts
    scrape_configs: []
  recording_rules.yml: |
    {}
  rules: |
    {}
---
# Source: aperture-controller/templates/operator-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: my-release-aperture-controller-operator
  labels:
    app.kubernetes.io/name: aperture-controller
    helm.sh/chart: aperture-controller-v2.34.0
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: aperture-controller-manager
rules:
- apiGroups:
  - admissionregistration.k8s.io
  resources:
  - validatingwebhookconfigurations
  - mutatingwebhookconfigurations
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - apps
  resources:
  - deployments
  - statefulsets
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - fluxninja.com
  resources:
  - controllers
  - policies
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - fluxninja.com
  resources:
  - controllers/finalizers
  - policies/finalizers
  verbs:
  - update
- apiGroups:
  - fluxninja.com
  resources:
  - controllers/status
  - policies/status
  verbs:
  - get
  - patch
  - update
- apiGroups:
  - ""
  resources:
  - configmaps
  - secrets
  - serviceaccounts
  - services
  - pods
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - create
  - get
  - list
  - patch
  - watch
- apiGroups:
  - coordination.k8s.io
  resources:
  - leases
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - patch
  - delete
- apiGroups:
  - rbac.authorization.k8s.io
  resources:
  - clusterrolebindings
  - clusterroles
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - policy
  resources:
  - poddisruptionbudgets
  verbs:
  - delete
  - get
  - list
---
# Source: aperture-controller/templates/operator-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: my-release-aperture-controller-operator
  labels:
    app.kubernetes.io/name: aperture-controller
    helm.sh/chart: aperture-controller-v2.34.0
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: aperture-controller-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: my-release-aperture-controller-operator
subjects:
- kind: ServiceAccount
  name: my-release-aperture-controller-operator
  namespace: aperture-controller-v2.34.0.tgz
---
# Source: aperture-controller/charts/etcd/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-etcd-headless
  namespace: "aperture-controller-v2.34.0.tgz"
  labels:
    app.kubernetes.io/name: etcd
    helm.sh/chart: etcd-8.9.0
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: client
      port: 2379
      targetPort: client
    - name: peer
      port: 2380
      targetPort: peer
  selector:
    app.kubernetes.io/name: etcd
    app.kubernetes.io/instance: my-release
---
# Source: aperture-controller/charts/etcd/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-etcd
  namespace: "aperture-controller-v2.34.0.tgz"
  labels:
    app.kubernetes.io/name: etcd
    helm.sh/chart: etcd-8.9.0
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
  annotations:
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: "client"
      port: 2379
      targetPort: client
      nodePort: null
    - name: "peer"
      port: 2380
      targetPort: peer
      nodePort: null
  selector:
    app.kubernetes.io/name: etcd
    app.kubernetes.io/instance: my-release
---
# Source: aperture-controller/charts/prometheus/templates/server/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    component: "server"
    app: prometheus
    release: my-release
    chart: prometheus-15.18.0
    heritage: Helm
  name: my-release-prometheus-server-headless
  namespace: aperture-controller-v2.34.0.tgz
spec:
  clusterIP: None
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 9090

  selector:
    component: "server"
    app: prometheus
    release: my-release
---
# Source: aperture-controller/charts/prometheus/templates/server/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    component: "server"
    app: prometheus
    release: my-release
    chart: prometheus-15.18.0
    heritage: Helm
  name: my-release-prometheus-server
  namespace: aperture-controller-v2.34.0.tgz
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 9090
  selector:
    component: "server"
    app: prometheus
    release: my-release
  sessionAffinity: None
  type: "ClusterIP"
---
# Source: aperture-controller/templates/operator-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-aperture-controller-manager
  namespace: aperture-controller-v2.34.0.tgz
  labels:
    app.kubernetes.io/name: aperture-controller
    helm.sh/chart: aperture-controller-v2.34.0
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: aperture-controller-manager
spec:
  ports:
    - name: https
      port: 8443
      protocol: TCP
      targetPort: https
    - name: webhook-server
      port: 443
      protocol: TCP
      targetPort: webhook-server
  selector:
    app.kubernetes.io/name: aperture-controller
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: aperture-controller-manager
---
# Source: aperture-controller/templates/operator-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-aperture-controller-manager
  namespace: aperture-controller-v2.34.0.tgz
  labels:
    app.kubernetes.io/name: aperture-controller
    helm.sh/chart: aperture-controller-v2.34.0
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: aperture-controller-manager
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: aperture-controller
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: aperture-controller-manager
  strategy: 
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      annotations:
        sidecar.fluxninja.com/injection: "false"
      labels:
        app.kubernetes.io/name: aperture-controller
        helm.sh/chart: aperture-controller-v2.34.0
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: aperture-controller-manager
    spec:
      
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: aperture-controller
                    app.kubernetes.io/instance: my-release
                    app.kubernetes.io/component: aperture-operator
                namespaces:
                  - "aperture-controller-v2.34.0.tgz"
                topologyKey: kubernetes.io/hostname
              weight: 1
      serviceAccountName: my-release-aperture-controller-operator
      securityContext:
        fsGroup: 1001
        runAsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
        supplementalGroups: []
      terminationGracePeriodSeconds: 10
      containers:
        - name: aperture-operator
          image: docker.io/fluxninja/aperture-operator:2.34.0
          imagePullPolicy: Always
          securityContext:
            allowPrivilegeEscalation: false
            privileged: false
            readOnlyRootFilesystem: false
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
          command:
            - /aperture-operator
            - --controller
          args:
            - --leader-elect=True
          env:
            - name: APERTURE_OPERATOR_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: APERTURE_OPERATOR_SERVICE_NAME
              value: my-release-aperture-controller-manager
          envFrom:
          ports:
            - containerPort: 9443
              name: webhook-server
              protocol: TCP
            - containerPort: 8080
              name: metrics
              protocol: TCP
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
            httpGet:
              path: /healthz
              port: 8081
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
            httpGet:
              path: /readyz
              port: 8081
          startupProbe:
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
            httpGet:
              path: /metrics
              port: metrics
          volumeMounts:
            - mountPath: /tmp/k8s-webhook-server/serving-certs
              name: cert
      volumes:
        - emptyDir: {}
          name: cert
---
# Source: aperture-controller/charts/etcd/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-etcd
  namespace: "aperture-controller-v2.34.0.tgz"
  labels:
    app.kubernetes.io/name: etcd
    helm.sh/chart: etcd-8.9.0
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: etcd
      app.kubernetes.io/instance: my-release
  serviceName: my-release-etcd-headless
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: etcd
        helm.sh/chart: etcd-8.9.0
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/managed-by: Helm
      annotations:
    spec:
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: etcd
                    app.kubernetes.io/instance: my-release
                namespaces:
                  - "aperture-controller-v2.34.0.tgz"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      serviceAccountName: "default"
      containers:
        - name: etcd
          image: docker.io/bitnami/etcd:3.5.8-debian-11-r0
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: MY_STS_NAME
              value: "my-release-etcd"
            - name: ETCDCTL_API
              value: "3"
            - name: ETCD_ON_K8S
              value: "yes"
            - name: ETCD_START_FROM_SNAPSHOT
              value: "no"
            - name: ETCD_DISASTER_RECOVERY
              value: "no"
            - name: ETCD_NAME
              value: "$(MY_POD_NAME)"
            - name: ETCD_DATA_DIR
              value: "/bitnami/etcd/data"
            - name: ETCD_LOG_LEVEL
              value: "info"
            - name: ALLOW_NONE_AUTHENTICATION
              value: "yes"
            - name: ETCD_AUTH_TOKEN
              value: "simple"
            - name: ETCD_ADVERTISE_CLIENT_URLS
              value: "http://$(MY_POD_NAME).my-release-etcd-headless.aperture-controller-v2.34.0.tgz.svc.cluster.local:2379,http://my-release-etcd.aperture-controller-v2.34.0.tgz.svc.cluster.local:2379"
            - name: ETCD_LISTEN_CLIENT_URLS
              value: "http://0.0.0.0:2379"
            - name: ETCD_INITIAL_ADVERTISE_PEER_URLS
              value: "http://$(MY_POD_NAME).my-release-etcd-headless.aperture-controller-v2.34.0.tgz.svc.cluster.local:2380"
            - name: ETCD_LISTEN_PEER_URLS
              value: "http://0.0.0.0:2380"
            - name: ETCD_AUTO_COMPACTION_MODE
              value: "periodic"
            - name: ETCD_AUTO_COMPACTION_RETENTION
              value: "24"
            - name: ETCD_CLUSTER_DOMAIN
              value: "my-release-etcd-headless.aperture-controller-v2.34.0.tgz.svc.cluster.local"
          envFrom:
          ports:
            - name: client
              containerPort: 2379
              protocol: TCP
            - name: peer
              containerPort: 2380
              protocol: TCP
          livenessProbe:
            exec:
              command:
                - /opt/bitnami/scripts/etcd/healthcheck.sh
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
          readinessProbe:
            exec:
              command:
                - /opt/bitnami/scripts/etcd/healthcheck.sh
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: data
              mountPath: /bitnami/etcd
      volumes:
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: aperture-controller/charts/prometheus/templates/server/sts.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    component: "server"
    app: prometheus
    release: my-release
    chart: prometheus-15.18.0
    heritage: Helm
  name: my-release-prometheus-server
  namespace: aperture-controller-v2.34.0.tgz
spec:
  serviceName: my-release-prometheus-server-headless
  selector:
    matchLabels:
      component: "server"
      app: prometheus
      release: my-release
  replicas: 1
  podManagementPolicy: OrderedReady
  template:
    metadata:
      labels:
        component: "server"
        app: prometheus
        release: my-release
        chart: prometheus-15.18.0
        heritage: Helm
    spec:
      enableServiceLinks: true
      serviceAccountName: my-release-prometheus-server
      containers:
        - name: prometheus-server-configmap-reload
          image: "jimmidyson/configmap-reload:v0.5.0"
          imagePullPolicy: "IfNotPresent"
          args:
            - --volume-dir=/etc/config
            - --webhook-url=http://127.0.0.1:9090/-/reload
          resources:
            {}
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
              readOnly: true

        - name: prometheus-server
          image: "quay.io/prometheus/prometheus:v2.33.5"
          imagePullPolicy: "IfNotPresent"
          args:
            - --storage.tsdb.retention.time=15d
            - --config.file=/etc/config/prometheus.yml
            - --storage.tsdb.path=/data
            - --web.console.libraries=/etc/prometheus/console_libraries
            - --web.console.templates=/etc/prometheus/consoles
            - --web.enable-remote-write-receiver
          ports:
            - containerPort: 9090
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            timeoutSeconds: 4
            failureThreshold: 3
            successThreshold: 1
          livenessProbe:
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 15
            timeoutSeconds: 10
            failureThreshold: 3
            successThreshold: 1
          resources:
            {}
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
            - name: storage-volume
              mountPath: /data
              subPath: ""
      hostNetwork: false
      dnsPolicy: ClusterFirst
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      terminationGracePeriodSeconds: 300
      volumes:
        - name: config-volume
          configMap:
            name: my-release-prometheus-server
  volumeClaimTemplates:
    - metadata:
        name: storage-volume
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: "8Gi"
---
# Source: aperture-controller/templates/post-install-hook.yaml
# We have updated the default parameters for built-in Etcd and Prometheus,
# which may leave some resources behind. This job will clean up the old
# resources. If you have customized the Etcd or Prometheus parameters, you
# should manually clean up the old resources.
apiVersion: batch/v1
kind: Job
metadata:
  name: my-release-aperture-controller-post-install-hook
  namespace: aperture-controller-v2.34.0.tgz
  labels:
    app.kubernetes.io/name: aperture-controller
    helm.sh/chart: aperture-controller-v2.34.0
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: aperture-controller-manager
  annotations:
spec:
  ttlSecondsAfterFinished: 10
  backoffLimit: 0
  template:
    metadata:
      annotations:
        sidecar.fluxninja.com/injection: "false"
      labels:
        app.kubernetes.io/name: aperture-controller
        helm.sh/chart: aperture-controller-v2.34.0
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: aperture-controller-manager
    spec:
      serviceAccountName: my-release-aperture-controller-operator
      restartPolicy: Never
      containers:
      - name: post-install-job
        image: docker.io/bitnami/kubectl:latest
        imagePullPolicy: Always
        resources:
          limits: {}
          requests: {}
        command:
          - "/bin/sh"
          - "-xc"
          - |
            kubectl delete service,deployment --selector=app.kubernetes.io/name=kube-state-metrics,app.kubernetes.io/instance=my-release,app.kubernetes.io/managed-by=Helm -n aperture-controller-v2.34.0.tgz
            kubectl delete clusterrole,clusterrolebinding,serviceaccount --selector=app.kubernetes.io/name=kube-state-metrics,app.kubernetes.io/instance=my-release,app.kubernetes.io/managed-by=Helm
            kubectl wait statefulset my-release-prometheus-server -n aperture-controller-v2.34.0.tgz --for=jsonpath='{.status.readyReplicas}'=1 --timeout=600s
            kubectl label --overwrite=true --selector=app=prometheus,release=my-release,heritage=Helm pod,service,statefulset -n aperture-controller-v2.34.0.tgz app.kubernetes.io/managed-by=Helm
            kubectl annotate --overwrite=true --selector=app=prometheus,release=my-release,heritage=Helm pod,service,statefulset -n aperture-controller-v2.34.0.tgz meta.helm.sh/release-name=my-release
            kubectl annotate --overwrite=true --selector=app=prometheus,release=my-release,heritage=Helm pod,service,statefulset -n aperture-controller-v2.34.0.tgz meta.helm.sh/release-namespace=aperture-controller-v2.34.0.tgz
            kubectl delete deployment --selector=app=prometheus,release=my-release,heritage=Helm -n aperture-controller-v2.34.0.tgz
            kubectl delete clusterrole,clusterrolebinding --selector=app=prometheus,release=my-release,heritage=Helm
            kubectl delete pdb --selector=app.kubernetes.io/name=etcd,app.kubernetes.io/instance=my-release,app.kubernetes.io/managed-by=Helm -n aperture-controller-v2.34.0.tgz
---
# Source: aperture-controller/templates/controller-cr.yaml
apiVersion: fluxninja.com/v1alpha1
kind: Controller
metadata:
  name: my-release
  namespace: aperture-controller-v2.34.0.tgz
  labels:
    app.kubernetes.io/name: aperture-controller
    helm.sh/chart: aperture-controller-v2.34.0
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: aperture-controller-manager
spec:
  image: 
    pullPolicy: IfNotPresent
    pullSecrets: []
    registry: docker.io/fluxninja
    repository: aperture-controller
    tag: "2.34.0"
  serviceAccount:
    create: true
  livenessProbe:
    enabled: true
  readinessProbe:
    enabled: true
  initContainers:
    - name: wait-for-etcd
      image: docker.io/bitnami/etcd:3.5.8-debian-11-r0
      imagePullPolicy: IfNotPresent
      resources:
        limits: {}
        requests: {}
      command:
        - 'sh'
        - '-c'
        - >
          while (etcdctl --endpoints $(yq -r '.etcd.endpoints[]' /etc/aperture/aperture-controller/config/aperture-controller.yaml) endpoint health); res=$?; [ $res != 0 ]; do
            echo "Waiting for Etcd to be Healthy";
          done;
          echo "Etcd is healthy."
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
        - mountPath: /etc/aperture/aperture-controller/config
          name: aperture-controller-config
    - name: wait-for-prometheus
      image: docker.io/linuxserver/yq:3.1.0
      imagePullPolicy: IfNotPresent
      resources:
        limits: {}
        requests: {}
      command:
        - 'sh'
        - '-c'
        - >
          while [ "$(curl -s -o /dev/null -w '%{http_code}' $(yq -r '.prometheus.address' /etc/aperture/aperture-controller/config/aperture-controller.yaml)/-/ready)" != "200" ] ; do
            echo "Waiting for Prometheus to be Ready"; sleep 2;
          done;
          echo "Prometheus is ready."
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
        - mountPath: /etc/aperture/aperture-controller/config
          name: aperture-controller-config
  config:
    etcd:
      endpoints: [http://my-release-etcd:2379]
    prometheus:
      address: http://my-release-prometheus-server:80
---
# Source: aperture-controller/templates/operator-mutatingwebhook.yaml
apiVersion: admissionregistration.k8s.io/v1
kind: MutatingWebhookConfiguration
metadata:
  name: aperture-controller-defaulter
  labels:
    app.kubernetes.io/name: aperture-controller
    helm.sh/chart: aperture-controller-v2.34.0
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: aperture-operator
webhooks:
- admissionReviewVersions:
  - v1
  clientConfig:
    service:
      name: my-release-aperture-controller-manager
      namespace: aperture-controller-v2.34.0.tgz
      path: /controller-defaulter
  failurePolicy: Fail
  name: controller-defaulter.fluxninja.com
  rules:
  - apiGroups:
    - fluxninja.com
    apiVersions:
    - v1alpha1
    operations:
    - CREATE
    - UPDATE
    resources:
    - controllers
  sideEffects: None
---
# Source: aperture-controller/templates/operator-pre-delete-hook.yaml
# Deleting the Aperture Custom Resource creating with the chart in the 'pre-delete' hook.
# The CR required the operator to gracefully delete all the resources and if the chart is
# deleted before deleting the CR, it won't get cleaned up properly.
apiVersion: batch/v1
kind: Job
metadata:
  name: my-release-aperture-controller-hook
  namespace: aperture-controller-v2.34.0.tgz
  labels:
    app.kubernetes.io/name: aperture-controller
    helm.sh/chart: aperture-controller-v2.34.0
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: aperture-controller-manager
  annotations:
    "helm.sh/hook": pre-delete
    "helm.sh/hook-weight": "-5"
    "helm.sh/hook-delete-policy": hook-succeeded
spec:
  ttlSecondsAfterFinished: 10
  backoffLimit: 0
  template:
    metadata:
      annotations:
        sidecar.fluxninja.com/injection: "false"
      labels:
        app.kubernetes.io/name: aperture-controller
        helm.sh/chart: aperture-controller-v2.34.0
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: aperture-controller-manager
    spec:
      serviceAccountName: my-release-aperture-controller-operator
      restartPolicy: Never
      containers:
      - name: pre-delete-job
        image: docker.io/bitnami/kubectl:latest
        imagePullPolicy: Always
        resources:
          limits: {}
          requests: {}
        command:
          - "/bin/sh"
          - "-xc"
          - "kubectl delete controller my-release -n aperture-controller-v2.34.0.tgz; while (kubectl get controller my-release -n aperture-controller-v2.34.0.tgz); ret=$?; [ $ret -eq 0 ]; do echo deleting; sleep 2; done"
