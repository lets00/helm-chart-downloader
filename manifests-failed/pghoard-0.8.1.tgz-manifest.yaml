---
# Source: pghoard/templates/secret.yaml
# XXX Not maintained, see wiremind/template/pghoard_secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-release-pghoard
  labels:
    app: pghoard
    chart: pghoard-0.8.1
    release: my-release
    heritage: Helm
data:
  PG_HOST: Q0hBTkdFTUU=
  PG_PORT: "NTQzMg=="
  PG_USER: Q0hBTkdFTUU=
  PG_PASSWORD: "Q0hBTkdFTUU="
---
# Source: pghoard/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-pghoard
  labels:
    heritage: Helm
    release: my-release
    chart: pghoard-0.8.1
    app: pghoard
data:
  PGHOARD_STORAGE_TYPE: "local"
  PGHOARD_BASEBACKUP_COUNT: "30"
  PGHOARD_BASEBACKUP_HOUR: "4"
  PGHOARD_BASEBACKUP_INTERVAL_HOURS: "24"
  PGHOARD_DIRECTORY: /var/lib/pghoard
  PGHOARD_ACTIVE_BACKUP_MODE: pg_receivexlog
  # docker-pghoard creates a "site" (i.e backup) named $HOSTNAME, so we hijack it so that site name is stable
  HOSTNAME: my-release-pghoard
---
# Source: pghoard/templates/service-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-pghoard-headless
  labels:
    app.kubernetes.io/name: pghoard
    helm.sh/chart: pghoard-0.8.1
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: backup
spec:
  type: ClusterIP
  clusterIP: None
  ports:
  - name: pghoard
    port: 16000
    targetPort: pghoard
  selector:
    app.kubernetes.io/name: pghoard
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: backup
---
# Source: pghoard/templates/deployment-restore.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-pghoard-restore
  labels:
    app.kubernetes.io/name: pghoard
    helm.sh/chart: pghoard-0.8.1
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: restore
spec:
  replicas: 0
  selector:
    matchLabels:
      app.kubernetes.io/name: pghoard
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: restore
  template:
    metadata:
      labels:
        app.kubernetes.io/name: pghoard
        helm.sh/chart: pghoard-0.8.1
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: restore
        my-release-postgresql-client: "true"  # Allows postgresql netpol, from the postgresql helm chart, to accept pghoard
      annotations:
        checksum/config: 91e50f9a14fb4bcc82f3cc59b6e20e8b9519fc88b73e2e31a3708e8670da0314
    spec:
      containers:
        - name: pghoard-restore
          image: "wiremind/pghoard:12-2019-11-26"
          command: ['/bin/bash', '/restore.sh']
          imagePullPolicy: IfNotPresent
          env:
            - name: PGHOARD_RESTORE_SITE
              value: my-release-pghoard
          envFrom:
          - secretRef:
              name: my-release-pghoard
          - configMapRef:
              name: my-release-pghoard
          resources:
            {}
          volumeMounts:
            - name: pghoard
              mountPath: /var/lib/pghoard
      restartPolicy: Always
      volumes:
        - name: pghoard
          # Note: see https://github.com/kubernetes/kubernetes/issues/60903
          persistentVolumeClaim:
            claimName: data-my-release-pghoard-0
            # pghoard in restore mode will write a satusfile in the site directory.
            # XXX for integrity reasons, this should be set to true. We do not want a restore to mess with our backup.
            readOnly: false
      affinity:
        # Force to be on same node than backup in order to be able to mount Persistent Volume
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/name: pghoard
                app.kubernetes.io/instance: my-release
                app.kubernetes.io/component: backup
            topologyKey: kubernetes.io/hostname
---
# Source: pghoard/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-pghoard
  labels:
    app.kubernetes.io/name: pghoard
    helm.sh/chart: pghoard-0.8.1
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: backup
spec:
  replicas: 1
  serviceName: my-release-pghoard-headless
  selector:
    matchLabels:
      app.kubernetes.io/name: pghoard
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: backup
  template:
    metadata:
      labels:
        app.kubernetes.io/name: pghoard
        helm.sh/chart: pghoard-0.8.1
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: backup
        my-release-postgresql-client: "true"  # Allows postgresql netpol, from the postgresql helm chart, to accept pghoard
      annotations:
        checksum/config: 91e50f9a14fb4bcc82f3cc59b6e20e8b9519fc88b73e2e31a3708e8670da0314
    spec:
      containers:
        - name: pghoard
          image: "wiremind/pghoard:12-2019-11-26"
          imagePullPolicy: IfNotPresent
          envFrom:
          - secretRef:
              name: my-release-pghoard
          - configMapRef:
              name: my-release-pghoard
          resources:
            {}
          volumeMounts:
          - name: data
            mountPath: /var/lib/pghoard
          
          ports:
          - name: pghoard
            containerPort: 16000
      restartPolicy: Always
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "1Gi"
---
# Source: pghoard/templates/crontab-restore-autocheck.yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: my-release-pghoard-check
  labels:
    app.kubernetes.io/name: pghoard
    helm.sh/chart: pghoard-0.8.1
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: restore-autocheck
spec:
  schedule: 0 12 * * *
  startingDeadlineSeconds: 600
  concurrencyPolicy: "Forbid"
  failedJobsHistoryLimit: 1
  successfulJobsHistoryLimit: 1
  jobTemplate:
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            app.kubernetes.io/name: pghoard
            helm.sh/chart: pghoard-0.8.1
            app.kubernetes.io/instance: my-release
            app.kubernetes.io/managed-by: Helm
            app.kubernetes.io/component: restore-autocheck
            my-release-postgresql-client: "true"  # Allows postgresql netpol, from the postgresql helm chart, to accept pghoard
          annotations:
            checksum/config: 91e50f9a14fb4bcc82f3cc59b6e20e8b9519fc88b73e2e31a3708e8670da0314
        spec:
          containers:
            - name: pghoard-restore
              image: "wiremind/pghoard:12-2019-11-26"
              command: ['/bin/bash', '/restore.sh']
              imagePullPolicy: IfNotPresent
              env:
                - name: PGHOARD_RESTORE_SITE
                  value: my-release-pghoard
                - name: RESTORE_CHECK_COMMAND
                  value: "SELECT * FROM my_table;"
              envFrom:
              - secretRef:
                  name: my-release-pghoard
              - configMapRef:
                  name: my-release-pghoard
              volumeMounts:
                - name: pghoard
                  mountPath: /var/lib/pghoard
          restartPolicy: Never
          volumes:
            - name: pghoard
              # Note: see https://github.com/kubernetes/kubernetes/issues/60903
              persistentVolumeClaim:
                claimName: data-my-release-pghoard-0
                # pghoard in restore mode will write a satusfile in the site directory.
                # XXX for integrity reasons, this should be set to true. We do not want a restore to mess with our backup.
                readOnly: false
          affinity:
            # Force to be on same node than backup in order to be able to mount Persistent Volume
            podAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
              - labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: pghoard
                    app.kubernetes.io/instance: my-release
                    app.kubernetes.io/component: backup
                topologyKey: kubernetes.io/hostname
