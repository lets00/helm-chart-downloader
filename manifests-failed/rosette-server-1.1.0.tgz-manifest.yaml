---
# Source: rosette-server/templates/hooks/sa-patch-deployment.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-rosette-server-patch-deployment-sa
  labels:
    helm.sh/chart: rosette-server-1.1.0
    app.kubernetes.io/name: rosette-server
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.29.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: rosette-server/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-rosette-server
  labels:
    helm.sh/chart: rosette-server-1.1.0
    app.kubernetes.io/name: rosette-server
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.29.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: rosette-server/templates/cm-conf.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-rosette-server-conf
  labels:
    helm.sh/chart: rosette-server-1.1.0
    app.kubernetes.io/name: rosette-server
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.29.0"
    app.kubernetes.io/managed-by: Helm
data:
  java_opts.conf: ""
  log4j2.xml: |
    <?xml version="1.0" encoding="UTF-8"?>
    <Configuration monitorInterval="60">
        <Properties>
            <Property name="log-path">../logs</Property>
            <Property name="archive">${log-path}/archive</Property>
        </Properties>
        <Appenders>
            <Console name="Console-Appender" target="SYSTEM_OUT">
                <PatternLayout>
                    <pattern>
                        [%-5level] %d{yyyy-MM-dd HH:mm:ss.SSS} [%t] %c - %msg%n
                    </pattern>>
                </PatternLayout>
            </Console>
            <RollingFile name="File-Appender"
                         fileName="${log-path}/rosapi.log"
                         filePattern="${archive}/rosapi.log.%d{yyyy-MM-dd-hh-mm}.gz">
                <PatternLayout pattern="[%-5level] %d{yyyy-MM-dd HH:mm:ss.SSS} [%t] %c - %msg%n"/>
                <Policies>
                    <SizeBasedTriggeringPolicy size="30 MB"/>
                </Policies>
                <DefaultRolloverStrategy max="30"/>
            </RollingFile>
    <!--
            <RollingFile name="Request-Appender"
                         fileName="${log-path}/request-tracker.log"
                         filePattern="${archive}/request-tracker.log.%d{yyyy-MM-dd-hh-mm}.gz">
                <PatternLayout pattern="[%-5level] %d{yyyy-MM-dd HH:mm:ss.SSS} [%t] %c - %msg%n"/>
                <Policies>
                    <SizeBasedTriggeringPolicy size="30 MB"/>
                </Policies>
                <DefaultRolloverStrategy max="30"/>
            </RollingFile>
    -->
            <RollingFile name="500-Exception-Appender"
                         fileName="${log-path}/500-exception.log"
                         filePattern="${archive}/500-exception.log.%d{yyyy-MM-dd-hh-mm}.gz">
                <PatternLayout pattern="[%-5level] %d{yyyy-MM-dd HH:mm:ss.SSS} [%t] %c - %msg%n"/>
                <RegexFilter regex=".*Exception processing ticket.*" onMatch="ACCEPT" onMismatch="DENY"/>
                <Policies>
                    <SizeBasedTriggeringPolicy size="30 MB"/>
                </Policies>
                <DefaultRolloverStrategy max="30"/>
            </RollingFile>
        </Appenders>
        <Loggers>
            <Root level="warn">
                <AppenderRef ref="File-Appender" level="warn"/>
                <AppenderRef ref="Console-Appender" level="warn"/>
            </Root>
            <Logger name="org.eclipse.jetty.server.handler" level="error"/>
    <!--
            <Logger name="com.basistech.ws.logrequesttracker" level="info" additivity="false">
                <AppenderRef ref="Request-Appender"/>
            </Logger>
    -->
            <Logger name="com.basistech.ws" level="info">
                <AppenderRef ref="500-Exception-Appender" level="info"/>
            </Logger>
        </Loggers>
    </Configuration>
  logging.properties: |
    org.apache.aries.spifly = ERROR
    org.apache.cxf.bus.blueprint = ERROR
  wrapper-license.conf: |
    #encoding=UTF-8
    wrapper.license.type=DEV
    wrapper.license.id=202309140000048
    wrapper.license.licensee=Basis Technology Corp.
    wrapper.license.group=Rosette API
    wrapper.license.dev_application=Rosette Server Edition
    wrapper.license.features=pro, 64bit
    wrapper.license.upgrade_term.begin_date=2010-11-09
    wrapper.license.upgrade_term.end_date=2024-11-09
    wrapper.license.key.1=ea46-e866-05ea-db35
    wrapper.license.key.2=1775-b49e-0ec4-9726
    wrapper.license.key.3=85e3-6f8b-fe69-bf92
    wrapper.license.key.4=8deb-59b8-6a2d-2fd3
  wrapper.conf: |
    #encoding=UTF-8
    # Configuration files must begin with a line specifying the encoding
    #  of the the file.
    #********************************************************************
    # Wrapper License Properties (Ignored by Community Edition)
    #********************************************************************
    # Professional and Standard Editions of the Wrapper require a valid
    #  License Key to start.  Licenses can be purchased or a trial license
    #  requested on the following pages:
    # http://wrapper.tanukisoftware.com/purchase
    # http://wrapper.tanukisoftware.com/trial
    # Include file problems can be debugged by leaving only one '#'
    #  at the beginning of the following line:
    ##include.debug
    # The Wrapper will look for either of the following optional files for a
    #  valid License Key.  License Key properties can optionally be included
    #  directly in this configuration file.
    #include ../conf/wrapper-license.conf
    #include ../conf/wrapper-license-%WRAPPER_HOST_NAME%.conf
    # The following property will output information about which License Key(s)
    #  are being found, and can aid in resolving any licensing problems.
    #wrapper.license.debug=TRUE
    #********************************************************************
    # Wrapper Localization
    #********************************************************************
    # Specify the language and locale which the Wrapper should use.
    wrapper.lang=en_US # en_US or ja_JP
    # Specify the location of the language resource files (*.mo).
    wrapper.lang.folder=../lang
    #********************************************************************
    # Wrapper Java Properties
    #********************************************************************
    # Java Application
    #  Locate the java binary on the system PATH:
    wrapper.java.command=%JAVA_HOME%/bin/java
    #  Specify a specific java binary:
    #set.JAVA_HOME=/java/path
    #wrapper.java.command=%JAVA_HOME%/bin/java
    # Tell the Wrapper to log the full generated Java command line.
    wrapper.java.command.loglevel=INFO
    #********************************************************************
    # Basis Technology Customizations
    #********************************************************************
    # Java Main class.  This class must implement the WrapperListener interface
    #  or guarantee that the WrapperManager class is initialized.  Helper
    #  classes are provided to do this for you.
    #  See the following page for details:
    #  http://wrapper.tanukisoftware.com/doc/english/integrate.html
    wrapper.java.mainclass=com.basistech.ws.launcher.tanuki.RosapiTanukiLauncher
    # Log level for notices about missing Java Classpath entries.
    wrapper.java.classpath.missing.loglevel=WARN
    # Java Classpath (include wrapper.jar)  Add class path elements as
    #  needed starting from 1
    wrapper.java.classpath.1=../framework/*.jar
    wrapper.java.classpath.2=../lib/wrapper.jar
    # Java Library Path (location of Wrapper.DLL or libwrapper.so)
    wrapper.java.library.path.1=../lib
    # Java Bits.  On applicable platforms, tells the JVM to run in 32 or 64-bit mode.
    #wrapper.java.additional.auto_bits=TRUE
    #********************************************************************
    # Basis Technology Customizations
    #********************************************************************
    # Rosette Enterprise will ignore 'rosapi roots' by spotting the existence of the 'roots' directory.
    # Java Additional Parameters
    wrapper.java.additional.101=-Drosapi.roots=%ROSAPI_ROOTS%
    wrapper.java.additional.102=-Dcom.sun.management.jmxremote
    wrapper.java.additional.103=-Drosapi.config=../launcher/config
    wrapper.java.additional.104=-Drosapi.port=8181
    # Cloud-specific (2xx) customizations are added to the bottom this file.
    # WS-2248
    wrapper.java.additional.302=-Dorg.bytedeco.javacpp.maxPhysicalBytes=0
    # WS-2622: disable crypto-policies alignment
    # https://access.redhat.com/documentation/en-us/openjdk/17/html/configuring_openjdk_17_on_rhel_with_fips/config-fips-in-openjdk
    wrapper.java.additional.331=-Djava.security.disableSystemPropertiesFile=true
    # Logging (6xx)
    wrapper.java.additional.600=-Drosapi.framework.logger=OFF
    wrapper.java.additional.601=-Djava.util.logging.config.file=../conf/logging.properties
    wrapper.java.additional.602=-Djorg.eclipse.jetty.LEVEL=OFF
    wrapper.java.additional.603=-Dlog4j.configurationFile=../conf/log4j2.xml
    # Suppress warnings in Java 11 runtime.  Only activate with Java 9 or higher.  Remove after WS-2440
    wrapper.java.additional.711.java_version.min=9
    wrapper.java.additional.711=--add-opens=java.base/java.lang=ALL-UNNAMED
    wrapper.java.additional.712.java_version.min=9
    wrapper.java.additional.712=--add-opens=java.base/java.net=ALL-UNNAMED
    # Implicitly flush stdout after each line of output is sent to the console.
    # If piping the console output of the Wrapper process into another application (eg. docker compose)
    # the output won't flush in real time with FALSE
    wrapper.console.flush=TRUE
    #********************************************************************
    # Basis Technology Debugging
    #********************************************************************
    # Uncomment to enable remote debugging
    #wrapper.java.additional.701=-Xdebug -Xnoagent -Djava.compiler=NONE
    #wrapper.java.additional.702=-Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=5005
    # Consult with http://docs.oracle.com/javase/8/docs/technotes/guides/management/agent.html
    # and configure remote monitoring and management accordingly.
    #wrapper.java.additional.801=-Dcom.sun.management.jmxremote.port=1099
    # ...
    # if the below isn't set to true, the wrapper will assume process is hung while debugging
    #wrapper.java.detect_debug_jvm=true
    #********************************************************************
    # End
    #********************************************************************
    # Initial Java Heap Size (in MB)
    #wrapper.java.initmemory=3
    wrapper.java.initmemory=8192
    # Maximum Java Heap Size (in MB)
    #wrapper.java.maxmemory=64
    wrapper.java.maxmemory=16384
    # Number of seconds to allow between the time that the Wrapper launches the JVM process
    # The default value is "30 seconds".
    wrapper.startup.timeout=300
    #include ../conf/wrapper-override.conf
    #********************************************************************
    # Wrapper Logging Properties
    #********************************************************************
    # Enables Debug output from the Wrapper.
    # wrapper.debug=TRUE
    # Format of output for the console.  (See docs for formats)
    wrapper.console.format=PM
    # Log Level for console output.  (See docs for log levels)
    wrapper.console.loglevel=INFO
    # Log file to use for wrapper output logging.
    wrapper.logfile=../logs/wrapper.log
    # Format of output for the log file.  (See docs for formats)
    wrapper.logfile.format=LPTM
    # Log Level for log file output.  (See docs for log levels)
    wrapper.logfile.loglevel=INFO
    # Maximum size that the log file will be allowed to grow to before
    #  the log is rolled. Size is specified in bytes.  The default value
    #  of 0, disables log rolling.  May abbreviate with the 'k' (kb) or
    #  'm' (mb) suffix.  For example: 10m = 10 megabytes.
    wrapper.logfile.maxsize=10m
    # Maximum number of rolled log files which will be allowed before old
    #  files are deleted.  The default value of 0 implies no limit.
    wrapper.logfile.maxfiles=5
    # Log Level for sys/event log output.  (See docs for log levels)
    wrapper.syslog.loglevel=NONE
    #********************************************************************
    # Wrapper General Properties
    #********************************************************************
    # Allow for the use of non-contiguous numbered properties
    wrapper.ignore_sequence_gaps=TRUE
    # Do not start if the pid file already exists.
    #wrapper.pidfile.strict=TRUE
    # Title to use when running as a console
    wrapper.console.title=Rosette Server Edition
    #********************************************************************
    # Wrapper JVM Checks
    #********************************************************************
    # Detect DeadLocked Threads in the JVM. (Requires Standard Edition)
    wrapper.check.deadlock=TRUE
    wrapper.check.deadlock.interval=60
    wrapper.check.deadlock.action=RESTART
    wrapper.check.deadlock.output=FULL
    # Out Of Memory detection.
    #  Ignore -verbose:class output to avoid false positives.
    #wrapper.filter.trigger.1000=[Loaded java.lang.OutOfMemoryError
    #wrapper.filter.action.1000=NONE
    # (Simple match)
    #wrapper.filter.trigger.1001=java.lang.OutOfMemoryError
    # (Only match text in stack traces if -XX:+PrintClassHistogram is being used.)
    #wrapper.filter.trigger.1001=Exception in thread "*" java.lang.OutOfMemoryError
    #wrapper.filter.allow_wildcards.1001=TRUE
    #wrapper.filter.action.1001=RESTART
    #wrapper.filter.message.1001=The JVM has run out of memory.
    #********************************************************************
    # Basis Technology Customizations
    #********************************************************************
    # interferes with license check at top, not really wanted. Karaf should not exit.
    #wrapper.on_exit.default=RESTART
    wrapper.on_exit.3=SHUTDOWN
    # Ping timeout / interval for detecting frozen JVM
    wrapper.ping.timeout=310
    wrapper.ping.interval=10
    wrapper.ping.timeout.action=DEBUG
    #********************************************************************
    # Wrapper Email Notifications. (Requires Professional Edition)
    #********************************************************************
    # Common Event Email settings.
    #wrapper.event.default.email.debug=TRUE
    #wrapper.event.default.email.smtp.host=<SMTP_Host>
    #wrapper.event.default.email.smtp.port=25
    #wrapper.event.default.email.subject=[%WRAPPER_HOSTNAME%:%WRAPPER_NAME%:%WRAPPER_EVENT_NAME%] Event Notification
    #wrapper.event.default.email.sender=<Sender email>
    #wrapper.event.default.email.recipient=<Recipient email>
    # Configure the log attached to event emails.
    #wrapper.event.default.email.maillog=ATTACHMENT
    #wrapper.event.default.email.maillog.lines=50
    #wrapper.event.default.email.maillog.format=LPTM
    #wrapper.event.default.email.maillog.loglevel=INFO
    # Enable specific event emails.
    #wrapper.event.wrapper_start.email=TRUE
    #wrapper.event.jvm_prelaunch.email=TRUE
    #wrapper.event.jvm_start.email=TRUE
    #wrapper.event.jvm_started.email=TRUE
    #wrapper.event.jvm_deadlock.email=TRUE
    #wrapper.event.jvm_stop.email=TRUE
    #wrapper.event.jvm_stopped.email=TRUE
    #wrapper.event.jvm_restart.email=TRUE
    #wrapper.event.jvm_failed_invocation.email=TRUE
    #wrapper.event.jvm_max_failed_invocations.email=TRUE
    #wrapper.event.jvm_kill.email=TRUE
    #wrapper.event.jvm_killed.email=TRUE
    #wrapper.event.jvm_unexpected_exit.email=TRUE
    #wrapper.event.wrapper_stop.email=TRUE
    # Specify custom mail content
    #wrapper.event.jvm_restart.email.body=The JVM was restarted.\n\nPlease check on its status.\n
    #********************************************************************
    # Wrapper Windows Service Properties
    #********************************************************************
    # WARNING - Do not modify any of these properties when an application
    #  using this configuration file has been installed as a service.
    #  Please uninstall the service before modifying this section.  The
    #  service can then be reinstalled.
    # TODO: These won't be right for worker versus front-end, but Windows is not a short-term concern.
    # Name of the service
    wrapper.name=rosapi-launcher
    # Display name of the service
    wrapper.displayname=Rosette Server Edition
    # Description of the service
    wrapper.description=Rosette Server Edition
    # Service dependencies.  Add dependencies as needed starting from 1
    wrapper.ntservice.dependency.1=
    # Mode in which the service is installed.  AUTO_START, DELAY_START or DEMAND_START
    wrapper.ntservice.starttype=AUTO_START
    # Allow the service to interact with the desktop (Windows NT/2000/XP only).
    wrapper.ntservice.interactive=FALSE
    # Allow the current user to perform certain actions without being prompted for
    #  administrator credentials. (Requires Professional Edition)
    #wrapper.ntservice.permissions.1.account=CURRENT_USER
    #wrapper.ntservice.permissions.1.allow=START, STOP, PAUSE_RESUME
---
# Source: rosette-server/templates/cm-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-rosette-server-config
  labels:
    helm.sh/chart: rosette-server-1.1.0
    app.kubernetes.io/name: rosette-server
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.29.0"
    app.kubernetes.io/managed-by: Helm
data:
  com.basistech.downloadextract.cfg: |
    # downloaderExtractorConfigFilePathname is the pathname of downloader extractor configuration file.
    downloaderExtractorConfigFilePathname=${rosapi.config}/rosapi/downloaderExtractor.yaml
    # constraintsPathname is the pathname of constraints file.
    constraintsPathname=${rosapi.config}/rosapi/constraints.yaml
  com.basistech.rli.cfg: |
    # INTERNAL USE ONLY
    # This file compensates for the fact that the RLI factory config does not allow a default value for some options.
    # shortStringThreshold is the the minimum length of a long string, for short string processing.
    # The shortStringThreshold does not support as many languages as the long string algorithm.
    # values: change only if directed by engineering
    shortStringThreshold=141
  com.basistech.ws.addresses.cfg: |-
    # maxFieldLength is the maximum length of a field in an address to process
    # values: default is 500
    #maxFieldLength=500
  com.basistech.ws.cxf.cfg: |2
  
    # urlBase is the host and port for the apache cxf
    # values: must match the host/port entries in org.apache.cxf.http.jetty-main.cfg
    urlBase=http://0.0.0.0:${rosapi.port}/rest
  com.basistech.ws.dedupe.cfg: |
    # This file compensates for the fact that the RNI factory config does not allow a default value for this option.
    # maxNameLength sets the maximum name length for name deduplication
    # values: 500 is the recommended, and default, value.
    #maxNameLength=500
  com.basistech.ws.doc.cfg: |
    # docRoot is the path to the documentation
    docRoot=${rosapi.config}/../../doc
    # enable turns on the document server
    # values = true|false
    enable=true
  com.basistech.ws.fe.health.cfg: |
    # INTERNAL USE ONLY
    # outstandingRequestThreshold sets the maximum number of outstanding requests allowed before 'unhealthy' is returned.
    outstandingRequestThreshold=500
    # cloudwatchReport enables cloudwatch reporting of the total number of outstanding requests.
    #cloudwatchReport=true
    # cloudwatchReportInterval specifies the reporting interval.
    # This only applies if cloudwatchReport is true
    #cloudwatchReportInterval=PT1M (default)
    # asyncReponseTimeoutMs sets a timeout value for checking callback failure.
    # If for whatever reason the callback failed to show up, we would like to time it out
    # so that the queue size is decremented accordingly.
    # values: Make this slightly bigger than worker ELB timeout when deployed in AWS
    asyncResponseTimeoutMs=310000
  com.basistech.ws.frontend.cfg: |
    # INTERNAL USE ONLY
    # bindingVersions specifies the range of versions accepted from the client in [maven format](https://maven.apache.org/enforcer/enforcer-rules/versionRanges.html).
    bindingVersions=[0.10,2.0)
    # urlPrefix is appended to the CXF service prefix. We currently set this parameter to `/v1` and the CXF parameter to `/rest`, so that all of the front end URLs start with `/rest/v1`.
    urlPrefix=/v1
    # constraintsPathname is the pathname of the file defining request constraints
    constraintsPathname=${rosapi.config}/rosapi/constraints.yaml
    # profile data root folder that may contain profile-id/{rex,tcat} etc
    # Caveat:  The folder structure beneath the root will be different if rosapi.feature.CUSTOM_PROFILE_UNDER_APP_ID is active.
    #profile-data-root=file:///<where custom roots live>
  com.basistech.ws.local.usage.tracker.cfg: |
    # INTERNAL USE ONLY
    # suppress disables local usage tracker, default is false
    #suppress: true
    # report interval in minutes, default is 1 minute
    #reportInterval: 1
    # Usage tracker root customizes the location of the rosette-usage.yaml file
    # default location is  <rosette>/server/launcher/config.
    # Change it to preferred location. Example: /var/log
    #usage-tracker-root: /var/log
    # enabled is a new flag that can be used to enable/disable local usage tracking.
    # the suppress flag will be deprecated in a later release
    # default is true.
    # enabled: true
  com.basistech.ws.metrics.prometheus.cfg: ""
  com.basistech.ws.requesttracker.cfg: |
    # INTERNAL USE ONLY
    #
    # set mongoUri to a valid uri to enable request logging, it's recommend to use w=-1 for async logging
    # that doesn't block the response to the caller
    #   see http://docs.mongodb.org/manual/reference/connection-string/ for details
    #
    # mongoUri = mongodb://{host}:{port}/{raas_req_db}.{raas_req_collection}?w=-1&maxPoolSize=300
    # level    = none | partial | all
    #
    # For example
    # mongoUri is the address of the mongo instance
    #mongoUri=mongodb://localhost:27017/requestTrackerDb.requestTrackerCollection?w=-1&maxPoolSize=300
    # trackerLevel indicates what should be tracked
    # values: none | partial | all
    #trackerLevel=all
  com.basistech.ws.rni.cfg: |
    # This file compensates for the fact that the RNI factory config does not allow a default value for this option.
    # maxNameLength is the maximum length of a name to process
    # values: default is 500
    #maxNameLength=500
  com.basistech.ws.rnt.cfg: |
    # This file compensates for the fact that the RNI factory config does not allow a default value for this option.
    # maxNameLength is the maximum length of a name to process
    # values: default is 500
    #maxNameLength=500
  com.basistech.ws.tracker.logstash.cfg: |
    # logstashAddress is the host:port for the logstash tcp input plugin.
    #logstashAddress=
    # maxPendingMessages is he size of the blocking queue that stores the messages for transit. If this fills, new messages spill into the log.
    # Default is 10000
    #maxPendingMessages=
  com.basistech.ws.transport.embedded.cfg: |
    # DEPRECATED
    # Please configure workerThreadCount in com.basistech.ws.worker.cfg
  com.basistech.ws.transport.http.cfg: |
    # INTERNAL USE ONLY
    # rulesPathname is the pathname to transport rules
    rulesPathname=${rosapi.config}/rosapi/transport-rules.tsv
    # There's a desire to make this shorter to bail on things that have gotten stuck.
    # workerTimeout is the amount of time to allow a worker to complete a ticket before timing out and failing it. This is in ISO format, see the javadoc for `Duration#parse`. `PT30S` is '30 seconds.'
    workerTimeout=PT310S
    # Flow control parameters
    # workQueue.initialMaxOutstanding is for each distinct worker URL, the front-end will only allow this many requests to be pending.
    workQueue.initialMaxOutstanding=10
    # workQueue.decreaseThreshold controls the maxoutstanding value. If the average queue depth for a worker grows above this number, the 'maxOutstanding' value shrinks by decreaseAmount.
    workQueue.decreaseThreshold=20
    # workQueue.decreaseAmount controls the max number of outstanding requests by indicating how much to decrease the max number of outstanding requests when the queue size hits the 'decreaseThreshold'.
    workQueue.decreaseAmount=10
    # workQueue.increaseThreshold defines the max outstanding limit lower limit. If the queue size drops below this value, the max outstanding limit is increased by increaseAmount.
    workQueue.increaseThreshold=5
    # workQueue.increaseAmount controls how much to increase the max number of outstanding requests when the queue size falls to the increaseThreshold.
    workQueue.increaseAmount=10
    # workQueue.averageWindowTime controls the window time interval for averaging the queue size over this time interval, in ISO format.
    workQueue.averageWindowTime=PT1M
    # workQueue.adaptIntervalTime controls how often to compare the average queue size to the thresholds.
    workQueue.adaptIntervalTime=PT1M
    # maxHttpConnectionsPerUrl is the Maximum HTTP Connections Per URL default (1000).
    maxHttpConnectionsPerUrl=1000
    # Retry parameters
    # maxRetries is the number of maximum retries.
    maxRetries=3
    # maxRetryDelay is the maximum delay for retry. If it is longer than the delay, don't retry.
    maxRetryDelay=PT130S
    # retryBackoffMultiple is a factor to multiply with retryCount and retryBackoffTime to delay the retry.
    retryBackoffMultiple=2
    # retryBackoffTime is the etry backoff time.
    retryBackoffTime=PT0.05S
    # connectionTimeToLive controls how long HttpClient connections should be kept, default to 1-day given we have a custom resolver to monitor DNS changes now
    connectionTimeToLive=P1D
    # dataFormat is the transport serialization format. Values can be SMILE (default) or JSON.
    #dataFormat=SMILE
  com.basistech.ws.worker.cfg: |
    # configurationFilePathname is the pathname to the worker service configuration yaml.
    configurationFilePathname=${rosapi.config}/rosapi/worker-config.yaml
    # <component>-root is the pathname to the root directory for various component.
    rbl-root=${rosapi.roots}/rbl/7.47.2.c73.0
    rct-root=${rosapi.roots}/rct/3.0.18.c73.0
    ascent-root=${rosapi.roots}/ascent/3.0.0.c73.0
    tcat-root=${rosapi.roots}/tcat/3.0.0.c73.0
    rli-root=${rosapi.roots}/rli/7.23.12.c73.0
    rex-root=${rosapi.roots}/rex/7.55.11.c73.0
    flinx-root=${rosapi.roots}/rex/7.55.11.c73.0/flinx
    relax-root=${rosapi.roots}/relax/4.0.0.c73.0
    dp-root=${rosapi.roots}/nlp4j/2.0.0.c73.0
    rni-rnt-root=${rosapi.roots}/rni-rnt/7.45.0.c73.0
    tvec-root=${rosapi.roots}/tvec/7.0.0.c73.0
    topics-root=${rosapi.roots}/topics/3.0.0.c73.0
    # healthCheckQueueSizeThreshold is used for ELB health check, change the default as desired. Default is 20.
    #healthCheckQueueSizeThreshold=20
    # workerThreadTimeout is the timeout value for worker threads. To avoid diminished return or even degradation
    # It is recommended to keep the workerThreadTimeout < the maximum session (HTTP) timeout in RESTful.  In
    # embedded mode, the value is entirely up to the user.
    workerThreadTimeout=PT5M
    # workerThreadCount is the number of threads in the worker that are created to do the actual work. Default is 2.
    # it is probably best to not go above 2-3x the number of physical cores on the host machine.
    #workerThreadCount=2
    # cloudwatchReport indicates whether queue depth information will be reported by the worker to cloudwatch.
    #cloudwatchReport=false
    # cloudwatchReportInterval is the time interval in between reports to cloudwatch. The default is 'PT1M' -- report once per minute.
    #cloudwatchReportInterval=PT1M
    # warm up worker upon activation. The default is false.
    warmUpWorker=false
    # profile data root folder that may contain profile-id/{rex,tcat} etc
    # Caveat:  The folder structure beneath the root will be different if rosapi.feature.CUSTOM_PROFILE_UNDER_APP_ID is active.
    #profile-data-root=file:///<where custom roots live>
    # download and text extractor
    enableDte=true
    # profileDeletionMonitorInterval controls how often to check for a profile deletion. The default is PT5S.
    # Only used when profile-data-root is specified.
    profileDeletionMonitorInterval=PT5S
    # overrideEndpointsPathname declares an endpoint override yaml file allowing for selectively turning on/off specific endpoints. This may be used to aid in debugging processes.
    #overrideEndpointsPathname=${rosapi.config}/endpoint-override.yaml
  endpoint-override.yaml: '# Use enabledEndpoints list instead'
  jetty-ssl-config.xml: |-
    <beans xmlns="http://www.springframework.org/schema/beans"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           xmlns:http="http://cxf.apache.org/transports/http/configuration"
           xmlns:httpj="http://cxf.apache.org/transports/http-jetty/configuration"
           xmlns:sec="http://cxf.apache.org/configuration/security"
           xsi:schemaLocation="
            http://www.springframework.org/schema/beans                 http://www.springframework.org/schema/beans/spring-beans.xsd
            http://cxf.apache.org/transports/http/configuration         http://cxf.apache.org/schemas/configuration/http-conf.xsd
            http://cxf.apache.org/transports/http-jetty/configuration   http://cxf.apache.org/schemas/configuration/http-jetty.xsd
            http://cxf.apache.org/configuration/security                http://cxf.apache.org/schemas/configuration/security.xsd">
        <httpj:engine-factory id="rosette-server-engine-config">
            <httpj:engine port="#{ systemProperties['rosapi.port'] }">
                <httpj:tlsServerParameters>
                    <sec:clientAuthentication required="false" />
                    <sec:keyManagers keyPassword="[key-pass]">
                        <sec:keyStore type="JKS" password="[keystore-pass]"
                                      file="path/to/keystore.jks"/>
                    </sec:keyManagers>
                    <sec:trustManagers>
                        <sec:keyStore type="JKS" password="[truststore-pass]"
                                      file="path/to/truststore.jks"/>
                    </sec:trustManagers>
                </httpj:tlsServerParameters>
            </httpj:engine>
        </httpj:engine-factory>
    </beans>
  org.apache.cxf.http.jetty-main.cfg: |
    # Reference: see HTTPJettyTransportActivator in CXF
    # The service is a managed service. So this file has to have "-xxx" at the end. This allows separate configurations for different ports; each such file gives parameters for a single port.
    # The host/port must match the urlBase property in com.basistech.ws.cxf.cfg.
    # host is the engine listen address.
    # values: hostname or IP
    host=0.0.0.0
    # port is the jetty port.
    port=${rosapi.port}
    # continuationsEnabled indicates that continuations will be checked.
    # values: true | false, default: true
    continuationsEnabled=true
    # reuseAddress allows the reuse of the service address.
    reuseAddress=true
    # maxIdleTime is the maximum idle time for a jetty connection. Timer is reset whenever there are any read or write actions on the underlying stream.
    maxIdleTime=330000
  org.apache.cxf.osgi.cfg: |
    # The contents of this file are not actually used since we are no longer using a servlet container. We assume that the only CXF services are us.
    # For the worker, this means urls like /rest/worker.
    # The client adds /v1 for /rest/v1 in com.basistech.frontend.cfg
    # org.apache.cxf.servlet.context is the prefix for all URLs for CXF services. We set this to `/rest`.
    org.apache.cxf.servlet.context=/rest
---
# Source: rosette-server/templates/cm-init-scripts.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-rosette-server-init-scripts
data:
  init.sh: |+
    #!/bin/bash
  
    OVERRIDE_DIR="/override"
    CONFIG_DIR="/configs"
    INIT_SCRIPTS_DIR="/init-scripts"
    ROSAPI_DIR="/rosapi"
  
    if [[ ! -d $OVERRIDE_DIR ]]; then
      echo "/override directory doesn't exist."
      exit 1
    fi
    if [[ ! -d $CONFIG_DIR ]]; then
      echo "/configs directory doesn't exist."
      exit 1
    fi
    if [[ ! -d $INIT_SCRIPTS_DIR ]]; then
      echo "/init-scripts directory doesn't exist."
      exit 1
    fi
    if [[ ! -d $ROSAPI_DIR ]]; then
      echo "/rosapi directory doesn't exist."
      exit 1
    fi
  
    echo "Configuring enabled endpoints"
    if [[ -z $ENDPOINTS ]]; then
      echo "Endpoints are not set. Select endpoints in values.yaml to use Rosette Server"
      exit 1
    fi
    bash ${INIT_SCRIPTS_DIR}/override-endpoints.sh
  
    echo "Configuring root versions"
    if [[ ! -f ${INIT_SCRIPTS_DIR}/rootVersions.sh ]]; then
      echo "rootVersions.sh is missing in ${INIT_SCRIPTS_DIR}"
      exit 1
    fi
    bash ${INIT_SCRIPTS_DIR}/override-roots-versions.sh
  
    if [[ -z $COREF_URL ]]; then
      echo "Indoc coref is not enabled. Not configuring"
    else
      echo "Configuring indoc coref connection"
      bash ${INIT_SCRIPTS_DIR}/configure-indoc-coref-connection.sh
    fi
  
    if [[ -z $CUSTOM_PROFILES_PATH ]]; then
      echo "Custom profiles are not enabled. Not configuring"
    else
      echo "Configuring custom profiles"
      bash ${INIT_SCRIPTS_DIR}/configure-custom-profiles.sh
    fi
  
  
  override-endpoints.sh: |-
    #!/bin/bash
  
    cp /configs/com.basistech.ws.worker.cfg /override
    sed -i "s|^\s*overrideEndpointsPathname|#overrideEndpointsPathname|g" /override/com.basistech.ws.worker.cfg
    echo -e "\noverrideEndpointsPathname=/rosette/server/override/config/enabled-endpoints.yaml\n" >> /override/com.basistech.ws.worker.cfg
    echo "endpoints:" > /override/enabled-endpoints.yaml
    for endpoint in $ENDPOINTS; do
      echo "- /${endpoint}" >> /override/enabled-endpoints.yaml;
    done
  override-roots-versions.sh: |+
    #!/bin/bash
  
    # rootVersions.sh is defined in templates/cm-init-scripts.yaml
    source /init-scripts/rootVersions.sh
  
    function update-root-version() {
      local FILE=$1
      local KEY=$2
      local ROOT=$3
      local VERSION=$4
  
      # Comment out the key
      sed -i "s|^\s*${KEY}|#${KEY}|g" ${FILE}
      # Add version from values.yaml
      echo -e "\n${KEY}=\${rosapi.roots}/${ROOT}/${VERSION}\n" >> ${FILE}
    }
  
    function update-flinx-version() {
      local FILE=$1
      local VERSION=$2
  
      # Comment out the key
      sed -i "s|^\s*flinx-root|#flinx-root|g" ${FILE}
      # Add version from values.yaml
      echo -e "\nflinx-root=\${rosapi.roots}/rex/${VERSION}/flinx\n" >> ${FILE}
    }
  
    WORKER_CFG="/override/com.basistech.ws.worker.cfg"
    # Should be already in the override partition from the previous init scripts, but just in case something changes in the
    # future, we'll copy it over if it's not there.
    if [[ ! -f $WORKER_CFG ]]; then
      cp "/configs/com.basistech.ws.worker.cfg" $WORKER_CFG
    fi
    update-root-version $WORKER_CFG "ascent-root" "ascent" $ASCENT
    update-root-version $WORKER_CFG "dp-root" "nlp4j" $NLP4J
    update-root-version $WORKER_CFG "rbl-root" "rbl" $RBL
    update-root-version $WORKER_CFG "rct-root" "rct" $RCT
    update-root-version $WORKER_CFG "relax-root" "relax" $RELAX
    update-root-version $WORKER_CFG "rex-root" "rex" $REX
    update-flinx-version $WORKER_CFG $REX
    update-root-version $WORKER_CFG "rli-root" "rli" $RLI
    update-root-version $WORKER_CFG "rni-rnt-root" "rni-rnt" $RNIRNT
    update-root-version $WORKER_CFG "tcat-root" "tcat" $TCAT
    update-root-version $WORKER_CFG "topics-root" "topics" $TOPICS
    update-root-version $WORKER_CFG "tvec-root" "tvec" $TVEC
  
  
  configure-indoc-coref-connection.sh: |-
    #!/bin/bash
  
    cp /rosapi/rex-factory-config.yaml /override/rex-factory-config.yaml
    sed -i "s|^\s*indocCorefServerUrl|#indocCorefServerUrl|g" /override/rex-factory-config.yaml
    echo -e "\nindocCorefServerUrl: http://$COREF_URL\n" >> /override/rex-factory-config.yaml
  configure-custom-profiles.sh: |
    #!/bin/bash
  
    WORKER_CFG="/override/com.basistech.ws.worker.cfg"
    # Should be already in the override partition from the previous init scripts, but just in case something changes in the
    # future, we'll copy it over if it's not there.
    if [[ ! -f $WORKER_CFG ]]; then
      cp "/configs/com.basistech.ws.worker.cfg" $WORKER_CFG
    fi
  
  
    sed -i "s|^\s*profile-data-root|#profile-data-root|g" $WORKER_CFG
    echo -e "\nprofile-data-root=file://${CUSTOM_PROFILES_PATH}\n" >> $WORKER_CFG
  
    FRONTEND_CFG="/override/com.basistech.ws.frontend.cfg"
    if [[ ! -f $FRONTEND_CFG ]]; then
      cp "/configs/com.basistech.ws.frontend.cfg" $FRONTEND_CFG
    fi
  
    sed -i "s|^\s*profile-data-root|#profile-data-root|g" $FRONTEND_CFG
    echo -e "\nprofile-data-root=file://${CUSTOM_PROFILES_PATH}\n" >> $FRONTEND_CFG
  rootVersions.sh: |2+
    ASCENT=3.0.0.c73.0
    NLP4J=2.0.0.c73.0
    RBL=7.47.2.c73.0
    RCT=3.0.18.c73.0
    RELAX=4.0.0.c73.0
    REX=7.55.11.c73.0
    RLI=7.23.12.c73.0
    RNIRNT=7.45.0.c73.0
    TCAT=3.0.0.c73.0
    TOPICS=3.0.0.c73.0
    TVEC=7.0.0.c73.0
---
# Source: rosette-server/templates/cm-rosapi.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-rosette-server-rosapi
  labels:
    helm.sh/chart: rosette-server-1.1.0
    app.kubernetes.io/name: rosette-server
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.29.0"
    app.kubernetes.io/managed-by: Helm
data:
  analyze-factory-config.yaml: |
    #Whether to tokenize and analyze emoticons.
    emoticons: true
    #Whether the produced analyzers should disambiguate their results.
    #Disambiguation is not supported for all languages.
    disambiguate: false
    #Whether to tokenize and analyze URLs.
    urls: true
    #Whether or not to do analysis. If this is false, the annotator will only do
    #tokenization.
    analyze: true
    #Indicates whether the single language annotators should convert part of speech
    #tags to their corresponding universal versions. If true, the universal tags are
    #returned. If false, the traditional tags are returned.
    universalPosTags: false
    #Whether to tokenize and analyze email addresses.
    emailAddresses: true
    #The root directory of the RBL-JE installation. Setting it sets default values
    #for otherwise required options.
    rootDirectory: ${rbl-root}
    #Turns on fragment boundary detection.
    fragmentBoundaryDetection: true
    #Configures the maximum number of tokens that can be in a line that is
    #considered short for the purposes of fragment boundary detection
    #maxTokensForShortLine: 6
    #Whether to use different tokenizers for different scripts (other than the
    #overall script). If false, uses the tokenizer for the
    #'defaultTokenizationLanguage'. Applies only to statistical segmentation
    #languages.
    #tokenizeForScript: false
    #Controls the tokenization of contractions. If this is true, then contractions
    #such as "can't" are delivered as *multiple* tokens. By default, they are
    #delivered as one.
    #tokenizeContractions: false
    #The highest conversion level that will be used by the Chinese Script Converter.
    #conversionLevel: "lexemic"
    #Whether to consider punctuation between alphanumeric characters as a break. For
    #example, the text "www.basistech.com" is segmented as a single token when the
    #option is not set, but as five tokens when it is set: "www", ".", "basistech",
    #".", "com".
    #
    #Has no effect when 'consistentLatinSegmentation' is set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #breakAtAlphaNumIntraWordPunct: false
    #Whether to add a separator character between readings when concatenating
    #readings by character.
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #readingsSeparateSyllables: false
    #The language for RBL
    #language: null
    #Selects which tokenizer to use. "fst" selects a tokenizer that uses an
    #FST to determine word breaks, which is supported for Czech, Dutch, English,
    #French, German, Greek, Hungarian, Italian, Polish, Portuguese, Romanian, Russian,
    #and Spanish. "icu" selects a tokenizer that that uses ICU break rules to
    #determine word breaks. This is supported for all languages other than Chinese and
    #Japanese. "spaceless_lexical" selects a tokenizer that uses a lexical and rule-based
    #approach to determine word breaks in input without spaces between words. This is
    #supported for Chinese and Japanese. "spaceless_statistical" selects a tokenizer
    #that uses a statistical model to determine word breaks in input without spaces.
    #This is supported for Chinese, Japanese, Korean, and Thai. "default" selects the default
    #type for each language. The default type is "spaceless_statistical" for Chinese,
    #Japanese, and Thai, and "icu" for all other languages.
    #tokenizerType: "default"
    #Selects which disambiguator to use for Hebrew. dnn is deep neural network,
    #dictionary is a dictionary-based reranker, and perceptron is a perceptron.
    #
    #disambiguatorType: "perceptron"
    #Additional user defined segmentation dictionary configurations.
    #
    #You are required to set the 'language' and the 'dict' for every configuration.
    #Configurations missing one of those will be ignored.
    #segmentationUserDefinedDictionaries: [ ]
    #Enables faster (but less accurate) part of speech disambiguation for English.
    #alternativeEnglishDisambiguation: false
    #The path of the RBL-JE license file.
    #licensePath: "${rootDirectory}/licenses/rlp-license.xml"
    #The path to the lemma/compound dictionary directory.
    #dictionaryDirectory: "${rootDirectory}/dicts"
    #The representation of readings. Possible values (case-insensitive) are:
    #"tone_marks" (pinyin with diacritics over the appropriate vowels),
    #"tone_numbers" (pinyin with a number from 1 to 4 suffixed to each syllable, or
    #no number for neutral tone), "no_tones" (pinyin without tones), "cjktex"
    #(macros for the CJKTeX pinyin.sty style).
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #readingType: "tone_marks"
    #Whether to add readings to morphological analyses. The annotator will try to
    #add readings by whole words. If it cannot, it will fall back to concatenating
    #the readings of individual characters.
    #
    #Has no effect when 'alternativeTokenization' is not set.
    #readings: false
    #'tokenizeContractions' uses a default set of per-language rules. This option
    #allows the application to replace those rules.
    #customTokenizeContractionRules: null
    #Whether to join sequences of Katakana tokens adjacent to a middle dot token.
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the language
    #is not Japanese.
    #joinKatakanaNextToMiddleDot: true
    #Whether to add parts of speech to morphological analyses.
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the
    #language is not Chinese or Japanese.
    #pos: true
    #Additional user defined tokenization rules configurations.
    #
    #You are required to set the 'language' and the 'rules' for every configuration.
    #Configurations missing one of those will be ignored.
    #tokenizationUserRules: [ ]
    #Deprecated, replaced by 'tokenizerType'
    #alternativeJapaneseTokenization: false
    #Directs the use of an alternative tokenization algorithm. The specifics depend
    #on the language. Deprecated, replaced by 'tokenizerType'.
    #alternativeTokenization: false
    #Enables faster (but less accurate) part of speech disambiguation for Spanish.
    #alternativeSpanishDisambiguation: false
    #Configures the fragment boundary delimiters.
    #fragmentBoundaryDelimiters: "\u0009\u000B\u000C"
    #The maximum number of entries in the analysis cache. Larger values (up to some
    #empirical limit) increase throughput at the expense of extra memory. A value of
    #zero turns off caching completely.
    #analysisCacheSize: 100000
    #The minimum length of non-native text to be considered for a script change. A
    #script change indicates a boundary between tokens, so the length may influence
    #how a mixed-script string is tokenized.
    #
    #Has no effect when 'consistentLatinSegmentation' or when 'tokenizerType'
    #is not 'spaceless_lexical' or the language is not Chinese or Japanese.
    #minLengthForScriptChange: 10
    #Whether to skip directly to the fallback behavior of 'readings' without
    #considering readings for whole words.
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #readingByCharacter: false
    #Turns on Unicode NFKC normalization before tokenization.
    #nfkcNormalize: false
    #The language to which the Chinese Script Converter is converting.
    #targetLanguage: null
    #Whether to split prefixes off of unknown Hebrew words.
    #guessHebrewPrefixes: false
    #Whether to decompose compounds.
    #decomposeCompounds: true
    #Enables faster (but less accurate) part of speech disambiguation for Greek.
    #alternativeGreekDisambiguation: false
    #Turns on FST tokenization for supported languages. Deprecated,
    #replaced by 'tokenizerType'.
    #fstTokenize: false
    #Whether to add ADM extended properties to tokens. No extended properties
    #are currently enabled by this option. Deprecated.
    #deliverExtendedAttributes: false
    #Whether to tokenize and analyze @mentions.
    #atMentions: false
    #Whether to tokenize and analyze hashtags.
    #hashtags: false
    #Whether to return the surface forms of compound components.
    #compoundComponentSurfaceForms: false
    #Whether to segment place names from their suffixes. For example, "東京都" (Tokyo Metropolis) is segmented as two tokens ("東京" and
    #"都").
    #
    #Has no effect when 'alternativeTokenization' is not set.
    #separatePlaceNameFromSuffix: true
    #If true, analysis and tokenization are case-sensitive; otherwise
    #case-insensitive.
    #caseSensitive: true
    #Whether to treat whitespace as a number separator. If set, the text "1995 1996"
    #is segmented as two tokens; otherwise, the same text is segmented as a single
    #token.
    #
    #Has no effect when 'consistentLatinSegmentation' is set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #whiteSpaceIsNumberSep: false
    #A list of paths to many-to-one normalization dictionaries. Paths are separated
    #by the OS-specific path separator. If 'rootDirectory' is specified, the
    #substring "${rootDirectory}" is replaced with the root directory. If the
    #value is null, many-to-one normalization is not done.
    #normalizationDictionaryPaths: null
    #The XML license content. This overrides 'licensePath'.
    #licenseString: null
    #The language to use for script regions other than the script of the overall
    #language. Applies only to statistical segmentation languages.
    #defaultTokenizationLanguage: "xxx"
    #Whether to ignore whitespace separators when segmenting input text. When not
    #set, JLA will treat whitespace separators as morpheme delimiters. Note that
    #Japanese orthography allows a newline to occur in the middle of a word.
    #
    #Has no effect when 'whitespaceTokenization' is set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #ignoreSeparators: true
    #Whether to segment each run of numbers or Latin letters into its own token,
    #without splitting on medial number/word joiners.
    #
    #For example, "12.34AB-CD)E-" is split into "12.34", "AB-CD", ")", "E", and "-".
    #
    #Has no effect when 'consistentLatinSegmentation' is set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #segmentNonJapanese: true
    #Whether to recursively decompose each token into smaller tokens, if the token
    #is marked in the dictionary as being decomposable.
    #
    #For example, if only 'decomposeCompounds' is set, the token
    #"徳島大学分子酵素学研究センター" will only be decomposed into six smaller tokens:
    #"徳島大学", "分子", "酵素", "学", "研究", "センター".
    #However, because "徳島大学" is marked decomposable in the dictionary, if deep
    #decompounding is enabled, the first token will be further decomposed into two
    #more tokens for a total of seven tokens from the original string:
    #"徳島", "大学", "分子", "酵素", "学", "研究", and "センター".
    #None of these seven tokens is further decomposable.
    #
    #Has no effect when 'decomposeCompounds' is not set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #deepCompoundDecomposition: false
    #Whether to use "v" instead of "ü" in pinyin readings, a common substitution in
    #environments that lack diacritics. The value is ignored when 'readingType' is
    #"cjktex" or "tone_marks", which always use "v" and "ü" respectively. It is
    #probably most useful when 'readingType' is "tone_numbers".
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #useVForUDiaeresis: false
    #Indicates whether to treat whitespace as a morpheme delimiter.
    #
    #Has no effect if the language is not Chinese or Japanese or
    #'tokenizerType' is not 'spaceless_lexical'.
    #whitespaceTokenization: false
    #Specifies the POS tag set to use.
    #posTagSet: "basis"
    #'universalPosTags' uses a set of per-language files to convert from Basis POS
    #tags to Universal POS Tags. This option allows the conversion to a different
    #set of POS tags.
    #customPosTags: null
    #Whether to provide consistent segmentation of embedded text not in Japanese
    #script, where "consistency" is measured against the behavior of the Chinese,
    #Japanese, and Korean segmenters previously available in Basis Technology's
    #native API, RLP. If this is true, then the setting of 'segmentNonJapanese' is
    #ignored.
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the language
    #is not Chinese or Japanese.
    #consistentLatinSegmentation: true
    #Indicates whether Hebrew root forms will be generated.
    #includeHebrewRoots: false
    #If a non primary script region is less than this length, and adjacent to a
    #primary script region, it is appended to the primary script region. Applies
    #only to statistical segmentation languages.
    #minNonPrimaryScriptRegionLength: 10
    #Deprecated, replaced by 'breakAtAlphaNumIntraWordPunct',
    #'consistentLatinSegmentation', 'decomposeCompounds',
    #'deepCompoundDecomposition', 'favorUserDictionary', 'generateAll',
    #'ignoreSeparators', 'ignoreStopwords', 'minLengthForScriptChange', 'pos',
    #'readingByCharacter', 'readings', 'readingsSeparateSyllables', 'readingType',
    #'segmentNonJapanese', 'separateNumbersFromCounters',
    #'separatePlaceNameFromSuffix', 'useVForUDiaeresis', 'whiteSpaceIsNumberSep', and
    #'whitespaceTokenization'
    #alternativeJapaneseTokenizationOptions: null
    #Indicates whether the analyzers should return extended tags, such as
    #morphological tags, with the analysis. Currently, they can only be returned
    #with the raw analysis. If true, the extended tags are returned if possible. If
    #false, extended tags are not returned.
    #deliverExtendedTags: false
    #Whether to return all the readings for a token. For characters with multiple
    #readings, all the readings are returned in brackets and separated by
    #semicolons.
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #generateAll: false
    #Whether to favor words in the user dictionary during segmentation.
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the
    #language is not Chinese or Japanese.
    #favorUserDictionary: false
    #Request query processing. RBL may change its behavior to reflect the fact that
    #query input is often not in full sentences; typically, this disables
    #disambiguation.
    #query: false
    #Whether to filter stopwords out of the output.
    #ignoreStopwords: false
    #A directory where perceptron or other statistical model files may be found.
    #modelDirectory: "${rootDirectory}/models"
    #Additional user defined analysis dictionary configurations.
    #
    #You are required to set the 'language', 'dict', and 'options' for every
    #configuration. Configurations missing one of those will be ignored.
    #analysisUserDefinedDictionaries: [ ]
    #Whether to return numbers and counters as separate tokens. For example, "二匹"
    #(2 small animals) is split into "二" and "匹".
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the
    #language is not Chinese or Japanese.
    #separateNumbersFromCounters: true
  cat-factory-config.yaml: |
    #Sets the root directory. The file structure under this directory should look like:
    #models/
    #  eng/
    #    combined-iab-qag/...
    #
    #This option is required if the component configuration does not supply a 'modelDirectory'.
    rootDirectory: ${tcat-root}
    #Enable elbow thresholding
    #elbowThresholding: false
    #Maximum number of results to return
    #maxResults: Integer.MAX_VALUE
    #Uses 'elbow thresholding' to determine the number of results to return
    #
    #elbowThresholding: false
  constraints.yaml: |2
  
    # The following properties were determined as optimal during early rounds of performance tests targeting < 2 second response time.
    # Larger values may cause sustained degradation of system performance.  These values may change as components are improved.
    # maxInputRawByteSize is the maximum number of bytes per raw doc
    # recommend not to exceed 10,000,000
    maxInputRawByteSize   : 614400
    # maxInputRawTextSize is the maximum number of characters per submission
    # recommend not to exceed 1,000,000
    maxInputRawTextSize   : 50000
    # maxNameDedupeListSize is the maximum number of names to be deduplicated
    # recommend not to exceed  100,000.
    maxNameDedupeListSize : 1000
  default-only-tokenization-factory-config.yaml: |
    #Whether or not to do analysis. If this is false, the annotator will only do
    #tokenization.
    analyze: false
    #The root directory of the RBL-JE installation. Setting it sets default values
    #for otherwise required options.
    rootDirectory: ${rbl-root}
    #Configures the maximum number of tokens that can be in a line that is
    #considered short for the purposes of fragment boundary detection
    #maxTokensForShortLine: 6
    #Whether to use different tokenizers for different scripts (other than the
    #overall script). If false, uses the tokenizer for the
    #'defaultTokenizationLanguage'. Applies only to statistical segmentation
    #languages.
    #tokenizeForScript: false
    #Turns on fragment boundary detection.
    #fragmentBoundaryDetection: true
    #Controls the tokenization of contractions. If this is true, then contractions
    #such as "can't" are delivered as *multiple* tokens. By default, they are
    #delivered as one.
    #tokenizeContractions: false
    #The highest conversion level that will be used by the Chinese Script Converter.
    #conversionLevel: "lexemic"
    #Whether to consider punctuation between alphanumeric characters as a break. For
    #example, the text "www.basistech.com" is segmented as a single token when the
    #option is not set, but as five tokens when it is set: "www", ".", "basistech",
    #".", "com".
    #
    #Has no effect when 'consistentLatinSegmentation' is set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #breakAtAlphaNumIntraWordPunct: false
    #Whether to add a separator character between readings when concatenating
    #readings by character.
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #readingsSeparateSyllables: false
    #The language for RBL
    #language: null
    #Selects which tokenizer to use. "fst" selects a tokenizer that uses an
    #FST to determine word breaks, which is supported for Czech, Dutch, English,
    #French, German, Greek, Hungarian, Italian, Polish, Portuguese, Romanian, Russian,
    #and Spanish. "icu" selects a tokenizer that that uses ICU break rules to
    #determine word breaks. This is supported for all languages other than Chinese and
    #Japanese. "spaceless_lexical" selects a tokenizer that uses a lexical and rule-based
    #approach to determine word breaks in input without spaces between words. This is
    #supported for Chinese and Japanese. "spaceless_statistical" selects a tokenizer
    #that uses a statistical model to determine word breaks in input without spaces.
    #This is supported for Chinese, Japanese, Korean, and Thai. "default" selects the default
    #type for each language. The default type is "spaceless_statistical" for Chinese,
    #Japanese, and Thai, and "icu" for all other languages.
    #tokenizerType: "default"
    #Selects which disambiguator to use for Hebrew. dnn is deep neural network,
    #dictionary is a dictionary-based reranker, and perceptron is a perceptron.
    #
    #disambiguatorType: "perceptron"
    #Additional user defined segmentation dictionary configurations.
    #
    #You are required to set the 'language' and the 'dict' for every configuration.
    #Configurations missing one of those will be ignored.
    #segmentationUserDefinedDictionaries: [ ]
    #Whether to tokenize and analyze email addresses.
    #emailAddresses: false
    #Enables faster (but less accurate) part of speech disambiguation for English.
    #alternativeEnglishDisambiguation: false
    #The path of the RBL-JE license file.
    #licensePath: "${rootDirectory}/licenses/rlp-license.xml"
    #The path to the lemma/compound dictionary directory.
    #dictionaryDirectory: "${rootDirectory}/dicts"
    #The representation of readings. Possible values (case-insensitive) are:
    #"tone_marks" (pinyin with diacritics over the appropriate vowels),
    #"tone_numbers" (pinyin with a number from 1 to 4 suffixed to each syllable, or
    #no number for neutral tone), "no_tones" (pinyin without tones), "cjktex"
    #(macros for the CJKTeX pinyin.sty style).
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #readingType: "tone_marks"
    #Whether to add readings to morphological analyses. The annotator will try to
    #add readings by whole words. If it cannot, it will fall back to concatenating
    #the readings of individual characters.
    #
    #Has no effect when 'alternativeTokenization' is not set.
    #readings: false
    #'tokenizeContractions' uses a default set of per-language rules. This option
    #allows the application to replace those rules.
    #customTokenizeContractionRules: null
    #Whether to join sequences of Katakana tokens adjacent to a middle dot token.
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the language
    #is not Japanese.
    #joinKatakanaNextToMiddleDot: true
    #Whether to add parts of speech to morphological analyses.
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the
    #language is not Chinese or Japanese.
    #pos: true
    #Additional user defined tokenization rules configurations.
    #
    #You are required to set the 'language' and the 'rules' for every configuration.
    #Configurations missing one of those will be ignored.
    #tokenizationUserRules: [ ]
    #Indicates whether the single language annotators should convert part of speech
    #tags to their corresponding universal versions. If true, the universal tags are
    #returned. If false, the traditional tags are returned.
    #universalPosTags: false
    #Deprecated, replaced by 'tokenizerType'
    #alternativeJapaneseTokenization: false
    #Directs the use of an alternative tokenization algorithm. The specifics depend
    #on the language. Deprecated, replaced by 'tokenizerType'.
    #alternativeTokenization: false
    #Enables faster (but less accurate) part of speech disambiguation for Spanish.
    #alternativeSpanishDisambiguation: false
    #Configures the fragment boundary delimiters.
    #fragmentBoundaryDelimiters: "\u0009\u000B\u000C"
    #The maximum number of entries in the analysis cache. Larger values (up to some
    #empirical limit) increase throughput at the expense of extra memory. A value of
    #zero turns off caching completely.
    #analysisCacheSize: 100000
    #The minimum length of non-native text to be considered for a script change. A
    #script change indicates a boundary between tokens, so the length may influence
    #how a mixed-script string is tokenized.
    #
    #Has no effect when 'consistentLatinSegmentation' or when 'tokenizerType'
    #is not 'spaceless_lexical' or the language is not Chinese or Japanese.
    #minLengthForScriptChange: 10
    #Whether to skip directly to the fallback behavior of 'readings' without
    #considering readings for whole words.
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #readingByCharacter: false
    #Turns on Unicode NFKC normalization before tokenization.
    #nfkcNormalize: false
    #The language to which the Chinese Script Converter is converting.
    #targetLanguage: null
    #Whether to split prefixes off of unknown Hebrew words.
    #guessHebrewPrefixes: false
    #Whether to decompose compounds.
    #decomposeCompounds: true
    #Enables faster (but less accurate) part of speech disambiguation for Greek.
    #alternativeGreekDisambiguation: false
    #Turns on FST tokenization for supported languages. Deprecated,
    #replaced by 'tokenizerType'.
    #fstTokenize: false
    #Whether to add ADM extended properties to tokens. No extended properties
    #are currently enabled by this option. Deprecated.
    #deliverExtendedAttributes: false
    #Whether to tokenize and analyze @mentions.
    #atMentions: false
    #Whether to tokenize and analyze hashtags.
    #hashtags: false
    #Whether to return the surface forms of compound components.
    #compoundComponentSurfaceForms: false
    #Whether to segment place names from their suffixes. For example, "東京都" (Tokyo Metropolis) is segmented as two tokens ("東京" and
    #"都").
    #
    #Has no effect when 'alternativeTokenization' is not set.
    #separatePlaceNameFromSuffix: true
    #If true, analysis and tokenization are case-sensitive; otherwise
    #case-insensitive.
    #caseSensitive: true
    #Whether to treat whitespace as a number separator. If set, the text "1995 1996"
    #is segmented as two tokens; otherwise, the same text is segmented as a single
    #token.
    #
    #Has no effect when 'consistentLatinSegmentation' is set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #whiteSpaceIsNumberSep: false
    #A list of paths to many-to-one normalization dictionaries. Paths are separated
    #by the OS-specific path separator. If 'rootDirectory' is specified, the
    #substring "${rootDirectory}" is replaced with the root directory. If the
    #value is null, many-to-one normalization is not done.
    #normalizationDictionaryPaths: null
    #The XML license content. This overrides 'licensePath'.
    #licenseString: null
    #The language to use for script regions other than the script of the overall
    #language. Applies only to statistical segmentation languages.
    #defaultTokenizationLanguage: "xxx"
    #Whether to ignore whitespace separators when segmenting input text. When not
    #set, JLA will treat whitespace separators as morpheme delimiters. Note that
    #Japanese orthography allows a newline to occur in the middle of a word.
    #
    #Has no effect when 'whitespaceTokenization' is set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #ignoreSeparators: true
    #Whether to segment each run of numbers or Latin letters into its own token,
    #without splitting on medial number/word joiners.
    #
    #For example, "12.34AB-CD)E-" is split into "12.34", "AB-CD", ")", "E", and "-".
    #
    #Has no effect when 'consistentLatinSegmentation' is set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #segmentNonJapanese: true
    #Whether to recursively decompose each token into smaller tokens, if the token
    #is marked in the dictionary as being decomposable.
    #
    #For example, if only 'decomposeCompounds' is set, the token
    #"徳島大学分子酵素学研究センター" will only be decomposed into six smaller tokens:
    #"徳島大学", "分子", "酵素", "学", "研究", "センター".
    #However, because "徳島大学" is marked decomposable in the dictionary, if deep
    #decompounding is enabled, the first token will be further decomposed into two
    #more tokens for a total of seven tokens from the original string:
    #"徳島", "大学", "分子", "酵素", "学", "研究", and "センター".
    #None of these seven tokens is further decomposable.
    #
    #Has no effect when 'decomposeCompounds' is not set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #deepCompoundDecomposition: false
    #Whether to use "v" instead of "ü" in pinyin readings, a common substitution in
    #environments that lack diacritics. The value is ignored when 'readingType' is
    #"cjktex" or "tone_marks", which always use "v" and "ü" respectively. It is
    #probably most useful when 'readingType' is "tone_numbers".
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #useVForUDiaeresis: false
    #Indicates whether to treat whitespace as a morpheme delimiter.
    #
    #Has no effect if the language is not Chinese or Japanese or
    #'tokenizerType' is not 'spaceless_lexical'.
    #whitespaceTokenization: false
    #Specifies the POS tag set to use.
    #posTagSet: "basis"
    #'universalPosTags' uses a set of per-language files to convert from Basis POS
    #tags to Universal POS Tags. This option allows the conversion to a different
    #set of POS tags.
    #customPosTags: null
    #Whether to tokenize and analyze URLs.
    #urls: false
    #Whether to provide consistent segmentation of embedded text not in Japanese
    #script, where "consistency" is measured against the behavior of the Chinese,
    #Japanese, and Korean segmenters previously available in Basis Technology's
    #native API, RLP. If this is true, then the setting of 'segmentNonJapanese' is
    #ignored.
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the language
    #is not Chinese or Japanese.
    #consistentLatinSegmentation: true
    #Indicates whether Hebrew root forms will be generated.
    #includeHebrewRoots: false
    #If a non primary script region is less than this length, and adjacent to a
    #primary script region, it is appended to the primary script region. Applies
    #only to statistical segmentation languages.
    #minNonPrimaryScriptRegionLength: 10
    #Deprecated, replaced by 'breakAtAlphaNumIntraWordPunct',
    #'consistentLatinSegmentation', 'decomposeCompounds',
    #'deepCompoundDecomposition', 'favorUserDictionary', 'generateAll',
    #'ignoreSeparators', 'ignoreStopwords', 'minLengthForScriptChange', 'pos',
    #'readingByCharacter', 'readings', 'readingsSeparateSyllables', 'readingType',
    #'segmentNonJapanese', 'separateNumbersFromCounters',
    #'separatePlaceNameFromSuffix', 'useVForUDiaeresis', 'whiteSpaceIsNumberSep', and
    #'whitespaceTokenization'
    #alternativeJapaneseTokenizationOptions: null
    #Indicates whether the analyzers should return extended tags, such as
    #morphological tags, with the analysis. Currently, they can only be returned
    #with the raw analysis. If true, the extended tags are returned if possible. If
    #false, extended tags are not returned.
    #deliverExtendedTags: false
    #Whether the produced analyzers should disambiguate their results.
    #Disambiguation is not supported for all languages.
    #disambiguate: true
    #Whether to return all the readings for a token. For characters with multiple
    #readings, all the readings are returned in brackets and separated by
    #semicolons.
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #generateAll: false
    #Whether to favor words in the user dictionary during segmentation.
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the
    #language is not Chinese or Japanese.
    #favorUserDictionary: false
    #Whether to tokenize and analyze emoticons.
    #emoticons: false
    #Request query processing. RBL may change its behavior to reflect the fact that
    #query input is often not in full sentences; typically, this disables
    #disambiguation.
    #query: false
    #Whether to filter stopwords out of the output.
    #ignoreStopwords: false
    #A directory where perceptron or other statistical model files may be found.
    #modelDirectory: "${rootDirectory}/models"
    #Additional user defined analysis dictionary configurations.
    #
    #You are required to set the 'language', 'dict', and 'options' for every
    #configuration. Configurations missing one of those will be ignored.
    #analysisUserDefinedDictionaries: [ ]
    #Whether to return numbers and counters as separate tokens. For example, "二匹"
    #(2 small animals) is split into "二" and "匹".
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the
    #language is not Chinese or Japanese.
    #separateNumbersFromCounters: true
  downloaderExtractor.yaml: |
    # urlContentDownloader is the configuration for downloading URL content.
    urlContentDownloader:
      # processTimeoutMs is the timeout in milliseconds for processing URL downloader component.
      # values: dependent on network
      processTimeoutMs      : 10000
      # connectTimeout is the timeout in milliseconds for connecting to the URL.
      # values: dependent on network
      connectTimeout        : 5000
      # readTimeout is the timeout in milliseconds for reading content from URL.
      # values: dependent on network
      readTimeout           : 3000
      # maxRedirects is the maximum number of HTTP redirects allowed.
      # values: typically, lower is faster
      maxRedirects          : 10
      # maxRetries is the maximum number of retries to attempt.
      # values: dependent on network
      maxRetries            : 1
      # retryInterval is the retry interval in milliseconds.
      # values: arbitrary
      retryInterval         : 1000
    # proxy is the configuration for an OPTIONAL proxy.
    #proxy:
      # proxyType is the type of proxy (required).
      # values: HTTP or SOCKS
      #proxyType             : HTTP
      # inetSocketAddress is the IP of the proxy (required).
      #inetSocketAddress     : 10.2.3.4
      # inetSocketPort is the port of the proxy (required).
      #inetSocketPort        : 3128
      # username is the username for the proxy (optional).
      #username              : username
      # password is the password for the proxy (optional).
      #password              : password
    # textExtractor is the configuration for text extractor.
    textExtractor:
      # processTimeoutMs is the timeout in milliseconds for processing TextExtractionProcessor component.
      # values: arbitrary
      processTimeoutMs      : 10000
      # Deprecated
      # maxHeapSize is the maximum heap size of forked process in MB.
      # values: dependent on server
      # maxHeapSize           : 64
      # htmlExtractor is the HTML extractor class.
      htmlExtractor         : de.l3s.boilerpipe.extractors.ArticleExtractor
      # supportedParsers is the list of parsers to run (required).
      supportedParsers:
        - org.apache.tika.parser.html.HtmlParser
        - org.apache.tika.parser.txt.TXTParser
  dp-factory-config.yaml: |
    rootDirectory: ${dp-root}
    rootDirectory: ${dp-root}
    #language: { }
    #language: { }
  event-extractor-factory-config.yaml: |
    #Base URL of ETS in the format
    #http://<ETS_HOST>:<ETS_PORT>/ets (example: http://ets.customer.net:9999/ets) .
    #Set to empty to disable.
    eventTrainingServerUrl: null
    #Milliseconds to wait for the connection to be established.
    #0 for an infinite timeout.
    #-1 for system default.
    #connectionTimeout: -1
    #Proxy server port(optional).
    #proxyServerPort: not set
    #Milliseconds to wait for data to arrive.
    #0 for an infinite timeout.
    #-1 for system default.
    #socketTimeout: -1
    #Proxy server host(optional).
    #proxyServer: not set
  framework-override.properties: |
    # disable stacktrace in Felix SCR
    ds.showerrors=false
  indoc-coref-server-factory-config.yaml: |
    #serverUrl: http://localhost:5000/
  rbl-factory-config.yaml: |
    #Whether to tokenize and analyze emoticons.
    emoticons: true
    #Whether to tokenize and analyze URLs.
    urls: true
    #Whether to tokenize and analyze hashtags.
    hashtags: true
    #Whether to tokenize and analyze @mentions.
    atMentions: true
    #Directs the use of an alternative tokenization algorithm. The specifics depend
    #on the language. Deprecated, replaced by 'tokenizerType'.
    alternativeTokenization: true
    #Indicates whether the single language annotators should convert part of speech
    #tags to their corresponding universal versions. If true, the universal tags are
    #returned. If false, the traditional tags are returned.
    universalPosTags: true
    #Whether to tokenize and analyze email addresses.
    emailAddresses: true
    #The root directory of the RBL-JE installation. Setting it sets default values
    #for otherwise required options.
    rootDirectory: ${rbl-root}
    #Configures the maximum number of tokens that can be in a line that is
    #considered short for the purposes of fragment boundary detection
    #maxTokensForShortLine: 6
    #Whether to use different tokenizers for different scripts (other than the
    #overall script). If false, uses the tokenizer for the
    #'defaultTokenizationLanguage'. Applies only to statistical segmentation
    #languages.
    #tokenizeForScript: false
    #Turns on fragment boundary detection.
    #fragmentBoundaryDetection: true
    #Controls the tokenization of contractions. If this is true, then contractions
    #such as "can't" are delivered as *multiple* tokens. By default, they are
    #delivered as one.
    #tokenizeContractions: false
    #The highest conversion level that will be used by the Chinese Script Converter.
    #conversionLevel: "lexemic"
    #Whether to consider punctuation between alphanumeric characters as a break. For
    #example, the text "www.basistech.com" is segmented as a single token when the
    #option is not set, but as five tokens when it is set: "www", ".", "basistech",
    #".", "com".
    #
    #Has no effect when 'consistentLatinSegmentation' is set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #breakAtAlphaNumIntraWordPunct: false
    #Whether to add a separator character between readings when concatenating
    #readings by character.
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #readingsSeparateSyllables: false
    #The language for RBL
    #language: null
    #Selects which tokenizer to use. "fst" selects a tokenizer that uses an
    #FST to determine word breaks, which is supported for Czech, Dutch, English,
    #French, German, Greek, Hungarian, Italian, Polish, Portuguese, Romanian, Russian,
    #and Spanish. "icu" selects a tokenizer that that uses ICU break rules to
    #determine word breaks. This is supported for all languages other than Chinese and
    #Japanese. "spaceless_lexical" selects a tokenizer that uses a lexical and rule-based
    #approach to determine word breaks in input without spaces between words. This is
    #supported for Chinese and Japanese. "spaceless_statistical" selects a tokenizer
    #that uses a statistical model to determine word breaks in input without spaces.
    #This is supported for Chinese, Japanese, Korean, and Thai. "default" selects the default
    #type for each language. The default type is "spaceless_statistical" for Chinese,
    #Japanese, and Thai, and "icu" for all other languages.
    #tokenizerType: "default"
    #Selects which disambiguator to use for Hebrew. dnn is deep neural network,
    #dictionary is a dictionary-based reranker, and perceptron is a perceptron.
    #
    #disambiguatorType: "perceptron"
    #Additional user defined segmentation dictionary configurations.
    #
    #You are required to set the 'language' and the 'dict' for every configuration.
    #Configurations missing one of those will be ignored.
    #segmentationUserDefinedDictionaries: [ ]
    #Enables faster (but less accurate) part of speech disambiguation for English.
    #alternativeEnglishDisambiguation: false
    #The path of the RBL-JE license file.
    #licensePath: "${rootDirectory}/licenses/rlp-license.xml"
    #The path to the lemma/compound dictionary directory.
    #dictionaryDirectory: "${rootDirectory}/dicts"
    #The representation of readings. Possible values (case-insensitive) are:
    #"tone_marks" (pinyin with diacritics over the appropriate vowels),
    #"tone_numbers" (pinyin with a number from 1 to 4 suffixed to each syllable, or
    #no number for neutral tone), "no_tones" (pinyin without tones), "cjktex"
    #(macros for the CJKTeX pinyin.sty style).
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #readingType: "tone_marks"
    #Whether to add readings to morphological analyses. The annotator will try to
    #add readings by whole words. If it cannot, it will fall back to concatenating
    #the readings of individual characters.
    #
    #Has no effect when 'alternativeTokenization' is not set.
    #readings: false
    #'tokenizeContractions' uses a default set of per-language rules. This option
    #allows the application to replace those rules.
    #customTokenizeContractionRules: null
    #Whether to join sequences of Katakana tokens adjacent to a middle dot token.
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the language
    #is not Japanese.
    #joinKatakanaNextToMiddleDot: true
    #Whether to add parts of speech to morphological analyses.
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the
    #language is not Chinese or Japanese.
    #pos: true
    #Additional user defined tokenization rules configurations.
    #
    #You are required to set the 'language' and the 'rules' for every configuration.
    #Configurations missing one of those will be ignored.
    #tokenizationUserRules: [ ]
    #Deprecated, replaced by 'tokenizerType'
    #alternativeJapaneseTokenization: false
    #Enables faster (but less accurate) part of speech disambiguation for Spanish.
    #alternativeSpanishDisambiguation: false
    #Configures the fragment boundary delimiters.
    #fragmentBoundaryDelimiters: "\u0009\u000B\u000C"
    #The maximum number of entries in the analysis cache. Larger values (up to some
    #empirical limit) increase throughput at the expense of extra memory. A value of
    #zero turns off caching completely.
    #analysisCacheSize: 100000
    #The minimum length of non-native text to be considered for a script change. A
    #script change indicates a boundary between tokens, so the length may influence
    #how a mixed-script string is tokenized.
    #
    #Has no effect when 'consistentLatinSegmentation' or when 'tokenizerType'
    #is not 'spaceless_lexical' or the language is not Chinese or Japanese.
    #minLengthForScriptChange: 10
    #Whether to skip directly to the fallback behavior of 'readings' without
    #considering readings for whole words.
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #readingByCharacter: false
    #Turns on Unicode NFKC normalization before tokenization.
    #nfkcNormalize: false
    #The language to which the Chinese Script Converter is converting.
    #targetLanguage: null
    #Whether to split prefixes off of unknown Hebrew words.
    #guessHebrewPrefixes: false
    #Whether to decompose compounds.
    #decomposeCompounds: true
    #Enables faster (but less accurate) part of speech disambiguation for Greek.
    #alternativeGreekDisambiguation: false
    #Turns on FST tokenization for supported languages. Deprecated,
    #replaced by 'tokenizerType'.
    #fstTokenize: false
    #Whether to add ADM extended properties to tokens. No extended properties
    #are currently enabled by this option. Deprecated.
    #deliverExtendedAttributes: false
    #Whether or not to do analysis. If this is false, the annotator will only do
    #tokenization.
    #analyze: true
    #Whether to return the surface forms of compound components.
    #compoundComponentSurfaceForms: false
    #Whether to segment place names from their suffixes. For example, "東京都" (Tokyo Metropolis) is segmented as two tokens ("東京" and
    #"都").
    #
    #Has no effect when 'alternativeTokenization' is not set.
    #separatePlaceNameFromSuffix: true
    #If true, analysis and tokenization are case-sensitive; otherwise
    #case-insensitive.
    #caseSensitive: true
    #Whether to treat whitespace as a number separator. If set, the text "1995 1996"
    #is segmented as two tokens; otherwise, the same text is segmented as a single
    #token.
    #
    #Has no effect when 'consistentLatinSegmentation' is set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #whiteSpaceIsNumberSep: false
    #A list of paths to many-to-one normalization dictionaries. Paths are separated
    #by the OS-specific path separator. If 'rootDirectory' is specified, the
    #substring "${rootDirectory}" is replaced with the root directory. If the
    #value is null, many-to-one normalization is not done.
    #normalizationDictionaryPaths: null
    #The XML license content. This overrides 'licensePath'.
    #licenseString: null
    #The language to use for script regions other than the script of the overall
    #language. Applies only to statistical segmentation languages.
    #defaultTokenizationLanguage: "xxx"
    #Whether to ignore whitespace separators when segmenting input text. When not
    #set, JLA will treat whitespace separators as morpheme delimiters. Note that
    #Japanese orthography allows a newline to occur in the middle of a word.
    #
    #Has no effect when 'whitespaceTokenization' is set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #ignoreSeparators: true
    #Whether to segment each run of numbers or Latin letters into its own token,
    #without splitting on medial number/word joiners.
    #
    #For example, "12.34AB-CD)E-" is split into "12.34", "AB-CD", ")", "E", and "-".
    #
    #Has no effect when 'consistentLatinSegmentation' is set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #segmentNonJapanese: true
    #Whether to recursively decompose each token into smaller tokens, if the token
    #is marked in the dictionary as being decomposable.
    #
    #For example, if only 'decomposeCompounds' is set, the token
    #"徳島大学分子酵素学研究センター" will only be decomposed into six smaller tokens:
    #"徳島大学", "分子", "酵素", "学", "研究", "センター".
    #However, because "徳島大学" is marked decomposable in the dictionary, if deep
    #decompounding is enabled, the first token will be further decomposed into two
    #more tokens for a total of seven tokens from the original string:
    #"徳島", "大学", "分子", "酵素", "学", "研究", and "センター".
    #None of these seven tokens is further decomposable.
    #
    #Has no effect when 'decomposeCompounds' is not set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #deepCompoundDecomposition: false
    #Whether to use "v" instead of "ü" in pinyin readings, a common substitution in
    #environments that lack diacritics. The value is ignored when 'readingType' is
    #"cjktex" or "tone_marks", which always use "v" and "ü" respectively. It is
    #probably most useful when 'readingType' is "tone_numbers".
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #useVForUDiaeresis: false
    #Indicates whether to treat whitespace as a morpheme delimiter.
    #
    #Has no effect if the language is not Chinese or Japanese or
    #'tokenizerType' is not 'spaceless_lexical'.
    #whitespaceTokenization: false
    #Specifies the POS tag set to use.
    #posTagSet: "basis"
    #'universalPosTags' uses a set of per-language files to convert from Basis POS
    #tags to Universal POS Tags. This option allows the conversion to a different
    #set of POS tags.
    #customPosTags: null
    #Whether to provide consistent segmentation of embedded text not in Japanese
    #script, where "consistency" is measured against the behavior of the Chinese,
    #Japanese, and Korean segmenters previously available in Basis Technology's
    #native API, RLP. If this is true, then the setting of 'segmentNonJapanese' is
    #ignored.
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the language
    #is not Chinese or Japanese.
    #consistentLatinSegmentation: true
    #Indicates whether Hebrew root forms will be generated.
    #includeHebrewRoots: false
    #If a non primary script region is less than this length, and adjacent to a
    #primary script region, it is appended to the primary script region. Applies
    #only to statistical segmentation languages.
    #minNonPrimaryScriptRegionLength: 10
    #Deprecated, replaced by 'breakAtAlphaNumIntraWordPunct',
    #'consistentLatinSegmentation', 'decomposeCompounds',
    #'deepCompoundDecomposition', 'favorUserDictionary', 'generateAll',
    #'ignoreSeparators', 'ignoreStopwords', 'minLengthForScriptChange', 'pos',
    #'readingByCharacter', 'readings', 'readingsSeparateSyllables', 'readingType',
    #'segmentNonJapanese', 'separateNumbersFromCounters',
    #'separatePlaceNameFromSuffix', 'useVForUDiaeresis', 'whiteSpaceIsNumberSep', and
    #'whitespaceTokenization'
    #alternativeJapaneseTokenizationOptions: null
    #Indicates whether the analyzers should return extended tags, such as
    #morphological tags, with the analysis. Currently, they can only be returned
    #with the raw analysis. If true, the extended tags are returned if possible. If
    #false, extended tags are not returned.
    #deliverExtendedTags: false
    #Whether the produced analyzers should disambiguate their results.
    #Disambiguation is not supported for all languages.
    #disambiguate: true
    #Whether to return all the readings for a token. For characters with multiple
    #readings, all the readings are returned in brackets and separated by
    #semicolons.
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #generateAll: false
    #Whether to favor words in the user dictionary during segmentation.
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the
    #language is not Chinese or Japanese.
    #favorUserDictionary: false
    #Request query processing. RBL may change its behavior to reflect the fact that
    #query input is often not in full sentences; typically, this disables
    #disambiguation.
    #query: false
    #Whether to filter stopwords out of the output.
    #ignoreStopwords: false
    #A directory where perceptron or other statistical model files may be found.
    #modelDirectory: "${rootDirectory}/models"
    #Additional user defined analysis dictionary configurations.
    #
    #You are required to set the 'language', 'dict', and 'options' for every
    #configuration. Configurations missing one of those will be ignored.
    #analysisUserDefinedDictionaries: [ ]
    #Whether to return numbers and counters as separate tokens. For example, "二匹"
    #(2 small animals) is split into "二" and "匹".
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the
    #language is not Chinese or Japanese.
    #separateNumbersFromCounters: true
  rct-factory-config.yaml: |
    #The root directory of the RCT installation.
    #Providing this removes the need to provide a 'licenseString'
    rootDirectory: ${rct-root}
    #The XML string containing the RCT license.
    #
    #licenseString: ${rootDirectory}/licenses/rct-license.xml
    #When true, enables "reverse transliteration" (Arabic to Arabizi).
    # Otherwise, the transliterator will do normal transliteration (Arabizi to Arabic)
    #reversed: false
  relax-factory-config.yaml: |
    processors:
    - targeted
    - triggered
    - treePathPattern
    threadPoolSize: 1
    nlp4jRoot: ${dp-root}
    relaxRoot: ${relax-root}
    #confidenceModelPath: ""
    #entityFabricatorClassName: ""
    #adjunctsToArgs: false
    #generatePatternId: false
    #generateReport: false
    #splitConj: false
    #useRblTokensInternally: false
    #normalizePredicates: false
    #resolvePronouns: false
    #bundleLicenseString: ""
    #licenseString: ""
    #licenseFile: ""
    #allowedRelationships: [ ]
    #language: { }
    #processorTypes: [ ]
    #calculateConfidence: false
    #confidenceThreshold: 0.0
    #calculateSalience: false
    #removeNonSalient: false
  rex-factory-config.yaml: |
    # The REX root directory. A REX root directory contains language models and necessary configuration
    # files.
    # This value is stored as a java.nio.file.Path.  Strings or URIs are valid values.  E.g.
    # "/path/to/rex-root" or "file:///path/to/rex-root"
    rootDirectory: ${rex-root}
    # The RBL root directory for REX to use.
    # This value is stored as a java.nio.file.Path.  Strings or URIs are valid values.  E.g.
    # "/path/to/rex-root/rbl-je" or "file:///path/to/rex-root/rbl-je"
    rblRootDirectory: ${rex-root}/rbl-je
    #The entity types to be excluded from extraction.
    # This property accepts a list of values and the values are stored as Strings.
    # Valid values:  Person, Location, Organization, Product, Title
    # Other valid values are possible if you are using DB Pedia Types or have defined custom
    # entity types.
    #excludedEntityTypes: null
    # The option to search for and link mentions to knowledge base entities with disambiguation model.
    # Enabling this option also enables calculateConfidence.
    # Valid values:  true, false
    #linkEntities: false
    # Additional files used to produce statistical entities for the given language.
    # This property accepts a list of values and the values are comma separated trios.
    # The trios of values specify the language, case-sensitivity and the model file,
    # separated by commas. See caseSensitivity for valid values.
    # For example:
    # - eng,caseSensitive,english-model.bin
    # - jpn,automatic,japanese-model.bin
    #statisticalModels: null
    #Milliseconds to wait for data to arrive. 0 for an infinite timeout. -1 for system default.
    #socketTimeout: -1
    # An option to calculate entity-chain salience with statistical-based calculation
    # (returns 0 or 1) or simple calculation (returns score between 0 and 1).
    # Valid values: true, false
    #statSalienceMode: true
    # The confidence value threshold below which entities extracted by the statistical processor
    # are ignored.
    # This property is stored as a Float.
    # Valid values:  -1.0 to 1.0
    #confidenceThreshold: -1.0
    # The confidence value threshold below which linking results by the kbLinker processor are
    # ignored.
    # This property is stored as a Float.
    # Valid values:  -1.0 to 1.0
    #linkingConfidenceThreshold: -1.0
    # Register a custom processor class. See AppDev guide for more details on custom processors.
    # This property accepts a list of values and the values are stored as Strings.
    # - "/path/to/MyCustomProcessor.class"
    #customProcessorClasses: null
    #Indoc-Coref server URL.
    #indocCorefServerUrl: not set
    # Apply joiner rules after redactor (old behavior).
    # Valid values: true, false
    #runJoinerPostRedactor: false
    # Additional gazetteer files used to produce entities for the given language.
    # This property accepts a list of values and the values are stored as a java.nio.file.Path.
    # Strings or URIs are valid values.  Entries are added below in the form:
    # - "/path/<language_code>/accept/example.txt"
    #acceptGazetteers: null
    # Enables the splitting of IDENTIFIER:MONEY regex matches into IDENTIFIER:CURRENCY_AMT and IDENTIFIER:CURRENCY_TYPE
    # where the amount of the currency will be represented in one entity and the type will be
    # represented in another.
    #regexCurrencySplit: false
    # An option for document entity resolution (also known as entity chaining).
    # These values are enums and should not be quoted.
    # Valid values:  HIGH, STANDARD, STANDARD_MINUS or NULL
    #indocType: STANDARD
    # The option to disable usage of the disambiguation model when linking.
    # Valid values: true, false",
    #disableLinkerDisambiguation: false
    # Additional joiner rules files.
    # This property accepts a list of values and the values are stored as a java.nio.file.Path.
    # Strings or URIs are valid values.  Entries are added below in the form:
    # - "/path/to/custom-joiners/my-joiner-rules.xml"
    #joinerRuleFiles: null
    # The option to allow partial gazetteer matches. For the purposes of this setting, a partial
    # match is one that does not line up with token boundaries as determined by the internal
    # tokenizer. This only applies to accept gazetteers.
    # Valid values:  true, false
    #allowPartialGazetteerMatches: false
    # Extract entities in structured regions with NER (perceptron, dnn) models if there are enough
    # tokens.
    # Valid values:  Integers greater than or equal to 0.
    #structuredRegionProcessingSentenceTokensMin: 0
    # An option to adjust offsets according to normalization.
    # Valid values: true, false
    #adjustNormalizedOffsets: false
    # The capitalization (aka 'case') used in the input texts. Processing standard documents
    # requires caseSensitive, which is the default. Documents with all-caps, no-caps or headline
    # capitalization may yield higher accuracy if processed with the caseInsensitive value.
    # These values are enums and should not be quoted.
    # Valid values: automatic, caseSensitive, caseInsensitive
    #caseSensitivity: caseSensitive
    # The option to choose whether to look for candidate entities in text or link pre-existing mentions (extracted by other processors) to knowledge base entities with disambiguation model.
    # Valid values:  text, entities
    #linkMentionMode: text
    # An option to calculate entity confidence values.
    # Valid values:  true, false
    #calculateConfidence: false
    # The option to retain social media symbols ('@' and '#') in normalized output.
    # Valid values: true, false
    #retainSocialMediaSymbols: false
    # The overlay directory. An overlay directory is a directory shaped like the 'data' directory.
    # REX will look for files in both the overlay directory and the root directory, using files from
    # both locations. However, if a file exists in both places (as identified by its path relative
    # to the overlay or root data directory), REX prefers the version in the overlay directory.
    # If REX finds a zero-length file in the overlay directory, it ignores both that file and any
    # corresponding file in the root data directory.
    # This value is stored as a java.nio.file.Path.  Strings or URIs are valid values.  E.g.
    # "/path/to/overlay-data" or "file:///path/to/overlay-data"
    #dataOverlayDirectory: null
    #Proxy server host for Indoc-Coref server (optional).
    #proxyServer: not set
    # The option to add supplemental regex files, usually for entity types that are excluded by
    # default. The supplemental regex files are located at data/regex/<lang>/accept/supplemental and
    # are not used unless specified.
    # Strings or URIs are valid values.  Entries are added below in the form:
    # - "${rex-root}/data/regex/<lang>/accept/supplemental/date-regexes.xml"
    # - "data/regex/<lang>/accept/supplemental/time-regexes.xml"
    #supplementalRegularExpressionPaths: null
    # Additional regex files used to reject entities.
    # This property accepts a list of values and the values are stored as a java.nio.file.Path.
    # Strings or URIs are valid values.  Entries are added below in the form:
    # - "/path/<language_code>/reject/regexes.xml"
    #rejectRegularExpressionSets: null
    # The option to prefer length over weights during redaction. If true, the redactor will always
    # choose a longer entity over a shorter one if the two overlap, regardless of their user-defined
    # weights. In this case, if the lengths are the same, then weight is used to disambiguate the
    # entities. If false, the redactor will choose the higher weighted entity when two overlap,
    # regardless of the length of the entity string. In this case, if the weights are the
    # same, then the redactor will choose the longer of the two entities.
    # Valid values: true, false
    #redactorPreferLength: true
    # Regular expressions and gazetteers may be configured to match tokens partially independent
    # from token boundaries. If true, reported offsets correspond to token boundaries.
    # Valid values: true, false
    #snapToTokenBoundaries: true
    # The option to resolve pronouns to person entities.
    # Valid values: true, false
    #resolvePronouns: false
    # Enable the REX Training Server (RTS) decoder and specify the workspace to be used.
    # Valid values:  "aQuotedWorkspaceIdString" (Pre 2.0)
    #                "workspace/model" (2.0 and later)
    #rtsDecoder: null
    # Additional gazetteer files used to reject entities for the given language.
    # This property accepts a list of values and the values are stored as a java.nio.file.Path.
    # Strings or URIs are valid values.  Entries are added below in the form:
    # - "/path/<language_code>/reject/example.txt"
    #rejectGazetteers: null
    # Custom list of Knowledge Bases for the linker, in order of priority.
    # This property accepts a list of values and the values are stored as a java.nio.file.Path.
    # Strings or URIs are valid values.  Entries are added below in the form:
    # - "${rex-root}/data/flinx/data/kb/<custom_kb_name>/"
    #kbs: null
    #Proxy server port for Indoc-Coref server (optional).
    #proxyServerPort: not set
    # The maximum number of entities for in-document coreference resolution (a.k.a. chaining).
    # Valid values:  Integers greater than 0.
    #maxResolvedEntities: 2000
    #Set to true to enable the indoc-coref-server. Valid values: true, false
    #useIndocServer: false
    # The maximum number of tokens allowed in an entity returned by Statistical Entity Extractor.
    # Entity Redactor discards entities from Statistical Entity Extractor with more than this number
    # of tokens.
    # Valid values:  Integers greater than 0.
    #maxEntityTokens: 8
    # List the set of active processors for an entity extraction run. All processors are active by
    # default. This method provides a way to turn off selected processors. The order of the
    # processors cannot be changed. Note that turning off redactor can cause overlapping and unsorted
    # entities to be returned.
    #
    # Default processors: acceptGazetteer, acceptRegex, rejectGazetteer, rejectRegex, statistical,
    #                     indocCoref, redactor, joiner
    #
    # Valid values: deepNeuralNetwork, indocCoref, acceptGazetteer, joiner, rejectRegex,
    #               statistical, nameClassifier, rejectGazetteer, acceptRegex, redactor, kbLinker,
    #               pronominalResolver
    #
    # These values are enums and should not be quoted.  Entries are added below in the form:
    # - acceptGazetteer
    # - acceptRegex
    #
    #processors: null
    #Milliseconds until a connection is established. 0 for an infinite timeout. -1 for system default.
    #connectionTimeout: -1
    # Enable only the REX Training Server (RTS) decoder (rtsDecoder must be used with this
    # parameter)
    # Valid values:  true, false
    #onlyRtsDecoder: false
    # The option to keep existing annotated text entities.
    # Valid values: true, false
    #keepEntitiesInInput: false
    # The option to assign default confidence value 1.0 to non-statistical entities instead of
    # null.
    # Valid values: true, false
    #useDefaultConfidence: false
    # Set extraction method to use on structured regions.
    # These values are enums and should not be quoted.
    # Valid values:  nerModel, nameClassifier, none
    #structuredRegionProcessingType: NULL
    # Custom processors to add to annotators. See AppDev guide for more details on custom processors.
    # This property accepts a list of values and the values are stored as Strings.
    # - "/path/to/MyCustomProcessor.java"
    #customProcessors: null
    # An option to calculate entity-chain salience values.
    # Valid values: true, false
    #calculateSalience: false
    # Additional files used to produce regex entities.
    # This property accepts a list of values and the values are stored as a java.nio.file.Path.
    # Strings or URIs are valid values.  Entries are added below in the form:
    # - "/path/<language_code>/accept/regexes.xml"
    #acceptRegularExpressionSets: null
  rex-no-resolution-factory-config.yaml: |
    # The REX root directory. A REX root directory contains language models and necessary configuration
    # files.
    # This value is stored as a java.nio.file.Path.  Strings or URIs are valid values.  E.g.
    # "/path/to/rex-root" or "file:///path/to/rex-root"
    rootDirectory: ${rex-root}
    # The RBL root directory for REX to use.
    # This value is stored as a java.nio.file.Path.  Strings or URIs are valid values.  E.g.
    # "/path/to/rex-root/rbl-je" or "file:///path/to/rex-root/rbl-je"
    rblRootDirectory: ${rex-root}/rbl-je
    # The option to resolve pronouns to person entities.
    # Valid values: true, false
    resolvePronouns: false
    # An option to calculate entity confidence values.
    # Valid values:  true, false
    calculateConfidence: false
    # The capitalization (aka 'case') used in the input texts. Processing standard documents
    # requires caseSensitive, which is the default. Documents with all-caps, no-caps or headline
    # capitalization may yield higher accuracy if processed with the caseInsensitive value.
    # These values are enums and should not be quoted.
    # Valid values: automatic, caseSensitive, caseInsensitive
    caseSensitivity: automatic
    # The option to search for and link mentions to knowledge base entities with disambiguation model.
    # Enabling this option also enables calculateConfidence.
    # Valid values:  true, false
    linkEntities: true
    #The entity types to be excluded from extraction.
    # This property accepts a list of values and the values are stored as Strings.
    # Valid values:  Person, Location, Organization, Product, Title
    # Other valid values are possible if you are using DB Pedia Types or have defined custom
    # entity types.
    #excludedEntityTypes: null
    # Additional files used to produce statistical entities for the given language.
    # This property accepts a list of values and the values are comma separated trios.
    # The trios of values specify the language, case-sensitivity and the model file,
    # separated by commas. See caseSensitivity for valid values.
    # For example:
    # - eng,caseSensitive,english-model.bin
    # - jpn,automatic,japanese-model.bin
    #statisticalModels: null
    #Milliseconds to wait for data to arrive. 0 for an infinite timeout. -1 for system default.
    #socketTimeout: -1
    # An option to calculate entity-chain salience with statistical-based calculation
    # (returns 0 or 1) or simple calculation (returns score between 0 and 1).
    # Valid values: true, false
    #statSalienceMode: true
    # The confidence value threshold below which entities extracted by the statistical processor
    # are ignored.
    # This property is stored as a Float.
    # Valid values:  -1.0 to 1.0
    #confidenceThreshold: -1.0
    # The confidence value threshold below which linking results by the kbLinker processor are
    # ignored.
    # This property is stored as a Float.
    # Valid values:  -1.0 to 1.0
    #linkingConfidenceThreshold: -1.0
    # Register a custom processor class. See AppDev guide for more details on custom processors.
    # This property accepts a list of values and the values are stored as Strings.
    # - "/path/to/MyCustomProcessor.class"
    #customProcessorClasses: null
    #Indoc-Coref server URL.
    #indocCorefServerUrl: not set
    # Apply joiner rules after redactor (old behavior).
    # Valid values: true, false
    #runJoinerPostRedactor: false
    # Additional gazetteer files used to produce entities for the given language.
    # This property accepts a list of values and the values are stored as a java.nio.file.Path.
    # Strings or URIs are valid values.  Entries are added below in the form:
    # - "/path/<language_code>/accept/example.txt"
    #acceptGazetteers: null
    # Enables the splitting of IDENTIFIER:MONEY regex matches into IDENTIFIER:CURRENCY_AMT and IDENTIFIER:CURRENCY_TYPE
    # where the amount of the currency will be represented in one entity and the type will be
    # represented in another.
    #regexCurrencySplit: false
    # An option for document entity resolution (also known as entity chaining).
    # These values are enums and should not be quoted.
    # Valid values:  HIGH, STANDARD, STANDARD_MINUS or NULL
    #indocType: STANDARD
    # The option to disable usage of the disambiguation model when linking.
    # Valid values: true, false",
    #disableLinkerDisambiguation: false
    # Additional joiner rules files.
    # This property accepts a list of values and the values are stored as a java.nio.file.Path.
    # Strings or URIs are valid values.  Entries are added below in the form:
    # - "/path/to/custom-joiners/my-joiner-rules.xml"
    #joinerRuleFiles: null
    # The option to allow partial gazetteer matches. For the purposes of this setting, a partial
    # match is one that does not line up with token boundaries as determined by the internal
    # tokenizer. This only applies to accept gazetteers.
    # Valid values:  true, false
    #allowPartialGazetteerMatches: false
    # Extract entities in structured regions with NER (perceptron, dnn) models if there are enough
    # tokens.
    # Valid values:  Integers greater than or equal to 0.
    #structuredRegionProcessingSentenceTokensMin: 0
    # An option to adjust offsets according to normalization.
    # Valid values: true, false
    #adjustNormalizedOffsets: false
    # The option to choose whether to look for candidate entities in text or link pre-existing mentions (extracted by other processors) to knowledge base entities with disambiguation model.
    # Valid values:  text, entities
    #linkMentionMode: text
    # The option to retain social media symbols ('@' and '#') in normalized output.
    # Valid values: true, false
    #retainSocialMediaSymbols: false
    # The overlay directory. An overlay directory is a directory shaped like the 'data' directory.
    # REX will look for files in both the overlay directory and the root directory, using files from
    # both locations. However, if a file exists in both places (as identified by its path relative
    # to the overlay or root data directory), REX prefers the version in the overlay directory.
    # If REX finds a zero-length file in the overlay directory, it ignores both that file and any
    # corresponding file in the root data directory.
    # This value is stored as a java.nio.file.Path.  Strings or URIs are valid values.  E.g.
    # "/path/to/overlay-data" or "file:///path/to/overlay-data"
    #dataOverlayDirectory: null
    #Proxy server host for Indoc-Coref server (optional).
    #proxyServer: not set
    # The option to add supplemental regex files, usually for entity types that are excluded by
    # default. The supplemental regex files are located at data/regex/<lang>/accept/supplemental and
    # are not used unless specified.
    # Strings or URIs are valid values.  Entries are added below in the form:
    # - "${rex-root}/data/regex/<lang>/accept/supplemental/date-regexes.xml"
    # - "data/regex/<lang>/accept/supplemental/time-regexes.xml"
    #supplementalRegularExpressionPaths: null
    # Additional regex files used to reject entities.
    # This property accepts a list of values and the values are stored as a java.nio.file.Path.
    # Strings or URIs are valid values.  Entries are added below in the form:
    # - "/path/<language_code>/reject/regexes.xml"
    #rejectRegularExpressionSets: null
    # The option to prefer length over weights during redaction. If true, the redactor will always
    # choose a longer entity over a shorter one if the two overlap, regardless of their user-defined
    # weights. In this case, if the lengths are the same, then weight is used to disambiguate the
    # entities. If false, the redactor will choose the higher weighted entity when two overlap,
    # regardless of the length of the entity string. In this case, if the weights are the
    # same, then the redactor will choose the longer of the two entities.
    # Valid values: true, false
    #redactorPreferLength: true
    # Regular expressions and gazetteers may be configured to match tokens partially independent
    # from token boundaries. If true, reported offsets correspond to token boundaries.
    # Valid values: true, false
    #snapToTokenBoundaries: true
    # Enable the REX Training Server (RTS) decoder and specify the workspace to be used.
    # Valid values:  "aQuotedWorkspaceIdString" (Pre 2.0)
    #                "workspace/model" (2.0 and later)
    #rtsDecoder: null
    # Additional gazetteer files used to reject entities for the given language.
    # This property accepts a list of values and the values are stored as a java.nio.file.Path.
    # Strings or URIs are valid values.  Entries are added below in the form:
    # - "/path/<language_code>/reject/example.txt"
    #rejectGazetteers: null
    # Custom list of Knowledge Bases for the linker, in order of priority.
    # This property accepts a list of values and the values are stored as a java.nio.file.Path.
    # Strings or URIs are valid values.  Entries are added below in the form:
    # - "${rex-root}/data/flinx/data/kb/<custom_kb_name>/"
    #kbs: null
    #Proxy server port for Indoc-Coref server (optional).
    #proxyServerPort: not set
    # The maximum number of entities for in-document coreference resolution (a.k.a. chaining).
    # Valid values:  Integers greater than 0.
    #maxResolvedEntities: 2000
    #Set to true to enable the indoc-coref-server. Valid values: true, false
    #useIndocServer: false
    # The maximum number of tokens allowed in an entity returned by Statistical Entity Extractor.
    # Entity Redactor discards entities from Statistical Entity Extractor with more than this number
    # of tokens.
    # Valid values:  Integers greater than 0.
    #maxEntityTokens: 8
    # List the set of active processors for an entity extraction run. All processors are active by
    # default. This method provides a way to turn off selected processors. The order of the
    # processors cannot be changed. Note that turning off redactor can cause overlapping and unsorted
    # entities to be returned.
    #
    # Default processors: acceptGazetteer, acceptRegex, rejectGazetteer, rejectRegex, statistical,
    #                     indocCoref, redactor, joiner
    #
    # Valid values: deepNeuralNetwork, indocCoref, acceptGazetteer, joiner, rejectRegex,
    #               statistical, nameClassifier, rejectGazetteer, acceptRegex, redactor, kbLinker,
    #               pronominalResolver
    #
    # These values are enums and should not be quoted.  Entries are added below in the form:
    # - acceptGazetteer
    # - acceptRegex
    #
    #processors: null
    #Milliseconds until a connection is established. 0 for an infinite timeout. -1 for system default.
    #connectionTimeout: -1
    # Enable only the REX Training Server (RTS) decoder (rtsDecoder must be used with this
    # parameter)
    # Valid values:  true, false
    #onlyRtsDecoder: false
    # The option to keep existing annotated text entities.
    # Valid values: true, false
    #keepEntitiesInInput: false
    # The option to assign default confidence value 1.0 to non-statistical entities instead of
    # null.
    # Valid values: true, false
    #useDefaultConfidence: false
    # Set extraction method to use on structured regions.
    # These values are enums and should not be quoted.
    # Valid values:  nerModel, nameClassifier, none
    #structuredRegionProcessingType: NULL
    # Custom processors to add to annotators. See AppDev guide for more details on custom processors.
    # This property accepts a list of values and the values are stored as Strings.
    # - "/path/to/MyCustomProcessor.java"
    #customProcessors: null
    # An option to calculate entity-chain salience values.
    # Valid values: true, false
    #calculateSalience: false
    # Additional files used to produce regex entities.
    # This property accepts a list of values and the values are stored as a java.nio.file.Path.
    # Strings or URIs are valid values.  Entries are added below in the form:
    # - "/path/<language_code>/accept/regexes.xml"
    #acceptRegularExpressionSets: null
  rli-factory-config.yaml: |
    #Root directory of your RLI-JE installation.
    rootDirectory: ${rli-root}
    #This option is deprecated. Use 'languageWeightAdjustments'.
    #
    #A language hint.
    #
    #The weight is a float from 1 to 99 (the default is 1.0). Standard analysis and
    #short-string analysis use this hint differently.
    #
    #In standard analysis the hint reduces the distance of the input profile from the
    #specified language's profile by the specified weight (treated as a percentage).
    #For example "nld" with the default weight reduces the distance from the Dutch
    #language profile by 1.0%. Another example: "deu" with a weight of 50.005
    #reduces the distance of the input profile from the German language profile by
    #50.005%. If the language is already known, then use a large hint weight to
    #suppress language detection, leaving only encoding and script detection to be
    #performed.
    #
    #For short-string analysis, the hint weight provides a greater boost to
    #confidence than for standard analysis. Confidence is boosted by 100/(100 - weight).
    #For example, a 50% French weight boosts French confidence by a factor of 2, and
    #75% French weight boosts confidence by a factor of 4.
    #languageHint: null
    #The short-string threshold.
    #
    #By default, the short-string threshold is 0 and short-string language detection
    #is inactive. To turn it on, set the threshold to a non-negative integer, such as
    #6. If the string contains fewer characters than this threshold, RLI-JE performs
    #short-string language detection. When RLI-JE is performing short-string language
    #detection, only 'languageHint' and language weight adjustments are used. The
    #other options are ignored.
    #shortStringThreshold: 0
    #Whether to force a language boundary when the script changes
    #breakRegionOnScriptBoundary: true
    #The maximum number of results to return.
    #maxResults: 5
    #The invalidity threshold.
    #
    #If the input profile distance measure is smaller than invalidityThreshold/100 ×
    #maximumProfileDistance, the result is flagged as valid. maximumProfileDistance
    #is the maximum distance any input text can have. If the threshold is 0, all
    #results are flagged as invalid. If the value is 100, all results are flagged
    #valid. The accepted value range is [0, 100].
    #invalidityThreshold: 99.0
    #Enables language region detection.
    #multilingual: false
    #The minimum length for a language region (the amount of text examined in a
    #script region). This is only used for language region detection.
    #minRegionLength: 20
    #The minimum number of valid non-whitespace characters required for
    #identification. If the input data has less than the minimum number of valid
    #characters, no detection is performed.
    #minValidChars: 4
    #The maximum region length. This is only used for language region detection.
    #maxRegionLength: 65536
    #A set of language weight adjustments.
    #
    #Use this setting to change the language weight for a language, for the purpose
    #of detecting another language that is present in the document. The weight
    #parameter is a non-negative percentage that is applied to the language weight;
    #70 reduces the weight to 70% of its default value. For example, you may be
    #interested in detecting German in a document that is mostly English but also
    #contains some German. Reduce the language weight for English or increase the
    #language weight for German.
    #languageWeightAdjustments: [ ]
    #This option is deprecated. Use 'languageWeightAdjustments'.
    #
    #An encoding hint.
    #
    #The weight is a float from 1 to 100 (the default is 3.0). The hint reduces the
    #distance from the input profile from the specified encoding's profile by the
    #specified weight (treated as a percentage). For example, "Ascii" with the
    #default weight reduces the distance from the input profile by 1.0%. The value of
    #100 forces only those results which match the encoding hint to be considered
    #during detection.
    #encodingHint: null
    #Whether to ignore script and encoding differences in language detection results.
    #By default, results that share a language but have different scripts or
    #encodings, such as Simplified Chinese and Traditional Chinese, are returned as
    #separate results. If script and encoding differences are ignored, only one
    #result is returned per language.
    #uniqueLanguages: false
    #The maximum number of results to return.
    #minNonScriptioContinuaRegionLength: 5
    #Enables language region detection.
    #koreanDialects: false
    #The XML license content. This overrides 'licensePath'.
    #licenseString: ${rootDirectory}/licenses/rlp-license.xml
    #The profile depth.
    #
    #Profile depth is the maximum number of n-grams to be used in the input profile.
    #If the depth is 100, the 100 most frequent n-grams are included in the input
    #profile. A small depth improves detection speed but reduces detection accuracy.
    #profileDepth: 1200
    #Path to a file containing your RLI-JE XML license. If you don't provide this
    #option, or 'licenseString', RLI-JE assumes your license is
    #"${rootDirectory}/licenses/rlp-license.xml".
    #licenseFile: ${rootDirectory}/licenses/rlp-license.xml
    #The ambiguity threshold.
    #
    #If the distance difference between the input profile and a candidate built-in
    #profile is smaller than ambiguityThreshold/100 × bestProfileDistance, the result
    #is flagged as ambiguous. bestProfileDistance is the distance measure of the best
    #matching profile. If the threshold is 0 then all results will be flagged as
    #unambiguous. If the value is 100 then all results will be flagged as ambiguous.
    #The accepted value range is [0, 100].
    #ambiguityThreshold: 2.0
  rni-dedupe-factory-config.yaml: |
    #The path to the root directory.
    rootDirectory: ${rni-rnt-root}
    #The XML license content.
    #licenseString: null
  rni-factory-config.yaml: ""
  rnt-factory-config.yaml: |
    #The path to the root directory
    rootDirectory: ${rni-rnt-root}
    #The XML license content.
    #licenseString: null
    #A mapping of languages to transliteration schemes.
    #schemeMap: null
  semantic-vectors-factory-config.yaml: |
    rblRootDirectory: ${rbl-root}
    rootDirectory: ${tvec-root}
    #numClosest: 0
    #queryLanguage: { }
    #resultLanguages: [ ]
    #embeddingsMode: { }
    #forceReadOOV: { }
    #allowWriteOOV: { }
    #language: { }
    #granularities: [ ]
    #embeddingsMode: { }
    #maxResults: 0
  sent-factory-config.yaml: |
    #Sets the root directory.
    #The file structure under this directory should look like:
    #data/
    #  dnn/
    #    eng/...
    #  svm/
    #    eng/...
    #    spa/...
    #licenses/
    #  rlp-license.xml
    #This option is required.
    rootDirectory: ${ascent-root}
    #The XML license content. This overrides 'licensePath'.
    #licenseString: null
    #Set the type of model.
    #Options are SVM and DNN.
    #modelType: SVM
    #The path of the license file.
    #If not set, a default model will be used.
    #licensePath: "${rootDirectory}/licenses/rlp-license.xml"
  similar-terms-factory-config.yaml: |
    rblRootDirectory: ${rbl-root}
    rootDirectory: ${tvec-root}
    #numClosest: 0
    #queryLanguage: { }
    #resultLanguages: [ ]
    #embeddingsMode: { }
    #forceReadOOV: { }
    #allowWriteOOV: { }
    #language: { }
    #granularities: [ ]
    #embeddingsMode: { }
    #maxResults: 0
  tokenization-factory-config.yaml: |
    #Whether to tokenize and analyze emoticons.
    emoticons: true
    #Whether the produced analyzers should disambiguate their results.
    #Disambiguation is not supported for all languages.
    disambiguate: false
    #Whether to tokenize and analyze URLs.
    urls: true
    #Whether to tokenize and analyze hashtags.
    hashtags: true
    #Whether to tokenize and analyze @mentions.
    atMentions: true
    #Whether or not to do analysis. If this is false, the annotator will only do
    #tokenization.
    analyze: false
    #Directs the use of an alternative tokenization algorithm. The specifics depend
    #on the language. Deprecated, replaced by 'tokenizerType'.
    alternativeTokenization: true
    #Indicates whether the single language annotators should convert part of speech
    #tags to their corresponding universal versions. If true, the universal tags are
    #returned. If false, the traditional tags are returned.
    universalPosTags: false
    #Whether to tokenize and analyze email addresses.
    emailAddresses: true
    #The root directory of the RBL-JE installation. Setting it sets default values
    #for otherwise required options.
    rootDirectory: ${rbl-root}
    #Configures the maximum number of tokens that can be in a line that is
    #considered short for the purposes of fragment boundary detection
    #maxTokensForShortLine: 6
    #Whether to use different tokenizers for different scripts (other than the
    #overall script). If false, uses the tokenizer for the
    #'defaultTokenizationLanguage'. Applies only to statistical segmentation
    #languages.
    #tokenizeForScript: false
    #Turns on fragment boundary detection.
    #fragmentBoundaryDetection: true
    #Controls the tokenization of contractions. If this is true, then contractions
    #such as "can't" are delivered as *multiple* tokens. By default, they are
    #delivered as one.
    #tokenizeContractions: false
    #The highest conversion level that will be used by the Chinese Script Converter.
    #conversionLevel: "lexemic"
    #Whether to consider punctuation between alphanumeric characters as a break. For
    #example, the text "www.basistech.com" is segmented as a single token when the
    #option is not set, but as five tokens when it is set: "www", ".", "basistech",
    #".", "com".
    #
    #Has no effect when 'consistentLatinSegmentation' is set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #breakAtAlphaNumIntraWordPunct: false
    #Whether to add a separator character between readings when concatenating
    #readings by character.
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #readingsSeparateSyllables: false
    #The language for RBL
    #language: null
    #Selects which tokenizer to use. "fst" selects a tokenizer that uses an
    #FST to determine word breaks, which is supported for Czech, Dutch, English,
    #French, German, Greek, Hungarian, Italian, Polish, Portuguese, Romanian, Russian,
    #and Spanish. "icu" selects a tokenizer that that uses ICU break rules to
    #determine word breaks. This is supported for all languages other than Chinese and
    #Japanese. "spaceless_lexical" selects a tokenizer that uses a lexical and rule-based
    #approach to determine word breaks in input without spaces between words. This is
    #supported for Chinese and Japanese. "spaceless_statistical" selects a tokenizer
    #that uses a statistical model to determine word breaks in input without spaces.
    #This is supported for Chinese, Japanese, Korean, and Thai. "default" selects the default
    #type for each language. The default type is "spaceless_statistical" for Chinese,
    #Japanese, and Thai, and "icu" for all other languages.
    #tokenizerType: "default"
    #Selects which disambiguator to use for Hebrew. dnn is deep neural network,
    #dictionary is a dictionary-based reranker, and perceptron is a perceptron.
    #
    #disambiguatorType: "perceptron"
    #Additional user defined segmentation dictionary configurations.
    #
    #You are required to set the 'language' and the 'dict' for every configuration.
    #Configurations missing one of those will be ignored.
    #segmentationUserDefinedDictionaries: [ ]
    #Enables faster (but less accurate) part of speech disambiguation for English.
    #alternativeEnglishDisambiguation: false
    #The path of the RBL-JE license file.
    #licensePath: "${rootDirectory}/licenses/rlp-license.xml"
    #The path to the lemma/compound dictionary directory.
    #dictionaryDirectory: "${rootDirectory}/dicts"
    #The representation of readings. Possible values (case-insensitive) are:
    #"tone_marks" (pinyin with diacritics over the appropriate vowels),
    #"tone_numbers" (pinyin with a number from 1 to 4 suffixed to each syllable, or
    #no number for neutral tone), "no_tones" (pinyin without tones), "cjktex"
    #(macros for the CJKTeX pinyin.sty style).
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #readingType: "tone_marks"
    #Whether to add readings to morphological analyses. The annotator will try to
    #add readings by whole words. If it cannot, it will fall back to concatenating
    #the readings of individual characters.
    #
    #Has no effect when 'alternativeTokenization' is not set.
    #readings: false
    #'tokenizeContractions' uses a default set of per-language rules. This option
    #allows the application to replace those rules.
    #customTokenizeContractionRules: null
    #Whether to join sequences of Katakana tokens adjacent to a middle dot token.
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the language
    #is not Japanese.
    #joinKatakanaNextToMiddleDot: true
    #Whether to add parts of speech to morphological analyses.
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the
    #language is not Chinese or Japanese.
    #pos: true
    #Additional user defined tokenization rules configurations.
    #
    #You are required to set the 'language' and the 'rules' for every configuration.
    #Configurations missing one of those will be ignored.
    #tokenizationUserRules: [ ]
    #Deprecated, replaced by 'tokenizerType'
    #alternativeJapaneseTokenization: false
    #Enables faster (but less accurate) part of speech disambiguation for Spanish.
    #alternativeSpanishDisambiguation: false
    #Configures the fragment boundary delimiters.
    #fragmentBoundaryDelimiters: "\u0009\u000B\u000C"
    #The maximum number of entries in the analysis cache. Larger values (up to some
    #empirical limit) increase throughput at the expense of extra memory. A value of
    #zero turns off caching completely.
    #analysisCacheSize: 100000
    #The minimum length of non-native text to be considered for a script change. A
    #script change indicates a boundary between tokens, so the length may influence
    #how a mixed-script string is tokenized.
    #
    #Has no effect when 'consistentLatinSegmentation' or when 'tokenizerType'
    #is not 'spaceless_lexical' or the language is not Chinese or Japanese.
    #minLengthForScriptChange: 10
    #Whether to skip directly to the fallback behavior of 'readings' without
    #considering readings for whole words.
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #readingByCharacter: false
    #Turns on Unicode NFKC normalization before tokenization.
    #nfkcNormalize: false
    #The language to which the Chinese Script Converter is converting.
    #targetLanguage: null
    #Whether to split prefixes off of unknown Hebrew words.
    #guessHebrewPrefixes: false
    #Whether to decompose compounds.
    #decomposeCompounds: true
    #Enables faster (but less accurate) part of speech disambiguation for Greek.
    #alternativeGreekDisambiguation: false
    #Turns on FST tokenization for supported languages. Deprecated,
    #replaced by 'tokenizerType'.
    #fstTokenize: false
    #Whether to add ADM extended properties to tokens. No extended properties
    #are currently enabled by this option. Deprecated.
    #deliverExtendedAttributes: false
    #Whether to return the surface forms of compound components.
    #compoundComponentSurfaceForms: false
    #Whether to segment place names from their suffixes. For example, "東京都" (Tokyo Metropolis) is segmented as two tokens ("東京" and
    #"都").
    #
    #Has no effect when 'alternativeTokenization' is not set.
    #separatePlaceNameFromSuffix: true
    #If true, analysis and tokenization are case-sensitive; otherwise
    #case-insensitive.
    #caseSensitive: true
    #Whether to treat whitespace as a number separator. If set, the text "1995 1996"
    #is segmented as two tokens; otherwise, the same text is segmented as a single
    #token.
    #
    #Has no effect when 'consistentLatinSegmentation' is set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #whiteSpaceIsNumberSep: false
    #A list of paths to many-to-one normalization dictionaries. Paths are separated
    #by the OS-specific path separator. If 'rootDirectory' is specified, the
    #substring "${rootDirectory}" is replaced with the root directory. If the
    #value is null, many-to-one normalization is not done.
    #normalizationDictionaryPaths: null
    #The XML license content. This overrides 'licensePath'.
    #licenseString: null
    #The language to use for script regions other than the script of the overall
    #language. Applies only to statistical segmentation languages.
    #defaultTokenizationLanguage: "xxx"
    #Whether to ignore whitespace separators when segmenting input text. When not
    #set, JLA will treat whitespace separators as morpheme delimiters. Note that
    #Japanese orthography allows a newline to occur in the middle of a word.
    #
    #Has no effect when 'whitespaceTokenization' is set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #ignoreSeparators: true
    #Whether to segment each run of numbers or Latin letters into its own token,
    #without splitting on medial number/word joiners.
    #
    #For example, "12.34AB-CD)E-" is split into "12.34", "AB-CD", ")", "E", and "-".
    #
    #Has no effect when 'consistentLatinSegmentation' is set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #segmentNonJapanese: true
    #Whether to recursively decompose each token into smaller tokens, if the token
    #is marked in the dictionary as being decomposable.
    #
    #For example, if only 'decomposeCompounds' is set, the token
    #"徳島大学分子酵素学研究センター" will only be decomposed into six smaller tokens:
    #"徳島大学", "分子", "酵素", "学", "研究", "センター".
    #However, because "徳島大学" is marked decomposable in the dictionary, if deep
    #decompounding is enabled, the first token will be further decomposed into two
    #more tokens for a total of seven tokens from the original string:
    #"徳島", "大学", "分子", "酵素", "学", "研究", and "センター".
    #None of these seven tokens is further decomposable.
    #
    #Has no effect when 'decomposeCompounds' is not set or when
    #'tokenizerType' is not 'spaceless_lexical' or the language is not
    #Chinese or Japanese.
    #deepCompoundDecomposition: false
    #Whether to use "v" instead of "ü" in pinyin readings, a common substitution in
    #environments that lack diacritics. The value is ignored when 'readingType' is
    #"cjktex" or "tone_marks", which always use "v" and "ü" respectively. It is
    #probably most useful when 'readingType' is "tone_numbers".
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #useVForUDiaeresis: false
    #Indicates whether to treat whitespace as a morpheme delimiter.
    #
    #Has no effect if the language is not Chinese or Japanese or
    #'tokenizerType' is not 'spaceless_lexical'.
    #whitespaceTokenization: false
    #Specifies the POS tag set to use.
    #posTagSet: "basis"
    #'universalPosTags' uses a set of per-language files to convert from Basis POS
    #tags to Universal POS Tags. This option allows the conversion to a different
    #set of POS tags.
    #customPosTags: null
    #Whether to provide consistent segmentation of embedded text not in Japanese
    #script, where "consistency" is measured against the behavior of the Chinese,
    #Japanese, and Korean segmenters previously available in Basis Technology's
    #native API, RLP. If this is true, then the setting of 'segmentNonJapanese' is
    #ignored.
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the language
    #is not Chinese or Japanese.
    #consistentLatinSegmentation: true
    #Indicates whether Hebrew root forms will be generated.
    #includeHebrewRoots: false
    #If a non primary script region is less than this length, and adjacent to a
    #primary script region, it is appended to the primary script region. Applies
    #only to statistical segmentation languages.
    #minNonPrimaryScriptRegionLength: 10
    #Deprecated, replaced by 'breakAtAlphaNumIntraWordPunct',
    #'consistentLatinSegmentation', 'decomposeCompounds',
    #'deepCompoundDecomposition', 'favorUserDictionary', 'generateAll',
    #'ignoreSeparators', 'ignoreStopwords', 'minLengthForScriptChange', 'pos',
    #'readingByCharacter', 'readings', 'readingsSeparateSyllables', 'readingType',
    #'segmentNonJapanese', 'separateNumbersFromCounters',
    #'separatePlaceNameFromSuffix', 'useVForUDiaeresis', 'whiteSpaceIsNumberSep', and
    #'whitespaceTokenization'
    #alternativeJapaneseTokenizationOptions: null
    #Indicates whether the analyzers should return extended tags, such as
    #morphological tags, with the analysis. Currently, they can only be returned
    #with the raw analysis. If true, the extended tags are returned if possible. If
    #false, extended tags are not returned.
    #deliverExtendedTags: false
    #Whether to return all the readings for a token. For characters with multiple
    #readings, all the readings are returned in brackets and separated by
    #semicolons.
    #
    #Has no effect when 'readings' is not set or when 'tokenizerType' is not
    #'spaceless_lexical' or the language is not Chinese or Japanese.
    #generateAll: false
    #Whether to favor words in the user dictionary during segmentation.
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the
    #language is not Chinese or Japanese.
    #favorUserDictionary: false
    #Request query processing. RBL may change its behavior to reflect the fact that
    #query input is often not in full sentences; typically, this disables
    #disambiguation.
    #query: false
    #Whether to filter stopwords out of the output.
    #ignoreStopwords: false
    #A directory where perceptron or other statistical model files may be found.
    #modelDirectory: "${rootDirectory}/models"
    #Additional user defined analysis dictionary configurations.
    #
    #You are required to set the 'language', 'dict', and 'options' for every
    #configuration. Configurations missing one of those will be ignored.
    #analysisUserDefinedDictionaries: [ ]
    #Whether to return numbers and counters as separate tokens. For example, "二匹"
    #(2 small animals) is split into "二" and "匹".
    #
    #Has no effect when 'tokenizerType' is not 'spaceless_lexical' or the
    #language is not Chinese or Japanese.
    #separateNumbersFromCounters: true
  topics-factory-config.yaml: |
    #the path to the topics data root directory
    rootDirectory: ${topics-root}
  transport-rules.tsv: "/address-similarity\t*\tlocal:\n/categories\t*\tlocal:\n/dte\t*\tlocal:\n/entities\t*\tlocal:\n/events\t*\tlocal:\n/language\t*\tlocal:\n/morphology\t*\tlocal:\n/name-deduplication\t*\tlocal:\n/name-similarity\t*\tlocal:\n/name-translation\t*\tlocal:\n/relationships\t*\tlocal:\n/semantics/similar\t*\tlocal:\n/semantics/vector\t*\tlocal:\n/sentences\t*\tlocal:\n/sentiment\t*\tlocal:\n/syntax/dependencies\t*\tlocal:\n/text-embedding\t*\tlocal:\n/tokens\t*\tlocal:\n/topics\t*\tlocal:\n/transliteration\t*\tlocal:\n"
  worker-config.yaml: |
    # components: defines a list of component
    # Each component sets up one or more SDK factories, as defined in the
    # rosette-osgi project. These are objects that implement
    # `RosetteComponentService`, and can deliver SDK factories in return for
    # YAML configuration of those factories.
    components:
    # categorization via tcat
    - componentName: categorization
      factories:
        default: cat-factory-config.yaml
    - componentName: dependency-parsing
      factories:
        default: dp-factory-config.yaml
    # RBL
    - componentName: base-linguistics
      factories:
        default: rbl-factory-config.yaml
        statistically-tokenize: default-only-tokenization-factory-config.yaml
        tokenize: tokenization-factory-config.yaml
        analyze: analyze-factory-config.yaml
    # RCT
    - componentName: arabic-chat-transliterator
      factories:
        default: rct-factory-config.yaml
    # REX
    - componentName: entity-extraction
      factories:
        default: rex-factory-config.yaml
        no-resolution: rex-no-resolution-factory-config.yaml
    # Events
    - componentName: event-extractor
      factories:
        default: event-extractor-factory-config.yaml
    # RELAX
    - componentName: relationship-extraction
      factories:
        default: relax-factory-config.yaml
    # RLI
    - componentName: language-identification
      factories:
        default: rli-factory-config.yaml
    # RNT
    - componentName: name-translation
      factories:
        default: rnt-factory-config.yaml
    # RNI
    - componentName: name-similarity
      factories:
        default: rni-factory-config.yaml
    # RNI Dedupe
    - componentName: name-deduplication
      factories:
        default: rni-dedupe-factory-config.yaml
     #sentiment via tcat
    - componentName: sentiment
      factories:
        default: sent-factory-config.yaml
    # TVEC
    - componentName: semantic-vectors
      factories:
        default: semantic-vectors-factory-config.yaml
    - componentName: similar-terms
      factories:
        default: similar-terms-factory-config.yaml
    # Topics
    - componentName: topics
      factories:
        default: topics-factory-config.yaml
    # Note: We don't have a factory because there's no SDK service
    - componentName: address-similarity
    - componentName: record-similarity
    #
    # Pipelines
    #
    # To put the factories to work, the configuration defines a series
    # of pipelines. Each pipeline handles a endpoint+language pair; either
    # language or endpoint can be '*' to mean 'any'.  The pipeline is a
    # series of components to run, defined in `steps`. For each component,
    # the configuration calls out the factory for it's SDK.
    # To specify any language, write `[ '*' ]`. Genre is optional.
    textPipelines:
    - endpoint: /syntax/dependencies
      languages: [ '*' ]
      steps:
      - componentName: dependency-parsing
    # Language detection. Not used, done in front end, delete?
    - endpoint: /language
      languages: [ '*' ]
      steps:
      - componentName: language-identification
    # Morphology
    - endpoint: /morphology
      languages: [ '*' ]
      steps:
      - componentName: base-linguistics
    # Entities
    - endpoint: /entities
      languages: [ '*' ]
      steps:
      - componentName: entity-extraction
      # events
    - endpoint: /events
      languages: [ '*' ]
      steps:
        - componentName: event-extractor
    # relationships
    - endpoint: /relationships
      languages: [ '*' ]
      steps:
      - componentName: base-linguistics
        factoryName: analyze
      - componentName: entity-extraction
      - componentName: dependency-parsing
      - componentName: relationship-extraction
    # sentiment
    - endpoint: /sentiment
      languages: [ 'ara', 'eng', 'fas', 'fra', 'jpn', 'spa', 'uen' ]
      steps:
      - componentName: entity-extraction
      - componentName: sentiment
    # categories
    - endpoint: /categories
      languages: [ 'eng', 'uen' ]
      steps:
      - componentName: base-linguistics
        factoryName: tokenize
      - componentName: categorization
    # semantic vectors
    - endpoint: /semantics/vector
      languages: [ 'ara', 'deu', 'eng', 'fas', 'fra', 'heb', 'hun', 'ita', 'jpn', 'kor', 'por', 'qkp', 'qkr', 'rus', 'spa', 'tgl', 'urd', 'zho' ]
      steps:
      - componentName: base-linguistics
        factoryName: statistically-tokenize
      - componentName: semantic-vectors
    - endpoint: /text-embedding
      languages: [ 'ara', 'deu', 'eng', 'fas', 'fra', 'heb', 'hun', 'ita', 'jpn', 'kor', 'por', 'qkp', 'qkr', 'rus', 'spa', 'tgl', 'urd', 'zho' ]
      steps:
      - componentName: base-linguistics
        factoryName: statistically-tokenize
      - componentName: semantic-vectors
    # similar terms
    - endpoint: /semantics/similar
      languages: [ 'ara', 'deu', 'eng', 'fas', 'fra', 'heb', 'hun', 'ita', 'jpn', 'kor', 'por', 'qkp', 'qkr', 'rus', 'spa', 'tgl', 'urd', 'zho' ]
      steps:
      - componentName: base-linguistics
        factoryName: statistically-tokenize
      - componentName: similar-terms
    # sentences
    - endpoint: /sentences
      languages: [ '*' ]
      steps:
        - componentName: base-linguistics
          factoryName: tokenize
    # tokens
    - endpoint: /tokens
      languages: [ '*' ]
      steps:
        - componentName: base-linguistics
          factoryName: tokenize
    # transliteration
    - endpoint: /transliteration
      languages: [ '*' ]
      steps:
      - componentName: arabic-chat-transliterator
    # topics
    - endpoint: /topics
      languages: [ 'eng' ]
      steps:
      - componentName: base-linguistics
        factoryName: analyze
      - componentName: entity-extraction
        factoryName: no-resolution
      - componentName: topics
    # Non-TextPipelines
    otherEndpoints:
    - endpoint: /name-translation
      componentName: name-translation
    - endpoint: /name-similarity
      componentName: name-similarity
    - endpoint: /name-deduplication
      componentName: name-deduplication
    - endpoint: /address-similarity
      componentName: address-similarity
    - endpoint: /record-similarity
      componentName: record-similarity
---
# Source: rosette-server/templates/hooks/cm-roots-override-files.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-rosette-server-roots-configs-override-scripts
data:
  new-entry.txt: |2+
  override-entry.txt: |2+
  delete-entry.txt: |2+
  install-upgrade.sh: |
    #!/bin/bash
    OPERATION_INFO_DIR=$(dirname $0)
    source "$OPERATION_INFO_DIR/utils.sh"
    OVERRIDE="/override"
    BACKUP="/backup"
    ROOTS_DIR="/roots"
    if [[ ! -d "$ROOTS_DIR" ]]; then
      error "Roots volume not found"
      exit 1
    fi
    if [[ -d "$BACKUP" ]]; then
      info "Backup volume found"
    else
      warn "Backup volume not found. Using roots volume"
      BACKUP="/$ROOTS_DIR/backup"
      if [[ ! -d "$BACKUP" ]]; then
        info "Creating backup directory on roots volume"
        mkdir "$BACKUP"
      fi
    fi
    # On install delete previous backups
    if [[ "$HELM_RELEASE_REVISION" -eq 1 ]]; then
      info "Previous backup found for ${HELM_RELEASE_NAME}. As a new release is being installed the previous backup is deleted."
      rm -rf "$BACKUP/$HELM_RELEASE_NAME"
    fi
    WORKDIR="$BACKUP/$HELM_RELEASE_NAME/revision-$HELM_RELEASE_REVISION"
    if [[ -d "$WORKDIR" ]]; then
      error "Backup volume already has rollback data for $HELM_RELEASE_NAME and revision $HELM_RELEASE_REVISION"
      exit 1
    fi
    info "Creating $WORKDIR"
    mkdir -p "$WORKDIR"
    REVERSE_OPS="$WORKDIR/reverse.operations"
    echo -e "-" > "$REVERSE_OPS"
    DELETED_DIR="$WORKDIR/deleted"
    mkdir -p "$DELETED_DIR"
    OVERWRITTEN_DIR="$WORKDIR/overwritten"
    mkdir -p "$OVERWRITTEN_DIR"
  
    startlogging "$WORKDIR/install-upgrade"
    # These files contain the addition, deletion and override operations to complete. They are using the information from the 'rootsOverride'
    # section of "values.yaml" and constructed in "templates/hooks/cm-roots-override-files.yaml".
    info "Copying operation files to $WORKDIR"
    cp "$OPERATION_INFO_DIR/new-entry.txt" "$WORKDIR/new-entry.txt"
    cp "$OPERATION_INFO_DIR/override-entry.txt" "$WORKDIR/override-entry.txt"
    cp "$OPERATION_INFO_DIR/delete-entry.txt" "$WORKDIR/delete-entry.txt"
    echo "$SEPARATOR" > "$WORKDIR/separator.txt"
  
    info "----Processing entry DELETION operations----"
    while read -r line; do
      ROOT=""
      ROOT=$(sed "s#\(.*\)${SEPARATOR}.*${SEPARATOR}.*#\1#" <<< $line)
      ROOT_VERSION=""
      ROOT_VERSION=$(sed "s#.*${SEPARATOR}\(.*\)${SEPARATOR}.*#\1#" <<< $line)
      TARGET_PATH=""
      TARGET_PATH=$(sed "s#.*${SEPARATOR}.*${SEPARATOR}/\?\(.*\)#\1#" <<< $line)
      TARGET_PATH_INVALID=""
      TARGET_PATH_INVALID=$(validate-target-path "$TARGET_PATH")
      if [[ $ROOT == "rnirnt" ]]; then
        ROOT="rni-rnt"
      fi
      TARGET="$ROOTS_DIR/$ROOT/$ROOT_VERSION/$TARGET_PATH"
      SKIP_DELETION=0
      if [[ "$TARGET_PATH_INVALID" -eq 1 ]]; then
        warn "Skipping deletion of $ROOT/$ROOT_VERSION/$TARGET_PATH. Target path is invalid. Make sure it is not empty or '/' and it doesn't contain '..'"
        SKIP_DELETION=1
      fi
      if [[ ! -e "$TARGET" ]]; then
        warn "Skipping deletion of $ROOT/$ROOT_VERSION/$TARGET_PATH as it doesn't exist"
        SKIP_DELETION=1
      fi
      if [[ $SKIP_DELETION -eq 0 ]]; then
        BACKUP_DIR="$ROOT/$ROOT_VERSION/$TARGET_PATH"
        BACKUP_DIR="$DELETED_DIR/${BACKUP_DIR//\//-}"
        mkdir -p "$BACKUP_DIR"
        cp -r "$TARGET" "$BACKUP_DIR"
        rm -rf "${TARGET}"
        sed -i "1 iDEL${SEPARATOR}${ROOT}${SEPARATOR}${ROOT_VERSION}${SEPARATOR}${TARGET_PATH}" "$REVERSE_OPS"
        info "Deleted $ROOT/$ROOT_VERSION/$TARGET_PATH"
      fi
    done < "$WORKDIR/delete-entry.txt"
  
    info "----Processing entry ADDITION operations----"
    while read -r line; do
      ROOT=""
      ROOT=$(sed "s#\(.*\)${SEPARATOR}.*${SEPARATOR}.*${SEPARATOR}.*#\1#" <<< $line)
      ROOT_VERSION=""
      ROOT_VERSION=$(sed "s#.*${SEPARATOR}\(.*\)${SEPARATOR}.*${SEPARATOR}.*#\1#" <<< $line)
      ORIGIN_PATH=""
      ORIGIN_PATH=$(sed "s#.*${SEPARATOR}.*${SEPARATOR}/\?\(.*\)${SEPARATOR}.*#\1#" <<< $line)
      TARGET_PATH=""
      TARGET_PATH=$(sed "s#.*${SEPARATOR}.*${SEPARATOR}.*${SEPARATOR}/\?\(.*\)#\1#" <<< $line)
      TARGET_PATH_INVALID=""
      TARGET_PATH_INVALID=$(validate-target-path "$TARGET_PATH")
      if [[ $ROOT == "rnirnt" ]]; then
        ROOT="rni-rnt"
      fi
      TARGET="$ROOTS_DIR/$ROOT/$ROOT_VERSION/$TARGET_PATH"
      ORIGIN="$OVERRIDE/$ORIGIN_PATH"
      SKIP_ADDITION=0
      if [[ ! -d "$OVERRIDE" ]]; then
        warn "Skipping adding $ORIGIN_PATH to $ROOT/$ROOT_VERSION/$TARGET_PATH. No override volume found."
        SKIP_ADDITION=1
      fi
      if [[ "$TARGET_PATH_INVALID" -eq 1 ]]; then
        warn "Skipping adding $ORIGIN_PATH to $ROOT/$ROOT_VERSION/$TARGET_PATH. Target path is invalid. Make sure it is not empty or '/' and it doesn't contain '..'"
        SKIP_ADDITION=1
      fi
      if [[ -e "$TARGET" ]]; then
        warn "Skipping adding $ORIGIN_PATH to $ROOT/$ROOT_VERSION/$TARGET_PATH as a file already exists there"
        SKIP_ADDITION=1
      fi
      if [[ ! -e "$ORIGIN" ]]; then
        warn "Skipping adding $ORIGIN_PATH to $ROOT/$ROOT_VERSION/$TARGET_PATH as $ORIGIN_PATH does not exist in the override volume"
        SKIP_ADDITION=1
      fi
      if [[ $SKIP_ADDITION -eq 0 ]]; then
        cp -r "$ORIGIN" "$TARGET"
        sed -i "1 iADD${SEPARATOR}${ROOT}${SEPARATOR}${ROOT_VERSION}${SEPARATOR}${TARGET_PATH}" "$REVERSE_OPS"
        info "Added $ORIGIN_PATH to $ROOT/$ROOT_VERSION/$TARGET_PATH"
      fi
    done < "$WORKDIR/new-entry.txt"
  
    info "----Processing entry OVERRIDE operations----"
    while read -r line; do
      ROOT=""
      ROOT=$(sed "s#\(.*\)${SEPARATOR}.*${SEPARATOR}.*${SEPARATOR}.*#\1#" <<< $line)
      ROOT_VERSION=""
      ROOT_VERSION=$(sed "s#.*${SEPARATOR}\(.*\)${SEPARATOR}.*${SEPARATOR}.*#\1#" <<< $line)
      ORIGIN_PATH=""
      ORIGIN_PATH=$(sed "s#.*${SEPARATOR}.*${SEPARATOR}/\?\(.*\)${SEPARATOR}.*#\1#" <<< $line)
      TARGET_PATH=""
      TARGET_PATH=$(sed "s#.*${SEPARATOR}.*${SEPARATOR}.*${SEPARATOR}/\?\(.*\)#\1#" <<< $line)
      TARGET_PATH_INVALID=""
      TARGET_PATH_INVALID=$(validate-target-path "$TARGET_PATH")
      if [[ $ROOT == "rnirnt" ]]; then
        ROOT="rni-rnt"
      fi
      TARGET="$ROOTS_DIR/$ROOT/$ROOT_VERSION/$TARGET_PATH"
      ORIGIN="$OVERRIDE/$ORIGIN_PATH"
      SKIP_OVERRIDE=0
      if [[ ! -d "$OVERRIDE" ]]; then
        warn "Skipping overriding $ROOT/$ROOT_VERSION/$TARGET_PATH with $ORIGIN_PATH. No override volume found."
        SKIP_OVERRIDE=1
      fi
      if [[ "$TARGET_PATH_INVALID" -eq 1 ]]; then
        warn "Skipping overriding $ROOT/$ROOT_VERSION/$TARGET_PATH with $ORIGIN_PATH. Target path is invalid. Make sure it is not empty or '/' and it doesn't contain '..'"
        SKIP_OVERRIDE=1
      fi
      if [[ ! -e "$TARGET" ]]; then
        warn "$ROOT/$ROOT_VERSION/$TARGET_PATH doesn't exist. Skipping override."
        SKIP_OVERRIDE=1
      fi
      if [[ ! -e "$ORIGIN" ]]; then
        warn "$ORIGIN_PATH doesn't exist in override volume. Cannot overwrite $ROOT/$ROOT_VERSION/$TARGET_PATH. Skipping override."
        SKIP_OVERRIDE=1
      fi
      if [[ -f "$TARGET" ]]; then
        if [[ ! -f "$ORIGIN" ]]; then
          warn "$ROOT/$ROOT_VERSION/$TARGET_PATH is a file. It cannot be overwritten by $ORIGIN_PATH because it is not a file. Skipping override."
          SKIP_OVERRIDE=1
        fi
      fi
      if [[ -d "$TARGET" ]]; then
        if [[ ! -d "$ORIGIN" ]]; then
          warn "$ROOT/$ROOT_VERSION/$TARGET_PATH is a directory. It cannot be overwritten by $ORIGIN_PATH because it is not a directory. Skipping override."
          SKIP_OVERRIDE=1
        fi
      fi
      if [[ $SKIP_OVERRIDE -eq 0 ]]; then
        BACKUP_DIR="$ROOT/$ROOT_VERSION/$TARGET_PATH"
        BACKUP_DIR="$OVERWRITTEN_DIR/${BACKUP_DIR//\//-}"
        mkdir -p "$BACKUP_DIR"
        cp -r "$TARGET" "$BACKUP_DIR"
        rm -rf "$TARGET"
        cp -r "$ORIGIN" "$TARGET"
        sed -i "1 iOVR${SEPARATOR}${ROOT}${SEPARATOR}${ROOT_VERSION}${SEPARATOR}${TARGET_PATH}" "$REVERSE_OPS"
        info "Overwritten $ROOT/$ROOT_VERSION/$TARGET_PATH with $ORIGIN_PATH"
      fi
    done < "$WORKDIR/override-entry.txt"
  
    info "Rolling the Rosette Server deployment"
    rollout-restart-rosette-server-deployment
  rollback.sh: |-
    #!/bin/bash
    OPERATION_INFO_DIR=$(dirname $0)
    source "$OPERATION_INFO_DIR/utils.sh"
    ROOTS_DIR="/roots"
    if [[ ! -d "$ROOTS_DIR" ]]; then
      error "Roots volume not found"
      exit 1
    fi
    BACKUP="/backup"
    if [[ ! -d "$BACKUP" ]]; then
      warn "No backup volume found"
      if [[ -d "$ROOTS_DIR/backup" ]]; then
        info "Roots volume has backup directory. Using it as the backup volume"
        BACKUP="$ROOTS_DIR/backup"
      else
        error "Backup volume not found"
        exit 1
      fi
    fi
    RELEASE_BACKUP="$BACKUP/$HELM_RELEASE_NAME"
    if [[ ! -d "$RELEASE_BACKUP" ]]; then
      error "Backup volume doesn't have a a directory for ${HELM_RELEASE_NAME} release with the previous changes. These are required for rollback."
      exit 1
    fi
    CURRENT_REVISION=0
    for revision in $(ls $RELEASE_BACKUP); do
      _REVISION=$(sed "s#revision-\(.*\)#\1#" <<< $revision)
      if [[ $_REVISION -gt $CURRENT_REVISION ]]; then
        CURRENT_REVISION=$_REVISION
      fi
    done
    DESIRED_REVISION=$HELM_RELEASE_REVISION
    HELM_RELEASE_REVISION=$((CURRENT_REVISION + 1))
    info "Rollback found that current version is $CURRENT_REVISION, the new rollback revision number is $HELM_RELEASE_REVISION and the desired rollback version is $DESIRED_REVISION"
    WORKDIR="$BACKUP/$HELM_RELEASE_NAME/revision-$HELM_RELEASE_REVISION"
    if [[ -d "$WORKDIR" ]]; then
      error "Backup volume already has rollback data for $HELM_RELEASE_NAME and revision $HELM_RELEASE_REVISION"
      exit 1
    fi
    info "Creating $WORKDIR"
    mkdir -p "$WORKDIR"
    REVERSE_OPS="$WORKDIR/reverse.operations"
    echo -e "-" > "$REVERSE_OPS"
    DELETED_DIR="$WORKDIR/deleted"
    mkdir -p "$DELETED_DIR"
    OVERWRITTEN_DIR="$WORKDIR/overwritten"
    mkdir -p "$OVERWRITTEN_DIR"
  
    startlogging "$WORKDIR/rollback"
    info "Creating rollback marker"
    ROLLBACK_MARKER="$WORKDIR/rollback.marker"
    echo -e "in-progress" > "$ROLLBACK_MARKER"
    echo "$SEPARATOR" > "$WORKDIR/separator.txt"
  
    while [[ $CURRENT_REVISION -gt $DESIRED_REVISION ]]; do
      info "-----The current revision of the rosette roots is ${CURRENT_REVISION}----"
      REVISION_DIRECTORY="$RELEASE_BACKUP/revision-${CURRENT_REVISION}"
      DO_REVERSE_OPERATIONS=1
      if [[ ! -d "$REVISION_DIRECTORY" ]]; then
        warn "No data found for ${REVISION_DIRECTORY}. This can be because override of the rosette root directories wasn't enabled for this revision. If no changes were made to the roots during this revision this shouldn't cause a problem. Skipping rollback of revision ${CURRENT_REVISION}"
        DO_REVERSE_OPERATIONS=0
        CURRENT_REVISION=$((CURRENT_REVISION - 1))
      fi
      # The current revision is a rollback
      if [[ -f "$REVISION_DIRECTORY/rollback.marker" ]]; then
        info "The revision is a rollback revision"
        ROLLBACK_TARGET_REVISION=$(cat "$REVISION_DIRECTORY/rollback.marker")
        if [[ "$ROLLBACK_TARGET_REVISION" != "in-progress" ]]; then
          if [[ $ROLLBACK_TARGET_REVISION -ge $DESIRED_REVISION ]]; then
            info "Roots state in revision ${CURRENT_REVISION} is the same as in revision ${ROLLBACK_TARGET_REVISION}. Continuing from revision ${ROLLBACK_TARGET_REVISION}"
            CURRENT_REVISION=$ROLLBACK_TARGET_REVISION
            DO_REVERSE_OPERATIONS=0
          fi
        else
          warn "This rollback wasn't completed correctly. No target revision found. Defaulting to rolling back operations"
        fi
      fi
      if [[ $DO_REVERSE_OPERATIONS -eq 1 ]]; then
        if [[ -f "$REVISION_DIRECTORY/reverse.operations" ]]; then
          info "Reversing operations of revision ${CURRENT_REVISION}"
          REVISION_SEPARATOR=""
          if [[ -f "$REVISION_DIRECTORY/separator.txt" ]]; then
            REVISION_SEPARATOR=$(cat "$REVISION_DIRECTORY/separator.txt")
          else
            warn "No separator string found in revision ${CURRENT_REVISION}. Defaulting to current separator ${SEPARATOR}"
            REVISION_SEPARATOR=$SEPARATOR
          fi
          while read -r line; do
            if [[ $line != "-" ]]; then # last line, do nothing
              OP=""
              OP=$(sed "s#\(.*\)${REVISION_SEPARATOR}.*${REVISION_SEPARATOR}.*${REVISION_SEPARATOR}.*#\1#" <<< $line)
              ROOT=""
              ROOT=$(sed "s#.*${REVISION_SEPARATOR}\(.*\)${REVISION_SEPARATOR}.*${REVISION_SEPARATOR}.*#\1#" <<< $line)
              ROOT_VERSION=""
              ROOT_VERSION=$(sed "s#.*${REVISION_SEPARATOR}.*${REVISION_SEPARATOR}\(.*\)${REVISION_SEPARATOR}.*#\1#" <<< $line)
              TARGET_PATH=""
              TARGET_PATH=$(sed "s#.*${REVISION_SEPARATOR}.*${REVISION_SEPARATOR}.*${REVISION_SEPARATOR}\(.*\)#\1#" <<< $line)
              TARGET_PATH_INVALID=""
              TARGET_PATH_INVALID=$(validate-target-path "$TARGET_PATH")
              REVERSE_STEP=1
              if [[ $OP == "" ]] || [[ $ROOT == "" ]]  || [[ $ROOT_VERSION == "" ]]; then
                error "Invalid reverse operations data line: ${line}. Some required information is missing. The line should be <operation><separator><root><separator><root-version><separator><file-path>. Skipping the reversal of this operation"
                REVERSE_STEP=0
              fi
              if [[ "$TARGET_PATH_INVALID" -eq 1 ]]; then
                warn "Skipping reversing operation $OP of $ROOT/$ROOT_VERSION/$TARGET_PATH. Target path is invalid. Make sure it is not empty or '/' and it doesn't contain '..'"
                REVERSE_STEP=0
              fi
              if [[ $REVERSE_STEP -eq 1 ]]; then
                TARGET="$ROOTS_DIR/$ROOT/$ROOT_VERSION/$TARGET_PATH"
                OPERATION_VALID=0
  
                if [[ $OP == "ADD" ]]; then
                  OPERATION_VALID=1
                  info "Reversing ADDITION of $ROOT/$ROOT_VERSION/$TARGET_PATH. Deleting it."
                  if [[ ! -e "$TARGET" ]]; then
                    warn "Skipping deletion of $ROOT/$ROOT_VERSION/$TARGET_PATH as it doesn't exist"
                  else
                    BACKUP_DIR="$ROOT/$ROOT_VERSION/$TARGET_PATH"
                    BACKUP_DIR="$DELETED_DIR/${BACKUP_DIR//\//-}"
                    mkdir -p "$BACKUP_DIR"
                    cp -r "$TARGET" "$BACKUP_DIR"
                    rm -rf "${TARGET}"
                    sed -i "1 iDEL${SEPARATOR}${ROOT}${SEPARATOR}${ROOT_VERSION}${SEPARATOR}${TARGET_PATH}" "$REVERSE_OPS"
                    info "Deleted $ROOT/$ROOT_VERSION/$TARGET_PATH"
                  fi
                fi
  
                if [[ $OP == "DEL" ]]; then
                  OPERATION_VALID=1
                  info "Reversing DELETION of $ROOT/$ROOT_VERSION/$TARGET_PATH. Re-adding it."
                  REVISION_DELETED_DIR="$REVISION_DIRECTORY/deleted"
                  OPERATION_BACKUP_DIR="$ROOT/$ROOT_VERSION/$TARGET_PATH"
                  ENTRY_NAME=$(basename $TARGET_PATH)
                  ORIGIN="$REVISION_DELETED_DIR/${OPERATION_BACKUP_DIR//\//-}/$ENTRY_NAME"
                  SKIP_ADDITION=0
                  if [[ -e "$TARGET" ]]; then
                    warn "Skipping re-adding $ROOT/$ROOT_VERSION/$TARGET_PATH as a file already exists there"
                    SKIP_ADDITION=1
                  fi
                  if [[ ! -e "$ORIGIN" ]]; then
                    warn "Skipping re-adding $ROOT/$ROOT_VERSION/$TARGET_PATH as no backup was found"
                    SKIP_ADDITION=1
                  fi
                  if [[ $SKIP_ADDITION -eq 0 ]]; then
                    cp -r "$ORIGIN" "$TARGET"
                    sed -i "1 iADD${SEPARATOR}${ROOT}${SEPARATOR}${ROOT_VERSION}${SEPARATOR}${TARGET_PATH}" "$REVERSE_OPS"
                    info "Added the backup to $ROOT/$ROOT_VERSION/$TARGET_PATH"
                  fi
                fi
  
                if [[ $OP == "OVR" ]]; then
                  OPERATION_VALID=1
                  info "Reversing OVERRIDE of $ROOT/$ROOT_VERSION/$TARGET_PATH. Overriding it with the backed up version."
                  REVISION_OVERRIDE_DIR="$REVISION_DIRECTORY/overwritten"
                  OPERATION_BACKUP_DIR="$ROOT/$ROOT_VERSION/$TARGET_PATH"
                  ENTRY_NAME=$(basename $TARGET_PATH)
                  ORIGIN="$REVISION_OVERRIDE_DIR/${OPERATION_BACKUP_DIR//\//-}/$ENTRY_NAME"
                  SKIP_OVERRIDE=0
                  if [[ ! -e "$TARGET" ]]; then
                    warn "$ROOT/$ROOT_VERSION/$TARGET_PATH doesn't exist. Skipping override reversal."
                    SKIP_OVERRIDE=1
                  fi
                  if [[ ! -e "$ORIGIN" ]]; then
                    warn "No backup was found from the original override. Cannot overwrite $ROOT/$ROOT_VERSION/$TARGET_PATH. Skipping override reversal."
                    SKIP_OVERRIDE=1
                  fi
                  if [[ -f "$TARGET" ]]; then
                    if [[ ! -f "$ORIGIN" ]]; then
                      warn "$ROOT/$ROOT_VERSION/$TARGET_PATH is a file. It cannot be overwritten by the backup because it is not a file. Skipping override."
                      SKIP_OVERRIDE=1
                    fi
                  fi
                  if [[ -d "$TARGET" ]]; then
                    if [[ ! -d "$ORIGIN" ]]; then
                      warn "$ROOT/$ROOT_VERSION/$TARGET_PATH is a directory. It cannot be overwritten by the backup because it is not a directory. Skipping override."
                      SKIP_OVERRIDE=1
                    fi
                  fi
                  if [[ $SKIP_OVERRIDE -eq 0 ]]; then
                    BACKUP_DIR="$ROOT/$ROOT_VERSION/$TARGET_PATH"
                    BACKUP_DIR="$OVERWRITTEN_DIR/${BACKUP_DIR//\//-}"
                    mkdir -p "$BACKUP_DIR"
                    cp -r "$TARGET" "$BACKUP_DIR"
                    rm -rf "$TARGET"
                    cp -r "$ORIGIN" "$TARGET"
                    sed -i "1 iOVR${SEPARATOR}${ROOT}${SEPARATOR}${ROOT_VERSION}${SEPARATOR}${TARGET_PATH}" "$REVERSE_OPS"
                    info "Overwritten $ROOT/$ROOT_VERSION/$TARGET_PATH with the backed up version"
                  fi
                fi
  
                if [[ $OPERATION_VALID -eq 0 ]]; then
                  error "Unknown operation $OP. Skipping reversal of $line"
                fi
              fi
            fi
          done < "$REVISION_DIRECTORY/reverse.operations"
        else
          error "No reverse operations file found in revision ${CURRENT_REVISION}. Cannot reverse changes. Skipping rollback of revision ${CURRENT_REVISION}"
        fi
        CURRENT_REVISION=$((CURRENT_REVISION - 1))
      fi
    done
    if [[ $CURRENT_REVISION -eq $DESIRED_REVISION ]]; then
      info "Done! Roots state is the same as it was at revision $DESIRED_REVISION"
      echo -e "$DESIRED_REVISION" > "$ROLLBACK_MARKER"
    fi
  
    info "Rolling the Rosette Server deployment"
    rollout-restart-rosette-server-deployment
  utils.sh: |-
    #!/bin/bash
    _LOGFILE="/dev/null"
    function startlogging() {
      local TS=$(date +%d-%m-%y_%H-%M)
      if [[ ! -z "$1" ]]; then
        _LOGFILE="${1}.$TS.log"
        if [[ ! -f "${_LOGFILE}" ]]; then
          printf "Logging %s at %s\n" "${1}" $TS > "${_LOGFILE}"
        fi
      else
        echo "1 is empty. It is $1"
        _LOGFILE="/dev/null"
      fi
    }
  
    function stoplogging() {
      _LOGFILE="/dev/null"
    }
  
    function info() {
      echo -e "[INFO] $1" | tee -a "${_LOGFILE}" >&1
    }
  
    function warn() {
      echo -e "[WARNING] $1" | tee -a "${_LOGFILE}" >&1
    }
  
    function error() {
      echo -e "[ERROR] $1" | tee -a "${_LOGFILE}" >&1
    }
  
    function validate-target-path() {
      _TARGET_PATH=$1
      RESULT=0
      if [[ "$_TARGET_PATH" == "" ]] ; then
        RESULT=1
      fi
      ## Disallow going up the file tree, to prevent override/deletion of files outside of the roots
      if [[ "$_TARGET_PATH" =~ \.\. ]]; then
        RESULT=1
      fi
      echo $RESULT
    }
  
    function rollout-restart-rosette-server-deployment() {
      # Point to the internal API server hostname
      APISERVER=https://kubernetes.default.svc
  
      # Path to ServiceAccount token
      SERVICEACCOUNT=/var/run/secrets/kubernetes.io/serviceaccount
  
      # Read this Pod's namespace
      NAMESPACE=$(cat ${SERVICEACCOUNT}/namespace)
  
      # Read the ServiceAccount bearer token
      TOKEN=$(cat ${SERVICEACCOUNT}/token)
  
      # Reference the internal certificate authority (CA)
      CACERT=${SERVICEACCOUNT}/ca.crt
  
      CURRENT_TIME=$(date)
  
      # Update the Rosette Server deployment template to force rolling the pods
      curl -s -o /dev/null --cacert ${CACERT} --header "Authorization: Bearer ${TOKEN}" --header "Content-type: application/strategic-merge-patch+json" -X PATCH \
      ${APISERVER}/apis/apps/v1/namespaces/${NAMESPACE}/deployments/${RS_DEPLOYMENT} \
      -d "{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"rootsExtracted\":\"${CURRENT_TIME}\"}}}}}"
    }
---
# Source: rosette-server/templates/pvc-roots.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-release-rosette-server-roots-claim
  labels:
    helm.sh/chart: rosette-server-1.1.0
    app.kubernetes.io/name: rosette-server
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.29.0"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: rosette-server
      app.kubernetes.io/instance: my-release
  storageClassName: ""
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 150Gi
---
# Source: rosette-server/templates/hooks/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: my-release-rosette-server-role
rules:
  - apiGroups: ["apps"]
    resources: ["deployments"]
    resourceNames: [ "my-release-rosette-server"]
    verbs: ["get", "patch"]
---
# Source: rosette-server/templates/hooks/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: my-release-rosette-server-role-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: my-release-rosette-server-role
subjects:
  - kind: ServiceAccount
    name: my-release-rosette-server-patch-deployment-sa
---
# Source: rosette-server/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-rosette-server
  labels:
    helm.sh/chart: rosette-server-1.1.0
    app.kubernetes.io/name: rosette-server
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.29.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8181
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: rosette-server
    app.kubernetes.io/instance: my-release
---
# Source: rosette-server/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-rosette-server
  labels:
    helm.sh/chart: rosette-server-1.1.0
    app.kubernetes.io/name: rosette-server
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.29.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: rosette-server
      app.kubernetes.io/instance: my-release
  template:
    metadata:
      annotations:
        checksum/conf: 75c10f11782571deaf891b610c654d50733805c592b45a8f0cc7b56c7e7d3fb5
        checksum/config: 86a35cd044f950341ddd2cc8262c115d923eafc9552111a5215de442a1c0bbb1
        checksum/rosapi: 4f49a377772b247a22294f079b1c5f32a5b41a31c501e8e6feead8a3fa9c4032
        checksum/license-file: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
        checksum/enabledLanguages: 4fde15d476decec249afda86559979c3c651a24c41de63cb2f92d339f43fb737
        checksum/roots-versions: 8c273e842faf1d3590818726c2f751e045d3d7edab5a02de518d3b3a7d61a092
        
      labels:
        app.kubernetes.io/name: rosette-server
        app.kubernetes.io/instance: my-release
    spec:
      serviceAccountName: my-release-rosette-server
      securityContext:
        {}
      initContainers:
        - name: rosette-server-init
          image: "rosette/server-enterprise:1.29.0"
          command: [ 'bash', '-c',
                     'bash /init-scripts/init.sh'
          ]
          env:
            - name : ENDPOINTS
              value: language
          volumeMounts:
            - mountPath: "/configs"
              name: config
            - mountPath: "/rosapi"
              name: config-rosapi
            - mountPath: "/override"
              name: init-override
            - mountPath: "/init-scripts"
              name: init-scripts
      containers:
        - name: rosette-server
          securityContext:
            {}
          image: "rosette/server-enterprise:1.29.0"
          command: ["/bin/bash","-c"]
          args: ["/rosette/server/bin/launch.sh console"]
          imagePullPolicy: IfNotPresent
          env:
            - name: ROSAPI_LICENSE_PATH
              value: /rosette/server/config/rosapi/rosette-license.xml
          ports:
            - name: http
              containerPort: 8181
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /rest/v1/info
              port: http
            failureThreshold: 3
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 5
          readinessProbe:
            httpGet:
              path: /rest/v1/info
              port: http
            failureThreshold: 3
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 5
          volumeMounts:
            - mountPath: "/rosette/server/config/rosapi/rosette-license.xml"
              subPath: "rosette-license.xml"
              name: rosette-license
              readOnly: true
            - mountPath: "/rosette/server/launcher/config/com.basistech.downloadextract.cfg"
              name: config
              subPath: com.basistech.downloadextract.cfg
            - mountPath: "/rosette/server/launcher/config/com.basistech.rli.cfg"
              name: config
              subPath: com.basistech.rli.cfg
            - mountPath: "/rosette/server/launcher/config/com.basistech.ws.addresses.cfg"
              name: config
              subPath: com.basistech.ws.addresses.cfg
            - mountPath: "/rosette/server/launcher/config/com.basistech.ws.cxf.cfg"
              name: config
              subPath: com.basistech.ws.cxf.cfg
            - mountPath: "/rosette/server/launcher/config/com.basistech.ws.dedupe.cfg"
              name: config
              subPath: com.basistech.ws.dedupe.cfg
            - mountPath: "/rosette/server/launcher/config/com.basistech.ws.doc.cfg"
              name: config
              subPath: com.basistech.ws.doc.cfg
            - mountPath: "/rosette/server/launcher/config/com.basistech.ws.fe.health.cfg"
              name: config
              subPath: com.basistech.ws.fe.health.cfg
            - mountPath: "/rosette/server/launcher/config/com.basistech.ws.frontend.cfg"
              name: config
              subPath: com.basistech.ws.frontend.cfg
            - mountPath: "/rosette/server/launcher/config/com.basistech.ws.local.usage.tracker.cfg"
              name: config
              subPath: com.basistech.ws.local.usage.tracker.cfg
            - mountPath: "/rosette/server/launcher/config/com.basistech.ws.metrics.prometheus.cfg"
              name: config
              subPath: com.basistech.ws.metrics.prometheus.cfg
            - mountPath: "/rosette/server/launcher/config/com.basistech.ws.requesttracker.cfg"
              name: config
              subPath: com.basistech.ws.requesttracker.cfg
            - mountPath: "/rosette/server/launcher/config/com.basistech.ws.rni.cfg"
              name: config
              subPath: com.basistech.ws.rni.cfg
            - mountPath: "/rosette/server/launcher/config/com.basistech.ws.rnt.cfg"
              name: config
              subPath: com.basistech.ws.rnt.cfg
            - mountPath: "/rosette/server/launcher/config/com.basistech.ws.tracker.logstash.cfg"
              name: config
              subPath: com.basistech.ws.tracker.logstash.cfg
            - mountPath: "/rosette/server/launcher/config/com.basistech.ws.transport.embedded.cfg"
              name: config
              subPath: com.basistech.ws.transport.embedded.cfg
            - mountPath: "/rosette/server/launcher/config/com.basistech.ws.transport.http.cfg"
              name: config
              subPath: com.basistech.ws.transport.http.cfg
            - mountPath: "/rosette/server/launcher/config/endpoint-override.yaml"
              name: config
              subPath: endpoint-override.yaml
            - mountPath: "/rosette/server/launcher/config/jetty-ssl-config.xml"
              name: config
              subPath: jetty-ssl-config.xml
            - mountPath: "/rosette/server/launcher/config/org.apache.cxf.http.jetty-main.cfg"
              name: config
              subPath: org.apache.cxf.http.jetty-main.cfg
            - mountPath: "/rosette/server/launcher/config/org.apache.cxf.osgi.cfg"
              name: config
              subPath: org.apache.cxf.osgi.cfg
            - mountPath: "/rosette/server/launcher/config/rosapi"
              name: config-rosapi
            - mountPath: "/rosette/server/conf"
              name: server-conf
            - mountPath: "/rosette/server/roots"
              name: roots
            - mountPath: "/rosette/server/launcher/config/com.basistech.ws.worker.cfg"
              name: init-override
              subPath: "com.basistech.ws.worker.cfg"
            - mountPath: "/rosette/server/override/config/enabled-endpoints.yaml"
              name: init-override
              subPath: "enabled-endpoints.yaml"

          resources:
            requests:
              ephemeral-storage: 2Gi
      volumes:
        - name: rosette-license
          secret:
            secretName: 
        - name: config
          configMap:
            name: my-release-rosette-server-config
        - name: config-rosapi
          configMap:
            name: my-release-rosette-server-rosapi
        - name: server-conf
          configMap:
            name: my-release-rosette-server-conf
        - name: roots
          persistentVolumeClaim:
            claimName: my-release-rosette-server-roots-claim
        - name: init-scripts
          configMap:
            name: my-release-rosette-server-init-scripts
        - name: init-override
          emptyDir:
            sizeLimit: 5Mi
---
# Source: rosette-server/templates/hooks/postinstall-upgrade-roots-hook.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: my-release-populate-roots
  annotations:
    "helm.sh/hook": post-install,post-upgrade
    "helm.sh/hook-delete-policy": hook-succeeded,hook-failed
    "helm.sh/hook-weight": "0"
spec:
  template:
    metadata:
      name: my-release-populate-roots
    spec:
      serviceAccountName: my-release-rosette-server-patch-deployment-sa
      volumes:
        - name: roots-storage
          persistentVolumeClaim:
            claimName: my-release-rosette-server-roots-claim
        - name: roots-config-override-scripts
          configMap:
            name: my-release-rosette-server-roots-configs-override-scripts
      restartPolicy: Never
      initContainers:
        # These roots cover, language, morph, sent, tokens
        - name: my-release-populate-rli
          image: "rosette/root-rli:7.23.12.c73.0"
          volumeMounts:
            - mountPath: "/roots-vol"
              name: roots-storage
        - name: my-release-populate-rbl
          image: "rosette/root-rbl:7.47.2.c73.0"
          volumeMounts:
            - mountPath: "/roots-vol"
              name: roots-storage
      containers:
        - name: rosette-roots-override
          image: "rosette/server-enterprise:1.29.0"
          command: ['bash','-c','source /workspace/utils.sh && info "Roots override is not enabled" && info "Rolling the Rosette Server deployment" && rollout-restart-rosette-server-deployment']
          env:
            - name: HELM_RELEASE_NAME
              value: "my-release"
            - name: HELM_RELEASE_REVISION
              value: "1"
            - name: SEPARATOR
              value: "&&&"
            - name: RS_DEPLOYMENT
              value: "my-release-rosette-server"
          volumeMounts:
            - mountPath: "/roots"
              name: roots-storage
            - mountPath: "/workspace"
              name: roots-config-override-scripts
---
# Source: rosette-server/templates/hooks/postrollback-roots-hook.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: my-release-populate-roots
  annotations:
    "helm.sh/hook": post-rollback
    "helm.sh/hook-delete-policy": hook-succeeded,hook-failed
    "helm.sh/hook-weight": "0"
spec:
  template:
    metadata:
      name: my-release-populate-roots
    spec:
      serviceAccountName: my-release-rosette-server-patch-deployment-sa
      volumes:
        - name: roots-storage
          persistentVolumeClaim:
            claimName: my-release-rosette-server-roots-claim
        - name: roots-config-override-scripts
          configMap:
            name: my-release-rosette-server-roots-configs-override-scripts
      restartPolicy: Never
      initContainers:
        # These roots cover, language, morph, sent, tokens
        - name: my-release-populate-rli
          image: "rosette/root-rli:7.23.12.c73.0"
          volumeMounts:
            - mountPath: "/roots-vol"
              name: roots-storage
        - name: my-release-populate-rbl
          image: "rosette/root-rbl:7.47.2.c73.0"
          volumeMounts:
            - mountPath: "/roots-vol"
              name: roots-storage
      containers:
        - name: rosette-roots-override
          image: "rosette/server-enterprise:1.29.0"
          command: ['bash','-c','source /workspace/utils.sh && info "Roots override is not enabled" && info "Rolling the Rosette Server deployment" && rollout-restart-rosette-server-deployment']
          env:
            - name: HELM_RELEASE_NAME
              value: "my-release"
            - name: HELM_RELEASE_REVISION
              value: "1"
            - name: SEPARATOR
              value: "&&&"
            - name: RS_DEPLOYMENT
              value: "my-release-rosette-server"
          volumeMounts:
            - mountPath: "/roots"
              name: roots-storage
            - mountPath: "/workspace"
              name: roots-config-override-scripts
