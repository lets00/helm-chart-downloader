---
# Source: pysequila/charts/jupyterhub/templates/hub/pdb.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: hub
  labels:
    component: hub
    app: jupyterhub
    release: my-release
    chart: jupyterhub-0.9.1
    heritage: Helm
spec:
  minAvailable: 1
  selector:
    matchLabels:
      component: hub
      app: jupyterhub
      release: my-release
---
# Source: pysequila/charts/jupyterhub/templates/proxy/pdb.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: proxy
  labels:
    component: proxy
    app: jupyterhub
    release: my-release
    chart: jupyterhub-0.9.1
    heritage: Helm
spec:
  minAvailable: 1
  selector:
    matchLabels:
      component: proxy
      app: jupyterhub
      release: my-release
---
# Source: pysequila/charts/jupyterhub/templates/scheduling/user-placeholder/pdb.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: user-placeholder
  labels:
    component: user-placeholder
    app: jupyterhub
    release: my-release
    chart: jupyterhub-0.9.1
    heritage: Helm
spec:
  minAvailable: 0
  selector:
    matchLabels:
      component: user-placeholder
      app: jupyterhub
      release: my-release
---
# Source: pysequila/charts/jupyterhub/templates/scheduling/user-scheduler/pdb.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: user-scheduler
  labels:
    component: user-scheduler
    app: jupyterhub
    release: my-release
    chart: jupyterhub-0.9.1
    heritage: Helm
spec:
  minAvailable: 1
  selector:
    matchLabels:
      component: user-scheduler
      app: jupyterhub
      release: my-release
---
# Source: pysequila/charts/jupyterhub/templates/hub/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: hub
  labels:
    component: hub
    app: jupyterhub
    release: my-release
    chart: jupyterhub-0.9.1
    heritage: Helm
---
# Source: pysequila/charts/jupyterhub/templates/scheduling/user-scheduler/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: user-scheduler
  labels:
    component: user-scheduler
    app: jupyterhub
    release: my-release
    chart: jupyterhub-0.9.1
    heritage: Helm
---
# Source: pysequila/charts/jupyterhub/templates/hub/secret.yaml
kind: Secret
apiVersion: v1
metadata:
  name: hub-secret
  labels:
    component: hub
    app: jupyterhub
    release: my-release
    chart: jupyterhub-0.9.1
    heritage: Helm
type: Opaque
data:
  proxy.token: "Y2hhbmdlbWU="
  values.yaml: "YXV0aDoge30KaHViOgogIHNlcnZpY2VzOiB7fQ=="
---
# Source: pysequila/charts/jupyterhub/templates/hub/configmap.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: hub-config
  labels:
    component: hub
    app: jupyterhub
    release: my-release
    chart: jupyterhub-0.9.1
    heritage: Helm
data:
  values.yaml: |
    Chart:
      Name: jupyterhub
      Version: 0.9.1
    Release:
      Name: my-release
      Namespace: pysequila-0.1.3.tgz
      Service: Helm
    auth:
      admin:
        access: true
      dummy: {}
      ldap:
        dn:
          search: {}
          user: {}
        user: {}
      state:
        enabled: false
      type: dummy
      whitelist: {}
    cull:
      concurrency: 10
      enabled: true
      every: 600
      maxAge: 0
      removeNamedServers: false
      timeout: 3600
      users: false
    custom: {}
    debug:
      enabled: false
    hub:
      allowNamedServers: false
      annotations: {}
      baseUrl: /
      concurrentSpawnLimit: 64
      consecutiveFailureLimit: 5
      db:
        pvc:
          accessModes:
          - ReadWriteOnce
          annotations: {}
          selector: {}
          storage: 1Gi
        type: sqlite-pvc
      deploymentStrategy:
        type: Recreate
      extraConfig: {}
      extraContainers: []
      extraVolumeMounts: []
      extraVolumes: []
      fsGid: 1000
      image:
        name: jupyterhub/k8s-hub
        tag: 0.9.1
      imagePullSecret:
        enabled: false
      initContainers: []
      labels: {}
      livenessProbe:
        enabled: false
        initialDelaySeconds: 30
        periodSeconds: 10
      networkPolicy:
        egress:
        - to:
          - ipBlock:
              cidr: 0.0.0.0/0
        enabled: false
        ingress: []
      nodeSelector: {}
      pdb:
        enabled: true
        minAvailable: 1
      readinessProbe:
        enabled: true
        initialDelaySeconds: 0
        periodSeconds: 10
      resources:
        requests:
          cpu: 200m
          memory: 512Mi
      service:
        annotations: {}
        ports: {}
        type: ClusterIP
      services: {}
      templatePaths: []
      templateVars: {}
      uid: 1000
    scheduling:
      corePods:
        nodeAffinity:
          matchNodePurpose: prefer
      podPriority:
        defaultPriority: 0
        enabled: false
        globalDefault: false
        userPlaceholderPriority: -10
      userPlaceholder:
        enabled: true
        replicas: 0
      userPods:
        nodeAffinity:
          matchNodePurpose: prefer
      userScheduler:
        enabled: true
        image:
          name: gcr.io/google_containers/kube-scheduler-amd64
          tag: v1.13.12
        logLevel: 4
        nodeSelector: {}
        pdb:
          enabled: true
          minAvailable: 1
        policy: {}
        replicas: 2
        resources:
          requests:
            cpu: 50m
            memory: 256Mi
    singleuser:
      cloudMetadata:
        enabled: false
        ip: 169.254.169.254
      cmd: jupyterhub-singleuser
      cpu: {}
      defaultUrl: /lab
      events: true
      extraAnnotations: {}
      extraContainers: []
      extraEnv:
        HOME: /home/jovyan
        PYSEQUILA_VERSION: 0.1.6
        SEQUILA_VERSION: 0.5.16
        TMP_HOME: /tmp/jovyan
      extraLabels:
        hub.jupyter.org/network-access-hub: "true"
      extraNodeAffinity:
        preferred: []
        required: []
      extraPodAffinity:
        preferred: []
        required: []
      extraPodAntiAffinity:
        preferred: []
        required: []
      extraPodConfig: {}
      extraResource:
        guarantees: {}
        limits: {}
      extraTolerations: []
      fsGid: 100
      image:
        name: biodatageeks/pysequila-notebook
        pullPolicy: IfNotPresent
        tag: 0.1.3-ga5af501
      imagePullSecret:
        enabled: false
      initContainers: []
      lifecycleHooks:
        postStart:
          exec:
            command:
            - sh
            - -c
            - |
              mkdir -p $HOME/.local/share/jupyter/kernels/; cp -r $TMP_HOME/venv $HOME; cp -r $TMP_HOME/.sdkman $HOME; cp -r  $TMP_HOME/.local/share/jupyter/kernels/pysequila/ $HOME/.local/share/jupyter/kernels/;
      memory:
        guarantee: 1G
      networkPolicy:
        egress:
        - to:
          - ipBlock:
              cidr: 0.0.0.0/0
              except:
              - 169.254.169.254/32
        enabled: false
        ingress: []
      networkTools:
        image:
          name: jupyterhub/k8s-network-tools
          tag: 0.9.1
      nodeSelector: {}
      startTimeout: 300
      storage:
        capacity: 10Gi
        dynamic:
          pvcNameTemplate: claim-{username}{servername}
          storageAccessModes:
          - ReadWriteOnce
          storageClass: standard
          volumeNameTemplate: volume-{username}{servername}
        extraLabels: {}
        extraVolumeMounts: []
        extraVolumes: []
        homeMountPath: /home/jovyan
        static:
          subPath: '{username}'
        type: dynamic
      uid: 1000
  cull_idle_servers.py: |
    #!/usr/bin/env python3
    # Imported from https://github.com/jupyterhub/jupyterhub/blob/6b1046697/examples/cull-idle/cull_idle_servers.py
    """script to monitor and cull idle single-user servers
  
    Caveats:
  
    last_activity is not updated with high frequency,
    so cull timeout should be greater than the sum of:
  
    - single-user websocket ping interval (default: 30s)
    - JupyterHub.last_activity_interval (default: 5 minutes)
  
    You can run this as a service managed by JupyterHub with this in your config::
  
  
        c.JupyterHub.services = [
            {
                'name': 'cull-idle',
                'admin': True,
                'command': 'python cull_idle_servers.py --timeout=3600'.split(),
            }
        ]
  
    Or run it manually by generating an API token and storing it in `JUPYTERHUB_API_TOKEN`:
  
        export JUPYTERHUB_API_TOKEN=`jupyterhub token`
        python cull_idle_servers.py [--timeout=900] [--url=http://127.0.0.1:8081/hub/api]
    """
  
    from datetime import datetime, timezone
    from functools import partial
    import json
    import os
  
    try:
        from urllib.parse import quote
    except ImportError:
        from urllib import quote
  
    import dateutil.parser
  
    from tornado.gen import coroutine, multi
    from tornado.locks import Semaphore
    from tornado.log import app_log
    from tornado.httpclient import AsyncHTTPClient, HTTPRequest
    from tornado.ioloop import IOLoop, PeriodicCallback
    from tornado.options import define, options, parse_command_line
  
  
    def parse_date(date_string):
        """Parse a timestamp
  
        If it doesn't have a timezone, assume utc
  
        Returned datetime object will always be timezone-aware
        """
        dt = dateutil.parser.parse(date_string)
        if not dt.tzinfo:
            # assume na√Øve timestamps are UTC
            dt = dt.replace(tzinfo=timezone.utc)
        return dt
  
  
    def format_td(td):
        """
        Nicely format a timedelta object
  
        as HH:MM:SS
        """
        if td is None:
            return "unknown"
        if isinstance(td, str):
            return td
        seconds = int(td.total_seconds())
        h = seconds // 3600
        seconds = seconds % 3600
        m = seconds // 60
        seconds = seconds % 60
        return f"{h:02}:{m:02}:{seconds:02}"
  
  
    @coroutine
    def cull_idle(
        url,
        api_token,
        inactive_limit,
        cull_users=False,
        remove_named_servers=False,
        max_age=0,
        concurrency=10,
    ):
        """Shutdown idle single-user servers
  
        If cull_users, inactive *users* will be deleted as well.
        """
        auth_header = {
            'Authorization': 'token %s' % api_token,
        }
        req = HTTPRequest(
            url=url + '/users',
            headers=auth_header,
        )
        now = datetime.now(timezone.utc)
        client = AsyncHTTPClient()
  
        if concurrency:
            semaphore = Semaphore(concurrency)
            @coroutine
            def fetch(req):
                """client.fetch wrapped in a semaphore to limit concurrency"""
                yield semaphore.acquire()
                try:
                    return (yield client.fetch(req))
                finally:
                    yield semaphore.release()
        else:
            fetch = client.fetch
  
        resp = yield fetch(req)
        users = json.loads(resp.body.decode('utf8', 'replace'))
        futures = []
  
        @coroutine
        def handle_server(user, server_name, server):
            """Handle (maybe) culling a single server
  
            Returns True if server is now stopped (user removable),
            False otherwise.
            """
            log_name = user['name']
            if server_name:
                log_name = '%s/%s' % (user['name'], server_name)
            if server.get('pending'):
                app_log.warning(
                    "Not culling server %s with pending %s",
                    log_name, server['pending'])
                return False
  
            if server.get('started'):
                age = now - parse_date(server['started'])
            else:
                # started may be undefined on jupyterhub < 0.9
                age = None
  
            # check last activity
            # last_activity can be None in 0.9
            if server['last_activity']:
                inactive = now - parse_date(server['last_activity'])
            else:
                # no activity yet, use start date
                # last_activity may be None with jupyterhub 0.9,
                # which introduces the 'started' field which is never None
                # for running servers
                inactive = age
  
            should_cull = (inactive is not None and
                           inactive.total_seconds() >= inactive_limit)
            if should_cull:
                app_log.info(
                    "Culling server %s (inactive for %s)",
                    log_name, format_td(inactive))
  
            if max_age and not should_cull:
                # only check started if max_age is specified
                # so that we can still be compatible with jupyterhub 0.8
                # which doesn't define the 'started' field
                if age is not None and age.total_seconds() >= max_age:
                    app_log.info(
                        "Culling server %s (age: %s, inactive for %s)",
                        log_name, format_td(age), format_td(inactive))
                    should_cull = True
  
            if not should_cull:
                app_log.debug(
                    "Not culling server %s (age: %s, inactive for %s)",
                    log_name, format_td(age), format_td(inactive))
                return False
  
            body = None
            if server_name:
                # culling a named server
                # A named server can be stopped and kept available to the user
                # for starting again or stopped and removed. To remove the named
                # server we have to pass an additional option in the body of our
                # DELETE request.
                delete_url = url + "/users/%s/servers/%s" % (
                    quote(user['name']),
                    quote(server['name']),
                )
                if remove_named_servers:
                    body = json.dumps({"remove": True})
            else:
                delete_url = url + '/users/%s/server' % quote(user['name'])
  
            req = HTTPRequest(
                url=delete_url,
                method='DELETE',
                headers=auth_header,
                body=body,
                allow_nonstandard_methods=True,
            )
            resp = yield fetch(req)
            if resp.code == 202:
                app_log.warning(
                    "Server %s is slow to stop",
                    log_name,
                )
                # return False to prevent culling user with pending shutdowns
                return False
            return True
  
        @coroutine
        def handle_user(user):
            """Handle one user.
  
            Create a list of their servers, and async exec them.  Wait for
            that to be done, and if all servers are stopped, possibly cull
            the user.
            """
            # shutdown servers first.
            # Hub doesn't allow deleting users with running servers.
            # named servers contain the 'servers' dict
            if 'servers' in user:
                servers = user['servers']
            # Otherwise, server data is intermingled in with the user
            # model
            else:
                servers = {}
                if user['server']:
                    servers[''] = {
                        'started': user.get('started'),
                        'last_activity': user['last_activity'],
                        'pending': user['pending'],
                        'url': user['server'],
                    }
            server_futures = [
                handle_server(user, server_name, server)
                for server_name, server in servers.items()
            ]
            results = yield multi(server_futures)
            if not cull_users:
                return
            # some servers are still running, cannot cull users
            still_alive = len(results) - sum(results)
            if still_alive:
                app_log.debug(
                    "Not culling user %s with %i servers still alive",
                    user['name'], still_alive)
                return False
  
            should_cull = False
            if user.get('created'):
                age = now - parse_date(user['created'])
            else:
                # created may be undefined on jupyterhub < 0.9
                age = None
  
            # check last activity
            # last_activity can be None in 0.9
            if user['last_activity']:
                inactive = now - parse_date(user['last_activity'])
            else:
                # no activity yet, use start date
                # last_activity may be None with jupyterhub 0.9,
                # which introduces the 'created' field which is never None
                inactive = age
  
            should_cull = (inactive is not None and
                           inactive.total_seconds() >= inactive_limit)
            if should_cull:
                app_log.info(
                    "Culling user %s (inactive for %s)",
                    user['name'], inactive)
  
            if max_age and not should_cull:
                # only check created if max_age is specified
                # so that we can still be compatible with jupyterhub 0.8
                # which doesn't define the 'started' field
                if age is not None and age.total_seconds() >= max_age:
                    app_log.info(
                        "Culling user %s (age: %s, inactive for %s)",
                        user['name'], format_td(age), format_td(inactive))
                    should_cull = True
  
            if not should_cull:
                app_log.debug(
                    "Not culling user %s (created: %s, last active: %s)",
                    user['name'], format_td(age), format_td(inactive))
                return False
  
            req = HTTPRequest(
                url=url + '/users/%s' % user['name'],
                method='DELETE',
                headers=auth_header,
            )
            yield fetch(req)
            return True
  
        for user in users:
            futures.append((user['name'], handle_user(user)))
  
        for (name, f) in futures:
            try:
                result = yield f
            except Exception:
                app_log.exception("Error processing %s", name)
            else:
                if result:
                    app_log.debug("Finished culling %s", name)
  
  
    if __name__ == '__main__':
        define(
            'url',
            default=os.environ.get('JUPYTERHUB_API_URL'),
            help="The JupyterHub API URL",
        )
        define('timeout', default=600, help="The idle timeout (in seconds)")
        define('cull_every', default=0,
               help="The interval (in seconds) for checking for idle servers to cull")
        define('max_age', default=0,
               help="The maximum age (in seconds) of servers that should be culled even if they are active")
        define('cull_users', default=False,
               help="""Cull users in addition to servers.
                    This is for use in temporary-user cases such as BinderHub.""",
               )
        define('remove_named_servers', default=False,
               help="""Remove named servers in addition to stopping them.
                    This is useful for a BinderHub that uses authentication and named servers.""",
               )
        define('concurrency', default=10,
               help="""Limit the number of concurrent requests made to the Hub.
  
                    Deleting a lot of users at the same time can slow down the Hub,
                    so limit the number of API requests we have outstanding at any given time.
                    """
               )
  
        parse_command_line()
        if not options.cull_every:
            options.cull_every = options.timeout // 2
        api_token = os.environ['JUPYTERHUB_API_TOKEN']
  
        try:
            AsyncHTTPClient.configure("tornado.curl_httpclient.CurlAsyncHTTPClient")
        except ImportError as e:
            app_log.warning(
                "Could not load pycurl: %s\n"
                "pycurl is recommended if you have a large number of users.",
                e)
  
        loop = IOLoop.current()
        cull = partial(
            cull_idle,
            url=options.url,
            api_token=api_token,
            inactive_limit=options.timeout,
            cull_users=options.cull_users,
            remove_named_servers=options.remove_named_servers,
            max_age=options.max_age,
            concurrency=options.concurrency,
        )
        # schedule first cull immediately
        # because PeriodicCallback doesn't start until the end of the first interval
        loop.add_callback(cull)
        # schedule periodic cull
        pc = PeriodicCallback(cull, 1e3 * options.cull_every)
        pc.start()
        try:
            loop.start()
        except KeyboardInterrupt:
            pass
  jupyterhub_config.py: "import os\nimport re\nimport sys\n\nfrom tornado.httpclient
    import AsyncHTTPClient\nfrom kubernetes import client\nfrom jupyterhub.utils import
    url_path_join\n\n# Make sure that modules placed in the same directory as the jupyterhub
    config are added to the pythonpath\nconfiguration_directory = os.path.dirname(os.path.realpath(__file__))\nsys.path.insert(0,
    configuration_directory)\n\nfrom z2jh import get_config, set_config_if_not_none\n\n#
    Configure JupyterHub to use the curl backend for making HTTP requests,\n# rather
    than the pure-python implementations. The default one starts\n# being too slow to
    make a large number of requests to the proxy API\n# at the rate required.\nAsyncHTTPClient.configure(\"tornado.curl_httpclient.CurlAsyncHTTPClient\")\n\n#
    Patch for CVE-2020-15110: change default template for named servers\n# with kubespawner
    0.11, {servername} *contains* a leading '-'\n# leading `{user}-{server}` to create
    `username--servername`,\n# preventing collision.\n# kubespawner 0.12 does not contain
    `-` in {servername},\n# and uses default name template `{username}--{servername}`\n#
    so this patch must not be used with kubespawner >= 0.12\n\nfrom distutils.version
    import LooseVersion as V\nfrom traitlets import default\nimport kubespawner\nfrom
    kubespawner import KubeSpawner\n\n\nclass PatchedKubeSpawner(KubeSpawner):\n    @default(\"pod_name_template\")\n
    \   def _default_pod_name_template(self):\n        if self.name:\n            return
    \"jupyter-{username}-{servername}\"\n        else:\n            return \"jupyter-{username}\"\n\n
    \   @default(\"pvc_name_template\")\n    def _default_pvc_name_template(self):\n
    \       if self.name:\n            return \"claim-{username}-{servername}\"\n        else:\n
    \           return \"claim-{username}\"\n\n\nkubespawner_version = getattr(kubespawner,
    \"__version__\", \"0.11\")\nif V(kubespawner_version) < V(\"0.11.999\"):\n    c.JupyterHub.spawner_class
    = PatchedKubeSpawner\nelse:\n    # 0.12 or greater, defaults are safe\n    c.JupyterHub.spawner_class
    = KubeSpawner\n\n# Connect to a proxy running in a different pod\nc.ConfigurableHTTPProxy.api_url
    = 'http://{}:{}'.format(os.environ['PROXY_API_SERVICE_HOST'], int(os.environ['PROXY_API_SERVICE_PORT']))\nc.ConfigurableHTTPProxy.should_start
    = False\n\n# Do not shut down user pods when hub is restarted\nc.JupyterHub.cleanup_servers
    = False\n\n# Check that the proxy has routes appropriately setup\nc.JupyterHub.last_activity_interval
    = 60\n\n# Don't wait at all before redirecting a spawning user to the progress page\nc.JupyterHub.tornado_settings
    = {\n    'slow_spawn_timeout': 0,\n}\n\n\ndef camelCaseify(s):\n    \"\"\"convert
    snake_case to camelCase\n\n    For the common case where some_value is set from
    someValue\n    so we don't have to specify the name twice.\n    \"\"\"\n    return
    re.sub(r\"_([a-z])\", lambda m: m.group(1).upper(), s)\n\n\n# configure the hub
    db connection\ndb_type = get_config('hub.db.type')\nif db_type == 'sqlite-pvc':\n
    \   c.JupyterHub.db_url = \"sqlite:///jupyterhub.sqlite\"\nelif db_type == \"sqlite-memory\":\n
    \   c.JupyterHub.db_url = \"sqlite://\"\nelse:\n    set_config_if_not_none(c.JupyterHub,
    \"db_url\", \"hub.db.url\")\n    \n\nfor trait, cfg_key in (\n    # Max number of
    servers that can be spawning at any one time\n    ('concurrent_spawn_limit', None),\n
    \   # Max number of servers to be running at one time\n    ('active_server_limit',
    None),\n    # base url prefix\n    ('base_url', None),\n    ('allow_named_servers',
    None),\n    ('named_server_limit_per_user', None),\n    ('authenticate_prometheus',
    None),\n    ('redirect_to_server', None),\n    ('shutdown_on_logout', None),\n    ('template_paths',
    None),\n    ('template_vars', None),\n):\n    if cfg_key is None:\n        cfg_key
    = camelCaseify(trait)\n    set_config_if_not_none(c.JupyterHub, trait, 'hub.' +
    cfg_key)\n\nc.JupyterHub.ip = os.environ['PROXY_PUBLIC_SERVICE_HOST']\nc.JupyterHub.port
    = int(os.environ['PROXY_PUBLIC_SERVICE_PORT'])\n\n# the hub should listen on all
    interfaces, so the proxy can access it\nc.JupyterHub.hub_ip = '0.0.0.0'\n\n# implement
    common labels\n# this duplicates the jupyterhub.commonLabels helper\ncommon_labels
    = c.KubeSpawner.common_labels = {}\ncommon_labels['app'] = get_config(\n    \"nameOverride\",\n
    \   default=get_config(\"Chart.Name\", \"jupyterhub\"),\n)\ncommon_labels['heritage']
    = \"jupyterhub\"\nchart_name = get_config('Chart.Name')\nchart_version = get_config('Chart.Version')\nif
    chart_name and chart_version:\n    common_labels['chart'] = \"{}-{}\".format(\n
    \       chart_name, chart_version.replace('+', '_'),\n    )\nrelease = get_config('Release.Name')\nif
    release:\n    common_labels['release'] = release\n\nc.KubeSpawner.namespace = os.environ.get('POD_NAMESPACE',
    'default')\n\n# Max number of consecutive failures before the Hub restarts itself\n#
    requires jupyterhub 0.9.2\nset_config_if_not_none(\n    c.Spawner,\n    'consecutive_failure_limit',\n
    \   'hub.consecutiveFailureLimit',\n)\n\nfor trait, cfg_key in (\n    ('start_timeout',
    None),\n    ('image_pull_policy', 'image.pullPolicy'),\n    ('events_enabled', 'events'),\n
    \   ('extra_labels', None),\n    ('extra_annotations', None),\n    ('uid', None),\n
    \   ('fs_gid', None),\n    ('service_account', 'serviceAccountName'),\n    ('storage_extra_labels',
    'storage.extraLabels'),\n    ('tolerations', 'extraTolerations'),\n    ('node_selector',
    None),\n    ('node_affinity_required', 'extraNodeAffinity.required'),\n    ('node_affinity_preferred',
    'extraNodeAffinity.preferred'),\n    ('pod_affinity_required', 'extraPodAffinity.required'),\n
    \   ('pod_affinity_preferred', 'extraPodAffinity.preferred'),\n    ('pod_anti_affinity_required',
    'extraPodAntiAffinity.required'),\n    ('pod_anti_affinity_preferred', 'extraPodAntiAffinity.preferred'),\n
    \   ('lifecycle_hooks', None),\n    ('init_containers', None),\n    ('extra_containers',
    None),\n    ('mem_limit', 'memory.limit'),\n    ('mem_guarantee', 'memory.guarantee'),\n
    \   ('cpu_limit', 'cpu.limit'),\n    ('cpu_guarantee', 'cpu.guarantee'),\n    ('extra_resource_limits',
    'extraResource.limits'),\n    ('extra_resource_guarantees', 'extraResource.guarantees'),\n
    \   ('environment', 'extraEnv'),\n    ('profile_list', None),\n    ('extra_pod_config',
    None),\n):\n    if cfg_key is None:\n        cfg_key = camelCaseify(trait)\n    set_config_if_not_none(c.KubeSpawner,
    trait, 'singleuser.' + cfg_key)\n\nimage = get_config(\"singleuser.image.name\")\nif
    image:\n    tag = get_config(\"singleuser.image.tag\")\n    if tag:\n        image
    = \"{}:{}\".format(image, tag)\n\n    c.KubeSpawner.image = image\n\nif get_config('singleuser.imagePullSecret.enabled'):\n
    \   c.KubeSpawner.image_pull_secrets = 'singleuser-image-credentials'\n\n# scheduling:\nif
    get_config('scheduling.userScheduler.enabled'):\n    c.KubeSpawner.scheduler_name
    = os.environ['HELM_RELEASE_NAME'] + \"-user-scheduler\"\nif get_config('scheduling.podPriority.enabled'):\n
    \   c.KubeSpawner.priority_class_name = os.environ['HELM_RELEASE_NAME'] + \"-default-priority\"\n\n#
    add node-purpose affinity\nmatch_node_purpose = get_config('scheduling.userPods.nodeAffinity.matchNodePurpose')\nif
    match_node_purpose:\n    node_selector = dict(\n        matchExpressions=[\n            dict(\n
    \               key=\"hub.jupyter.org/node-purpose\",\n                operator=\"In\",\n
    \               values=[\"user\"],\n            )\n        ],\n    )\n    if match_node_purpose
    == 'prefer':\n        c.KubeSpawner.node_affinity_preferred.append(\n            dict(\n
    \               weight=100,\n                preference=node_selector,\n            ),\n
    \       )\n    elif match_node_purpose == 'require':\n        c.KubeSpawner.node_affinity_required.append(node_selector)\n
    \   elif match_node_purpose == 'ignore':\n        pass\n    else:\n        raise
    ValueError(\"Unrecognized value for matchNodePurpose: %r\" % match_node_purpose)\n\n#
    add dedicated-node toleration\nfor key in (\n    'hub.jupyter.org/dedicated',\n
    \   # workaround GKE not supporting / in initial node taints\n    'hub.jupyter.org_dedicated',\n):\n
    \   c.KubeSpawner.tolerations.append(\n        dict(\n            key=key,\n            operator='Equal',\n
    \           value='user',\n            effect='NoSchedule',\n        )\n    )\n\n#
    Configure dynamically provisioning pvc\nstorage_type = get_config('singleuser.storage.type')\n\nif
    storage_type == 'dynamic':\n    pvc_name_template = get_config('singleuser.storage.dynamic.pvcNameTemplate')\n
    \   c.KubeSpawner.pvc_name_template = pvc_name_template\n    volume_name_template
    = get_config('singleuser.storage.dynamic.volumeNameTemplate')\n    c.KubeSpawner.storage_pvc_ensure
    = True\n    set_config_if_not_none(c.KubeSpawner, 'storage_class', 'singleuser.storage.dynamic.storageClass')\n
    \   set_config_if_not_none(c.KubeSpawner, 'storage_access_modes', 'singleuser.storage.dynamic.storageAccessModes')\n
    \   set_config_if_not_none(c.KubeSpawner, 'storage_capacity', 'singleuser.storage.capacity')\n\n
    \   # Add volumes to singleuser pods\n    c.KubeSpawner.volumes = [\n        {\n
    \           'name': volume_name_template,\n            'persistentVolumeClaim':
    {\n                'claimName': pvc_name_template\n            }\n        }\n    ]\n
    \   c.KubeSpawner.volume_mounts = [\n        {\n            'mountPath': get_config('singleuser.storage.homeMountPath'),\n
    \           'name': volume_name_template\n        }\n    ]\nelif storage_type ==
    'static':\n    pvc_claim_name = get_config('singleuser.storage.static.pvcName')\n
    \   c.KubeSpawner.volumes = [{\n        'name': 'home',\n        'persistentVolumeClaim':
    {\n            'claimName': pvc_claim_name\n        }\n    }]\n\n    c.KubeSpawner.volume_mounts
    = [{\n        'mountPath': get_config('singleuser.storage.homeMountPath'),\n        'name':
    'home',\n        'subPath': get_config('singleuser.storage.static.subPath')\n    }]\n\nc.KubeSpawner.volumes.extend(get_config('singleuser.storage.extraVolumes',
    []))\nc.KubeSpawner.volume_mounts.extend(get_config('singleuser.storage.extraVolumeMounts',
    []))\n\n# Gives spawned containers access to the API of the hub\nc.JupyterHub.hub_connect_ip
    = os.environ['HUB_SERVICE_HOST']\nc.JupyterHub.hub_connect_port = int(os.environ['HUB_SERVICE_PORT'])\n\n#
    Allow switching authenticators easily\nauth_type = get_config('auth.type')\nemail_domain
    = 'local'\n\ncommon_oauth_traits = (\n        ('client_id', None),\n        ('client_secret',
    None),\n        ('oauth_callback_url', 'callbackUrl'),\n)\n\nif auth_type == 'google':\n
    \   c.JupyterHub.authenticator_class = 'oauthenticator.GoogleOAuthenticator'\n    for
    trait, cfg_key in common_oauth_traits + (\n        ('hosted_domain', None),\n        ('login_service',
    None),\n    ):\n        if cfg_key is None:\n            cfg_key = camelCaseify(trait)\n
    \       set_config_if_not_none(c.GoogleOAuthenticator, trait, 'auth.google.' + cfg_key)\n
    \   email_domain = get_config('auth.google.hostedDomain')\nelif auth_type == 'github':\n
    \   c.JupyterHub.authenticator_class = 'oauthenticator.github.GitHubOAuthenticator'\n
    \   for trait, cfg_key in common_oauth_traits + (\n        ('github_organization_whitelist',
    'orgWhitelist'),\n    ):\n        if cfg_key is None:\n            cfg_key = camelCaseify(trait)\n
    \       set_config_if_not_none(c.GitHubOAuthenticator, trait, 'auth.github.' + cfg_key)\nelif
    auth_type == 'cilogon':\n    c.JupyterHub.authenticator_class = 'oauthenticator.CILogonOAuthenticator'\n
    \   for trait, cfg_key in common_oauth_traits:\n        if cfg_key is None:\n            cfg_key
    = camelCaseify(trait)\n        set_config_if_not_none(c.CILogonOAuthenticator, trait,
    'auth.cilogon.' + cfg_key)\nelif auth_type == 'gitlab':\n    c.JupyterHub.authenticator_class
    = 'oauthenticator.gitlab.GitLabOAuthenticator'\n    for trait, cfg_key in common_oauth_traits
    + (\n        ('gitlab_group_whitelist', None),\n        ('gitlab_project_id_whitelist',
    None),\n        ('gitlab_url', None),\n    ):\n        if cfg_key is None:\n            cfg_key
    = camelCaseify(trait)\n        set_config_if_not_none(c.GitLabOAuthenticator, trait,
    'auth.gitlab.' + cfg_key)\nelif auth_type == 'azuread':\n    c.JupyterHub.authenticator_class
    = 'oauthenticator.azuread.AzureAdOAuthenticator'\n    for trait, cfg_key in common_oauth_traits
    + (\n        ('tenant_id', None),\n        ('username_claim', None),\n    ):\n        if
    cfg_key is None:\n            cfg_key = camelCaseify(trait)\n\n        set_config_if_not_none(c.AzureAdOAuthenticator,
    trait, 'auth.azuread.' + cfg_key)\nelif auth_type == 'mediawiki':\n    c.JupyterHub.authenticator_class
    = 'oauthenticator.mediawiki.MWOAuthenticator'\n    for trait, cfg_key in common_oauth_traits
    + (\n        ('index_url', None),\n    ):\n        if cfg_key is None:\n            cfg_key
    = camelCaseify(trait)\n        set_config_if_not_none(c.MWOAuthenticator, trait,
    'auth.mediawiki.' + cfg_key)\nelif auth_type == 'globus':\n    c.JupyterHub.authenticator_class
    = 'oauthenticator.globus.GlobusOAuthenticator'\n    for trait, cfg_key in common_oauth_traits
    + (\n        ('identity_provider', None),\n    ):\n        if cfg_key is None:\n
    \           cfg_key = camelCaseify(trait)\n        set_config_if_not_none(c.GlobusOAuthenticator,
    trait, 'auth.globus.' + cfg_key)\nelif auth_type == 'hmac':\n    c.JupyterHub.authenticator_class
    = 'hmacauthenticator.HMACAuthenticator'\n    c.HMACAuthenticator.secret_key = bytes.fromhex(get_config('auth.hmac.secretKey'))\nelif
    auth_type == 'dummy':\n    c.JupyterHub.authenticator_class = 'dummyauthenticator.DummyAuthenticator'\n
    \   set_config_if_not_none(c.DummyAuthenticator, 'password', 'auth.dummy.password')\nelif
    auth_type == 'tmp':\n    c.JupyterHub.authenticator_class = 'tmpauthenticator.TmpAuthenticator'\nelif
    auth_type == 'lti':\n    c.JupyterHub.authenticator_class = 'ltiauthenticator.LTIAuthenticator'\n
    \   set_config_if_not_none(c.LTIAuthenticator, 'consumers', 'auth.lti.consumers')\nelif
    auth_type == 'ldap':\n    c.JupyterHub.authenticator_class = 'ldapauthenticator.LDAPAuthenticator'\n
    \   c.LDAPAuthenticator.server_address = get_config('auth.ldap.server.address')\n
    \   set_config_if_not_none(c.LDAPAuthenticator, 'server_port', 'auth.ldap.server.port')\n
    \   set_config_if_not_none(c.LDAPAuthenticator, 'use_ssl', 'auth.ldap.server.ssl')\n
    \   set_config_if_not_none(c.LDAPAuthenticator, 'allowed_groups', 'auth.ldap.allowedGroups')\n
    \   set_config_if_not_none(c.LDAPAuthenticator, 'bind_dn_template', 'auth.ldap.dn.templates')\n
    \   set_config_if_not_none(c.LDAPAuthenticator, 'lookup_dn', 'auth.ldap.dn.lookup')\n
    \   set_config_if_not_none(c.LDAPAuthenticator, 'lookup_dn_search_filter', 'auth.ldap.dn.search.filter')\n
    \   set_config_if_not_none(c.LDAPAuthenticator, 'lookup_dn_search_user', 'auth.ldap.dn.search.user')\n
    \   set_config_if_not_none(c.LDAPAuthenticator, 'lookup_dn_search_password', 'auth.ldap.dn.search.password')\n
    \   set_config_if_not_none(c.LDAPAuthenticator, 'lookup_dn_user_dn_attribute', 'auth.ldap.dn.user.dnAttribute')\n
    \   set_config_if_not_none(c.LDAPAuthenticator, 'escape_userdn', 'auth.ldap.dn.user.escape')\n
    \   set_config_if_not_none(c.LDAPAuthenticator, 'valid_username_regex', 'auth.ldap.dn.user.validRegex')\n
    \   set_config_if_not_none(c.LDAPAuthenticator, 'user_search_base', 'auth.ldap.dn.user.searchBase')\n
    \   set_config_if_not_none(c.LDAPAuthenticator, 'user_attribute', 'auth.ldap.dn.user.attribute')\nelif
    auth_type == 'custom':\n    # full_class_name looks like \"myauthenticator.MyAuthenticator\".\n
    \   # To create a docker image with this class availabe, you can just have the\n
    \   # following Dockerfile:\n    #   FROM jupyterhub/k8s-hub:v0.4\n    #   RUN pip3
    install myauthenticator\n    full_class_name = get_config('auth.custom.className')\n
    \   c.JupyterHub.authenticator_class = full_class_name\n    auth_class_name = full_class_name.rsplit('.',
    1)[-1]\n    auth_config = c[auth_class_name]\n    auth_config.update(get_config('auth.custom.config')
    or {})\nelse:\n    raise ValueError(\"Unhandled auth type: %r\" % auth_type)\n\nset_config_if_not_none(c.OAuthenticator,
    'scope', 'auth.scopes')\n\nset_config_if_not_none(c.Authenticator, 'enable_auth_state',
    'auth.state.enabled')\n\n# Enable admins to access user servers\nset_config_if_not_none(c.JupyterHub,
    'admin_access', 'auth.admin.access')\nset_config_if_not_none(c.Authenticator, 'admin_users',
    'auth.admin.users')\nset_config_if_not_none(c.Authenticator, 'whitelist', 'auth.whitelist.users')\n\nc.JupyterHub.services
    = []\n\nif get_config('cull.enabled', False):\n    cull_cmd = [\n        'python3',\n
    \       '/etc/jupyterhub/cull_idle_servers.py',\n    ]\n    base_url = c.JupyterHub.get('base_url',
    '/')\n    cull_cmd.append(\n        '--url=http://127.0.0.1:8081' + url_path_join(base_url,
    'hub/api')\n    )\n\n    cull_timeout = get_config('cull.timeout')\n    if cull_timeout:\n
    \       cull_cmd.append('--timeout=%s' % cull_timeout)\n\n    cull_every = get_config('cull.every')\n
    \   if cull_every:\n        cull_cmd.append('--cull-every=%s' % cull_every)\n\n
    \   cull_concurrency = get_config('cull.concurrency')\n    if cull_concurrency:\n
    \       cull_cmd.append('--concurrency=%s' % cull_concurrency)\n\n    if get_config('cull.users'):\n
    \       cull_cmd.append('--cull-users')\n\n    if get_config('cull.removeNamedServers'):\n
    \       cull_cmd.append('--remove-named-servers')\n\n    cull_max_age = get_config('cull.maxAge')\n
    \   if cull_max_age:\n        cull_cmd.append('--max-age=%s' % cull_max_age)\n\n
    \   c.JupyterHub.services.append({\n        'name': 'cull-idle',\n        'admin':
    True,\n        'command': cull_cmd,\n    })\n\nfor name, service in get_config('hub.services',
    {}).items():\n    # jupyterhub.services is a list of dicts, but\n    # in the helm
    chart it is a dict of dicts for easier merged-config\n    service.setdefault('name',
    name)\n    # handle camelCase->snake_case of api_token\n    api_token = service.pop('apiToken',
    None)\n    if api_token:\n        service['api_token'] = api_token\n    c.JupyterHub.services.append(service)\n\n\nset_config_if_not_none(c.Spawner,
    'cmd', 'singleuser.cmd')\nset_config_if_not_none(c.Spawner, 'default_url', 'singleuser.defaultUrl')\n\ncloud_metadata
    = get_config('singleuser.cloudMetadata', {})\n\nif not cloud_metadata.get('enabled',
    False):\n    # Use iptables to block access to cloud metadata by default\n    network_tools_image_name
    = get_config('singleuser.networkTools.image.name')\n    network_tools_image_tag
    = get_config('singleuser.networkTools.image.tag')\n    ip_block_container = client.V1Container(\n
    \       name=\"block-cloud-metadata\",\n        image=f\"{network_tools_image_name}:{network_tools_image_tag}\",\n
    \       command=[\n            'iptables',\n            '-A', 'OUTPUT',\n            '-d',
    cloud_metadata.get('ip', '169.254.169.254'),\n            '-j', 'DROP'\n        ],\n
    \       security_context=client.V1SecurityContext(\n            privileged=True,\n
    \           run_as_user=0,\n            capabilities=client.V1Capabilities(add=['NET_ADMIN'])\n
    \       )\n    )\n\n    c.KubeSpawner.init_containers.append(ip_block_container)\n\n\nif
    get_config('debug.enabled', False):\n    c.JupyterHub.log_level = 'DEBUG'\n    c.Spawner.debug
    = True\n\n\nextra_config = get_config('hub.extraConfig', {})\nif isinstance(extra_config,
    str):\n    from textwrap import indent, dedent\n    msg = dedent(\n    \"\"\"\n
    \   hub.extraConfig should be a dict of strings,\n    but found a single string
    instead.\n\n    extraConfig as a single string is deprecated\n    as of the jupyterhub
    chart version 0.6.\n\n    The keys can be anything identifying the\n    block of
    extra configuration.\n\n    Try this instead:\n\n        hub:\n          extraConfig:\n
    \           myConfig: |\n              {}\n\n    This configuration will still be
    loaded,\n    but you are encouraged to adopt the nested form\n    which enables
    easier merging of multiple extra configurations.\n    \"\"\"\n    )\n    print(\n
    \       msg.format(\n            indent(extra_config, ' ' * 10).lstrip()\n        ),\n
    \       file=sys.stderr\n    )\n    extra_config = {'deprecated string': extra_config}\n\nfor
    key, config_py in sorted(extra_config.items()):\n    print(\"Loading extra config:
    %s\" % key)\n    exec(config_py)\n"
  z2jh.py: |
    """
    Utility methods for use in jupyterhub_config.py and dynamic subconfigs.
  
    Methods here can be imported by extraConfig in values.yaml
    """
    from collections import Mapping
    from functools import lru_cache
    import os
  
    import yaml
  
  
    # memoize so we only load config once
    @lru_cache()
    def _load_config():
        """Load configuration from disk
  
        Memoized to only load once
        """
        cfg = {}
        for source in ('config', 'secret'):
            path = f"/etc/jupyterhub/{source}/values.yaml"
            if os.path.exists(path):
                print(f"Loading {path}")
                with open(path) as f:
                    values = yaml.safe_load(f)
                cfg = _merge_dictionaries(cfg, values)
            else:
                print(f"No config at {path}")
        return cfg
  
  
    def _merge_dictionaries(a, b):
        """Merge two dictionaries recursively.
  
        Simplified From https://stackoverflow.com/a/7205107
        """
        merged = a.copy()
        for key in b:
            if key in a:
                if isinstance(a[key], Mapping) and isinstance(b[key], Mapping):
                    merged[key] = _merge_dictionaries(a[key], b[key])
                else:
                    merged[key] = b[key]
            else:
                merged[key] = b[key]
        return merged
  
  
    def get_config(key, default=None):
        """
        Find a config item of a given name & return it
  
        Parses everything as YAML, so lists and dicts are available too
  
        get_config("a.b.c") returns config['a']['b']['c']
        """
        value = _load_config()
        # resolve path in yaml
        for level in key.split('.'):
            if not isinstance(value, dict):
                # a parent is a scalar or null,
                # can't resolve full path
                return default
            if level not in value:
                return default
            else:
                value = value[level]
        return value
  
  
    def set_config_if_not_none(cparent, name, key):
        """
        Find a config item of a given name, set the corresponding Jupyter
        configuration item if not None
        """
        data = get_config(key)
        if data is not None:
            setattr(cparent, name, data)
---
# Source: pysequila/charts/jupyterhub/templates/scheduling/user-scheduler/configmap.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: user-scheduler
  labels:
    component: user-scheduler
    app: jupyterhub
    release: my-release
    chart: jupyterhub-0.9.1
    heritage: Helm
data:
  policy.cfg: "{\"alwaysCheckAllPredicates\":false,\"apiVersion\":\"v1\",\"hardPodAffinitySymmetricWeight\":100,\"kind\":\"Policy\",\"predicates\":[{\"name\":\"PodFitsResources\"},{\"name\":\"HostName\"},{\"name\":\"PodFitsHostPorts\"},{\"name\":\"MatchNodeSelector\"},{\"name\":\"NoDiskConflict\"},{\"name\":\"PodToleratesNodeTaints\"},{\"name\":\"MaxEBSVolumeCount\"},{\"name\":\"MaxGCEPDVolumeCount\"},{\"name\":\"MaxAzureDiskVolumeCount\"},{\"name\":\"CheckVolumeBinding\"},{\"name\":\"NoVolumeZoneConflict\"},{\"name\":\"MatchInterPodAffinity\"}],\"priorities\":[{\"name\":\"NodePreferAvoidPodsPriority\",\"weight\":161051},{\"name\":\"NodeAffinityPriority\",\"weight\":14641},{\"name\":\"InterPodAffinityPriority\",\"weight\":1331},{\"name\":\"MostRequestedPriority\",\"weight\":121},{\"name\":\"ImageLocalityPriority\",\"weight\":11}]}"
---
# Source: pysequila/charts/jupyterhub/templates/hub/pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: hub-db-dir
  labels:
    component: hub
    app: jupyterhub
    release: my-release
    chart: jupyterhub-0.9.1
    heritage: Helm
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: "1Gi"
---
# Source: pysequila/charts/jupyterhub/templates/scheduling/user-scheduler/rbac.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: my-release-user-scheduler-complementary
  labels:
    component: user-scheduler-complementary
    app: jupyterhub
    release: my-release
    chart: jupyterhub-0.9.1
    heritage: Helm
rules:
  # Support leader elections
  - apiGroups: [""]
    resourceNames: ["user-scheduler"]
    resources: ["configmaps"]
    verbs: ["get", "update"]
  # Workaround for missing permission in system:kube-scheduler as of k8s 1.10.4
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  # Workaround for missing permission with rancher local-path-provisioner
  - apiGroups: [""]
    resources: ["persistentvolume", "persistentvolumeclaims"]
    verbs: ["update"]
---
# Source: pysequila/charts/jupyterhub/templates/scheduling/user-scheduler/rbac.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: my-release-user-scheduler-base
  labels:
    component: user-scheduler-base
    app: jupyterhub
    release: my-release
    chart: jupyterhub-0.9.1
    heritage: Helm
subjects:
  - kind: ServiceAccount
    name: user-scheduler
    namespace: pysequila-0.1.3.tgz
roleRef:
  kind: ClusterRole
  name: system:kube-scheduler
  apiGroup: rbac.authorization.k8s.io
---
# Source: pysequila/charts/jupyterhub/templates/scheduling/user-scheduler/rbac.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: my-release-user-scheduler-complementary
  labels:
    component: user-scheduler-complementary
    app: jupyterhub
    release: my-release
    chart: jupyterhub-0.9.1
    heritage: Helm
subjects:
  - kind: ServiceAccount
    name: user-scheduler
    namespace: pysequila-0.1.3.tgz
roleRef:
  kind: ClusterRole
  name: my-release-user-scheduler-complementary
  apiGroup: rbac.authorization.k8s.io
---
# Source: pysequila/charts/jupyterhub/templates/hub/rbac.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: hub
  labels:
    component: hub
    app: jupyterhub
    release: my-release
    chart: jupyterhub-0.9.1
    heritage: Helm
rules:
  - apiGroups: [""]       # "" indicates the core API group
    resources: ["pods", "persistentvolumeclaims"]
    verbs: ["get", "watch", "list", "create", "delete"]
  - apiGroups: [""]       # "" indicates the core API group
    resources: ["events"]
    verbs: ["get", "watch", "list"]
---
# Source: pysequila/charts/jupyterhub/templates/hub/rbac.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: hub
  labels:
    component: hub
    app: jupyterhub
    release: my-release
    chart: jupyterhub-0.9.1
    heritage: Helm
subjects:
  - kind: ServiceAccount
    name: hub
    namespace: pysequila-0.1.3.tgz
roleRef:
  kind: Role
  name: hub
  apiGroup: rbac.authorization.k8s.io
---
# Source: pysequila/charts/jupyterhub/templates/hub/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: hub
  labels:
    component: hub
    app: jupyterhub
    release: my-release
    chart: jupyterhub-0.9.1
    heritage: Helm
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/path: /hub/metrics
spec:
  type: ClusterIP
  selector:
    component: hub
    app: jupyterhub
    release: my-release
  ports:
    - protocol: TCP
      port: 8081
      targetPort: 8081
---
# Source: pysequila/charts/jupyterhub/templates/proxy/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: proxy-api
  labels:
    component: proxy-api
    app: jupyterhub
    release: my-release
    chart: jupyterhub-0.9.1
    heritage: Helm
spec:
  selector:
    component: proxy
    app: jupyterhub
    release: my-release
  ports:
    - protocol: TCP
      port: 8001
      targetPort: 8001
---
# Source: pysequila/charts/jupyterhub/templates/proxy/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: proxy-public
  labels:
    component: proxy-public
    app: jupyterhub
    release: my-release
    chart: jupyterhub-0.9.1
    heritage: Helm
spec:
  selector:
    # TODO: Refactor to utilize the helpers
    component: proxy
    release: my-release
  ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: 443
    - name: http
      port: 80
      protocol: TCP
      targetPort: 8000
      # allow proxy.service.nodePort for http
  type: LoadBalancer
---
# Source: pysequila/charts/jupyterhub/templates/image-puller/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: continuous-image-puller
  labels:
    component: continuous-image-puller
    app: jupyterhub
    release: my-release
    chart: jupyterhub-0.9.1
    heritage: Helm
spec:
  selector:
    matchLabels:
      component: continuous-image-puller
      app: jupyterhub
      release: my-release
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 100%
  template:
    metadata:
      labels:
        component: continuous-image-puller
        app: jupyterhub
        release: my-release
    spec:
      tolerations:
        - key: hub.jupyter.org_dedicated
          operator: Equal
          value: user
          effect: NoSchedule
        - key: hub.jupyter.org/dedicated
          operator: Equal
          value: user
          effect: NoSchedule
      nodeSelector: {}
      terminationGracePeriodSeconds: 0
      automountServiceAccountToken: false
      initContainers:
        - name: image-pull-singleuser
          image: biodatageeks/pysequila-notebook:0.1.3-ga5af501
          imagePullPolicy: IfNotPresent
          command:
            - /bin/sh
            - -c
            - echo "Pulling complete"
        - name: image-pull-metadata-block
          image: jupyterhub/k8s-network-tools:0.9.1
          command:
            - /bin/sh
            - -c
            - echo "Pulling complete"
      containers:
        - name: pause
          image: gcr.io/google_containers/pause:3.1
---
# Source: pysequila/charts/jupyterhub/templates/hub/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hub
  labels:
    component: hub
    app: jupyterhub
    release: my-release
    chart: jupyterhub-0.9.1
    heritage: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      component: hub
      app: jupyterhub
      release: my-release
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        component: hub
        app: jupyterhub
        release: my-release
        hub.jupyter.org/network-access-proxy-api: "true"
        hub.jupyter.org/network-access-proxy-http: "true"
        hub.jupyter.org/network-access-singleuser: "true"
      annotations:
        # This lets us autorestart when the secret changes!
        checksum/config-map: a0e2425060cf096c3fd3666b3db221b5f6e1b224f9237932182662698548c8f9
        checksum/secret: e14a9c2044049f5bf47e40d4bc5402b24d8b6cfd5ecafa3e2be572666f8ed815
    spec:
      nodeSelector: {}
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: hub.jupyter.org/node-purpose
                    operator: In
                    values: [core]
      volumes:
        - name: config
          configMap:
            name: hub-config
        - name: secret
          secret:
            secretName: hub-secret
        - name: hub-db-dir
          persistentVolumeClaim:
            claimName: hub-db-dir
      serviceAccountName: hub
      securityContext:
        fsGroup: 1000
      containers:
        - name: hub
          image: jupyterhub/k8s-hub:0.9.1
          command:
            - jupyterhub
            - --config
            - /etc/jupyterhub/jupyterhub_config.py
            - --upgrade-db
          volumeMounts:
            - mountPath: /etc/jupyterhub/jupyterhub_config.py
              subPath: jupyterhub_config.py
              name: config
            - mountPath: /etc/jupyterhub/z2jh.py
              subPath: z2jh.py
              name: config
            - mountPath: /etc/jupyterhub/cull_idle_servers.py
              subPath: cull_idle_servers.py
              name: config
            - mountPath: /etc/jupyterhub/config/
              name: config
            - mountPath: /etc/jupyterhub/secret/
              name: secret
            - mountPath: /srv/jupyterhub
              name: hub-db-dir
          resources:
            requests:
              cpu: 200m
              memory: 512Mi
          securityContext:
            runAsUser: 1000
            # Don't allow any process to execute as root inside the container
            allowPrivilegeEscalation: false
          env:
            - name: PYTHONUNBUFFERED
              value: "1"
            - name: HELM_RELEASE_NAME
              value: "my-release"
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: CONFIGPROXY_AUTH_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hub-secret
                  key: proxy.token
          ports:
            - containerPort: 8081
              name: hub
          readinessProbe:
            initialDelaySeconds: 0
            periodSeconds: 10
            httpGet:
              path: /hub/health
              port: hub
---
# Source: pysequila/charts/jupyterhub/templates/proxy/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: proxy
  labels:
    component: proxy
    app: jupyterhub
    release: my-release
    chart: jupyterhub-0.9.1
    heritage: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      component: proxy
      app: jupyterhub
      release: my-release
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        component: proxy
        app: jupyterhub
        release: my-release
        hub.jupyter.org/network-access-hub: "true"
        hub.jupyter.org/network-access-singleuser: "true"
      annotations:
        # This lets us autorestart when the secret changes!
        checksum/hub-secret: 0f886a3adec60c031da50f140c48d534e355ca5bc46abf6bc2e73a45fcc759f3
        checksum/proxy-secret: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
    spec:
      terminationGracePeriodSeconds: 60
      nodeSelector: {}
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: hub.jupyter.org/node-purpose
                    operator: In
                    values: [core]
      containers:
        - name: chp
          image: jupyterhub/configurable-http-proxy:4.2.1
          command:
            - configurable-http-proxy
            - --ip=0.0.0.0
            - --api-ip=0.0.0.0
            - --api-port=8001
            - --default-target=http://$(HUB_SERVICE_HOST):$(HUB_SERVICE_PORT)
            - --error-target=http://$(HUB_SERVICE_HOST):$(HUB_SERVICE_PORT)/hub/error
            - --port=8000
          resources:
            requests:
              cpu: 200m
              memory: 512Mi
          securityContext:
            # Don't allow any process to execute as root inside the container
            allowPrivilegeEscalation: false
          env:
            - name: CONFIGPROXY_AUTH_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hub-secret
                  key: proxy.token
          ports:
            - containerPort: 8000
              name: proxy-public
            - containerPort: 8001
              name: api
          livenessProbe:
            initialDelaySeconds: 30
            periodSeconds: 10
            httpGet:
              path: /_chp_healthz
              port: proxy-public
              scheme: HTTP
          readinessProbe:
            initialDelaySeconds: 0
            periodSeconds: 10
            httpGet:
              path: /_chp_healthz
              port: proxy-public
              scheme: HTTP
---
# Source: pysequila/charts/jupyterhub/templates/scheduling/user-scheduler/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-scheduler
  labels:
    component: user-scheduler
    app: jupyterhub
    release: my-release
    chart: jupyterhub-0.9.1
    heritage: Helm
spec:
  replicas: 2
  selector:
    matchLabels:
      component: user-scheduler
      app: jupyterhub
      release: my-release
  template:
    metadata:
      labels:
        component: user-scheduler
        app: jupyterhub
        release: my-release
      annotations:
        # This lets us autorestart when the configmap changes!
        checksum/config-map: 4eb9f529d6dee0c48b6101c7b66cc5b847580639be385625f49dbd065888f199
    spec:
      serviceAccountName: user-scheduler
      nodeSelector: {}
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: hub.jupyter.org/node-purpose
                    operator: In
                    values: [core]
      containers:
        - name: user-scheduler
          image: gcr.io/google_containers/kube-scheduler-amd64:v1.13.12
          command:
            - /usr/local/bin/kube-scheduler
            - --scheduler-name=my-release-user-scheduler
            - --policy-configmap=user-scheduler
            - --policy-configmap-namespace=pysequila-0.1.3.tgz
            - --lock-object-name=user-scheduler
            - --lock-object-namespace=pysequila-0.1.3.tgz
            - --leader-elect-resource-lock=configmaps
            - --v=4
          livenessProbe:
            httpGet:
              path: /healthz
              port: 10251
            initialDelaySeconds: 15
          readinessProbe:
            httpGet:
              path: /healthz
              port: 10251
          resources:
            requests:
              cpu: 50m
              memory: 256Mi
---
# Source: pysequila/charts/jupyterhub/templates/scheduling/user-placeholder/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: user-placeholder
  labels:
    component: user-placeholder
    app: jupyterhub
    release: my-release
    chart: jupyterhub-0.9.1
    heritage: Helm
spec:
  podManagementPolicy: Parallel
  replicas: 0
  selector:
    matchLabels:
      component: user-placeholder
      app: jupyterhub
      release: my-release
  serviceName: "user-placeholder"
  template:
    metadata:
      labels:
        component: user-placeholder
        app: jupyterhub
        release: my-release
    spec:
      schedulerName: my-release-user-scheduler
      tolerations:
        - key: hub.jupyter.org_dedicated
          operator: Equal
          value: user
          effect: NoSchedule
        - key: hub.jupyter.org/dedicated
          operator: Equal
          value: user
          effect: NoSchedule
      nodeSelector: {}
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: hub.jupyter.org/node-purpose
                    operator: In
                    values: [user]
      terminationGracePeriodSeconds: 0
      automountServiceAccountToken: false
      containers:
        - name: pause
          image: gcr.io/google_containers/pause:3.1
          resources:
            requests:
              memory: 1G
---
# Source: pysequila/charts/jupyterhub/templates/image-puller/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: hook-image-awaiter
  labels:
    component: image-puller
    app: jupyterhub
    release: my-release
    chart: jupyterhub-0.9.1
    heritage: Helm
    hub.jupyter.org/deletable: "true"
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
    "helm.sh/hook-weight": "0"
---
# Source: pysequila/charts/jupyterhub/templates/image-puller/rbac.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: hook-image-awaiter
  labels:
    component: image-puller
    app: jupyterhub
    release: my-release
    chart: jupyterhub-0.9.1
    heritage: Helm
    hub.jupyter.org/deletable: "true"
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
    "helm.sh/hook-weight": "0"
rules:
  - apiGroups: ["apps"]       # "" indicates the core API group
    resources: ["daemonsets"]
    verbs: ["get"]
---
# Source: pysequila/charts/jupyterhub/templates/image-puller/rbac.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: hook-image-awaiter
  labels:
    component: image-puller
    app: jupyterhub
    release: my-release
    chart: jupyterhub-0.9.1
    heritage: Helm
    hub.jupyter.org/deletable: "true"
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
    "helm.sh/hook-weight": "0"
subjects:
  - kind: ServiceAccount
    name: hook-image-awaiter
    namespace: pysequila-0.1.3.tgz
roleRef:
  kind: Role
  name: hook-image-awaiter
  apiGroup: rbac.authorization.k8s.io
---
# Source: pysequila/charts/jupyterhub/templates/image-puller/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: hook-image-puller
  labels:
    component: hook-image-puller
    app: jupyterhub
    release: my-release
    chart: jupyterhub-0.9.1
    heritage: Helm
    hub.jupyter.org/deletable: "true"
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
    "helm.sh/hook-weight": "-10"
spec:
  selector:
    matchLabels:
      component: hook-image-puller
      app: jupyterhub
      release: my-release
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 100%
  template:
    metadata:
      labels:
        component: hook-image-puller
        app: jupyterhub
        release: my-release
    spec:
      tolerations:
        - key: hub.jupyter.org_dedicated
          operator: Equal
          value: user
          effect: NoSchedule
        - key: hub.jupyter.org/dedicated
          operator: Equal
          value: user
          effect: NoSchedule
      nodeSelector: {}
      terminationGracePeriodSeconds: 0
      automountServiceAccountToken: false
      initContainers:
        - name: image-pull-singleuser
          image: biodatageeks/pysequila-notebook:0.1.3-ga5af501
          imagePullPolicy: IfNotPresent
          command:
            - /bin/sh
            - -c
            - echo "Pulling complete"
        - name: image-pull-metadata-block
          image: jupyterhub/k8s-network-tools:0.9.1
          command:
            - /bin/sh
            - -c
            - echo "Pulling complete"
      containers:
        - name: pause
          image: gcr.io/google_containers/pause:3.1
---
# Source: pysequila/charts/jupyterhub/templates/image-puller/job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: hook-image-awaiter
  labels:
    component: image-puller
    app: jupyterhub
    release: my-release
    chart: jupyterhub-0.9.1
    heritage: Helm
    hub.jupyter.org/deletable: "true"
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
    "helm.sh/hook-weight": "10"
spec:
  template:
    metadata:
      labels:
        component: image-puller
        app: jupyterhub
        release: my-release
    spec:
      restartPolicy: Never
      serviceAccountName: hook-image-awaiter
      containers:
        - image: jupyterhub/k8s-image-awaiter:0.9.1
          name: hook-image-awaiter
          imagePullPolicy: IfNotPresent
          command:
            - /image-awaiter
            - -ca-path=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            - -auth-token-path=/var/run/secrets/kubernetes.io/serviceaccount/token
            - -api-server-address=https://$(KUBERNETES_SERVICE_HOST):$(KUBERNETES_SERVICE_PORT)
            - -namespace=pysequila-0.1.3.tgz
            - -daemonset=hook-image-puller
