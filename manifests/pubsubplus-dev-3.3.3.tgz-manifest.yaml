---
# Source: pubsubplus-dev/templates/podModRbac.yaml
# Deployment requires the capability of patching the pod to indicate active state for load balancing
kind: ServiceAccount
apiVersion: v1
metadata:
  name: my-release-pubsubplus-dev-sa
  labels:
    app.kubernetes.io/name: pubsubplus-dev
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    helm.sh/chart: pubsubplus-dev-3.3.3
---
# Source: pubsubplus-dev/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-release-pubsubplus-dev-secrets
  labels:
    app.kubernetes.io/name: pubsubplus-dev
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    helm.sh/chart: pubsubplus-dev-3.3.3
type: Opaque
data:

  username_admin_password: "emV3SXdzUGFFeA=="
---
# Source: pubsubplus-dev/templates/solaceConfigMap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-pubsubplus-dev
  labels:
    app.kubernetes.io/name: pubsubplus-dev
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    helm.sh/chart: pubsubplus-dev-3.3.3
data:
  init.sh: |-
    export username_admin_passwordfilepath="/mnt/disks/secrets/username_admin_password"
    export username_admin_globalaccesslevel=admin
    export service_ssh_port='2222'
    export service_webtransport_port='8008'
    export service_webtransport_tlsport='1443'
    export service_semp_tlsport='1943'
    export logging_debug_output=all
    export system_scaling_maxconnectioncount="100"
    # Deal with the fact we cannot accept "-" in router names
    export routername=$(echo $(hostname) | sed 's/-//g')

  startup-broker.sh: |-
    #!/bin/bash
    APP=`basename "$0"`
    IFS='-' read -ra host_array <<< $(hostname)
    node_ordinal=${host_array[-1]}
    echo "`date` INFO: ${APP}-Node ordinal: ${node_ordinal}"
    echo "`date` INFO: ${APP}-Waiting for management API to become available"
    password=`cat /mnt/disks/secrets/username_admin_password`
    INITIAL_STARTUP_FILE=/var/lib/solace/var/k8s_initial_startup_marker
    loop_guard=60
    pause=10
    count=0
    # Wait for Solace Management API
    while [ ${count} -lt ${loop_guard} ]; do 
      if /mnt/disks/solace/semp_query.sh -n admin -p ${password} -u http://localhost:8080 -t ; then
        break
      fi
      run_time=$((${count} * ${pause}))
      ((count++))
      echo "`date` INFO: ${APP}-Waited ${run_time} seconds, Management API not yet accessible"
      sleep ${pause}
    done
    if [ ${count} -eq ${loop_guard} ]; then
      echo "`date` ERROR: ${APP}-Solace Management API never came up"  >&2
      exit 1 
    fi
    echo "`date` INFO: ${APP}-PubSub+ Event Broker bringup is complete for this node."
    # create startup file after PubSub+ Event Broker is up and running.  Create only if it does not exist
    if [[ ! -e ${INITIAL_STARTUP_FILE} ]]; then
        echo "PubSub+ Event Broker initial startup completed on `date`" > ${INITIAL_STARTUP_FILE}
    fi
    exit 0


  readiness_check.sh: |-
    #!/bin/bash
    APP=`basename "$0"`
    LOG_FILE=/usr/sw/var/k8s_readiness_check.log # STDOUT/STDERR goes to k8s event logs but gets cleaned out eventually. This will also persist it.
    if [ -f ${LOG_FILE} ] ; then
        tail -n 1000 ${LOG_FILE} > ${LOG_FILE}.tmp; mv -f ${LOG_FILE}.tmp ${LOG_FILE} || :  # Limit logs size
    fi
    exec > >(tee -a ${LOG_FILE}) 2>&1 # Setup logging
    FINAL_ACTIVITY_LOGGED_TRACKING_FILE=/tmp/final_activity_state_logged

    # Function to read Kubernetes metadata labels
    get_label () {
      # Params: $1 label name
      echo $(cat /etc/podinfo/labels | awk -F= '$1=="'${1}'"{print $2}' | xargs);
    }

    # Function to set Kubernetes metadata labels
    set_label () {
      # Params: $1 label name, $2 label set value
      #Prevent overdriving Kubernetes infra, don't set activity state to same as previous state
      previous_state=$(get_label "active")
      if [ "${2}" = "${previous_state}" ]; then
        #echo "`date` INFO: ${APP}-Current and Previous state match (${2}), not updating pod label"
        :
      else
        echo "`date` INFO: ${APP}-Updating pod label using K8s API from ${previous_state} to ${2}"
        echo "[{\"op\": \"add\", \"path\": \"/metadata/labels/${1}\", \"value\": \"${2}\" }]" > /tmp/patch_label.json
        K8S=https://kubernetes.default.svc.cluster.local:$KUBERNETES_SERVICE_PORT
        KUBE_TOKEN=$(</var/run/secrets/kubernetes.io/serviceaccount/token)
        CACERT=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        NAMESPACE=$(</var/run/secrets/kubernetes.io/serviceaccount/namespace)
        if ! curl -sS --output /dev/null --cacert $CACERT --connect-timeout 5 \
            --request PATCH --data "$(cat /tmp/patch_label.json)" \
            -H "Authorization: Bearer $KUBE_TOKEN" -H "Content-Type:application/json-patch+json" \
            $K8S/api/v1/namespaces/$NAMESPACE/pods/$HOSTNAME ; then
          # Label update didn't work this way, fall back to alternative legacy method to update label
          if ! curl -sSk --output /dev/null -H "Authorization: Bearer $KUBE_TOKEN" --request PATCH --data "$(cat /tmp/patch_label.json)" \
            -H "Content-Type:application/json-patch+json" \
            https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_PORT_443_TCP_PORT/api/v1/namespaces/$STATEFULSET_NAMESPACE/pods/$HOSTNAME ; then
            echo "`date` ERROR: ${APP}-Unable to update pod label, check access from pod to K8s API or RBAC authorization" >&2
            rm -f ${FINAL_ACTIVITY_LOGGED_TRACKING_FILE}; exit 1
          fi
        fi
      fi
    }


    # Function to get remote sync state
    get_router_remote_config_state() {
      # Params: $1 is property of config to return for router
      routerresults=`/mnt/disks/solace/semp_query.sh -n admin -p ${password} -u http://localhost:8080 \
                -q "<rpc><show><config-sync><database/><router/><remote/></config-sync></show></rpc>" \
                -v "/rpc-reply/rpc/show/config-sync/database/remote/tables/table[1]/source-router/${1}"`
      routerremotesync_result=`echo ${routerresults} | xmllint -xpath "string(returnInfo/valueSearchResult)" -`
      echo $routerremotesync_result
    }

    # Main logic: note that there are no re-tries here, if check fails then return not ready.
    # nonHA config
    health_result=`curl -s -o /dev/null -w "%{http_code}"  http://localhost:5550/health-check/guaranteed-active`
    case "${health_result}" in
      "200")
        if [ ! -f ${FINAL_ACTIVITY_LOGGED_TRACKING_FILE} ]; then
          echo "`date` INFO: ${APP}-nonHA Event Broker health check reported 200, message spool is up"
          touch ${FINAL_ACTIVITY_LOGGED_TRACKING_FILE}
          echo "`date` INFO: ${APP}-Server status check complete for this broker node"
          echo "`date` INFO: ${APP}-Changing pod label to active"
          exit 1
        fi
        set_label "active" "true"
        exit 0
        ;;
      "503")
        if [[ $(get_label "active") = "true" ]]; then echo "`date` INFO: ${APP}-nonHA Event Broker health check reported 503, message spool is down"; fi
        set_label "active" "false"
        echo "`date` INFO: ${APP}-Changing pod label to inactive"
        # Fail readiness check
        rm -f ${FINAL_ACTIVITY_LOGGED_TRACKING_FILE}; exit 1
        ;;
      *)
        echo "`date` WARN: ${APP}-nonHA Event Broker health check reported ${health_result}"
        set_label "active" "false"
        echo "`date` INFO: ${APP}-Changing pod label to inactive"
        # Fail readiness check
        rm -f ${FINAL_ACTIVITY_LOGGED_TRACKING_FILE}; exit 1
    esac
  semp_query.sh: |-
      #!/bin/bash
      APP=`basename "$0"`
      OPTIND=1         # Reset in case getopts has been used previously in the shell.
      # Initialize our own variables:
      count_search=""
      name=""
      password=""
      query=""
      url=""
      value_search=""
      test_connection_only=false
      script_name=$0
      verbose=0
      while getopts "c:n:p:q:u:v:t" opt; do
          case "$opt" in
          c)  count_search=$OPTARG
              ;;
          n)  username=$OPTARG
              ;;
          p)  password=$OPTARG
              ;;
          q)  query=$OPTARG
              ;;
          u)  url=$OPTARG
              ;;
          v)  value_search=$OPTARG
              ;;
          t)  test_connection_only=true
              ;;
          esac
      done
      shift $((OPTIND-1))
      [ "$1" = "--" ] && shift
      verbose=1
      #echo "`date` INFO: ${APP}-${script_name}: count_search=${count_search} ,username=${username} ,password=xxx query=${query} \
      #            ,url=${url} ,value_search=${value_search} ,Leftovers: $@" >&2
      if [[ ${url} = "" || ${username} = "" || ${password} = "" ]]; then
        echo "`date` ERROR: ${APP}-${script_name}: url, username, password are madatory fields" >&2
        echo  '<returnInfo><errorInfo>missing parameter</errorInfo></returnInfo>'
        exit 1
      fi
      if [ "`curl --write-out '%{http_code}' --silent --output /dev/null -u ${username}:${password} ${url}/SEMP -d '<rpc><show><version/></show></rpc>'`" != "200" ] ; then
        echo  "<returnInfo><errorInfo>management host is not responding</errorInfo></returnInfo>"
        exit 1
      fi
      if [ "$test_connection_only" = true ] ; then
        exit 0      # done here, connection is up
      fi
      query_response=`curl -sS -u ${username}:${password} ${url}/SEMP -d "${query}"`
      # Validate first char of response is "<", otherwise no hope of being valid xml
      if [[ ${query_response:0:1} != "<" ]] ; then
        echo  "<returnInfo><errorInfo>no valid xml returned</errorInfo></returnInfo>"
        exit 1
      fi
      query_response_code=`echo $query_response | xmllint -xpath 'string(/rpc-reply/execute-result/@code)' -`

      if [[ -z ${query_response_code} && ${query_response_code} != "ok" ]]; then
          echo  "<returnInfo><errorInfo>query failed -${query_response_code}-</errorInfo></returnInfo>"
          exit 1
      fi
      #echo "`date` INFO: ${APP}-${script_name}: query passed ${query_response_code}" >&2
      if [[ ! -z $value_search ]]; then
          value_result=`echo $query_response | xmllint -xpath "string($value_search)" -`
          echo  "<returnInfo><errorInfo></errorInfo><valueSearchResult>${value_result}</valueSearchResult></returnInfo>"
          exit 0
      fi
      if [[ ! -z $count_search ]]; then
          count_line=`echo $query_response | xmllint -xpath "$count_search" -`
          count_string=`echo $count_search | cut -d '"' -f 2`
          count_result=`echo ${count_line} | tr "><" "\n" | grep -c ${count_string}`
          echo  "<returnInfo><errorInfo></errorInfo><countSearchResult>${count_result}</countSearchResult></returnInfo>"
          exit 0
      fi
---
# Source: pubsubplus-dev/templates/podModRbac.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: my-release-pubsubplus-dev-podtagupdater
rules:
- apiGroups: [""] # "" indicates the core API group
  resources: ["pods"]
  verbs: ["patch"]
---
# Source: pubsubplus-dev/templates/podModRbac.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: my-release-pubsubplus-dev-serviceaccounts-to-podtagupdater
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: my-release-pubsubplus-dev-podtagupdater
subjects:
- kind: ServiceAccount
  name: my-release-pubsubplus-dev-sa
---
# Source: pubsubplus-dev/templates/service.yaml
# Load Service part of template
apiVersion: v1
kind: Service
metadata:
  name: my-release-pubsubplus-dev
  labels:
    app.kubernetes.io/name: pubsubplus-dev
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    helm.sh/chart: pubsubplus-dev-3.3.3
spec:
  type: LoadBalancer
  ports:
  - port: 2222
    targetPort: 2222
    protocol: TCP
    name: tcp-ssh
  - port: 8080
    targetPort: 8080
    protocol: TCP
    name: tcp-semp
  - port: 1943
    targetPort: 1943
    protocol: TCP
    name: tls-semp
  - port: 55555
    targetPort: 55555
    protocol: TCP
    name: tcp-smf
  - port: 55003
    targetPort: 55003
    protocol: TCP
    name: tcp-smfcomp
  - port: 55443
    targetPort: 55443
    protocol: TCP
    name: tls-smf
  - port: 55556
    targetPort: 55556
    protocol: TCP
    name: tcp-smfroute
  - port: 8008
    targetPort: 8008
    protocol: TCP
    name: tcp-web
  - port: 1443
    targetPort: 1443
    protocol: TCP
    name: tls-web
  - port: 9000
    targetPort: 9000
    protocol: TCP
    name: tcp-rest
  - port: 9443
    targetPort: 9443
    protocol: TCP
    name: tls-rest
  - port: 5672
    targetPort: 5672
    protocol: TCP
    name: tcp-amqp
  - port: 5671
    targetPort: 5671
    protocol: TCP
    name: tls-amqp
  - port: 1883
    targetPort: 1883
    protocol: TCP
    name: tcp-mqtt
  - port: 8883
    targetPort: 8883
    protocol: TCP
    name: tls-mqtt
  - port: 8000
    targetPort: 8000
    protocol: TCP
    name: tcp-mqttweb
  - port: 8443
    targetPort: 8443
    protocol: TCP
    name: tls-mqttweb
  selector:
    app.kubernetes.io/name: pubsubplus-dev
    app.kubernetes.io/instance: my-release
    active: "true"
---
# Source: pubsubplus-dev/templates/solaceStatefulSet.yaml
# Create the StatefulSet needed for redundancy
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-pubsubplus-dev
  labels:
    app.kubernetes.io/name: pubsubplus-dev
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    helm.sh/chart: pubsubplus-dev-3.3.3
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: pubsubplus-dev
      app.kubernetes.io/instance: my-release
  serviceName: my-release-pubsubplus-dev-discovery
  replicas: 1
  podManagementPolicy: Parallel
  updateStrategy: 
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: pubsubplus-dev
        app.kubernetes.io/instance: my-release
    spec:
      securityContext:
        fsGroup: 1000002
        runAsUser: 1000001
      serviceAccountName: my-release-pubsubplus-dev-sa
      terminationGracePeriodSeconds: 1200
      containers:
      - name: pubsubplus
        image: "solace/solace-pubsub-standard:latest"
        imagePullPolicy: IfNotPresent
        resources:
          requests:
            cpu: "1"
            memory: 3410Mi
          limits:
            cpu: "2"
            memory: 3410Mi
        livenessProbe:
          tcpSocket:
            port: 8080
          initialDelaySeconds: 300
          timeoutSeconds: 5
        readinessProbe:
          initialDelaySeconds: 30
          periodSeconds: 5
          exec:
            command:
            - /mnt/disks/solace/readiness_check.sh
        securityContext:
          privileged: false
        env:
        - name: STATEFULSET_NAME
          value: my-release-pubsubplus-dev
        - name: STATEFULSET_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: TZ
          value: :/usr/share/zoneinfo/UTC
        - name: UMASK
          value: "0022"
        command:
          - bash
          - "-ec"
          - |
            source /mnt/disks/solace/init.sh
            # not using postinstall hooks because of order dependencies
            # launch config check - readiness check script will be launched by readinessProbe
            nohup /mnt/disks/solace/startup-broker.sh &
            /usr/sbin/boot.sh
        lifecycle:
          preStop:
            exec:
              command:
                - bash
                - "-ec"
                - |
                  while ! pgrep solacedaemon ; do sleep 1; done
                  killall solacedaemon; 
                  while [ ! -d /usr/sw/var/db.upgrade ]; do sleep 1; done;
        ports:
          - containerPort: 2222
            protocol: TCP
          - containerPort: 8080
            protocol: TCP
          - containerPort: 1943
            protocol: TCP
          - containerPort: 55555
            protocol: TCP
          - containerPort: 55003
            protocol: TCP
          - containerPort: 55443
            protocol: TCP
          - containerPort: 55556
            protocol: TCP
          - containerPort: 8008
            protocol: TCP
          - containerPort: 1443
            protocol: TCP
          - containerPort: 9000
            protocol: TCP
          - containerPort: 9443
            protocol: TCP
          - containerPort: 5672
            protocol: TCP
          - containerPort: 5671
            protocol: TCP
          - containerPort: 1883
            protocol: TCP
          - containerPort: 8883
            protocol: TCP
          - containerPort: 8000
            protocol: TCP
          - containerPort: 8443
            protocol: TCP
        volumeMounts:
        - name: podinfo
          mountPath: /etc/podinfo        
        - name: config-map
          mountPath: /mnt/disks/solace
        - name: secrets
          mountPath: /mnt/disks/secrets
          readOnly: true
        - name: dshm
          mountPath: /dev/shm
        # use legacy multiple storage elements
        - name: data
          mountPath: /usr/sw/jail
          subPath: jail
        - name: data
          mountPath: /usr/sw/var
          subPath: var
        - name: data
          mountPath: /usr/sw/internalSpool
          subPath: internalSpool
        - name: data
          mountPath: /usr/sw/adb
          subPath: adb
        - name: data
          mountPath: /var/lib/solace/diags
          subPath: diags
        - name: data
          mountPath: /usr/sw/internalSpool/softAdb
          subPath: softAdb
      volumes:
        - name: podinfo
          downwardAPI:
            items:
              - path: "labels"
                fieldRef:
                  fieldPath: metadata.labels
        - name: config-map
          configMap:
            name: my-release-pubsubplus-dev
            defaultMode: 0755
        - name: secrets
          secret:
            secretName: my-release-pubsubplus-dev-secrets
            defaultMode: 0400
        - name: dshm
          emptyDir:
            medium: Memory
  # This is the default way to acquire volume for the data mount
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 10Gi
---
# Source: pubsubplus-dev/templates/podDisruptionBudget.yaml
# PodDisruptionBudget for  Statefulsets
---
# Source: pubsubplus-dev/templates/tests/test-semp-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "my-release-pubsubplus-dev-test"
  labels:
    app.kubernetes.io/name: pubsubplus-dev
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    helm.sh/chart: pubsubplus-dev-3.3.3
  annotations:
    "helm.sh/hook": test
spec:
  securityContext:
    fsGroup: 1000002
    runAsUser: 1000001
  containers:
    - name: my-release-pubsubplus-dev-test
      image: "solace/solace-pubsub-standard:latest"
      imagePullPolicy: IfNotPresent
      env:
        - name: SOLACE_HOST
          value: my-release-pubsubplus-dev
        - name: SOLACE_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-release-pubsubplus-dev-secrets
              key: username_admin_password
        - name: PORT_MAPPINGS
          value: "[map[containerPort:2222 name:tcp-ssh protocol:TCP servicePort:2222] map[containerPort:8080 name:tcp-semp protocol:TCP servicePort:8080] map[containerPort:1943 name:tls-semp protocol:TCP servicePort:1943] map[containerPort:55555 name:tcp-smf protocol:TCP servicePort:55555] map[containerPort:55003 name:tcp-smfcomp protocol:TCP servicePort:55003] map[containerPort:55443 name:tls-smf protocol:TCP servicePort:55443] map[containerPort:55556 name:tcp-smfroute protocol:TCP servicePort:55556] map[containerPort:8008 name:tcp-web protocol:TCP servicePort:8008] map[containerPort:1443 name:tls-web protocol:TCP servicePort:1443] map[containerPort:9000 name:tcp-rest protocol:TCP servicePort:9000] map[containerPort:9443 name:tls-rest protocol:TCP servicePort:9443] map[containerPort:5672 name:tcp-amqp protocol:TCP servicePort:5672] map[containerPort:5671 name:tls-amqp protocol:TCP servicePort:5671] map[containerPort:1883 name:tcp-mqtt protocol:TCP servicePort:1883] map[containerPort:8883 name:tls-mqtt protocol:TCP servicePort:8883] map[containerPort:8000 name:tcp-mqttweb protocol:TCP servicePort:8000] map[containerPort:8443 name:tls-mqttweb protocol:TCP servicePort:8443]]"
      command:
        - /bin/bash
        - -c
        - |
          # Get tcp-semp port out of PORT_MAPPINGS
          portmappings_array=(`awk -F']' '{ for(i=1;i<=NF;i++) print $i }' <<< $PORT_MAPPINGS | grep "tcp-semp"`)
          for i in ${portmappings_array[@]}; do if [[ "$i" == *"servicePort"* ]]; then SEMP_PORT="$(cut -d':' -f2 <<<$i)"; fi ; done
          echo "SEMP port: $SEMP_PORT"
          echo "Checking for successful SEMP access"
          if curl --write-out '%{http_code}' -u admin:$SOLACE_PASSWORD $SOLACE_HOST:$SEMP_PORT/SEMP | grep 200
            then echo "SEMP access successful"
            else echo "SEMP access failed"; exit 1
          fi
          exit 0
  restartPolicy: Never
