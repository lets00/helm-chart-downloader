---
# Source: authf/charts/elasticsearch/templates/poddisruptionbudget.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: "elasticsearch-master-pdb"
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: "elasticsearch-master"
---
# Source: authf/charts/microservice/templates/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-authf
  labels: 
    
    app.kubernetes.io/version: "0.8.1"
    helm.sh/chart: microservice-0.6.0
    
    
    app.kubernetes.io/name: authf
    app.kubernetes.io/instance: authf-v1
    
    app.cryptexlabs.com/apiVersion: v1
    
    app.kubernetes.io/managed-by: Helm
---
# Source: authf/charts/microservice/templates/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-authf-traefik-configurator
  labels: 
    
    app.kubernetes.io/version: "0.8.1"
    helm.sh/chart: microservice-0.6.0
    
    
    app.kubernetes.io/name: authf
    app.kubernetes.io/instance: authf-v1
    
    app.cryptexlabs.com/apiVersion: v1
    
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
---
# Source: authf/charts/microservice/templates/rbac.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: my-release-authf-traefik-configurator
subjects:
  - kind: ServiceAccount
    name: my-release-authf-traefik-configurator
    apiGroup: ""
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: ""
---
# Source: authf/charts/elasticsearch/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: elasticsearch-master
  labels:
    heritage: "Helm"
    release: "my-release"
    chart: "elasticsearch"
    app: "elasticsearch-master"
  annotations:
    {}
spec:
  type: ClusterIP
  selector:
    release: "my-release"
    chart: "elasticsearch"
    app: "elasticsearch-master"
  ports:
  - name: http
    protocol: TCP
    port: 9200
  - name: transport
    protocol: TCP
    port: 9300
---
# Source: authf/charts/elasticsearch/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: elasticsearch-master-headless
  labels:
    heritage: "Helm"
    release: "my-release"
    chart: "elasticsearch"
    app: "elasticsearch-master"
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  clusterIP: None # This is needed for statefulset hostnames like elasticsearch-0 to resolve
  # Create endpoints also if the related pod isn't ready
  publishNotReadyAddresses: true
  selector:
    app: "elasticsearch-master"
  ports:
  - name: http
    port: 9200
  - name: transport
    port: 9300
---
# Source: authf/charts/microservice/templates/service.yaml
---



apiVersion: v1
kind: Service
metadata:
  name: my-release-authf-rest
  labels: 
    
    app.kubernetes.io/version: "0.8.1"
    helm.sh/chart: microservice-0.6.0
    
    
    app.kubernetes.io/name: authf
    app.kubernetes.io/instance: authf-v1
    
    app.cryptexlabs.com/apiVersion: v1
    
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  selector: 
    app.kubernetes.io/name: authf
    app.kubernetes.io/instance: authf-v1
    
    app.cryptexlabs.com/apiVersion: v1
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: http
  ports:
    - port: 80
      name: "http"
      targetPort: "http"
      protocol: TCP
---
# Source: authf/charts/microservice/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-authf-http
  labels: 
    
    app.kubernetes.io/version: "0.8.1"
    helm.sh/chart: microservice-0.6.0
    
    
    app.kubernetes.io/name: authf
    app.kubernetes.io/instance: authf-v1
    
    app.cryptexlabs.com/apiVersion: v1
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: http
spec:
  selector:
    matchLabels: 
      app.kubernetes.io/name: authf
      app.kubernetes.io/instance: authf-v1
      
      app.cryptexlabs.com/apiVersion: v1
      
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/component: http
  replicas: 1
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels: 
        
        app.kubernetes.io/version: "0.8.1"
        helm.sh/chart: microservice-0.6.0
        
        
        app.kubernetes.io/name: authf
        app.kubernetes.io/instance: authf-v1
        
        app.cryptexlabs.com/apiVersion: v1
        
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: http
    spec:
    
      
      
      
      
      
      
      containers:
      
        
        
        
        - name: microservice
          
          image: "docker.io/cryptexlabs/authf:0.8.1"
          command:
            - yarn
            - start:prod
          imagePullPolicy: IfNotPresent
          
          
          resources:
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 150m
              memory: 512Mi
          
          
          ports:
            - name: "http"
              containerPort: 80
          
            
            
            
            
            
            
            
        
          
          
          startupProbe:
                    failureThreshold: 10
                    httpGet:
                      path: /healthz
                      port: http
                    initialDelaySeconds: 10
                    periodSeconds: 3
          readinessProbe:
                    failureThreshold: 3
                    httpGet:
                      path: /healthz
                      port: http
                    initialDelaySeconds: 10
                    periodSeconds: 10
          livenessProbe:
                    failureThreshold: 1
                    httpGet:
                      path: /healthz
                      port: http
                    initialDelaySeconds: 10
                    periodSeconds: 10
          
          
        
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: APP_VERSION
              value: "0.8.1"
            - name: API_VERSION
              value: "v1"
            - name: APP_PREFIX
              value: "api"
            - name: AUTHF_SERVER_CONFIG_PATH
              value: "/var/app/config/config/server-config.yaml"
            - name: DATABASE_TYPE
              value: "elasticsearch"
            - name: DOCS_PATH
              value: "/authf"
            - name: DOCS_PREFIX
              value: "docs"
            - name: ELASTICSEARCH_URL
              value: "http://elasticsearch-master:9200"
            - name: HTTP_PORT
              value: "80"
            - name: HTTP_SECURE
              value: "true"
            - name: JWT_ALGORITHM
              value: "RS256"
            - name: JWT_PRIVATE_KEY_PATH
              value: "/var/app/config/secrets/jwt-rs256.key"
            - name: JWT_PUBLIC_KEY_PATH
              value: "/var/app/config/secrets/jwt-rs256.pub"
            - name: LOG_LEVELS
              value: "info,error"
            
          volumeMounts:
            
            
            - name: server-configs
              mountPath: /var/app/config/config
            - name: my-release-authf-jwt-keys
              mountPath: /var/app/config/secrets
        
      
      volumes:
      
        
        
        
        
        - name: server-configs
          configMap:
            name: my-release-authf-server-configs
        - name: my-release-authf-jwt-keys
          secret:
            secretName: my-release-authf-jwt-keys
---
# Source: authf/charts/microservice/templates/hpa.yaml
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: my-release-authf-http
  labels: 
    
    app.kubernetes.io/version: "0.8.1"
    helm.sh/chart: microservice-0.6.0
    
    
    app.kubernetes.io/name: authf
    app.kubernetes.io/instance: authf-v1
    
    app.cryptexlabs.com/apiVersion: v1
    
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: http
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-release-authf-http
  maxReplicas: 20
  minReplicas: 1 
  metrics: 
    - resource:
        name: cpu
        target:
          averageUtilization: 50
          type: Utilization
      type: Resource
---
# Source: authf/charts/elasticsearch/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch-master
  labels:
    heritage: "Helm"
    release: "my-release"
    chart: "elasticsearch"
    app: "elasticsearch-master"
  annotations:
    esMajorVersion: "7"
spec:
  serviceName: elasticsearch-master-headless
  selector:
    matchLabels:
      app: "elasticsearch-master"
  replicas: 3
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  volumeClaimTemplates:
  - metadata:
      name: elasticsearch-master
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 30Gi
  template:
    metadata:
      name: "elasticsearch-master"
      labels:
        release: "my-release"
        chart: "elasticsearch"
        app: "elasticsearch-master"
      annotations:
        
    spec:
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - "elasticsearch-master"
            topologyKey: kubernetes.io/hostname
      terminationGracePeriodSeconds: 120
      volumes:
      enableServiceLinks: true
      initContainers:
      - name: configure-sysctl
        securityContext:
          runAsUser: 0
          privileged: true
        image: "docker.elastic.co/elasticsearch/elasticsearch:7.14.0"
        imagePullPolicy: "IfNotPresent"
        command: ["sysctl", "-w", "vm.max_map_count=262144"]
        resources:
          {}

      containers:
      - name: "elasticsearch"
        securityContext:
          capabilities:
            drop:
            - ALL
          runAsNonRoot: true
          runAsUser: 1000
        image: "docker.elastic.co/elasticsearch/elasticsearch:7.14.0"
        imagePullPolicy: "IfNotPresent"
        readinessProbe:
          exec:
            command:
              - sh
              - -c
              - |
                #!/usr/bin/env bash -e
                # If the node is starting up wait for the cluster to be ready (request params: "wait_for_status=green&timeout=1s" )
                # Once it has started only check that the node itself is responding
                START_FILE=/tmp/.es_start_file

                # Disable nss cache to avoid filling dentry cache when calling curl
                # This is required with Elasticsearch Docker using nss < 3.52
                export NSS_SDB_USE_CACHE=no

                http () {
                  local path="${1}"
                  local args="${2}"
                  set -- -XGET -s

                  if [ "$args" != "" ]; then
                    set -- "$@" $args
                  fi

                  if [ -n "${ELASTIC_USERNAME}" ] && [ -n "${ELASTIC_PASSWORD}" ]; then
                    set -- "$@" -u "${ELASTIC_USERNAME}:${ELASTIC_PASSWORD}"
                  fi

                  curl --output /dev/null -k "$@" "http://127.0.0.1:9200${path}"
                }

                if [ -f "${START_FILE}" ]; then
                  echo 'Elasticsearch is already running, lets check the node is healthy'
                  HTTP_CODE=$(http "/" "-w %{http_code}")
                  RC=$?
                  if [[ ${RC} -ne 0 ]]; then
                    echo "curl --output /dev/null -k -XGET -s -w '%{http_code}' \${BASIC_AUTH} http://127.0.0.1:9200/ failed with RC ${RC}"
                    exit ${RC}
                  fi
                  # ready if HTTP code 200, 503 is tolerable if ES version is 6.x
                  if [[ ${HTTP_CODE} == "200" ]]; then
                    exit 0
                  elif [[ ${HTTP_CODE} == "503" && "7" == "6" ]]; then
                    exit 0
                  else
                    echo "curl --output /dev/null -k -XGET -s -w '%{http_code}' \${BASIC_AUTH} http://127.0.0.1:9200/ failed with HTTP code ${HTTP_CODE}"
                    exit 1
                  fi

                else
                  echo 'Waiting for elasticsearch cluster to become ready (request params: "wait_for_status=green&timeout=1s" )'
                  if http "/_cluster/health?wait_for_status=green&timeout=1s" "--fail" ; then
                    touch ${START_FILE}
                    exit 0
                  else
                    echo 'Cluster is not yet ready (request params: "wait_for_status=green&timeout=1s" )'
                    exit 1
                  fi
                fi
          failureThreshold: 3
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 3
          timeoutSeconds: 5
        ports:
        - name: http
          containerPort: 9200
        - name: transport
          containerPort: 9300
        resources:
          limits:
            cpu: 1000m
            memory: 2Gi
          requests:
            cpu: 1000m
            memory: 2Gi
        env:
          - name: node.name
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: cluster.initial_master_nodes
            value: "elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,"
          - name: discovery.seed_hosts
            value: "elasticsearch-master-headless"
          - name: cluster.name
            value: "elasticsearch"
          - name: network.host
            value: "0.0.0.0"
          - name: node.data
            value: "true"
          - name: node.ingest
            value: "true"
          - name: node.master
            value: "true"
          - name: node.ml
            value: "true"
          - name: node.remote_cluster_client
            value: "true"
        volumeMounts:
          - name: "elasticsearch-master"
            mountPath: /usr/share/elasticsearch/data
---
# Source: authf/templates/init-jwt-secrets.job.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-authf-jwt-secrets-configurator
  labels: 
    
    app.kubernetes.io/version: '0.8.1'
    helm.sh/chart: authf-0.8.2
    
    app.kubernetes.io/name: authf
    app.kubernetes.io/instance: authf-v1
    
    app.cryptexlabs.com/apiVersion: v1
    
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade,post-delete
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
automountServiceAccountToken: true
---
# Source: authf/templates/init-jwt-secrets.job.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-authf-jwt-secrets-creator-scripts
  annotations:
    "helm.sh/hook-weight": "-1"
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
data:
  ensure-secrets-exist.sh: |
    if [[ ! $(kubectl get secret my-release-authf-jwt-keys 2>/dev/null ) ]]; then
      echo 'Secret my-release-authf-jwt-keys does not exist. Creating';

      openssl genrsa -out /tmp/mykey.pem 4096;
      openssl rsa -in /tmp/mykey.pem -pubout -out /tmp/pubkey.pem;

      jwt_rs256_key=$(base64 /tmp/mykey.pem -w 0);
      jwt_rs256_pub=$(base64 /tmp/pubkey.pem -w 0);

      cat >/tmp/jwt-secrets.yaml <<EOL
    apiVersion: v1
    kind: Secret
    type: Opaque
    metadata:
      name: my-release-authf-jwt-keys
      labels: 
        
        app.kubernetes.io/version: '0.8.1'
        helm.sh/chart: authf-0.8.2
        
        app.kubernetes.io/name: authf
        app.kubernetes.io/instance: authf-v1
        
        app.cryptexlabs.com/apiVersion: v1
        
        app.kubernetes.io/managed-by: Helm
    data:
      jwt-rs256.key: ${jwt_rs256_key}
      jwt-rs256.pub: ${jwt_rs256_pub}
    EOL

      kubectl create -f /tmp/jwt-secrets.yaml;
    else
      echo 'Secret my-release-authf-jwt-keys aready exists. Skipping';
    fi;
---
# Source: authf/templates/server-config.configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-authf-server-configs
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
data:
  server-config.yaml: | 
    {}
---
# Source: authf/templates/init-jwt-secrets.job.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: my-release-authf-jwt-secrets-configurator
  labels: 
    
    app.kubernetes.io/version: '0.8.1'
    helm.sh/chart: authf-0.8.2
    
    app.kubernetes.io/name: authf
    app.kubernetes.io/instance: authf-v1
    
    app.cryptexlabs.com/apiVersion: v1
    
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade,post-delete
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
rules:
  - apiGroups: [""]
    resources: ["secrets"]
    verbs: ["get", "watch", "list", "create"]
  - apiGroups: [""]
    resources: ["secrets"]
    resourceNames: ["my-release-authf-jwt-keys"]
    verbs: ["delete"]
---
# Source: authf/templates/init-jwt-secrets.job.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: my-release-authf-jwt-secrets-configurator
  labels: 
    
    app.kubernetes.io/version: '0.8.1'
    helm.sh/chart: authf-0.8.2
    
    app.kubernetes.io/name: authf
    app.kubernetes.io/instance: authf-v1
    
    app.cryptexlabs.com/apiVersion: v1
    
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade,post-delete
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
subjects:
  - kind: ServiceAccount
    name: my-release-authf-jwt-secrets-configurator
    apiGroup: ""
    namespace: default
roleRef:
  kind: Role
  name: my-release-authf-jwt-secrets-configurator
  apiGroup: ""
---
# Source: authf/charts/elasticsearch/templates/test/test-elasticsearch-health.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "my-release-ouvbl-test"
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-delete-policy": hook-succeeded
spec:
  securityContext:
    fsGroup: 1000
    runAsUser: 1000
  containers:
  - name: "my-release-scjje-test"
    image: "docker.elastic.co/elasticsearch/elasticsearch:7.14.0"
    imagePullPolicy: "IfNotPresent"
    command:
      - "sh"
      - "-c"
      - |
        #!/usr/bin/env bash -e
        curl -XGET --fail 'elasticsearch-master:9200/_cluster/health?wait_for_status=green&timeout=1s'
  restartPolicy: Never
---
# Source: authf/templates/init-jwt-secrets.job.yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-release-authf-jwt-secrets-creator
  labels: 
    
    app.kubernetes.io/version: '0.8.1'
    helm.sh/chart: authf-0.8.2
    
    app.kubernetes.io/name: authf
    app.kubernetes.io/instance: authf-v1
    
    app.cryptexlabs.com/apiVersion: v1
    
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
spec:
  serviceAccountName: my-release-authf-jwt-secrets-configurator
  containers:
    - name: jwt-secrets-maker
      image: "docker.io/bitnami/kubectl:latest"
      imagePullPolicy: IfNotPresent
      command:
        - /bin/bash
        - /var/scripts/ensure-secrets-exist.sh
      volumeMounts:
        - mountPath: /var/scripts
          name: scripts

  volumes:
    - name: scripts
      configMap:
        name: my-release-authf-jwt-secrets-creator-scripts

  restartPolicy: Never
  terminationGracePeriodSeconds: 0
---
# Source: authf/templates/init-jwt-secrets.job.yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-release-authf-jwt-secrets-destroyer
  labels: 
    
    app.kubernetes.io/version: '0.8.1'
    helm.sh/chart: authf-0.8.2
    
    app.kubernetes.io/name: authf
    app.kubernetes.io/instance: authf-v1
    
    app.cryptexlabs.com/apiVersion: v1
    
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": post-delete
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
spec:
  serviceAccountName: my-release-authf-jwt-secrets-configurator
  containers:
    - name: jwt-secrets-maker
      image: "docker.io/bitnami/kubectl:latest"
      imagePullPolicy: IfNotPresent
      command:
        - /bin/bash
        - -c
        - |
          kubectl delete secret my-release-authf-jwt-keys || exit 0

  restartPolicy: Never
  terminationGracePeriodSeconds: 0
