---
# Source: vela-core/templates/kubevela-controller.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-vela-core
  namespace: default
  labels:
    helm.sh/chart: vela-core-1.9.11
    app.kubernetes.io/name: vela-core
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.9.11"
    app.kubernetes.io/managed-by: Helm
---
# Source: vela-core/templates/addon_registry.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: vela-addon-registry
  namespace: default
data:
  registries: '{
  "KubeVela":{
    "name": "KubeVela",
    "helm": {
      "url": "https://kubevela.github.io/catalog/official"
    }
  }
}'
---
# Source: vela-core/templates/cluster-gateway/cluster-gateway.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-cluster-gateway-proxy-config
  namespace: default
data:
  config.yaml: |
    apiVersion: cluster.core.oam.dev/v1alpha1
    kind: ClusterGatewayProxyConfiguration
    spec:
      clientIdentityExchanger:
        rules:
          - name: super-user
            source:
              group: kubevela:ux
            type: PrivilegedIdentityExchanger
---
# Source: vela-core/templates/velaql/application-revision.yaml
apiVersion: "v1"
kind: "ConfigMap"
metadata:
  name: "application-revision-view"
  namespace: default
data:
  template: |
    import (
       "vela/op"
    )

    output: {
      op.#Read & {
        value: {
          apiVersion: "core.oam.dev/v1beta1"
            kind:       "ApplicationRevision"
            metadata: {
              name: parameter.name
              namespace: parameter.namespace
            }
        }
      }
    }

    parameter: {
       // +usage=Specify the name of the object
      name: string
      // +usage=Specify the namespace of the object
      namespace: *"default" | string
    }

    status: output.value
---
# Source: vela-core/templates/velaql/applied-resources.yaml
apiVersion: "v1"
kind:       "ConfigMap"
metadata: 
  name:      "service-applied-resources-view"
  namespace: default
data:
  template: |
      import (
          "vela/ql"
      )
      parameter: {
          appName:    string
          appNs:      string
          name?:      string
          cluster?:   string
          clusterNs?: string
      }
      response: ql.#ListAppliedResources & {
          app: {
              name:      parameter.appName
              namespace: parameter.appNs
              filter: {
                  if parameter.cluster != _|_ {
                      cluster: parameter.cluster
                  }
                  if parameter.clusterNs != _|_ {
                      clusterNamespace: parameter.clusterNs
                  }
                  if parameter.name != _|_ {
                      components: [parameter.name]
                  }
              }
          }
      }
      if response.err == _|_ {
          status: {
              resources: response.list
          }
      }
      if response.err != _|_ {
          status: {
              error: response.err
          }
      }
---
# Source: vela-core/templates/velaql/component-pod.yaml
apiVersion: v1
data:
  template: |
      import (
        "vela/ql"
      )

      parameter: {
        appName:    string
        appNs:      string
        name?:      string
        cluster?:   string
        clusterNs?: string
      }

      result: ql.#CollectPods & {
        app: {
          name:      parameter.appName
          namespace: parameter.appNs
          filter: {
            if parameter.cluster != _|_ {
              cluster: parameter.cluster
            }
            if parameter.clusterNs != _|_ {
              clusterNamespace: parameter.clusterNs
            }
            if parameter.name != _|_ {
              components: [parameter.name]
            }
          }
        }
      }

      if result.err == _|_ {
        status: {
          podList: [ for pod in result.list if pod.object != _|_ {
            cluster:   pod.cluster
            workload:  pod.workload
            component: pod.component
            metadata: {
              name:         pod.object.metadata.name
              namespace:    pod.object.metadata.namespace
              creationTime: pod.object.metadata.creationTimestamp
              labels:       pod.object.metadata.labels
              version: {
                if pod.publishVersion != _|_ {
                  publishVersion: pod.publishVersion
                }
                if pod.deployVersion != _|_ {
                  deployVersion: pod.deployVersion
                }
              }
            }
            status: {
              phase: pod.object.status.phase
              // refer to https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase
              if phase != "Pending" && phase != "Unknown" {
                if pod.object.status.podIP != _|_ {
                  podIP: pod.object.status.podIP
                }
                if pod.object.status.hostIP != _|_ {
                  hostIP: pod.object.status.hostIP
                }
                if pod.object.spec.nodeName != _|_ {
                  nodeName: pod.object.spec.nodeName
                }
              }
            }
          }]
        }
      }

      if result.err != _|_ {
        status: {
          error: result.err
        }
      }
kind: ConfigMap
metadata:
  name: component-pod-view
  namespace: default
---
# Source: vela-core/templates/velaql/component-service.yaml
apiVersion: v1
data:
  template: |
      import (
      "vela/ql"
      )

      parameter: {
        appName:    string
        appNs:      string
        name?:      string
        cluster?:   string
        clusterNs?: string
      }

      result: ql.#CollectServices & {
        app: {
          name:      parameter.appName
          namespace: parameter.appNs
          filter: {
            if parameter.cluster != _|_ {
              cluster: parameter.cluster
            }
            if parameter.clusterNs != _|_ {
              clusterNamespace: parameter.clusterNs
            }
            if parameter.name != _|_ {
              components: [parameter.name]
            }
          }
        }
      }

      if result.err == _|_ {
        status: {
          services: result.list
        }
      }

      if result.err != _|_ {
        status: {
          error: result.err
        }
      }
kind: ConfigMap
metadata:
  name: component-service-view
  namespace: default
---
# Source: vela-core/templates/velaql/endpoints.yaml
apiVersion: "v1"
kind:       "ConfigMap"
metadata: 
  name:      "service-endpoints-view"
  namespace: default
data:
  template: |
      import (
          "vela/ql"
      )
      parameter: {
          appName:    string
          appNs:      string
          name?:      string
          cluster?:   string
          clusterNs?: string
      }
      resources: ql.#CollectServiceEndpoints & {
          app: {
              name:      parameter.appName
              namespace: parameter.appNs
              filter: {
                  if parameter.cluster != _|_ {
                      cluster: parameter.cluster
                  }
                  if parameter.clusterNs != _|_ {
                      clusterNamespace: parameter.clusterNs
                  }
                  if parameter.name != _|_ {
                      components: [parameter.name]
                  }
              }
          }
      }
      if resources.err == _|_ {
          status: {
              endpoints: resources.list
          }
      }
      if resources.err != _|_ {
          status: {
              error: resources.err
          }
      }
---
# Source: vela-core/templates/velaql/resourceTree.yaml
apiVersion: "v1"
kind:       "ConfigMap"
metadata:
  name:      "application-resource-tree-view"
  namespace: default
data:
  template: |
    import (
        "vela/ql"
    )
    parameter: {
        appName:    string
        appNs:      string
        name?:      string
        cluster?:   string
        clusterNs?: string
        queryNewest?: bool
    }
    response: ql.#GetApplicationTree & {
        app: {
            name:      parameter.appName
            namespace: parameter.appNs
            filter: {
                if parameter.cluster != _|_ {
                    cluster: parameter.cluster
                }
                if parameter.clusterNs != _|_ {
                    clusterNamespace: parameter.clusterNs
                }
                if parameter.name != _|_ {
                    components: [parameter.name]
                }
                if parameter.queryNewest != _|_ {
                    queryNewest: parameter.queryNewest
                }
            }
        }
    }

    if response.err == _|_ {
        status: {
            resources: response.list
        }
    }
    if response.err != _|_ {
        status: {
            error: response.err
        }
    }
---
# Source: vela-core/templates/cluster-gateway/cluster-gateway.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: my-release-vela-core:cluster-gateway:proxy
rules:
  - apiGroups: [ "cluster.core.oam.dev" ]
    resources: [ "clustergateways/proxy" ]
    verbs: [ "get", "list", "watch", "create", "update", "patch", "delete" ]
---
# Source: vela-core/templates/cluster-gateway/cluster-gateway.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: my-release-vela-core:cluster-gateway:proxy
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: my-release-vela-core:cluster-gateway:proxy
subjects:
  - kind: Group
    name: kubevela:client
    apiGroup: rbac.authorization.k8s.io
  - kind: ServiceAccount
    name: my-release-vela-core
    namespace: default
---
# Source: vela-core/templates/kubevela-controller.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: my-release-vela-core:manager-rolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: "cluster-admin"
subjects:
  - kind: ServiceAccount
    name: my-release-vela-core
    namespace: default
---
# Source: vela-core/templates/kubevela-controller.yaml
# permissions to do leader election.
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: my-release-vela-core:leader-election-role
  namespace: default
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - configmaps/status
    verbs:
      - get
      - update
      - patch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
---
# Source: vela-core/templates/kubevela-controller.yaml
# permissions to read the view of VelaQL, schemas, and templates.
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: my-release-vela-core:template-reader-role
  namespace: default
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - configmaps/status
    verbs:
      - get
---
# Source: vela-core/templates/kubevela-controller.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: my-release-vela-core:leader-election-rolebinding
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: my-release-vela-core:leader-election-role
subjects:
  - kind: ServiceAccount
    name: my-release-vela-core
---
# Source: vela-core/templates/kubevela-controller.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: my-release-vela-core:template-reader-binding
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: my-release-vela-core:template-reader-role
subjects:
  - kind: Group
    name: template-reader
---
# Source: vela-core/templates/admission-webhooks/webhookService.yaml
apiVersion: v1
kind: Service
metadata:
  name: vela-core-webhook
  namespace: default
  labels:
    helm.sh/chart: vela-core-1.9.11
    app.kubernetes.io/name: vela-core
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.9.11"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 443
      targetPort: 9443
      protocol: TCP
      name: https
  selector:
    
      app.kubernetes.io/name: vela-core
      app.kubernetes.io/instance: my-release
---
# Source: vela-core/templates/cluster-gateway/cluster-gateway.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-cluster-gateway-service
  namespace: default
spec:
  selector:
    app.kubernetes.io/name: vela-core-cluster-gateway
    app.kubernetes.io/instance: my-release-cluster-gateway
  ports:
    - protocol: TCP
      port: 9443
      targetPort: 9443
---
# Source: vela-core/templates/cluster-gateway/cluster-gateway.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-cluster-gateway
  namespace: default
  labels:
    helm.sh/chart: vela-core-1.9.11
    app.kubernetes.io/name: vela-core
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.9.11"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: vela-core-cluster-gateway
      app.kubernetes.io/instance: my-release-cluster-gateway
  template:
    metadata:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: "9443"
        prometheus.io/scrape: "true"
        prometheus.io/scheme: "https"
      labels:
        app.kubernetes.io/name: vela-core-cluster-gateway
        app.kubernetes.io/instance: my-release-cluster-gateway
    spec:
      serviceAccountName: my-release-vela-core
      securityContext:
        {}
      containers:
        - name: my-release-vela-core-cluster-gateway
          securityContext:
            {}
          args:
            - "apiserver"
            - "--secure-port=9443"
            - "--secret-namespace=default"
            - "--feature-gates=APIPriorityAndFairness=false,ClientIdentityPenetration=false"
            - "--cluster-gateway-proxy-config=/etc/proxy-config/config.yaml"
            - "--tls-cert-file=/etc/k8s-cluster-gateway-certs/tls.crt"
            - "--tls-private-key-file=/etc/k8s-cluster-gateway-certs/tls.key"
            - "--authorization-always-allow-paths=/healthz,/readyz,/livez,/metrics"
          image: oamdev/cluster-gateway:v1.9.0-alpha.2
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              cpu: 500m
              memory: 200Mi
            requests:
              cpu: 50m
              memory: 20Mi
          ports:
            - containerPort: 9443
          volumeMounts:
            - mountPath: /etc/proxy-config
              name: proxy-config
          
            - mountPath: /etc/k8s-cluster-gateway-certs
              name: tls-cert-vol
              readOnly: true
      volumes:
        - configMap:
            defaultMode: 420
            name: my-release-cluster-gateway-proxy-config
          name: proxy-config
      
        - name: tls-cert-vol
          secret:
            defaultMode: 420
            secretName: my-release-vela-core-cluster-gateway-tls-v2
      
      affinity:
      
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: vela-core-cluster-gateway
                    app.kubernetes.io/instance: my-release-cluster-gateway
                topologyKey: kubernetes.io/hostname
              weight: 100
      
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
---
# Source: vela-core/templates/kubevela-controller.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-vela-core
  namespace: default
  labels:
    controller.oam.dev/name: vela-core
    helm.sh/chart: vela-core-1.9.11
    app.kubernetes.io/name: vela-core
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.9.11"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: vela-core
      app.kubernetes.io/instance: my-release
  template:
    metadata:
      labels:
        app.kubernetes.io/name: vela-core
        app.kubernetes.io/instance: my-release
        
      annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
    spec:
      serviceAccountName: my-release-vela-core
      securityContext:
        {}
      containers:
        - name: my-release
          securityContext:
            {}
          args:
            - "--metrics-addr=:8080"
            - "--enable-leader-election"
            
            
            
            - "--use-webhook=true"
            - "--webhook-port=9443"
            - "--webhook-cert-dir=/etc/k8s-webhook-certs"
            
            
            
            - "--optimize-mark-with-prob=0.1"
            
            
            - "--optimize-disable-component-revision"
            
            
            
            
            
            - "--health-addr=:9440"
            - "--system-definition-namespace=default"
            - "--application-revision-limit=2"
            - "--definition-revision-limit=2"
            
            - "--enable-cluster-gateway"
            
            - "--cluster-gateway-url=my-release-cluster-gateway-service:9443"
            
            - "--cluster-gateway-ca-file=/cluster-gateway-tls-cert/ca"
            
            
            
            
            - "--application-re-sync-period=5m"
            - "--concurrent-reconciles=4"
            - "--kube-api-qps=400"
            - "--kube-api-burst=600"
            - "--max-workflow-wait-backoff-time=60"
            - "--max-workflow-failed-backoff-time=300"
            - "--max-workflow-step-error-retry-times=10"
            - "--feature-gates=EnableSuspendOnFailure=false"
            - "--feature-gates=AuthenticateApplication=false"
            - "--feature-gates=GzipResourceTracker=false"
            - "--feature-gates=ZstdResourceTracker=true"
            - "--feature-gates=ApplyOnce=false"
            - "--feature-gates=MultiStageComponentApply=true"
            - "--feature-gates=GzipApplicationRevision=false"
            - "--feature-gates=ZstdApplicationRevision=true"
            - "--feature-gates=PreDispatchDryRun=true"
            - "--feature-gates=DisableBootstrapClusterInfo=false"
            - "--feature-gates=InformerCacheFilterUnnecessaryFields=true"
            - "--feature-gates=SharedDefinitionStorageForApplicationRevision=true"
            - "--feature-gates=DisableWorkflowContextConfigMapCache=true"
            
            
          image: oamdev/vela-core:v1.9.11
          imagePullPolicy: "Always"
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 50m
              memory: 20Mi
          
          ports:
            - containerPort: 9443
              name: webhook-server
              protocol: TCP
            - containerPort: 9440
              name: healthz
              protocol: TCP
          readinessProbe:
            httpGet:
              path: /readyz
              port: healthz
            initialDelaySeconds: 30
            periodSeconds: 5
            timeoutSeconds: 1
            failureThreshold: 3
            successThreshold: 1
          livenessProbe:
            httpGet:
              path: /healthz
              port: healthz
            initialDelaySeconds: 90
            periodSeconds: 5
            timeoutSeconds: 1
            failureThreshold: 3
            successThreshold: 1
          volumeMounts:
            - mountPath: /etc/k8s-webhook-certs
              name: tls-cert-vol
              readOnly: true
          
            - mountPath: /cluster-gateway-tls-cert
              name: tls-cert-vol-cg
              readOnly: true
          
          
      
      volumes:
        - name: tls-cert-vol
          secret:
            defaultMode: 420
            secretName: my-release-vela-core-admission
      
        - name: tls-cert-vol-cg
          secret:
            defaultMode: 420
            secretName: my-release-vela-core-cluster-gateway-tls-v2
      
      
      affinity:
      
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: vela-core
                    app.kubernetes.io/instance: my-release
                topologyKey: kubernetes.io/hostname
              weight: 100
---
# Source: vela-core/templates/cluster-gateway/cluster-gateway.yaml
# 1.  Check whether APIService ""v1alpha1.cluster.core.oam.dev" is already present in the cluster
# 2.a If the APIService doesn't exist, create it.
# 2.b If the APIService exists without helm-chart related annotation, skip creating it to the
#     cluster because the APIService can be managed by an external controller.
# 2.c If the APIService exists with valid helm-chart annotations, which means that the APIService
#     is previously managed by helm commands, hence update the APIService consistently.




apiVersion: apiregistration.k8s.io/v1
kind: APIService
metadata:
  name: v1alpha1.cluster.core.oam.dev
  annotations:
  labels:
    api: cluster-extension-apiserver
    apiserver: "true"
spec:
  version: v1alpha1
  group: cluster.core.oam.dev
  groupPriorityMinimum: 2000
  service:
    name: my-release-cluster-gateway-service
    namespace: default
    port: 9443
  versionPriority: 10
  insecureSkipTLSVerify: false
  
  caBundle: Cg==
---
# Source: vela-core/templates/defwithtemplate/cron-task.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/cron-task.cue
apiVersion: core.oam.dev/v1beta1
kind: ComponentDefinition
metadata:
  annotations:
    definition.oam.dev/description: Describes cron jobs that run code or a script to completion.
  name: cron-task
  namespace: default
spec:
  schematic:
    cue:
      template: |
        mountsArray: {
        	pvc: *[
        		for v in parameter.volumeMounts.pvc {
        			{
        				mountPath: v.mountPath
        				if v.subPath != _|_ {
        					subPath: v.subPath
        				}
        				name: v.name
        			}
        		},
        	] | []

        	configMap: *[
        			for v in parameter.volumeMounts.configMap {
        			{
        				mountPath: v.mountPath
        				if v.subPath != _|_ {
        					subPath: v.subPath
        				}
        				name: v.name
        			}
        		},
        	] | []

        	secret: *[
        		for v in parameter.volumeMounts.secret {
        			{
        				mountPath: v.mountPath
        				if v.subPath != _|_ {
        					subPath: v.subPath
        				}
        				name: v.name
        			}
        		},
        	] | []

        	emptyDir: *[
        			for v in parameter.volumeMounts.emptyDir {
        			{
        				mountPath: v.mountPath
        				if v.subPath != _|_ {
        					subPath: v.subPath
        				}
        				name: v.name
        			}
        		},
        	] | []

        	hostPath: *[
        			for v in parameter.volumeMounts.hostPath {
        			{
        				mountPath: v.mountPath
        				if v.subPath != _|_ {
        					subPath: v.subPath
        				}
        				name: v.name
        			}
        		},
        	] | []
        }
        volumesArray: {
        	pvc: *[
        		for v in parameter.volumeMounts.pvc {
        			{
        				name: v.name
        				persistentVolumeClaim: claimName: v.claimName
        			}
        		},
        	] | []

        	configMap: *[
        			for v in parameter.volumeMounts.configMap {
        			{
        				name: v.name
        				configMap: {
        					defaultMode: v.defaultMode
        					name:        v.cmName
        					if v.items != _|_ {
        						items: v.items
        					}
        				}
        			}
        		},
        	] | []

        	secret: *[
        		for v in parameter.volumeMounts.secret {
        			{
        				name: v.name
        				secret: {
        					defaultMode: v.defaultMode
        					secretName:  v.secretName
        					if v.items != _|_ {
        						items: v.items
        					}
        				}
        			}
        		},
        	] | []

        	emptyDir: *[
        			for v in parameter.volumeMounts.emptyDir {
        			{
        				name: v.name
        				emptyDir: medium: v.medium
        			}
        		},
        	] | []

        	hostPath: *[
        			for v in parameter.volumeMounts.hostPath {
        			{
        				name: v.name
        				hostPath: path: v.path
        			}
        		},
        	] | []
        }
        volumesList: volumesArray.pvc + volumesArray.configMap + volumesArray.secret + volumesArray.emptyDir + volumesArray.hostPath
        deDupVolumesArray: [
        	for val in [
        		for i, vi in volumesList {
        			for j, vj in volumesList if j < i && vi.name == vj.name {
        				_ignore: true
        			}
        			vi
        		},
        	] if val._ignore == _|_ {
        		val
        	},
        ]
        output: {
        	if context.clusterVersion.minor < 25 {
        		apiVersion: "batch/v1beta1"
        	}
        	if context.clusterVersion.minor >= 25 {
        		apiVersion: "batch/v1"
        	}
        	kind: "CronJob"
        	spec: {
        		schedule:                   parameter.schedule
        		concurrencyPolicy:          parameter.concurrencyPolicy
        		suspend:                    parameter.suspend
        		successfulJobsHistoryLimit: parameter.successfulJobsHistoryLimit
        		failedJobsHistoryLimit:     parameter.failedJobsHistoryLimit
        		if parameter.startingDeadlineSeconds != _|_ {
        			startingDeadlineSeconds: parameter.startingDeadlineSeconds
        		}
        		jobTemplate: {
        			metadata: {
        				labels: {
        					if parameter.labels != _|_ {
        						parameter.labels
        					}
        					"app.oam.dev/name":      context.appName
        					"app.oam.dev/component": context.name
        				}
        				if parameter.annotations != _|_ {
        					annotations: parameter.annotations
        				}
        			}
        			spec: {
        				parallelism: parameter.count
        				completions: parameter.count
        				if parameter.ttlSecondsAfterFinished != _|_ {
        					ttlSecondsAfterFinished: parameter.ttlSecondsAfterFinished
        				}
        				if parameter.activeDeadlineSeconds != _|_ {
        					activeDeadlineSeconds: parameter.activeDeadlineSeconds
        				}
        				backoffLimit: parameter.backoffLimit
        				template: {
        					metadata: {
        						labels: {
        							if parameter.labels != _|_ {
        								parameter.labels
        							}
        							"app.oam.dev/name":      context.appName
        							"app.oam.dev/component": context.name
        						}
        						if parameter.annotations != _|_ {
        							annotations: parameter.annotations
        						}
        					}
        					spec: {
        						restartPolicy: parameter.restart
        						containers: [{
        							name:  context.name
        							image: parameter.image
        							if parameter["imagePullPolicy"] != _|_ {
        								imagePullPolicy: parameter.imagePullPolicy
        							}
        							if parameter["cmd"] != _|_ {
        								command: parameter.cmd
        							}
        							if parameter["env"] != _|_ {
        								env: parameter.env
        							}
        							if parameter["cpu"] != _|_ {
        								resources: {
        									limits: cpu:   parameter.cpu
        									requests: cpu: parameter.cpu
        								}
        							}
        							if parameter["memory"] != _|_ {
        								resources: {
        									limits: memory:   parameter.memory
        									requests: memory: parameter.memory
        								}
        							}
        							if parameter["volumes"] != _|_ if parameter["volumeMounts"] == _|_ {
        								volumeMounts: [ for v in parameter.volumes {
        									{
        										mountPath: v.mountPath
        										name:      v.name
        									}}]
        							}
        							if parameter["volumeMounts"] != _|_ {
        								volumeMounts: mountsArray.pvc + mountsArray.configMap + mountsArray.secret + mountsArray.emptyDir + mountsArray.hostPath
        							}
        						}]
        						if parameter["volumes"] != _|_ if parameter["volumeMounts"] == _|_ {
        							volumes: [ for v in parameter.volumes {
        								{
        									name: v.name
        									if v.type == "pvc" {
        										persistentVolumeClaim: claimName: v.claimName
        									}
        									if v.type == "configMap" {
        										configMap: {
        											defaultMode: v.defaultMode
        											name:        v.cmName
        											if v.items != _|_ {
        												items: v.items
        											}
        										}
        									}
        									if v.type == "secret" {
        										secret: {
        											defaultMode: v.defaultMode
        											secretName:  v.secretName
        											if v.items != _|_ {
        												items: v.items
        											}
        										}
        									}
        									if v.type == "emptyDir" {
        										emptyDir: medium: v.medium
        									}
        								}}]
        						}
        						if parameter["volumeMounts"] != _|_ {
        							volumes: deDupVolumesArray
        						}
        						if parameter["imagePullSecrets"] != _|_ {
        							imagePullSecrets: [ for v in parameter.imagePullSecrets {
        								name: v
        							},
        							]
        						}
        						if parameter.hostAliases != _|_ {
        							hostAliases: [ for v in parameter.hostAliases {
        								ip:        v.ip
        								hostnames: v.hostnames
        							},
        							]
        						}
        					}
        				}
        			}
        		}
        	}
        }

        parameter: {
        	// +usage=Specify the labels in the workload
        	labels?: [string]: string

        	// +usage=Specify the annotations in the workload
        	annotations?: [string]: string

        	// +usage=Specify the schedule in Cron format, see https://en.wikipedia.org/wiki/Cron
        	schedule: string

        	// +usage=Specify deadline in seconds for starting the job if it misses scheduled
        	startingDeadlineSeconds?: int

        	// +usage=suspend subsequent executions
        	suspend: *false | bool

        	// +usage=Specifies how to treat concurrent executions of a Job
        	concurrencyPolicy: *"Allow" | "Allow" | "Forbid" | "Replace"

        	// +usage=The number of successful finished jobs to retain
        	successfulJobsHistoryLimit: *3 | int

        	// +usage=The number of failed finished jobs to retain
        	failedJobsHistoryLimit: *1 | int

        	// +usage=Specify number of tasks to run in parallel
        	// +short=c
        	count: *1 | int

        	// +usage=Which image would you like to use for your service
        	// +short=i
        	image: string

        	// +usage=Specify image pull policy for your service
        	imagePullPolicy?: "Always" | "Never" | "IfNotPresent"

        	// +usage=Specify image pull secrets for your service
        	imagePullSecrets?: [...string]

        	// +usage=Define the job restart policy, the value can only be Never or OnFailure. By default, it's Never.
        	restart: *"Never" | string

        	// +usage=Commands to run in the container
        	cmd?: [...string]

        	// +usage=Define arguments by using environment variables
        	env?: [...{
        		// +usage=Environment variable name
        		name: string
        		// +usage=The value of the environment variable
        		value?: string
        		// +usage=Specifies a source the value of this var should come from
        		valueFrom?: {
        			// +usage=Selects a key of a secret in the pod's namespace
        			secretKeyRef?: {
        				// +usage=The name of the secret in the pod's namespace to select from
        				name: string
        				// +usage=The key of the secret to select from. Must be a valid secret key
        				key: string
        			}
        			// +usage=Selects a key of a config map in the pod's namespace
        			configMapKeyRef?: {
        				// +usage=The name of the config map in the pod's namespace to select from
        				name: string
        				// +usage=The key of the config map to select from. Must be a valid secret key
        				key: string
        			}
        		}
        	}]

        	// +usage=Number of CPU units for the service, like `0.5` (0.5 CPU core), `1` (1 CPU core)
        	cpu?: string

        	// +usage=Specifies the attributes of the memory resource required for the container.
        	memory?: string

        	volumeMounts?: {
        		// +usage=Mount PVC type volume
        		pvc?: [...{
        			name:      string
        			mountPath: string
        			subPath?:  string
        			// +usage=The name of the PVC
        			claimName: string
        		}]
        		// +usage=Mount ConfigMap type volume
        		configMap?: [...{
        			name:        string
        			mountPath:   string
        			subPath?:    string
        			defaultMode: *420 | int
        			cmName:      string
        			items?: [...{
        				key:  string
        				path: string
        				mode: *511 | int
        			}]
        		}]
        		// +usage=Mount Secret type volume
        		secret?: [...{
        			name:        string
        			mountPath:   string
        			subPath?:    string
        			defaultMode: *420 | int
        			secretName:  string
        			items?: [...{
        				key:  string
        				path: string
        				mode: *511 | int
        			}]
        		}]
        		// +usage=Mount EmptyDir type volume
        		emptyDir?: [...{
        			name:      string
        			mountPath: string
        			subPath?:  string
        			medium:    *"" | "Memory"
        		}]
        		// +usage=Mount HostPath type volume
        		hostPath?: [...{
        			name:      string
        			mountPath: string
        			subPath?:  string
        			path:      string
        		}]
        	}

        	// +usage=Deprecated field, use volumeMounts instead.
        	volumes?: [...{
        		name:      string
        		mountPath: string
        		// +usage=Specify volume type, options: "pvc","configMap","secret","emptyDir", default to emptyDir
        		type: *"emptyDir" | "pvc" | "configMap" | "secret"
        		if type == "pvc" {
        			claimName: string
        		}
        		if type == "configMap" {
        			defaultMode: *420 | int
        			cmName:      string
        			items?: [...{
        				key:  string
        				path: string
        				mode: *511 | int
        			}]
        		}
        		if type == "secret" {
        			defaultMode: *420 | int
        			secretName:  string
        			items?: [...{
        				key:  string
        				path: string
        				mode: *511 | int
        			}]
        		}
        		if type == "emptyDir" {
        			medium: *"" | "Memory"
        		}
        	}]

        	// +usage=An optional list of hosts and IPs that will be injected into the pod's hosts file
        	hostAliases?: [...{
        		ip: string
        		hostnames: [...string]
        	}]

        	// +usage=Limits the lifetime of a Job that has finished
        	ttlSecondsAfterFinished?: int

        	// +usage=The duration in seconds relative to the startTime that the job may be continuously active before the system tries to terminate it
        	activeDeadlineSeconds?: int

        	// +usage=The number of retries before marking this job failed
        	backoffLimit: *6 | int

        	// +usage=Instructions for assessing whether the container is alive.
        	livenessProbe?: #HealthProbe

        	// +usage=Instructions for assessing whether the container is in a suitable state to serve traffic.
        	readinessProbe?: #HealthProbe
        }

        #HealthProbe: {

        	// +usage=Instructions for assessing container health by executing a command. Either this attribute or the httpGet attribute or the tcpSocket attribute MUST be specified. This attribute is mutually exclusive with both the httpGet attribute and the tcpSocket attribute.
        	exec?: {
        		// +usage=A command to be executed inside the container to assess its health. Each space delimited token of the command is a separate array element. Commands exiting 0 are considered to be successful probes, whilst all other exit codes are considered failures.
        		command: [...string]
        	}

        	// +usage=Instructions for assessing container health by executing an HTTP GET request. Either this attribute or the exec attribute or the tcpSocket attribute MUST be specified. This attribute is mutually exclusive with both the exec attribute and the tcpSocket attribute.
        	httpGet?: {
        		// +usage=The endpoint, relative to the port, to which the HTTP GET request should be directed.
        		path: string
        		// +usage=The TCP socket within the container to which the HTTP GET request should be directed.
        		port: int
        		httpHeaders?: [...{
        			name:  string
        			value: string
        		}]
        	}

        	// +usage=Instructions for assessing container health by probing a TCP socket. Either this attribute or the exec attribute or the httpGet attribute MUST be specified. This attribute is mutually exclusive with both the exec attribute and the httpGet attribute.
        	tcpSocket?: {
        		// +usage=The TCP socket within the container that should be probed to assess container health.
        		port: int
        	}

        	// +usage=Number of seconds after the container is started before the first probe is initiated.
        	initialDelaySeconds: *0 | int

        	// +usage=How often, in seconds, to execute the probe.
        	periodSeconds: *10 | int

        	// +usage=Number of seconds after which the probe times out.
        	timeoutSeconds: *1 | int

        	// +usage=Minimum consecutive successes for the probe to be considered successful after having failed.
        	successThreshold: *1 | int

        	// +usage=Number of consecutive failures required to determine the container is not alive (liveness probe) or not ready (readiness probe).
        	failureThreshold: *3 | int
        }
  workload:
    type: autodetects.core.oam.dev
---
# Source: vela-core/templates/defwithtemplate/daemon.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/daemon.cue
apiVersion: core.oam.dev/v1beta1
kind: ComponentDefinition
metadata:
  annotations:
    definition.oam.dev/description: Describes daemonset services in Kubernetes.
  name: daemon
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"strconv"
        )

        mountsArray: [
        	if parameter.volumeMounts != _|_ && parameter.volumeMounts.pvc != _|_ for v in parameter.volumeMounts.pvc {
        		{
        			mountPath: v.mountPath
        			if v.subPath != _|_ {
        				subPath: v.subPath
        			}
        			name: v.name
        		}
        	},

        	if parameter.volumeMounts != _|_ && parameter.volumeMounts.configMap != _|_ for v in parameter.volumeMounts.configMap {
        		{
        			mountPath: v.mountPath
        			if v.subPath != _|_ {
        				subPath: v.subPath
        			}
        			name: v.name
        		}
        	},

        	if parameter.volumeMounts != _|_ && parameter.volumeMounts.secret != _|_ for v in parameter.volumeMounts.secret {
        		{
        			mountPath: v.mountPath
        			if v.subPath != _|_ {
        				subPath: v.subPath
        			}
        			name: v.name
        		}
        	},

        	if parameter.volumeMounts != _|_ && parameter.volumeMounts.emptyDir != _|_ for v in parameter.volumeMounts.emptyDir {
        		{
        			mountPath: v.mountPath
        			if v.subPath != _|_ {
        				subPath: v.subPath
        			}
        			name: v.name
        		}
        	},

        	if parameter.volumeMounts != _|_ && parameter.volumeMounts.hostPath != _|_ for v in parameter.volumeMounts.hostPath {
        		{
        			mountPath: v.mountPath
        			if v.subPath != _|_ {
        				subPath: v.subPath
        			}
        			name: v.name
        		}
        	},
        ]

        volumesList: [
        	if parameter.volumeMounts != _|_ && parameter.volumeMounts.pvc != _|_ for v in parameter.volumeMounts.pvc {
        		{
        			name: v.name
        			persistentVolumeClaim: claimName: v.claimName
        		}
        	},

        	if parameter.volumeMounts != _|_ && parameter.volumeMounts.configMap != _|_ for v in parameter.volumeMounts.configMap {
        		{
        			name: v.name
        			configMap: {
        				defaultMode: v.defaultMode
        				name:        v.cmName
        				if v.items != _|_ {
        					items: v.items
        				}
        			}
        		}
        	},

        	if parameter.volumeMounts != _|_ && parameter.volumeMounts.secret != _|_ for v in parameter.volumeMounts.secret {
        		{
        			name: v.name
        			secret: {
        				defaultMode: v.defaultMode
        				secretName:  v.secretName
        				if v.items != _|_ {
        					items: v.items
        				}
        			}
        		}
        	},

        	if parameter.volumeMounts != _|_ && parameter.volumeMounts.emptyDir != _|_ for v in parameter.volumeMounts.emptyDir {
        		{
        			name: v.name
        			emptyDir: medium: v.medium
        		}
        	},

        	if parameter.volumeMounts != _|_ && parameter.volumeMounts.hostPath != _|_ for v in parameter.volumeMounts.hostPath {
        		{
        			name: v.name
        			hostPath: path: v.path
        		}
        	},
        ]

        deDupVolumesArray: [
        	for val in [
        		for i, vi in volumesList {
        			for j, vj in volumesList if j < i && vi.name == vj.name {
        				_ignore: true
        			}
        			vi
        		},
        	] if val._ignore == _|_ {
        		val
        	},
        ]

        output: {
        	apiVersion: "apps/v1"
        	kind:       "DaemonSet"
        	spec: {
        		selector: matchLabels: "app.oam.dev/component": context.name

        		template: {
        			metadata: {
        				labels: {
        					if parameter.labels != _|_ {
        						parameter.labels
        					}
        					if parameter.addRevisionLabel {
        						"app.oam.dev/revision": context.revision
        					}
        					"app.oam.dev/name":      context.appName
        					"app.oam.dev/component": context.name
        				}
        				if parameter.annotations != _|_ {
        					annotations: parameter.annotations
        				}
        			}

        			spec: {
        				containers: [{
        					name:  context.name
        					image: parameter.image
        					if parameter["port"] != _|_ && parameter["ports"] == _|_ {
        						ports: [{
        							containerPort: parameter.port
        						}]
        					}
        					if parameter["ports"] != _|_ {
        						ports: [ for v in parameter.ports {
        							{
        								containerPort: v.port
        								protocol:      v.protocol
        								if v.name != _|_ {
        									name: v.name
        								}
        								if v.name == _|_ {
        									name: "port-" + strconv.FormatInt(v.port, 10)
        								}
        							}}]
        					}

        					if parameter["imagePullPolicy"] != _|_ {
        						imagePullPolicy: parameter.imagePullPolicy
        					}

        					if parameter["cmd"] != _|_ {
        						command: parameter.cmd
        					}

        					if parameter["env"] != _|_ {
        						env: parameter.env
        					}

        					if context["config"] != _|_ {
        						env: context.config
        					}

        					if parameter["cpu"] != _|_ {
        						resources: {
        							limits: cpu:   parameter.cpu
        							requests: cpu: parameter.cpu
        						}
        					}

        					if parameter["memory"] != _|_ {
        						resources: {
        							limits: memory:   parameter.memory
        							requests: memory: parameter.memory
        						}
        					}

        					if parameter["volumes"] != _|_ && parameter["volumeMounts"] == _|_ {
        						volumeMounts: [ for v in parameter.volumes {
        							{
        								mountPath: v.mountPath
        								name:      v.name
        							}}]
        					}

        					if parameter["volumeMounts"] != _|_ {
        						volumeMounts: mountsArray
        					}

        					if parameter["livenessProbe"] != _|_ {
        						livenessProbe: parameter.livenessProbe
        					}

        					if parameter["readinessProbe"] != _|_ {
        						readinessProbe: parameter.readinessProbe
        					}

        				}]

        				if parameter["hostAliases"] != _|_ {
        					// +patchKey=ip
        					hostAliases: parameter.hostAliases
        				}

        				if parameter["imagePullSecrets"] != _|_ {
        					imagePullSecrets: [ for v in parameter.imagePullSecrets {
        						name: v
        					},
        					]
        				}

        				if parameter["volumes"] != _|_ && parameter["volumeMounts"] == _|_ {
        					volumes: [ for v in parameter.volumes {
        						{
        							name: v.name
        							if v.type == "pvc" {
        								persistentVolumeClaim: claimName: v.claimName
        							}
        							if v.type == "configMap" {
        								configMap: {
        									defaultMode: v.defaultMode
        									name:        v.cmName
        									if v.items != _|_ {
        										items: v.items
        									}
        								}
        							}
        							if v.type == "secret" {
        								secret: {
        									defaultMode: v.defaultMode
        									secretName:  v.secretName
        									if v.items != _|_ {
        										items: v.items
        									}
        								}
        							}
        							if v.type == "emptyDir" {
        								emptyDir: medium: v.medium
        							}
        						}
        					}]
        				}

        				if parameter["volumeMounts"] != _|_ {
        					volumes: deDupVolumesArray
        				}
        			}
        		}
        	}
        }

        exposePorts: [
        	if parameter.ports != _|_ for v in parameter.ports if v.expose == true {
        		port:       v.port
        		targetPort: v.port
        		if v.name != _|_ {
        			name: v.name
        		}
        		if v.name == _|_ {
        			name: "port-" + strconv.FormatInt(v.port, 10)
        		}
        	},
        ]

        outputs: {
        	if len(exposePorts) != 0 {
        		webserviceExpose: {
        			apiVersion: "v1"
        			kind:       "Service"
        			metadata: name: context.name
        			spec: {
        				selector: "app.oam.dev/component": context.name
        				ports: exposePorts
        				type:  parameter.exposeType
        			}
        		}
        	}
        }

        parameter: {
        	// +usage=Specify the labels in the workload
        	labels?: [string]: string

        	// +usage=Specify the annotations in the workload
        	annotations?: [string]: string

        	// +usage=Which image would you like to use for your service
        	// +short=i
        	image: string

        	// +usage=Specify image pull policy for your service
        	imagePullPolicy?: "Always" | "Never" | "IfNotPresent"

        	// +usage=Specify image pull secrets for your service
        	imagePullSecrets?: [...string]

        	// +ignore
        	// +usage=Deprecated field, please use ports instead
        	// +short=p
        	port?: int

        	// +usage=Which ports do you want customer traffic sent to, defaults to 80
        	ports?: [...{
        		// +usage=Number of port to expose on the pod's IP address
        		port: int
        		// +usage=Name of the port
        		name?: string
        		// +usage=Protocol for port. Must be UDP, TCP, or SCTP
        		protocol: *"TCP" | "UDP" | "SCTP"
        		// +usage=Specify if the port should be exposed
        		expose: *false | bool
        	}]

        	// +ignore
        	// +usage=Specify what kind of Service you want. options: "ClusterIP", "NodePort", "LoadBalancer", "ExternalName"
        	exposeType: *"ClusterIP" | "NodePort" | "LoadBalancer" | "ExternalName"

        	// +ignore
        	// +usage=If addRevisionLabel is true, the revision label will be added to the underlying pods
        	addRevisionLabel: *false | bool

        	// +usage=Commands to run in the container
        	cmd?: [...string]

        	// +usage=Define arguments by using environment variables
        	env?: [...{
        		// +usage=Environment variable name
        		name: string
        		// +usage=The value of the environment variable
        		value?: string
        		// +usage=Specifies a source the value of this var should come from
        		valueFrom?: {
        			// +usage=Selects a key of a secret in the pod's namespace
        			secretKeyRef?: {
        				// +usage=The name of the secret in the pod's namespace to select from
        				name: string
        				// +usage=The key of the secret to select from. Must be a valid secret key
        				key: string
        			}
        			// +usage=Selects a key of a config map in the pod's namespace
        			configMapKeyRef?: {
        				// +usage=The name of the config map in the pod's namespace to select from
        				name: string
        				// +usage=The key of the config map to select from. Must be a valid secret key
        				key: string
        			}
        		}
        	}]

        	// +usage=Number of CPU units for the service, like `0.5` (0.5 CPU core), `1` (1 CPU core)
        	cpu?: string

        	// +usage=Specifies the attributes of the memory resource required for the container.
        	memory?: string

        	volumeMounts?: {
        		// +usage=Mount PVC type volume
        		pvc?: [...{
        			name:      string
        			mountPath: string
        			// +usage=The name of the PVC
        			claimName: string
        		}]
        		// +usage=Mount ConfigMap type volume
        		configMap?: [...{
        			name:        string
        			mountPath:   string
        			defaultMode: *420 | int
        			cmName:      string
        			items?: [...{
        				key:  string
        				path: string
        				mode: *511 | int
        			}]
        		}]
        		// +usage=Mount Secret type volume
        		secret?: [...{
        			name:        string
        			mountPath:   string
        			defaultMode: *420 | int
        			secretName:  string
        			items?: [...{
        				key:  string
        				path: string
        				mode: *511 | int
        			}]
        		}]
        		// +usage=Mount EmptyDir type volume
        		emptyDir?: [...{
        			name:      string
        			mountPath: string
        			medium:    *"" | "Memory"
        		}]
        		// +usage=Mount HostPath type volume
        		hostPath?: [...{
        			name:              string
        			mountPath:         string
        			mountPropagation?: "None" | "HostToContainer" | "Bidirectional"
        			path:              string
        			readOnly?:         bool
        		}]
        	}

        	// +usage=Deprecated field, use volumeMounts instead.
        	volumes?: [...{
        		name:      string
        		mountPath: string
        		// +usage=Specify volume type, options: "pvc","configMap","secret","emptyDir", default to emptyDir
        		type: *"emptyDir" | "pvc" | "configMap" | "secret"
        		if type == "pvc" {
        			claimName: string
        		}
        		if type == "configMap" {
        			defaultMode: *420 | int
        			cmName:      string
        			items?: [...{
        				key:  string
        				path: string
        				mode: *511 | int
        			}]
        		}
        		if type == "secret" {
        			defaultMode: *420 | int
        			secretName:  string
        			items?: [...{
        				key:  string
        				path: string
        				mode: *511 | int
        			}]
        		}
        		if type == "emptyDir" {
        			medium: *"" | "Memory"
        		}
        	}]

        	// +usage=Instructions for assessing whether the container is alive.
        	livenessProbe?: #HealthProbe

        	// +usage=Instructions for assessing whether the container is in a suitable state to serve traffic.
        	readinessProbe?: #HealthProbe

        	// +usage=Specify the hostAliases to add
        	hostAliases?: [...{
        		ip: string
        		hostnames: [...string]
        	}]
        }

        #HealthProbe: {

        	// +usage=Instructions for assessing container health by executing a command. Either this attribute or the httpGet attribute or the tcpSocket attribute MUST be specified. This attribute is mutually exclusive with both the httpGet attribute and the tcpSocket attribute.
        	exec?: {
        		// +usage=A command to be executed inside the container to assess its health. Each space delimited token of the command is a separate array element. Commands exiting 0 are considered to be successful probes, whilst all other exit codes are considered failures.
        		command: [...string]
        	}

        	// +usage=Instructions for assessing container health by executing an HTTP GET request. Either this attribute or the exec attribute or the tcpSocket attribute MUST be specified. This attribute is mutually exclusive with both the exec attribute and the tcpSocket attribute.
        	httpGet?: {
        		// +usage=The endpoint, relative to the port, to which the HTTP GET request should be directed.
        		path: string
        		// +usage=The TCP socket within the container to which the HTTP GET request should be directed.
        		port:    int
        		host?:   string
        		scheme?: *"HTTP" | string
        		httpHeaders?: [...{
        			name:  string
        			value: string
        		}]
        	}

        	// +usage=Instructions for assessing container health by probing a TCP socket. Either this attribute or the exec attribute or the httpGet attribute MUST be specified. This attribute is mutually exclusive with both the exec attribute and the httpGet attribute.
        	tcpSocket?: {
        		// +usage=The TCP socket within the container that should be probed to assess container health.
        		port: int
        	}

        	// +usage=Number of seconds after the container is started before the first probe is initiated.
        	initialDelaySeconds: *0 | int

        	// +usage=How often, in seconds, to execute the probe.
        	periodSeconds: *10 | int

        	// +usage=Number of seconds after which the probe times out.
        	timeoutSeconds: *1 | int

        	// +usage=Minimum consecutive successes for the probe to be considered successful after having failed.
        	successThreshold: *1 | int

        	// +usage=Number of consecutive failures required to determine the container is not alive (liveness probe) or not ready (readiness probe).
        	failureThreshold: *3 | int
        }
  status:
    customStatus: |-
      ready: {
      	replicas: *0 | int
      } & {
      	if context.output.status.numberReady != _|_ {
      		replicas: context.output.status.numberReady
      	}
      }
      desired: {
      	replicas: *0 | int
      } & {
      	if context.output.status.desiredNumberScheduled != _|_ {
      		replicas: context.output.status.desiredNumberScheduled
      	}
      }
      message: "Ready:\(ready.replicas)/\(desired.replicas)"
    healthPolicy: |-
      ready: {
      	replicas: *0 | int
      } & {
      	if context.output.status.numberReady != _|_ {
      		replicas: context.output.status.numberReady
      	}
      }
      desired: {
      	replicas: *0 | int
      } & {
      	if context.output.status.desiredNumberScheduled != _|_ {
      		replicas: context.output.status.desiredNumberScheduled
      	}
      }
      current: {
      	replicas: *0 | int
      } & {
      	if context.output.status.currentNumberScheduled != _|_ {
      		replicas: context.output.status.currentNumberScheduled
      	}
      }
      updated: {
      	replicas: *0 | int
      } & {
      	if context.output.status.updatedNumberScheduled != _|_ {
      		replicas: context.output.status.updatedNumberScheduled
      	}
      }
      generation: {
      	metadata: context.output.metadata.generation
      	observed: *0 | int
      } & {
      	if context.output.status.observedGeneration != _|_ {
      		observed: context.output.status.observedGeneration
      	}
      }
      isHealth: (desired.replicas == ready.replicas) && (desired.replicas == updated.replicas) && (desired.replicas == current.replicas) && (generation.observed == generation.metadata || generation.observed > generation.metadata)
  workload:
    definition:
      apiVersion: apps/v1
      kind: DaemonSet
    type: daemonsets.apps
---
# Source: vela-core/templates/defwithtemplate/k8s-objects.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/k8s-objects.cue
apiVersion: core.oam.dev/v1beta1
kind: ComponentDefinition
metadata:
  annotations:
    definition.oam.dev/description: K8s-objects allow users to specify raw K8s objects in properties
  name: k8s-objects
  namespace: default
spec:
  schematic:
    cue:
      template: |
        output: parameter.objects[0]

        outputs: {
        	for i, v in parameter.objects {
        		if i > 0 {
        			"objects-\(i)": v
        		}
        	}
        }
        parameter: objects: [...{}]
  workload:
    type: autodetects.core.oam.dev
---
# Source: vela-core/templates/defwithtemplate/raw.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/deprecated/raw.cue
apiVersion: core.oam.dev/v1beta1
kind: ComponentDefinition
metadata:
  annotations:
    definition.oam.dev/description: Raw allow users to specify raw K8s object in properties. This definition is DEPRECATED, please use 'k8s-objects' instead.
  labels:
    custom.definition.oam.dev/deprecated: "true"
  name: raw
  namespace: default
spec:
  schematic:
    cue:
      template: |
        output: parameter
        parameter: {}
  workload:
    type: autodetects.core.oam.dev
---
# Source: vela-core/templates/defwithtemplate/ref-objects.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/ref-objects.cue
apiVersion: core.oam.dev/v1beta1
kind: ComponentDefinition
metadata:
  annotations:
    definition.oam.dev/description: Ref-objects allow users to specify ref objects to use. Notice that this component type have special handle logic.
  labels:
    custom.definition.oam.dev/ui-hidden: "true"
  name: ref-objects
  namespace: default
spec:
  schematic:
    cue:
      template: |
        #K8sObject: {
        	// +usage=The resource type for the Kubernetes objects
        	resource?: string
        	// +usage=The group name for the Kubernetes objects
        	group?: string
        	// +usage=If specified, fetch the Kubernetes objects with the name, exclusive to labelSelector
        	name?: string
        	// +usage=If specified, fetch the Kubernetes objects from the namespace. Otherwise, fetch from the application's namespace.
        	namespace?: string
        	// +usage=If specified, fetch the Kubernetes objects from the cluster. Otherwise, fetch from the local cluster.
        	cluster?: string
        	// +usage=If specified, fetch the Kubernetes objects according to the label selector, exclusive to name
        	labelSelector?: [string]: string
        	...
        }

        output: {
        	if len(parameter.objects) > 0 {
        		parameter.objects[0]
        	}
        	...
        }

        outputs: {
        	for i, v in parameter.objects {
        		if i > 0 {
        			"objects-\(i)": v
        		}
        	}
        }
        parameter: {
        	// +usage=If specified, application will fetch native Kubernetes objects according to the object description
        	objects?: [...#K8sObject]
        	// +usage=If specified, the objects in the urls will be loaded.
        	urls?: [...string]
        }
  status:
    customStatus: |-
      if context.output.apiVersion == "apps/v1" && context.output.kind == "Deployment" {
      	ready: {
      		readyReplicas: *0 | int
      	} & {
      		if context.output.status.readyReplicas != _|_ {
      			readyReplicas: context.output.status.readyReplicas
      		}
      	}
      	message: "Ready:\(ready.readyReplicas)/\(context.output.spec.replicas)"
      }
      if context.output.apiVersion != "apps/v1" || context.output.kind != "Deployment" {
      	message: ""
      }
    healthPolicy: |-
      if context.output.apiVersion == "apps/v1" && context.output.kind == "Deployment" {
      	ready: {
      		updatedReplicas:    *0 | int
      		readyReplicas:      *0 | int
      		replicas:           *0 | int
      		observedGeneration: *0 | int
      	} & {
      		if context.output.status.updatedReplicas != _|_ {
      			updatedReplicas: context.output.status.updatedReplicas
      		}
      		if context.output.status.readyReplicas != _|_ {
      			readyReplicas: context.output.status.readyReplicas
      		}
      		if context.output.status.replicas != _|_ {
      			replicas: context.output.status.replicas
      		}
      		if context.output.status.observedGeneration != _|_ {
      			observedGeneration: context.output.status.observedGeneration
      		}
      	}
      	isHealth: (context.output.spec.replicas == ready.readyReplicas) && (context.output.spec.replicas == ready.updatedReplicas) && (context.output.spec.replicas == ready.replicas) && (ready.observedGeneration == context.output.metadata.generation || ready.observedGeneration > context.output.metadata.generation)
      }
      if context.output.apiVersion != "apps/v1" || context.output.kind != "Deployment" {
      	isHealth: true
      }
  workload:
    type: autodetects.core.oam.dev
---
# Source: vela-core/templates/defwithtemplate/task.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/task.cue
apiVersion: core.oam.dev/v1beta1
kind: ComponentDefinition
metadata:
  annotations:
    definition.oam.dev/description: Describes jobs that run code or a script to completion.
  name: task
  namespace: default
spec:
  schematic:
    cue:
      template: |
        output: {
        	apiVersion: "batch/v1"
        	kind:       "Job"
        	spec: {
        		parallelism: parameter.count
        		completions: parameter.count
        		template: {
        			metadata: {
        				labels: {
        					if parameter.labels != _|_ {
        						parameter.labels
        					}
        					"app.oam.dev/name":      context.appName
        					"app.oam.dev/component": context.name
        				}
        				if parameter.annotations != _|_ {
        					annotations: parameter.annotations
        				}
        			}
        			spec: {
        				restartPolicy: parameter.restart
        				containers: [{
        					name:  context.name
        					image: parameter.image

        					if parameter["imagePullPolicy"] != _|_ {
        						imagePullPolicy: parameter.imagePullPolicy
        					}

        					if parameter["cmd"] != _|_ {
        						command: parameter.cmd
        					}

        					if parameter["env"] != _|_ {
        						env: parameter.env
        					}

        					if parameter["cpu"] != _|_ {
        						resources: {
        							limits: cpu:   parameter.cpu
        							requests: cpu: parameter.cpu
        						}
        					}

        					if parameter["memory"] != _|_ {
        						resources: {
        							limits: memory:   parameter.memory
        							requests: memory: parameter.memory
        						}
        					}

        					if parameter["volumes"] != _|_ {
        						volumeMounts: [ for v in parameter.volumes {
        							{
        								mountPath: v.mountPath
        								name:      v.name
        							}}]
        					}
        				}]

        				if parameter["volumes"] != _|_ {
        					volumes: [ for v in parameter.volumes {
        						{
        							name: v.name
        							if v.type == "pvc" {
        								persistentVolumeClaim: claimName: v.claimName
        							}
        							if v.type == "configMap" {
        								configMap: {
        									defaultMode: v.defaultMode
        									name:        v.cmName
        									if v.items != _|_ {
        										items: v.items
        									}
        								}
        							}
        							if v.type == "secret" {
        								secret: {
        									defaultMode: v.defaultMode
        									secretName:  v.secretName
        									if v.items != _|_ {
        										items: v.items
        									}
        								}
        							}
        							if v.type == "emptyDir" {
        								emptyDir: medium: v.medium
        							}
        						}}]
        				}

        				if parameter["imagePullSecrets"] != _|_ {
        					imagePullSecrets: [ for v in parameter.imagePullSecrets {
        						name: v
        					},
        					]
        				}

        			}
        		}
        	}
        }

        parameter: {
        	// +usage=Specify the labels in the workload
        	labels?: [string]: string

        	// +usage=Specify the annotations in the workload
        	annotations?: [string]: string

        	// +usage=Specify number of tasks to run in parallel
        	// +short=c
        	count: *1 | int

        	// +usage=Which image would you like to use for your service
        	// +short=i
        	image: string

        	// +usage=Specify image pull policy for your service
        	imagePullPolicy?: "Always" | "Never" | "IfNotPresent"

        	// +usage=Specify image pull secrets for your service
        	imagePullSecrets?: [...string]

        	// +usage=Define the job restart policy, the value can only be Never or OnFailure. By default, it's Never.
        	restart: *"Never" | string

        	// +usage=Commands to run in the container
        	cmd?: [...string]

        	// +usage=Define arguments by using environment variables
        	env?: [...{
        		// +usage=Environment variable name
        		name: string
        		// +usage=The value of the environment variable
        		value?: string
        		// +usage=Specifies a source the value of this var should come from
        		valueFrom?: {
        			// +usage=Selects a key of a secret in the pod's namespace
        			secretKeyRef?: {
        				// +usage=The name of the secret in the pod's namespace to select from
        				name: string
        				// +usage=The key of the secret to select from. Must be a valid secret key
        				key: string
        			}
        			// +usage=Selects a key of a config map in the pod's namespace
        			configMapKeyRef?: {
        				// +usage=The name of the config map in the pod's namespace to select from
        				name: string
        				// +usage=The key of the config map to select from. Must be a valid secret key
        				key: string
        			}
        		}
        	}]

        	// +usage=Number of CPU units for the service, like `0.5` (0.5 CPU core), `1` (1 CPU core)
        	cpu?: string

        	// +usage=Specifies the attributes of the memory resource required for the container.
        	memory?: string

        	// +usage=Declare volumes and volumeMounts
        	volumes?: [...{
        		name:      string
        		mountPath: string
        		// +usage=Specify volume type, options: "pvc","configMap","secret","emptyDir", default to emptyDir
        		type: *"emptyDir" | "pvc" | "configMap" | "secret"
        		if type == "pvc" {
        			claimName: string
        		}
        		if type == "configMap" {
        			defaultMode: *420 | int
        			cmName:      string
        			items?: [...{
        				key:  string
        				path: string
        				mode: *511 | int
        			}]
        		}
        		if type == "secret" {
        			defaultMode: *420 | int
        			secretName:  string
        			items?: [...{
        				key:  string
        				path: string
        				mode: *511 | int
        			}]
        		}
        		if type == "emptyDir" {
        			medium: *"" | "Memory"
        		}
        	}]

        	// +usage=Instructions for assessing whether the container is alive.
        	livenessProbe?: #HealthProbe

        	// +usage=Instructions for assessing whether the container is in a suitable state to serve traffic.
        	readinessProbe?: #HealthProbe
        }

        #HealthProbe: {

        	// +usage=Instructions for assessing container health by executing a command. Either this attribute or the httpGet attribute or the tcpSocket attribute MUST be specified. This attribute is mutually exclusive with both the httpGet attribute and the tcpSocket attribute.
        	exec?: {
        		// +usage=A command to be executed inside the container to assess its health. Each space delimited token of the command is a separate array element. Commands exiting 0 are considered to be successful probes, whilst all other exit codes are considered failures.
        		command: [...string]
        	}

        	// +usage=Instructions for assessing container health by executing an HTTP GET request. Either this attribute or the exec attribute or the tcpSocket attribute MUST be specified. This attribute is mutually exclusive with both the exec attribute and the tcpSocket attribute.
        	httpGet?: {
        		// +usage=The endpoint, relative to the port, to which the HTTP GET request should be directed.
        		path: string
        		// +usage=The TCP socket within the container to which the HTTP GET request should be directed.
        		port: int
        		httpHeaders?: [...{
        			name:  string
        			value: string
        		}]
        	}

        	// +usage=Instructions for assessing container health by probing a TCP socket. Either this attribute or the exec attribute or the httpGet attribute MUST be specified. This attribute is mutually exclusive with both the exec attribute and the httpGet attribute.
        	tcpSocket?: {
        		// +usage=The TCP socket within the container that should be probed to assess container health.
        		port: int
        	}

        	// +usage=Number of seconds after the container is started before the first probe is initiated.
        	initialDelaySeconds: *0 | int

        	// +usage=How often, in seconds, to execute the probe.
        	periodSeconds: *10 | int

        	// +usage=Number of seconds after which the probe times out.
        	timeoutSeconds: *1 | int

        	// +usage=Minimum consecutive successes for the probe to be considered successful after having failed.
        	successThreshold: *1 | int

        	// +usage=Number of consecutive failures required to determine the container is not alive (liveness probe) or not ready (readiness probe).
        	failureThreshold: *3 | int
        }
  status:
    customStatus: |-
      status: {
      	active:    *0 | int
      	failed:    *0 | int
      	succeeded: *0 | int
      } & {
      	if context.output.status.active != _|_ {
      		active: context.output.status.active
      	}
      	if context.output.status.failed != _|_ {
      		failed: context.output.status.failed
      	}
      	if context.output.status.succeeded != _|_ {
      		succeeded: context.output.status.succeeded
      	}
      }
      message: "Active/Failed/Succeeded:\(status.active)/\(status.failed)/\(status.succeeded)"
    healthPolicy: |-
      succeeded: *0 | int
      if context.output.status.succeeded != _|_ {
      	succeeded: context.output.status.succeeded
      }
      isHealth: succeeded == context.output.spec.parallelism
  workload:
    definition:
      apiVersion: batch/v1
      kind: Job
    type: jobs.batch
---
# Source: vela-core/templates/defwithtemplate/webservice.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/webservice.cue
apiVersion: core.oam.dev/v1beta1
kind: ComponentDefinition
metadata:
  annotations:
    definition.oam.dev/description: Describes long-running, scalable, containerized services that have a stable network endpoint to receive external network traffic from customers.
  name: webservice
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"strconv"
        	"strings"
        )

        mountsArray: [
        	if parameter.volumeMounts != _|_ && parameter.volumeMounts.pvc != _|_ for v in parameter.volumeMounts.pvc {
        		{
        			mountPath: v.mountPath
        			if v.subPath != _|_ {
        				subPath: v.subPath
        			}
        			name: v.name
        		}
        	},

        	if parameter.volumeMounts != _|_ && parameter.volumeMounts.configMap != _|_ for v in parameter.volumeMounts.configMap {
        		{
        			mountPath: v.mountPath
        			if v.subPath != _|_ {
        				subPath: v.subPath
        			}
        			name: v.name
        		}
        	},

        	if parameter.volumeMounts != _|_ && parameter.volumeMounts.secret != _|_ for v in parameter.volumeMounts.secret {
        		{
        			mountPath: v.mountPath
        			if v.subPath != _|_ {
        				subPath: v.subPath
        			}
        			name: v.name
        		}
        	},

        	if parameter.volumeMounts != _|_ && parameter.volumeMounts.emptyDir != _|_ for v in parameter.volumeMounts.emptyDir {
        		{
        			mountPath: v.mountPath
        			if v.subPath != _|_ {
        				subPath: v.subPath
        			}
        			name: v.name
        		}
        	},

        	if parameter.volumeMounts != _|_ && parameter.volumeMounts.hostPath != _|_ for v in parameter.volumeMounts.hostPath {
        		{
        			mountPath: v.mountPath
        			if v.subPath != _|_ {
        				subPath: v.subPath
        			}
        			name: v.name
        		}
        	},
        ]

        volumesList: [
        	if parameter.volumeMounts != _|_ && parameter.volumeMounts.pvc != _|_ for v in parameter.volumeMounts.pvc {
        		{
        			name: v.name
        			persistentVolumeClaim: claimName: v.claimName
        		}
        	},

        	if parameter.volumeMounts != _|_ && parameter.volumeMounts.configMap != _|_ for v in parameter.volumeMounts.configMap {
        		{
        			name: v.name
        			configMap: {
        				defaultMode: v.defaultMode
        				name:        v.cmName
        				if v.items != _|_ {
        					items: v.items
        				}
        			}
        		}
        	},

        	if parameter.volumeMounts != _|_ && parameter.volumeMounts.secret != _|_ for v in parameter.volumeMounts.secret {
        		{
        			name: v.name
        			secret: {
        				defaultMode: v.defaultMode
        				secretName:  v.secretName
        				if v.items != _|_ {
        					items: v.items
        				}
        			}
        		}
        	},

        	if parameter.volumeMounts != _|_ && parameter.volumeMounts.emptyDir != _|_ for v in parameter.volumeMounts.emptyDir {
        		{
        			name: v.name
        			emptyDir: medium: v.medium
        		}
        	},

        	if parameter.volumeMounts != _|_ && parameter.volumeMounts.hostPath != _|_ for v in parameter.volumeMounts.hostPath {
        		{
        			name: v.name
        			hostPath: path: v.path
        		}
        	},
        ]

        deDupVolumesArray: [
        	for val in [
        		for i, vi in volumesList {
        			for j, vj in volumesList if j < i && vi.name == vj.name {
        				_ignore: true
        			}
        			vi
        		},
        	] if val._ignore == _|_ {
        		val
        	},
        ]

        output: {
        	apiVersion: "apps/v1"
        	kind:       "Deployment"
        	spec: {
        		selector: matchLabels: "app.oam.dev/component": context.name

        		template: {
        			metadata: {
        				labels: {
        					if parameter.labels != _|_ {
        						parameter.labels
        					}
        					if parameter.addRevisionLabel {
        						"app.oam.dev/revision": context.revision
        					}
        					"app.oam.dev/name":      context.appName
        					"app.oam.dev/component": context.name
        				}
        				if parameter.annotations != _|_ {
        					annotations: parameter.annotations
        				}
        			}

        			spec: {
        				containers: [{
        					name:  context.name
        					image: parameter.image
        					if parameter["port"] != _|_ && parameter["ports"] == _|_ {
        						ports: [{
        							containerPort: parameter.port
        						}]
        					}
        					if parameter["ports"] != _|_ {
        						ports: [ for v in parameter.ports {
        							{
        								containerPort: {
        									if v.containerPort != _|_ {v.containerPort}
        									if v.containerPort == _|_ {v.port}
        								}
        								protocol: v.protocol
        								if v.name != _|_ {
        									name: v.name
        								}
        								if v.name == _|_ {
        									_name: {
        										if v.containerPort != _|_ {"port-" + strconv.FormatInt(v.containerPort, 10)}
        										if v.containerPort == _|_ {"port-" + strconv.FormatInt(v.port, 10)}
        									}
        									name: *_name | string
        									if v.protocol != "TCP" {
        										name: _name + "-" + strings.ToLower(v.protocol)
        									}
        								}
        							}}]
        					}

        					if parameter["imagePullPolicy"] != _|_ {
        						imagePullPolicy: parameter.imagePullPolicy
        					}

        					if parameter["cmd"] != _|_ {
        						command: parameter.cmd
        					}

        					if parameter["args"] != _|_ {
        						args: parameter.args
        					}

        					if parameter["env"] != _|_ {
        						env: parameter.env
        					}

        					if context["config"] != _|_ {
        						env: context.config
        					}

        					if parameter["cpu"] != _|_ {
        						resources: {
        							limits: cpu:   parameter.cpu
        							requests: cpu: parameter.cpu
        						}
        					}

        					if parameter["memory"] != _|_ {
        						resources: {
        							limits: memory:   parameter.memory
        							requests: memory: parameter.memory
        						}
        					}

        					if parameter["volumes"] != _|_ && parameter["volumeMounts"] == _|_ {
        						volumeMounts: [ for v in parameter.volumes {
        							{
        								mountPath: v.mountPath
        								name:      v.name
        							}}]
        					}

        					if parameter["volumeMounts"] != _|_ {
        						volumeMounts: mountsArray
        					}

        					if parameter["livenessProbe"] != _|_ {
        						livenessProbe: parameter.livenessProbe
        					}

        					if parameter["readinessProbe"] != _|_ {
        						readinessProbe: parameter.readinessProbe
        					}

        				}]

        				if parameter["hostAliases"] != _|_ {
        					// +patchKey=ip
        					hostAliases: parameter.hostAliases
        				}

        				if parameter["imagePullSecrets"] != _|_ {
        					imagePullSecrets: [ for v in parameter.imagePullSecrets {
        						name: v
        					},
        					]
        				}

        				if parameter["volumes"] != _|_ && parameter["volumeMounts"] == _|_ {
        					volumes: [ for v in parameter.volumes {
        						{
        							name: v.name
        							if v.type == "pvc" {
        								persistentVolumeClaim: claimName: v.claimName
        							}
        							if v.type == "configMap" {
        								configMap: {
        									defaultMode: v.defaultMode
        									name:        v.cmName
        									if v.items != _|_ {
        										items: v.items
        									}
        								}
        							}
        							if v.type == "secret" {
        								secret: {
        									defaultMode: v.defaultMode
        									secretName:  v.secretName
        									if v.items != _|_ {
        										items: v.items
        									}
        								}
        							}
        							if v.type == "emptyDir" {
        								emptyDir: medium: v.medium
        							}
        						}
        					}]
        				}

        				if parameter["volumeMounts"] != _|_ {
        					volumes: deDupVolumesArray
        				}
        			}
        		}
        	}
        }

        exposePorts: [
        	if parameter.ports != _|_ for v in parameter.ports if v.expose == true {
        		port: v.port
        		if v.containerPort != _|_ {targetPort: v.containerPort}
        		if v.containerPort == _|_ {targetPort: v.port}
        		if v.name != _|_ {name: v.name}
        		if v.name == _|_ {
        			_name: {
        				if v.containerPort != _|_ {
        					"port-" + strconv.FormatInt(v.containerPort, 10)
        				}
        				if v.containerPort == _|_ {
        					"port-" + strconv.FormatInt(v.port, 10)
        				}
        			}
        			name: *_name | string
        			if v.protocol != "TCP" {
        				name: _name + "-" + strings.ToLower(v.protocol)
        			}
        		}
        		if v.nodePort != _|_ && parameter.exposeType == "NodePort" {
        			nodePort: v.nodePort
        		}
        		if v.protocol != _|_ {
        			protocol: v.protocol
        		}
        	},
        ]

        outputs: {
        	if len(exposePorts) != 0 {
        		webserviceExpose: {
        			apiVersion: "v1"
        			kind:       "Service"
        			metadata: name: context.name
        			spec: {
        				selector: "app.oam.dev/component": context.name
        				ports: exposePorts
        				type:  parameter.exposeType
        			}
        		}
        	}
        }

        parameter: {
        	// +usage=Specify the labels in the workload
        	labels?: [string]: string

        	// +usage=Specify the annotations in the workload
        	annotations?: [string]: string

        	// +usage=Which image would you like to use for your service
        	// +short=i
        	image: string

        	// +usage=Specify image pull policy for your service
        	imagePullPolicy?: "Always" | "Never" | "IfNotPresent"

        	// +usage=Specify image pull secrets for your service
        	imagePullSecrets?: [...string]

        	// +ignore
        	// +usage=Deprecated field, please use ports instead
        	// +short=p
        	port?: int

        	// +usage=Which ports do you want customer traffic sent to, defaults to 80
        	ports?: [...{
        		// +usage=Number of port to expose on the pod's IP address
        		port: int
        		// +usage=Number of container port to connect to, defaults to port
        		containerPort?: int
        		// +usage=Name of the port
        		name?: string
        		// +usage=Protocol for port. Must be UDP, TCP, or SCTP
        		protocol: *"TCP" | "UDP" | "SCTP"
        		// +usage=Specify if the port should be exposed
        		expose: *false | bool
        		// +usage=exposed node port. Only Valid when exposeType is NodePort
        		nodePort?: int
        	}]

        	// +ignore
        	// +usage=Specify what kind of Service you want. options: "ClusterIP", "NodePort", "LoadBalancer"
        	exposeType: *"ClusterIP" | "NodePort" | "LoadBalancer"

        	// +ignore
        	// +usage=If addRevisionLabel is true, the revision label will be added to the underlying pods
        	addRevisionLabel: *false | bool

        	// +usage=Commands to run in the container
        	cmd?: [...string]

        	// +usage=Arguments to the entrypoint
        	args?: [...string]

        	// +usage=Define arguments by using environment variables
        	env?: [...{
        		// +usage=Environment variable name
        		name: string
        		// +usage=The value of the environment variable
        		value?: string
        		// +usage=Specifies a source the value of this var should come from
        		valueFrom?: {
        			// +usage=Selects a key of a secret in the pod's namespace
        			secretKeyRef?: {
        				// +usage=The name of the secret in the pod's namespace to select from
        				name: string
        				// +usage=The key of the secret to select from. Must be a valid secret key
        				key: string
        			}
        			// +usage=Selects a key of a config map in the pod's namespace
        			configMapKeyRef?: {
        				// +usage=The name of the config map in the pod's namespace to select from
        				name: string
        				// +usage=The key of the config map to select from. Must be a valid secret key
        				key: string
        			}
        		}
        	}]

        	// +usage=Number of CPU units for the service, like `0.5` (0.5 CPU core), `1` (1 CPU core)
        	cpu?: string

        	// +usage=Specifies the attributes of the memory resource required for the container.
        	memory?: string

        	volumeMounts?: {
        		// +usage=Mount PVC type volume
        		pvc?: [...{
        			name:      string
        			mountPath: string
        			subPath?:  string
        			// +usage=The name of the PVC
        			claimName: string
        		}]
        		// +usage=Mount ConfigMap type volume
        		configMap?: [...{
        			name:        string
        			mountPath:   string
        			subPath?:    string
        			defaultMode: *420 | int
        			cmName:      string
        			items?: [...{
        				key:  string
        				path: string
        				mode: *511 | int
        			}]
        		}]
        		// +usage=Mount Secret type volume
        		secret?: [...{
        			name:        string
        			mountPath:   string
        			subPath?:    string
        			defaultMode: *420 | int
        			secretName:  string
        			items?: [...{
        				key:  string
        				path: string
        				mode: *511 | int
        			}]
        		}]
        		// +usage=Mount EmptyDir type volume
        		emptyDir?: [...{
        			name:      string
        			mountPath: string
        			subPath?:  string
        			medium:    *"" | "Memory"
        		}]
        		// +usage=Mount HostPath type volume
        		hostPath?: [...{
        			name:      string
        			mountPath: string
        			subPath?:  string
        			path:      string
        		}]
        	}

        	// +usage=Deprecated field, use volumeMounts instead.
        	volumes?: [...{
        		name:      string
        		mountPath: string
        		// +usage=Specify volume type, options: "pvc","configMap","secret","emptyDir", default to emptyDir
        		type: *"emptyDir" | "pvc" | "configMap" | "secret"
        		if type == "pvc" {
        			claimName: string
        		}
        		if type == "configMap" {
        			defaultMode: *420 | int
        			cmName:      string
        			items?: [...{
        				key:  string
        				path: string
        				mode: *511 | int
        			}]
        		}
        		if type == "secret" {
        			defaultMode: *420 | int
        			secretName:  string
        			items?: [...{
        				key:  string
        				path: string
        				mode: *511 | int
        			}]
        		}
        		if type == "emptyDir" {
        			medium: *"" | "Memory"
        		}
        	}]

        	// +usage=Instructions for assessing whether the container is alive.
        	livenessProbe?: #HealthProbe

        	// +usage=Instructions for assessing whether the container is in a suitable state to serve traffic.
        	readinessProbe?: #HealthProbe

        	// +usage=Specify the hostAliases to add
        	hostAliases?: [...{
        		ip: string
        		hostnames: [...string]
        	}]
        }

        #HealthProbe: {

        	// +usage=Instructions for assessing container health by executing a command. Either this attribute or the httpGet attribute or the tcpSocket attribute MUST be specified. This attribute is mutually exclusive with both the httpGet attribute and the tcpSocket attribute.
        	exec?: {
        		// +usage=A command to be executed inside the container to assess its health. Each space delimited token of the command is a separate array element. Commands exiting 0 are considered to be successful probes, whilst all other exit codes are considered failures.
        		command: [...string]
        	}

        	// +usage=Instructions for assessing container health by executing an HTTP GET request. Either this attribute or the exec attribute or the tcpSocket attribute MUST be specified. This attribute is mutually exclusive with both the exec attribute and the tcpSocket attribute.
        	httpGet?: {
        		// +usage=The endpoint, relative to the port, to which the HTTP GET request should be directed.
        		path: string
        		// +usage=The TCP socket within the container to which the HTTP GET request should be directed.
        		port:    int
        		host?:   string
        		scheme?: *"HTTP" | string
        		httpHeaders?: [...{
        			name:  string
        			value: string
        		}]
        	}

        	// +usage=Instructions for assessing container health by probing a TCP socket. Either this attribute or the exec attribute or the httpGet attribute MUST be specified. This attribute is mutually exclusive with both the exec attribute and the httpGet attribute.
        	tcpSocket?: {
        		// +usage=The TCP socket within the container that should be probed to assess container health.
        		port: int
        	}

        	// +usage=Number of seconds after the container is started before the first probe is initiated.
        	initialDelaySeconds: *0 | int

        	// +usage=How often, in seconds, to execute the probe.
        	periodSeconds: *10 | int

        	// +usage=Number of seconds after which the probe times out.
        	timeoutSeconds: *1 | int

        	// +usage=Minimum consecutive successes for the probe to be considered successful after having failed.
        	successThreshold: *1 | int

        	// +usage=Number of consecutive failures required to determine the container is not alive (liveness probe) or not ready (readiness probe).
        	failureThreshold: *3 | int
        }
  status:
    customStatus: |-
      ready: {
      	readyReplicas: *0 | int
      } & {
      	if context.output.status.readyReplicas != _|_ {
      		readyReplicas: context.output.status.readyReplicas
      	}
      }
      message: "Ready:\(ready.readyReplicas)/\(context.output.spec.replicas)"
    healthPolicy: |-
      ready: {
      	updatedReplicas:    *0 | int
      	readyReplicas:      *0 | int
      	replicas:           *0 | int
      	observedGeneration: *0 | int
      } & {
      	if context.output.status.updatedReplicas != _|_ {
      		updatedReplicas: context.output.status.updatedReplicas
      	}
      	if context.output.status.readyReplicas != _|_ {
      		readyReplicas: context.output.status.readyReplicas
      	}
      	if context.output.status.replicas != _|_ {
      		replicas: context.output.status.replicas
      	}
      	if context.output.status.observedGeneration != _|_ {
      		observedGeneration: context.output.status.observedGeneration
      	}
      }
      _isHealth: (context.output.spec.replicas == ready.readyReplicas) && (context.output.spec.replicas == ready.updatedReplicas) && (context.output.spec.replicas == ready.replicas) && (ready.observedGeneration == context.output.metadata.generation || ready.observedGeneration > context.output.metadata.generation)
      isHealth: *_isHealth | bool
      if context.output.metadata.annotations != _|_ {
      	if context.output.metadata.annotations["app.oam.dev/disable-health-check"] != _|_ {
      		isHealth: true
      	}
      }
  workload:
    definition:
      apiVersion: apps/v1
      kind: Deployment
    type: deployments.apps
---
# Source: vela-core/templates/defwithtemplate/worker.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/worker.cue
apiVersion: core.oam.dev/v1beta1
kind: ComponentDefinition
metadata:
  annotations:
    definition.oam.dev/description: Describes long-running, scalable, containerized services that running at backend. They do NOT have network endpoint to receive external network traffic.
  labels:
    custom.definition.oam.dev/ui-hidden: "true"
  name: worker
  namespace: default
spec:
  schematic:
    cue:
      template: |
        mountsArray: [
        	if parameter.volumeMounts != _|_ && parameter.volumeMounts.pvc != _|_ for v in parameter.volumeMounts.pvc {
        		{
        			mountPath: v.mountPath
        			if v.subPath != _|_ {
        				subPath: v.subPath
        			}
        			name: v.name
        		}
        	},

        	if parameter.volumeMounts != _|_ && parameter.volumeMounts.configMap != _|_ for v in parameter.volumeMounts.configMap {
        		{
        			mountPath: v.mountPath
        			if v.subPath != _|_ {
        				subPath: v.subPath
        			}
        			name: v.name
        		}
        	},

        	if parameter.volumeMounts != _|_ && parameter.volumeMounts.secret != _|_ for v in parameter.volumeMounts.secret {
        		{
        			mountPath: v.mountPath
        			if v.subPath != _|_ {
        				subPath: v.subPath
        			}
        			name: v.name
        		}
        	},

        	if parameter.volumeMounts != _|_ && parameter.volumeMounts.emptyDir != _|_ for v in parameter.volumeMounts.emptyDir {
        		{
        			mountPath: v.mountPath
        			if v.subPath != _|_ {
        				subPath: v.subPath
        			}
        			name: v.name
        		}
        	},

        	if parameter.volumeMounts != _|_ && parameter.volumeMounts.hostPath != _|_ for v in parameter.volumeMounts.hostPath {
        		{
        			mountPath: v.mountPath
        			if v.subPath != _|_ {
        				subPath: v.subPath
        			}
        			name: v.name
        		}
        	},
        ]

        volumesList: [
        	if parameter.volumeMounts != _|_ && parameter.volumeMounts.pvc != _|_ for v in parameter.volumeMounts.pvc {
        		{
        			name: v.name
        			persistentVolumeClaim: claimName: v.claimName
        		}
        	},

        	if parameter.volumeMounts != _|_ && parameter.volumeMounts.configMap != _|_ for v in parameter.volumeMounts.configMap {
        		{
        			name: v.name
        			configMap: {
        				defaultMode: v.defaultMode
        				name:        v.cmName
        				if v.items != _|_ {
        					items: v.items
        				}
        			}
        		}
        	},

        	if parameter.volumeMounts != _|_ && parameter.volumeMounts.secret != _|_ for v in parameter.volumeMounts.secret {
        		{
        			name: v.name
        			secret: {
        				defaultMode: v.defaultMode
        				secretName:  v.secretName
        				if v.items != _|_ {
        					items: v.items
        				}
        			}
        		}
        	},

        	if parameter.volumeMounts != _|_ && parameter.volumeMounts.emptyDir != _|_ for v in parameter.volumeMounts.emptyDir {
        		{
        			name: v.name
        			emptyDir: medium: v.medium
        		}
        	},

        	if parameter.volumeMounts != _|_ && parameter.volumeMounts.hostPath != _|_ for v in parameter.volumeMounts.hostPath {
        		{
        			name: v.name
        			hostPath: path: v.path
        		}
        	},
        ]

        deDupVolumesArray: [
        	for val in [
        		for i, vi in volumesList {
        			for j, vj in volumesList if j < i && vi.name == vj.name {
        				_ignore: true
        			}
        			vi
        		},
        	] if val._ignore == _|_ {
        		val
        	},
        ]

        output: {
        	apiVersion: "apps/v1"
        	kind:       "Deployment"
        	spec: {
        		selector: matchLabels: "app.oam.dev/component": context.name

        		template: {
        			metadata: labels: {
        				"app.oam.dev/name":      context.appName
        				"app.oam.dev/component": context.name
        			}

        			spec: {
        				containers: [{
        					name:  context.name
        					image: parameter.image

        					if parameter["imagePullPolicy"] != _|_ {
        						imagePullPolicy: parameter.imagePullPolicy
        					}

        					if parameter["cmd"] != _|_ {
        						command: parameter.cmd
        					}

        					if parameter["env"] != _|_ {
        						env: parameter.env
        					}

        					if parameter["cpu"] != _|_ {
        						resources: {
        							limits: cpu:   parameter.cpu
        							requests: cpu: parameter.cpu
        						}
        					}

        					if parameter["memory"] != _|_ {
        						resources: {
        							limits: memory:   parameter.memory
        							requests: memory: parameter.memory
        						}
        					}

        					if parameter["volumes"] != _|_ && parameter["volumeMounts"] == _|_ {
        						volumeMounts: [ for v in parameter.volumes {
        							{
        								mountPath: v.mountPath
        								name:      v.name
        							}}]
        					}

        					if parameter["volumeMounts"] != _|_ {
        						volumeMounts: mountsArray
        					}

        					if parameter["livenessProbe"] != _|_ {
        						livenessProbe: parameter.livenessProbe
        					}

        					if parameter["readinessProbe"] != _|_ {
        						readinessProbe: parameter.readinessProbe
        					}

        				}]

        				if parameter["imagePullSecrets"] != _|_ {
        					imagePullSecrets: [ for v in parameter.imagePullSecrets {
        						name: v
        					},
        					]
        				}

        				if parameter["volumes"] != _|_ && parameter["volumeMounts"] == _|_ {
        					volumes: [ for v in parameter.volumes {
        						{
        							name: v.name
        							if v.type == "pvc" {
        								persistentVolumeClaim: claimName: v.claimName
        							}
        							if v.type == "configMap" {
        								configMap: {
        									defaultMode: v.defaultMode
        									name:        v.cmName
        									if v.items != _|_ {
        										items: v.items
        									}
        								}
        							}
        							if v.type == "secret" {
        								secret: {
        									defaultMode: v.defaultMode
        									secretName:  v.secretName
        									if v.items != _|_ {
        										items: v.items
        									}
        								}
        							}
        							if v.type == "emptyDir" {
        								emptyDir: medium: v.medium
        							}
        						}
        					}]
        				}
        				if parameter["volumeMounts"] != _|_ {
        					volumes: deDupVolumesArray
        				}
        			}
        		}
        	}
        }

        parameter: {
        	// +usage=Which image would you like to use for your service
        	// +short=i
        	image: string

        	// +usage=Specify image pull policy for your service
        	imagePullPolicy?: string

        	// +usage=Specify image pull secrets for your service
        	imagePullSecrets?: [...string]

        	// +usage=Commands to run in the container
        	cmd?: [...string]

        	// +usage=Define arguments by using environment variables
        	env?: [...{
        		// +usage=Environment variable name
        		name: string
        		// +usage=The value of the environment variable
        		value?: string
        		// +usage=Specifies a source the value of this var should come from
        		valueFrom?: {
        			// +usage=Selects a key of a secret in the pod's namespace
        			secretKeyRef?: {
        				// +usage=The name of the secret in the pod's namespace to select from
        				name: string
        				// +usage=The key of the secret to select from. Must be a valid secret key
        				key: string
        			}
        			// +usage=Selects a key of a config map in the pod's namespace
        			configMapKeyRef?: {
        				// +usage=The name of the config map in the pod's namespace to select from
        				name: string
        				// +usage=The key of the config map to select from. Must be a valid secret key
        				key: string
        			}
        		}
        	}]

        	// +usage=Number of CPU units for the service, like `0.5` (0.5 CPU core), `1` (1 CPU core)
        	cpu?: string

        	// +usage=Specifies the attributes of the memory resource required for the container.
        	memory?: string

        	volumeMounts?: {
        		// +usage=Mount PVC type volume
        		pvc?: [...{
        			name:      string
        			mountPath: string
        			// +usage=The name of the PVC
        			claimName: string
        		}]
        		// +usage=Mount ConfigMap type volume
        		configMap?: [...{
        			name:        string
        			mountPath:   string
        			defaultMode: *420 | int
        			cmName:      string
        			items?: [...{
        				key:  string
        				path: string
        				mode: *511 | int
        			}]
        		}]
        		// +usage=Mount Secret type volume
        		secret?: [...{
        			name:        string
        			mountPath:   string
        			defaultMode: *420 | int
        			secretName:  string
        			items?: [...{
        				key:  string
        				path: string
        				mode: *511 | int
        			}]
        		}]
        		// +usage=Mount EmptyDir type volume
        		emptyDir?: [...{
        			name:      string
        			mountPath: string
        			medium:    *"" | "Memory"
        		}]
        		// +usage=Mount HostPath type volume
        		hostPath?: [...{
        			name:      string
        			mountPath: string
        			path:      string
        		}]
        	}

        	// +usage=Deprecated field, use volumeMounts instead.
        	volumes?: [...{
        		name:      string
        		mountPath: string
        		// +usage=Specify volume type, options: "pvc","configMap","secret","emptyDir", default to emptyDir
        		type: *"emptyDir" | "pvc" | "configMap" | "secret"
        		if type == "pvc" {
        			claimName: string
        		}
        		if type == "configMap" {
        			defaultMode: *420 | int
        			cmName:      string
        			items?: [...{
        				key:  string
        				path: string
        				mode: *511 | int
        			}]
        		}
        		if type == "secret" {
        			defaultMode: *420 | int
        			secretName:  string
        			items?: [...{
        				key:  string
        				path: string
        				mode: *511 | int
        			}]
        		}
        		if type == "emptyDir" {
        			medium: *"" | "Memory"
        		}
        	}]

        	// +usage=Instructions for assessing whether the container is alive.
        	livenessProbe?: #HealthProbe

        	// +usage=Instructions for assessing whether the container is in a suitable state to serve traffic.
        	readinessProbe?: #HealthProbe
        }

        #HealthProbe: {

        	// +usage=Instructions for assessing container health by executing a command. Either this attribute or the httpGet attribute or the tcpSocket attribute MUST be specified. This attribute is mutually exclusive with both the httpGet attribute and the tcpSocket attribute.
        	exec?: {
        		// +usage=A command to be executed inside the container to assess its health. Each space delimited token of the command is a separate array element. Commands exiting 0 are considered to be successful probes, whilst all other exit codes are considered failures.
        		command: [...string]
        	}

        	// +usage=Instructions for assessing container health by executing an HTTP GET request. Either this attribute or the exec attribute or the tcpSocket attribute MUST be specified. This attribute is mutually exclusive with both the exec attribute and the tcpSocket attribute.
        	httpGet?: {
        		// +usage=The endpoint, relative to the port, to which the HTTP GET request should be directed.
        		path: string
        		// +usage=The TCP socket within the container to which the HTTP GET request should be directed.
        		port: int
        		httpHeaders?: [...{
        			name:  string
        			value: string
        		}]
        	}

        	// +usage=Instructions for assessing container health by probing a TCP socket. Either this attribute or the exec attribute or the httpGet attribute MUST be specified. This attribute is mutually exclusive with both the exec attribute and the httpGet attribute.
        	tcpSocket?: {
        		// +usage=The TCP socket within the container that should be probed to assess container health.
        		port: int
        	}

        	// +usage=Number of seconds after the container is started before the first probe is initiated.
        	initialDelaySeconds: *0 | int

        	// +usage=How often, in seconds, to execute the probe.
        	periodSeconds: *10 | int

        	// +usage=Number of seconds after which the probe times out.
        	timeoutSeconds: *1 | int

        	// +usage=Minimum consecutive successes for the probe to be considered successful after having failed.
        	successThreshold: *1 | int

        	// +usage=Number of consecutive failures required to determine the container is not alive (liveness probe) or not ready (readiness probe).
        	failureThreshold: *3 | int
        }
  status:
    customStatus: |-
      ready: {
      	readyReplicas: *0 | int
      } & {
      	if context.output.status.readyReplicas != _|_ {
      		readyReplicas: context.output.status.readyReplicas
      	}
      }
      message: "Ready:\(ready.readyReplicas)/\(context.output.spec.replicas)"
    healthPolicy: |-
      ready: {
      	updatedReplicas:    *0 | int
      	readyReplicas:      *0 | int
      	replicas:           *0 | int
      	observedGeneration: *0 | int
      } & {
      	if context.output.status.updatedReplicas != _|_ {
      		updatedReplicas: context.output.status.updatedReplicas
      	}
      	if context.output.status.readyReplicas != _|_ {
      		readyReplicas: context.output.status.readyReplicas
      	}
      	if context.output.status.replicas != _|_ {
      		replicas: context.output.status.replicas
      	}
      	if context.output.status.observedGeneration != _|_ {
      		observedGeneration: context.output.status.observedGeneration
      	}
      }
      isHealth: (context.output.spec.replicas == ready.readyReplicas) && (context.output.spec.replicas == ready.updatedReplicas) && (context.output.spec.replicas == ready.replicas) && (ready.observedGeneration == context.output.metadata.generation || ready.observedGeneration > context.output.metadata.generation)
  workload:
    definition:
      apiVersion: apps/v1
      kind: Deployment
    type: deployments.apps
---
# Source: vela-core/templates/admission-webhooks/mutatingWebhookConfiguration.yaml
apiVersion: admissionregistration.k8s.io/v1
kind: MutatingWebhookConfiguration
metadata:
  name: my-release-vela-core-admission
  namespace: default
webhooks:
  - clientConfig:
      caBundle: Cg==
      service:
        name: vela-core-webhook
        namespace: default
        path: /mutating-core-oam-dev-v1beta1-applications
    failurePolicy: Ignore
    name: mutating.core.oam.dev.v1beta1.applications
    admissionReviewVersions:
      - v1beta1
      - v1
    sideEffects: None
    rules:
      - apiGroups:
          - core.oam.dev
        apiVersions:
          - v1beta1
        operations:
          - CREATE
          - UPDATE
        resources:
          - applications
  - clientConfig:
      caBundle: Cg==
      service:
        name: vela-core-webhook
        namespace: default
        path: /mutating-core-oam-dev-v1beta1-componentdefinitions
    failurePolicy: Ignore
    name: mutating.core.oam-dev.v1beta1.componentdefinitions
    sideEffects: None
    admissionReviewVersions:
      - v1beta1
      - v1
    rules:
      - apiGroups:
          - core.oam.dev
        apiVersions:
          - v1beta1
        operations:
          - CREATE
          - UPDATE
        resources:
          - componentdefinitions
---
# Source: vela-core/templates/defwithtemplate/apply-once.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/apply-once.cue
apiVersion: core.oam.dev/v1beta1
kind: PolicyDefinition
metadata:
  annotations:
    definition.oam.dev/description: Allow configuration drift for applied resources, delivery the resource without continuously reconciliation.
  name: apply-once
  namespace: default
spec:
  schematic:
    cue:
      template: |
        #ApplyOnceStrategy: {
        	// +usage=When the strategy takes effect,e.g. onUpdate、onStateKeep
        	affect?: string
        	// +usage=Specify the path of the resource that allow configuration drift
        	path: [...string]
        }

        #ApplyOncePolicyRule: {
        	// +usage=Specify how to select the targets of the rule
        	selector?: #ResourcePolicyRuleSelector
        	// +usage=Specify the strategy for configuring the resource level configuration drift behaviour
        	strategy: #ApplyOnceStrategy
        }

        #ResourcePolicyRuleSelector: {
        	// +usage=Select resources by component names
        	componentNames?: [...string]
        	// +usage=Select resources by component types
        	componentTypes?: [...string]
        	// +usage=Select resources by oamTypes (COMPONENT or TRAIT)
        	oamTypes?: [...string]
        	// +usage=Select resources by trait types
        	traitTypes?: [...string]
        	// +usage=Select resources by resource types (like Deployment)
        	resourceTypes?: [...string]
        	// +usage=Select resources by their names
        	resourceNames?: [...string]
        }

        parameter: {
        	// +usage=Whether to enable apply-once for the whole application
        	enable: *false | bool
        	// +usage=Specify the rules for configuring apply-once policy in resource level
        	rules?: [...#ApplyOncePolicyRule]
        }
---
# Source: vela-core/templates/defwithtemplate/envbinding.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/deprecated/envbinding.cue
apiVersion: core.oam.dev/v1beta1
kind: PolicyDefinition
metadata:
  annotations:
    definition.oam.dev/description: Determining the destination where components should be deployed to, and support override configuration
  labels:
    custom.definition.oam.dev/deprecated: "true"
  name: envbinding
  namespace: default
spec:
  schematic:
    cue:
      template: |
        #PatchParams: {
        	// +usage=Specify the name of the patch component, if empty, all components will be merged
        	name?: string
        	// +usage=Specify the type of the patch component.
        	type?: string
        	properties?: {...}
        	traits?: [...{
        		type: string
        		properties?: {...}
        		// +usage=Specify if the trait should be remove, default false
        		disable: *false | bool
        	}]
        }

        parameter: envs: [...{
        	name: string
        	placement?: {
        		clusterSelector?: {
        			// +usage=Specify cluster name, defualt local
        			name: *"local" | string
        			labels?: [string]: string
        		}
        		namespaceSelector?: {
        			// +usage=Specify namespace name.
        			name?: string
        			labels?: [string]: string
        		}
        	}
        	selector?: components: [...string]
        	patch?: components: [...#PatchParams]
        }]
---
# Source: vela-core/templates/defwithtemplate/garbage-collect.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/garbage-collect.cue
apiVersion: core.oam.dev/v1beta1
kind: PolicyDefinition
metadata:
  annotations:
    definition.oam.dev/description: Configure the garbage collect behaviour for the application.
  name: garbage-collect
  namespace: default
spec:
  schematic:
    cue:
      template: |
        #GarbageCollectPolicyRule: {
        	// +usage=Specify how to select the targets of the rule
        	selector: #ResourcePolicyRuleSelector
        	// +usage=Specify the strategy for target resource to recycle
        	strategy: *"onAppUpdate" | "onAppDelete" | "never"
        	// +usage=Specify the deletion propagation strategy for target resource to delete
        	propagation?: "orphan" | "cascading"
        }

        #ResourcePolicyRuleSelector: {
        	// +usage=Select resources by component names
        	componentNames?: [...string]
        	// +usage=Select resources by component types
        	componentTypes?: [...string]
        	// +usage=Select resources by oamTypes (COMPONENT or TRAIT)
        	oamTypes?: [...string]
        	// +usage=Select resources by trait types
        	traitTypes?: [...string]
        	// +usage=Select resources by resource types (like Deployment)
        	resourceTypes?: [...string]
        	// +usage=Select resources by their names
        	resourceNames?: [...string]
        }

        parameter: {
        	// +usage=If set, it will override the default revision limit number and customize this number for the current application
        	applicationRevisionLimit?: int
        	// +usage=If is set, outdated versioned resourcetracker will not be recycled automatically, outdated resources will be kept until resourcetracker be deleted manually
        	keepLegacyResource: *false | bool
        	// +usage=If is set, continue to execute gc when the workflow fails, by default gc will be executed only after the workflow succeeds
        	continueOnFailure: *false | bool
        	// +usage=Specify the list of rules to control gc strategy at resource level, if one resource is controlled by multiple rules, first rule will be used
        	rules?: [...#GarbageCollectPolicyRule]
        }
---
# Source: vela-core/templates/defwithtemplate/override.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/override.cue
apiVersion: core.oam.dev/v1beta1
kind: PolicyDefinition
metadata:
  annotations:
    definition.oam.dev/description: Describe the configuration to override when deploying resources, it only works with specified `deploy` step in workflow.
  name: override
  namespace: default
spec:
  schematic:
    cue:
      template: |
        #PatchParams: {
        	// +usage=Specify the name of the patch component, if empty, all components will be merged
        	name?: string
        	// +usage=Specify the type of the patch component.
        	type?: string
        	// +usage=Specify the properties to override.
        	properties?: {...}
        	// +usage=Specify the traits to override.
        	traits?: [...{
        		// +usage=Specify the type of the trait to be patched.
        		type: string
        		// +usage=Specify the properties to override.
        		properties?: {...}
        		// +usage=Specify if the trait should be remove, default false
        		disable: *false | bool
        	}]
        }

        parameter: {
        	// +usage=Specify the overridden component configuration.
        	components: [...#PatchParams]
        	// +usage=Specify a list of component names to use, if empty, all components will be selected.
        	selector?: [...string]
        }
---
# Source: vela-core/templates/defwithtemplate/read-only.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/read-only.cue
apiVersion: core.oam.dev/v1beta1
kind: PolicyDefinition
metadata:
  annotations:
    definition.oam.dev/description: Configure the resources to be read-only in the application (no update / state-keep).
  name: read-only
  namespace: default
spec:
  schematic:
    cue:
      template: |
        #PolicyRule: {
        	// +usage=Specify how to select the targets of the rule
        	selector: #RuleSelector
        }

        #RuleSelector: {
        	// +usage=Select resources by component names
        	componentNames?: [...string]
        	// +usage=Select resources by component types
        	componentTypes?: [...string]
        	// +usage=Select resources by oamTypes (COMPONENT or TRAIT)
        	oamTypes?: [...string]
        	// +usage=Select resources by trait types
        	traitTypes?: [...string]
        	// +usage=Select resources by resource types (like Deployment)
        	resourceTypes?: [...string]
        	// +usage=Select resources by their names
        	resourceNames?: [...string]
        }

        parameter: {
        	// +usage=Specify the list of rules to control read only strategy at resource level.
        	// The selected resource will be read-only to the current application. If the target resource does
        	// not exist, error will be raised.
        	rules?: [...#PolicyRule]
        }
---
# Source: vela-core/templates/defwithtemplate/replication.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/replication.cue
apiVersion: core.oam.dev/v1beta1
kind: PolicyDefinition
metadata:
  annotations:
    definition.oam.dev/description: Describe the configuration to replicate components when deploying resources, it only works with specified `deploy` step in workflow.
  name: replication
  namespace: default
spec:
  schematic:
    cue:
      template: |
        parameter: {
        	// +usage=Spicify the keys of replication. Every key coresponds to a replication components
        	keys: [...string]
        	// +usage=Specify the components which will be replicated.
        	selector?: [...string]
        }
---
# Source: vela-core/templates/defwithtemplate/resource-update.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/resource-update.cue
apiVersion: core.oam.dev/v1beta1
kind: PolicyDefinition
metadata:
  annotations:
    definition.oam.dev/description: Configure the update strategy for selected resources.
  name: resource-update
  namespace: default
spec:
  schematic:
    cue:
      template: |
        #PolicyRule: {
        	// +usage=Specify how to select the targets of the rule
        	selector: #RuleSelector
        	// +usage=The update strategy for the target resources
        	strategy: #Strategy
        }

        #Strategy: {
        	// +usage=Specify the op for updating target resources
        	op: *"patch" | "replace"
        	// +usage=Specify which fields would trigger recreation when updated
        	recreateFields?: [...string]
        }

        #RuleSelector: {
        	// +usage=Select resources by component names
        	componentNames?: [...string]
        	// +usage=Select resources by component types
        	componentTypes?: [...string]
        	// +usage=Select resources by oamTypes (COMPONENT or TRAIT)
        	oamTypes?: [...string]
        	// +usage=Select resources by trait types
        	traitTypes?: [...string]
        	// +usage=Select resources by resource types (like Deployment)
        	resourceTypes?: [...string]
        	// +usage=Select resources by their names
        	resourceNames?: [...string]
        }

        parameter: {
        	// +usage=Specify the list of rules to control resource update strategy at resource level.
        	rules?: [...#PolicyRule]
        }
---
# Source: vela-core/templates/defwithtemplate/shared-resource.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/shared-resource.cue
apiVersion: core.oam.dev/v1beta1
kind: PolicyDefinition
metadata:
  annotations:
    definition.oam.dev/description: Configure the resources to be sharable across applications.
  name: shared-resource
  namespace: default
spec:
  schematic:
    cue:
      template: |
        #SharedResourcePolicyRule: {
        	// +usage=Specify how to select the targets of the rule
        	selector: #ResourcePolicyRuleSelector
        }

        #ResourcePolicyRuleSelector: {
        	// +usage=Select resources by component names
        	componentNames?: [...string]
        	// +usage=Select resources by component types
        	componentTypes?: [...string]
        	// +usage=Select resources by oamTypes (COMPONENT or TRAIT)
        	oamTypes?: [...string]
        	// +usage=Select resources by trait types
        	traitTypes?: [...string]
        	// +usage=Select resources by resource types (like Deployment)
        	resourceTypes?: [...string]
        	// +usage=Select resources by their names
        	resourceNames?: [...string]
        }

        parameter: {
        	// +usage=Specify the list of rules to control shared-resource strategy at resource level.
        	// The selected resource will be sharable across applications. (That means multiple applications
        	// can all read it without conflict, but only the first one can write it)
        	rules?: [...#SharedResourcePolicyRule]
        }
---
# Source: vela-core/templates/defwithtemplate/take-over.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/take-over.cue
apiVersion: core.oam.dev/v1beta1
kind: PolicyDefinition
metadata:
  annotations:
    definition.oam.dev/description: Configure the resources to be able to take over when it belongs to no application.
  name: take-over
  namespace: default
spec:
  schematic:
    cue:
      template: |
        #PolicyRule: {
        	// +usage=Specify how to select the targets of the rule
        	selector: #RuleSelector
        }

        #RuleSelector: {
        	// +usage=Select resources by component names
        	componentNames?: [...string]
        	// +usage=Select resources by component types
        	componentTypes?: [...string]
        	// +usage=Select resources by oamTypes (COMPONENT or TRAIT)
        	oamTypes?: [...string]
        	// +usage=Select resources by trait types
        	traitTypes?: [...string]
        	// +usage=Select resources by resource types (like Deployment)
        	resourceTypes?: [...string]
        	// +usage=Select resources by their names
        	resourceNames?: [...string]
        }

        parameter: {
        	// +usage=Specify the list of rules to control take over strategy at resource level.
        	// The selected resource will be able to be taken over by the current application when the resource belongs to no
        	// one.
        	rules?: [...#PolicyRule]
        }
---
# Source: vela-core/templates/defwithtemplate/topology.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/topology.cue
apiVersion: core.oam.dev/v1beta1
kind: PolicyDefinition
metadata:
  annotations:
    definition.oam.dev/description: Describe the destination where components should be deployed to.
  name: topology
  namespace: default
spec:
  schematic:
    cue:
      template: |
        parameter: {
        	// +usage=Specify the names of the clusters to select.
        	clusters?: [...string]
        	// +usage=Specify the label selector for clusters
        	clusterLabelSelector?: [string]: string
        	// +usage=Ignore empty cluster error
        	allowEmpty?: bool
        	// +usage=Deprecated: Use clusterLabelSelector instead.
        	clusterSelector?: [string]: string
        	// +usage=Specify the target namespace to deploy in the selected clusters, default inherit the original namespace.
        	namespace?: string
        }
---
# Source: vela-core/templates/defwithtemplate/affinity.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/affinity.cue
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Affinity specifies affinity and toleration K8s pod for your workload which follows the pod spec in path 'spec.template'.
  labels:
    custom.definition.oam.dev/ui-hidden: "true"
  name: affinity
  namespace: default
spec:
  appliesToWorkloads:
    - deployments.apps
    - statefulsets.apps
    - daemonsets.apps
    - jobs.batch
  podDisruptive: true
  schematic:
    cue:
      template: |
        patch: spec: template: spec: {
        	if parameter.podAffinity != _|_ {
        		affinity: podAffinity: {
        			if parameter.podAffinity.required != _|_ {
        				requiredDuringSchedulingIgnoredDuringExecution: [
        					for k in parameter.podAffinity.required {
        						if k.labelSelector != _|_ {
        							labelSelector: k.labelSelector
        						}
        						if k.namespace != _|_ {
        							namespace: k.namespace
        						}
        						topologyKey: k.topologyKey
        						if k.namespaceSelector != _|_ {
        							namespaceSelector: k.namespaceSelector
        						}
        					}]
        			}
        			if parameter.podAffinity.preferred != _|_ {
        				preferredDuringSchedulingIgnoredDuringExecution: [
        					for k in parameter.podAffinity.preferred {
        						weight:          k.weight
        						podAffinityTerm: k.podAffinityTerm
        					}]
        			}
        		}
        	}
        	if parameter.podAntiAffinity != _|_ {
        		affinity: podAntiAffinity: {
        			if parameter.podAntiAffinity.required != _|_ {
        				requiredDuringSchedulingIgnoredDuringExecution: [
        					for k in parameter.podAntiAffinity.required {
        						if k.labelSelector != _|_ {
        							labelSelector: k.labelSelector
        						}
        						if k.namespace != _|_ {
        							namespace: k.namespace
        						}
        						topologyKey: k.topologyKey
        						if k.namespaceSelector != _|_ {
        							namespaceSelector: k.namespaceSelector
        						}
        					}]
        			}
        			if parameter.podAntiAffinity.preferred != _|_ {
        				preferredDuringSchedulingIgnoredDuringExecution: [
        					for k in parameter.podAntiAffinity.preferred {
        						weight:          k.weight
        						podAffinityTerm: k.podAffinityTerm
        					}]
        			}
        		}
        	}
        	if parameter.nodeAffinity != _|_ {
        		affinity: nodeAffinity: {
        			if parameter.nodeAffinity.required != _|_ {
        				requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: [
        					for k in parameter.nodeAffinity.required.nodeSelectorTerms {
        						if k.matchExpressions != _|_ {
        							matchExpressions: k.matchExpressions
        						}
        						if k.matchFields != _|_ {
        							matchFields: k.matchFields
        						}
        					}]
        			}
        			if parameter.nodeAffinity.preferred != _|_ {
        				preferredDuringSchedulingIgnoredDuringExecution: [
        					for k in parameter.nodeAffinity.preferred {
        						weight:     k.weight
        						preference: k.preference
        					}]
        			}
        		}
        	}
        	if parameter.tolerations != _|_ {
        		tolerations: [
        			for k in parameter.tolerations {
        				if k.key != _|_ {
        					key: k.key
        				}
        				if k.effect != _|_ {
        					effect: k.effect
        				}
        				if k.value != _|_ {
        					value: k.value
        				}
        				operator: k.operator
        				if k.tolerationSeconds != _|_ {
        					tolerationSeconds: k.tolerationSeconds
        				}
        			}]
        	}
        }

        #labelSelector: {
        	matchLabels?: [string]: string
        	matchExpressions?: [...{
        		key:      string
        		operator: *"In" | "NotIn" | "Exists" | "DoesNotExist"
        		values?: [...string]
        	}]
        }

        #podAffinityTerm: {
        	labelSelector?: #labelSelector
        	namespaces?: [...string]
        	topologyKey:        string
        	namespaceSelector?: #labelSelector
        }

        #nodeSelecor: {
        	key:      string
        	operator: *"In" | "NotIn" | "Exists" | "DoesNotExist" | "Gt" | "Lt"
        	values?: [...string]
        }

        #nodeSelectorTerm: {
        	matchExpressions?: [...#nodeSelecor]
        	matchFields?: [...#nodeSelecor]
        }

        parameter: {
        	// +usage=Specify the pod affinity scheduling rules
        	podAffinity?: {
        		// +usage=Specify the required during scheduling ignored during execution
        		required?: [...#podAffinityTerm]
        		// +usage=Specify the preferred during scheduling ignored during execution
        		preferred?: [...{
        			// +usage=Specify weight associated with matching the corresponding podAffinityTerm
        			weight: int & >=1 & <=100
        			// +usage=Specify a set of pods
        			podAffinityTerm: #podAffinityTerm
        		}]
        	}
        	// +usage=Specify the pod anti-affinity scheduling rules
        	podAntiAffinity?: {
        		// +usage=Specify the required during scheduling ignored during execution
        		required?: [...#podAffinityTerm]
        		// +usage=Specify the preferred during scheduling ignored during execution
        		preferred?: [...{
        			// +usage=Specify weight associated with matching the corresponding podAffinityTerm
        			weight: int & >=1 & <=100
        			// +usage=Specify a set of pods
        			podAffinityTerm: #podAffinityTerm
        		}]
        	}
        	// +usage=Specify the node affinity scheduling rules for the pod
        	nodeAffinity?: {
        		// +usage=Specify the required during scheduling ignored during execution
        		required?: {
        			// +usage=Specify a list of node selector
        			nodeSelectorTerms: [...#nodeSelectorTerm]
        		}
        		// +usage=Specify the preferred during scheduling ignored during execution
        		preferred?: [...{
        			// +usage=Specify weight associated with matching the corresponding nodeSelector
        			weight: int & >=1 & <=100
        			// +usage=Specify a node selector
        			preference: #nodeSelectorTerm
        		}]
        	}
        	// +usage=Specify tolerant taint
        	tolerations?: [...{
        		key?:     string
        		operator: *"Equal" | "Exists"
        		value?:   string
        		effect?:  "NoSchedule" | "PreferNoSchedule" | "NoExecute"
        		// +usage=Specify the period of time the toleration
        		tolerationSeconds?: int
        	}]
        }
---
# Source: vela-core/templates/defwithtemplate/annotations.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/annotations.cue
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Add annotations on your workload. If it generates pod or job, add same annotations for generated pods.
  name: annotations
  namespace: default
spec:
  appliesToWorkloads:
    - '*'
  podDisruptive: true
  schematic:
    cue:
      template: |
        // +patchStrategy=jsonMergePatch
        patch: {
        	let annotationsContent = {
        		for k, v in parameter {
        			(k): v
        		}
        	}

        	metadata: annotations: annotationsContent
        	if context.output.spec != _|_ if context.output.spec.template != _|_ {
        		spec: template: metadata: annotations: annotationsContent
        	}
        	if context.output.spec != _|_ if context.output.spec.jobTemplate != _|_ {
        		spec: jobTemplate: metadata: annotations: annotationsContent
        	}
        	if context.output.spec != _|_ if context.output.spec.jobTemplate != _|_ if context.output.spec.jobTemplate.spec != _|_ if context.output.spec.jobTemplate.spec.template != _|_ {
        		spec: jobTemplate: spec: template: metadata: annotations: annotationsContent
        	}
        }
        parameter: [string]: string | null
---
# Source: vela-core/templates/defwithtemplate/command.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/command.cue
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Add command on K8s pod for your workload which follows the pod spec in path 'spec.template'
  name: command
  namespace: default
spec:
  appliesToWorkloads:
    - deployments.apps
    - statefulsets.apps
    - daemonsets.apps
    - jobs.batch
  schematic:
    cue:
      template: |
        #PatchParams: {
        	// +usage=Specify the name of the target container, if not set, use the component name
        	containerName: *"" | string
        	// +usage=Specify the command to use in the target container, if not set, it will not be changed
        	command: *null | [...string]
        	// +usage=Specify the args to use in the target container, if set, it will override existing args
        	args: *null | [...string]
        	// +usage=Specify the args to add in the target container, existing args will be kept, cannot be used with `args`
        	addArgs: *null | [...string]
        	// +usage=Specify the existing args to delete in the target container, cannot be used with `args`
        	delArgs: *null | [...string]
        }
        PatchContainer: {
        	_params:         #PatchParams
        	name:            _params.containerName
        	_baseContainers: context.output.spec.template.spec.containers
        	_matchContainers_: [ for _container_ in _baseContainers if _container_.name == name {_container_}]
        	_baseContainer: *_|_ | {...}
        	if len(_matchContainers_) == 0 {
        		err: "container \(name) not found"
        	}
        	if len(_matchContainers_) > 0 {
        		_baseContainer: _matchContainers_[0]
        		if _params.command != null {
        			// +patchStrategy=replace
        			command: _params.command
        		}
        		if (_params.addArgs != null || _params.delArgs != null) && _params.args != null {
        			err: "cannot set addArgs/delArgs and args at the same time"
        		}
        		_delArgs: {...}
        		if _params.delArgs != null {
        			_delArgs: {for k in _params.delArgs {(k): ""}}
        		}
        		if _params.delArgs == null {
        			_delArgs: {}
        		}
        		_args: [...string]
        		if _params.args != null {
        			_args: _params.args
        		}
        		if _params.args == null && _baseContainer.args != _|_ {
        			_args: _baseContainer.args
        		}
        		if _params.args == null && _baseContainer.args == _|_ {
        			_args: []
        		}
        		_argsMap: {for a in _args {(a): ""}}
        		_addArgs: [...string]
        		if _params.addArgs != null {
        			_addArgs: _params.addArgs
        		}
        		if _params.addArgs == null {
        			_addArgs: []
        		}

        		// +patchStrategy=replace
        		args: [ for a in _args if _delArgs[a] == _|_ {a}] + [ for a in _addArgs if _delArgs[a] == _|_ && _argsMap[a] == _|_ {a}]
        	}
        }
        // +patchStrategy=open
        patch: spec: template: spec: {
        	if parameter.containers == _|_ {
        		// +patchKey=name
        		containers: [{
        			PatchContainer & {_params: {
        				if parameter.containerName == "" {
        					containerName: context.name
        				}
        				if parameter.containerName != "" {
        					containerName: parameter.containerName
        				}
        				command: parameter.command
        				args:    parameter.args
        				addArgs: parameter.addArgs
        				delArgs: parameter.delArgs
        			}}
        		}]
        	}
        	if parameter.containers != _|_ {
        		// +patchKey=name
        		containers: [ for c in parameter.containers {
        			if c.containerName == "" {
        				err: "container name must be set for containers"
        			}
        			if c.containerName != "" {
        				PatchContainer & {_params: c}
        			}
        		}]
        	}
        }

        parameter: *#PatchParams | close({
        	// +usage=Specify the commands for multiple containers
        	containers: [...#PatchParams]
        })

        errs: [ for c in patch.spec.template.spec.containers if c.err != _|_ {c.err}]
---
# Source: vela-core/templates/defwithtemplate/configmap.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/deprecated/configmap.cue
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Create/Attach configmaps on K8s pod for your workload which follows the pod spec in path 'spec.template'. This definition is DEPRECATED, please specify configmap in 'storage' instead.
  labels:
    custom.definition.oam.dev/deprecated: "true"
  name: configmap
  namespace: default
spec:
  appliesToWorkloads:
    - '*'
  podDisruptive: true
  schematic:
    cue:
      template: |
        patch: spec: template: spec: {
        	containers: [{
        		// +patchKey=name
        		volumeMounts: [
        			for v in parameter.volumes {
        				{
        					name:      "volume-\(v.name)"
        					mountPath: v.mountPath
        					readOnly:  v.readOnly
        				}
        			},
        		]
        	}, ...]
        	// +patchKey=name
        	volumes: [
        		for v in parameter.volumes {
        			{
        				name: "volume-\(v.name)"
        				configMap: name: v.name
        			}
        		},
        	]
        }
        outputs: {
        	for v in parameter.volumes {
        		if v.data != _|_ {
        			(v.name): {
        				apiVersion: "v1"
        				kind:       "ConfigMap"
        				metadata: name: v.name
        				data: v.data
        			}
        		}
        	}
        }
        parameter: {
        	// +usage=Specify mounted configmap names and their mount paths in the container
        	volumes: [...{
        		name:      string
        		mountPath: string
        		readOnly:  *false | bool
        		data?: [string]: string
        	}]
        }
---
# Source: vela-core/templates/defwithtemplate/container-image.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/container-image.cue
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Set the image of the container.
  name: container-image
  namespace: default
spec:
  appliesToWorkloads:
    - deployments.apps
    - statefulsets.apps
    - daemonsets.apps
    - jobs.batch
  podDisruptive: true
  schematic:
    cue:
      template: |
        #PatchParams: {
        	// +usage=Specify the name of the target container, if not set, use the component name
        	containerName: *"" | string
        	// +usage=Specify the image of the container
        	image: string
        	// +usage=Specify the image pull policy of the container
        	imagePullPolicy: *"" | "IfNotPresent" | "Always" | "Never"
        }
        PatchContainer: {
        	_params:         #PatchParams
        	name:            _params.containerName
        	_baseContainers: context.output.spec.template.spec.containers
        	_matchContainers_: [ for _container_ in _baseContainers if _container_.name == name {_container_}]
        	_baseContainer: *_|_ | {...}
        	if len(_matchContainers_) == 0 {
        		err: "container \(name) not found"
        	}
        	if len(_matchContainers_) > 0 {
        		// +patchStrategy=retainKeys
        		image: _params.image

        		if _params.imagePullPolicy != "" {
        			// +patchStrategy=retainKeys
        			imagePullPolicy: _params.imagePullPolicy
        		}
        	}
        }
        patch: spec: template: spec: {
        	if parameter.containers == _|_ {
        		// +patchKey=name
        		containers: [{
        			PatchContainer & {_params: {
        				if parameter.containerName == "" {
        					containerName: context.name
        				}
        				if parameter.containerName != "" {
        					containerName: parameter.containerName
        				}
        				image:           parameter.image
        				imagePullPolicy: parameter.imagePullPolicy
        			}}
        		}]
        	}
        	if parameter.containers != _|_ {
        		// +patchKey=name
        		containers: [ for c in parameter.containers {
        			if c.containerName == "" {
        				err: "containerName must be set for containers"
        			}
        			if c.containerName != "" {
        				PatchContainer & {_params: c}
        			}
        		}]
        	}
        }

        parameter: #PatchParams | close({
        	// +usage=Specify the container image for multiple containers
        	containers: [...#PatchParams]
        })

        errs: [ for c in patch.spec.template.spec.containers if c.err != _|_ {c.err}]
---
# Source: vela-core/templates/defwithtemplate/container-ports.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/container-ports.cue
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Expose on the host and bind the external port to host to enable web traffic for your component.
  name: container-ports
  namespace: default
spec:
  appliesToWorkloads:
    - deployments.apps
    - statefulsets.apps
    - daemonsets.apps
    - jobs.batch
  podDisruptive: true
  schematic:
    cue:
      template: |
        import (
        	"strconv"
        	"strings"
        )

        #PatchParams: {
        	// +usage=Specify the name of the target container, if not set, use the component name
        	containerName: *"" | string
        	// +usage=Specify ports you want customer traffic sent to
        	ports: *[] | [...{
        		// +usage=Number of port to expose on the pod's IP address
        		containerPort: int
        		// +usage=Protocol for port. Must be UDP, TCP, or SCTP
        		protocol: *"TCP" | "UDP" | "SCTP"
        		// +usage=Number of port to expose on the host
        		hostPort?: int
        		// +usage=What host IP to bind the external port to.
        		hostIP?: string
        	}]
        }

        PatchContainer: {
        	_params:         #PatchParams
        	name:            _params.containerName
        	_baseContainers: context.output.spec.template.spec.containers
        	_matchContainers_: [ for _container_ in _baseContainers if _container_.name == name {_container_}]
        	_baseContainer: *_|_ | {...}
        	if len(_matchContainers_) == 0 {
        		err: "container \(name) not found"
        	}
        	if len(_matchContainers_) > 0 {
        		_baseContainer: _matchContainers_[0]
        		_basePorts:     _baseContainer.ports
        		if _basePorts == _|_ {
        			// +patchStrategy=replace
        			ports: [ for port in _params.ports {
        				containerPort: port.containerPort
        				protocol:      port.protocol
        				if port.hostPort != _|_ {
        					hostPort: port.hostPort
        				}
        				if port.hostIP != _|_ {
        					hostIP: port.hostIP
        				}
        			}]
        		}
        		if _basePorts != _|_ {
        			_basePortsMap: {for _basePort in _basePorts {(strings.ToLower(_basePort.protocol) + strconv.FormatInt(_basePort.containerPort, 10)): _basePort}}
        			_portsMap: {for port in _params.ports {(strings.ToLower(port.protocol) + strconv.FormatInt(port.containerPort, 10)): port}}
        			// +patchStrategy=replace
        			ports: [ for portVar in _basePorts {
        				containerPort: portVar.containerPort
        				protocol:      portVar.protocol
        				name:          portVar.name
        				_uniqueKey:    strings.ToLower(portVar.protocol) + strconv.FormatInt(portVar.containerPort, 10)
        				if _portsMap[_uniqueKey] != _|_ {
        					if _portsMap[_uniqueKey].hostPort != _|_ {
        						hostPort: _portsMap[_uniqueKey].hostPort
        					}
        					if _portsMap[_uniqueKey].hostIP != _|_ {
        						hostIP: _portsMap[_uniqueKey].hostIP
        					}
        				}
        			}] + [ for port in _params.ports if _basePortsMap[strings.ToLower(port.protocol)+strconv.FormatInt(port.containerPort, 10)] == _|_ {
        				if port.containerPort != _|_ {
        					containerPort: port.containerPort
        				}
        				if port.protocol != _|_ {
        					protocol: port.protocol
        				}
        				if port.hostPort != _|_ {
        					hostPort: port.hostPort
        				}
        				if port.hostIP != _|_ {
        					hostIP: port.hostIP
        				}
        			}]
        		}
        	}
        }

        patch: spec: template: spec: {
        	if parameter.containers == _|_ {
        		// +patchKey=name
        		containers: [{
        			PatchContainer & {_params: {
        				if parameter.containerName == "" {
        					containerName: context.name
        				}
        				if parameter.containerName != "" {
        					containerName: parameter.containerName
        				}
        				ports: parameter.ports
        			}}
        		}]
        	}
        	if parameter.containers != _|_ {
        		// +patchKey=name
        		containers: [ for c in parameter.containers {
        			if c.containerName == "" {
        				err: "container name must be set for containers"
        			}
        			if c.containerName != "" {
        				PatchContainer & {_params: c}
        			}
        		}]
        	}
        }

        parameter: *#PatchParams | close({
        	// +usage=Specify the container ports for multiple containers
        	containers: [...#PatchParams]
        })

        errs: [ for c in patch.spec.template.spec.containers if c.err != _|_ {c.err}]
---
# Source: vela-core/templates/defwithtemplate/cpuscaler.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/cpuscaler.cue
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Automatically scale the component based on CPU usage.
  name: cpuscaler
  namespace: default
spec:
  appliesToWorkloads:
    - deployments.apps
    - statefulsets.apps
  schematic:
    cue:
      template: |
        outputs: cpuscaler: {
        	apiVersion: "autoscaling/v1"
        	kind:       "HorizontalPodAutoscaler"
        	metadata: name: context.name
        	spec: {
        		scaleTargetRef: {
        			apiVersion: parameter.targetAPIVersion
        			kind:       parameter.targetKind
        			name:       context.name
        		}
        		minReplicas:                    parameter.min
        		maxReplicas:                    parameter.max
        		targetCPUUtilizationPercentage: parameter.cpuUtil
        	}
        }

        parameter: {
        	// +usage=Specify the minimal number of replicas to which the autoscaler can scale down
        	min: *1 | int
        	// +usage=Specify the maximum number of of replicas to which the autoscaler can scale up
        	max: *10 | int
        	// +usage=Specify the average CPU utilization, for example, 50 means the CPU usage is 50%
        	cpuUtil: *50 | int
        	// +usage=Specify the apiVersion of scale target
        	targetAPIVersion: *"apps/v1" | string
        	// +usage=Specify the kind of scale target
        	targetKind: *"Deployment" | string
        }
---
# Source: vela-core/templates/defwithtemplate/env.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/env.cue
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Add env on K8s pod for your workload which follows the pod spec in path 'spec.template'
  name: env
  namespace: default
spec:
  appliesToWorkloads:
    - deployments.apps
    - statefulsets.apps
    - daemonsets.apps
    - jobs.batch
  schematic:
    cue:
      template: |
        #PatchParams: {
        	// +usage=Specify the name of the target container, if not set, use the component name
        	containerName: *"" | string
        	// +usage=Specify if replacing the whole environment settings for the container
        	replace: *false | bool
        	// +usage=Specify the  environment variables to merge, if key already existing, override its value
        	env: [string]: string
        	// +usage=Specify which existing environment variables to unset
        	unset: *[] | [...string]
        }
        PatchContainer: {
        	_params: #PatchParams
        	name:    _params.containerName
        	_delKeys: {for k in _params.unset {(k): ""}}
        	_baseContainers: context.output.spec.template.spec.containers
        	_matchContainers_: [ for _container_ in _baseContainers if _container_.name == name {_container_}]
        	_baseContainer: *_|_ | {...}
        	if len(_matchContainers_) == 0 {
        		err: "container \(name) not found"
        	}
        	if len(_matchContainers_) > 0 {
        		_baseContainer: _matchContainers_[0]
        		_baseEnv:       _baseContainer.env
        		if _baseEnv == _|_ {
        			// +patchStrategy=replace
        			env: [ for k, v in _params.env if _delKeys[k] == _|_ {
        				name:  k
        				value: v
        			}]
        		}
        		if _baseEnv != _|_ {
        			_baseEnvMap: {for envVar in _baseEnv {(envVar.name): envVar}}
        			// +patchStrategy=replace
        			env: [ for envVar in _baseEnv if _delKeys[envVar.name] == _|_ && !_params.replace {
        				name: envVar.name
        				if _params.env[envVar.name] != _|_ {
        					value: _params.env[envVar.name]
        				}
        				if _params.env[envVar.name] == _|_ {
        					if envVar.value != _|_ {
        						value: envVar.value
        					}
        					if envVar.valueFrom != _|_ {
        						valueFrom: envVar.valueFrom
        					}
        				}
        			}] + [ for k, v in _params.env if _delKeys[k] == _|_ && (_params.replace || _baseEnvMap[k] == _|_) {
        				name:  k
        				value: v
        			}]
        		}
        	}
        }
        patch: spec: template: spec: {
        	if parameter.containers == _|_ {
        		// +patchKey=name
        		containers: [{
        			PatchContainer & {_params: {
        				if parameter.containerName == "" {
        					containerName: context.name
        				}
        				if parameter.containerName != "" {
        					containerName: parameter.containerName
        				}
        				replace: parameter.replace
        				env:     parameter.env
        				unset:   parameter.unset
        			}}
        		}]
        	}
        	if parameter.containers != _|_ {
        		// +patchKey=name
        		containers: [ for c in parameter.containers {
        			if c.containerName == "" {
        				err: "containerName must be set for containers"
        			}
        			if c.containerName != "" {
        				PatchContainer & {_params: c}
        			}
        		}]
        	}
        }

        parameter: *#PatchParams | close({
        	// +usage=Specify the environment variables for multiple containers
        	containers: [...#PatchParams]
        })

        errs: [ for c in patch.spec.template.spec.containers if c.err != _|_ {c.err}]
---
# Source: vela-core/templates/defwithtemplate/expose.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/expose.cue
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Expose port to enable web traffic for your component.
  name: expose
  namespace: default
spec:
  appliesToWorkloads:
    - deployments.apps
    - statefulsets.apps
  podDisruptive: false
  schematic:
    cue:
      template: |
        import (
        	"strconv"
        	"strings"
        )

        outputs: service: {
        	apiVersion: "v1"
        	kind:       "Service"
        	metadata: name:        context.name
        	metadata: annotations: parameter.annotations
        	spec: {
        		if parameter["matchLabels"] == _|_ {
        			selector: "app.oam.dev/component": context.name
        		}
        		if parameter["matchLabels"] != _|_ {
        			selector: parameter["matchLabels"]
        		}

        		// compatible with the old way
        		if parameter["port"] != _|_ if parameter["ports"] == _|_ {
        			ports: [
        				for p in parameter.port {
        					name:       "port-" + strconv.FormatInt(p, 10)
        					port:       p
        					targetPort: p
        				},
        			]
        		}
        		if parameter["ports"] != _|_ {
        			ports: [ for v in parameter.ports {
        				port:       v.port
        				targetPort: v.port
        				if v.name != _|_ {
        					name: v.name
        				}
        				if v.name == _|_ {
        					_name: "port-" + strconv.FormatInt(v.port, 10)
        					name:  *_name | string
        					if v.protocol != "TCP" {
        						name: _name + "-" + strings.ToLower(v.protocol)
        					}
        				}
        				if v.nodePort != _|_ if parameter.type == "NodePort" {
        					nodePort: v.nodePort
        				}
        				if v.protocol != _|_ {
        					protocol: v.protocol
        				}
        			},
        			]
        		}
        		type: parameter.type
        	}
        }
        parameter: {
        	// +usage=Deprecated, the old way to specify the exposion ports
        	port?: [...int]

        	// +usage=Specify portsyou want customer traffic sent to
        	ports?: [...{
        		// +usage=Number of port to expose on the pod's IP address
        		port: int
        		// +usage=Name of the port
        		name?: string
        		// +usage=Protocol for port. Must be UDP, TCP, or SCTP
        		protocol: *"TCP" | "UDP" | "SCTP"
        		// +usage=exposed node port. Only Valid when exposeType is NodePort
        		nodePort?: int
        	}]

        	// +usage=Specify the annotations of the exposed service
        	annotations: [string]: string

        	matchLabels?: [string]: string

        	// +usage=Specify what kind of Service you want. options: "ClusterIP","NodePort","LoadBalancer","ExternalName"
        	type: *"ClusterIP" | "NodePort" | "LoadBalancer" | "ExternalName"
        }
  stage: PostDispatch
  status:
    customStatus: |-
      message: *"" | string
      service: context.outputs.service
      if service.spec.type == "ClusterIP" {
      	message: "ClusterIP: \(service.spec.clusterIP)"
      }
      if service.spec.type == "LoadBalancer" {
      	status: service.status
      	isHealth: *false | bool
      	if status != _|_ if status.loadBalancer != _|_ if status.loadBalancer.ingress != _|_ if len(status.loadBalancer.ingress) > 0 if status.loadBalancer.ingress[0].ip != _|_ {
      		isHealth: true
      	}
      	if !isHealth {
      		message: "ExternalIP: Pending"
      	}
      	if isHealth {
      		message: "ExternalIP: \(status.loadBalancer.ingress[0].ip)"
      	}
      }
    healthPolicy: |-
      service: context.outputs.service
      if service.spec.type == "LoadBalancer" {
      	status: service.status
      	isHealth: *false | bool
      	if status != _|_ if status.loadBalancer != _|_ if status.loadBalancer.ingress != _|_ if len(status.loadBalancer.ingress) > 0 if status.loadBalancer.ingress[0].ip != _|_ {
      		isHealth: true
      	}
      }
      if service.spec.type != "LoadBalancer" {
      	isHealth: true
      }
---
# Source: vela-core/templates/defwithtemplate/gateway.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/gateway.cue
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Enable public web traffic for the component, the ingress API matches K8s v1.20+.
  name: gateway
  namespace: default
spec:
  appliesToWorkloads:
    - deployments.apps
    - statefulsets.apps
  podDisruptive: false
  schematic:
    cue:
      template: |
        import "strconv"

        let nameSuffix = {
        	if parameter.name != _|_ {"-" + parameter.name}
        	if parameter.name == _|_ {""}
        }

        let serviceMetaName = {
        	if (parameter.existingServiceName != _|_) {parameter.existingServiceName}
        	if (parameter.existingServiceName == _|_) {context.name + nameSuffix}
        }

        if (parameter.existingServiceName == _|_) {
        	let serviceOutputName = "service" + nameSuffix
        	outputs: (serviceOutputName): {
        		apiVersion: "v1"
        		kind:       "Service"
        		metadata: name: "\(serviceMetaName)"
        		spec: {
        			selector: "app.oam.dev/component": context.name
        			ports: [
        				for k, v in parameter.http {
        					name:       "port-" + strconv.FormatInt(v, 10)
        					port:       v
        					targetPort: v
        				},
        			]
        		}
        	}
        }

        let ingressOutputName = "ingress" + nameSuffix
        let ingressMetaName = context.name + nameSuffix
        legacyAPI: context.clusterVersion.minor < 19

        outputs: (ingressOutputName): {
        	if legacyAPI {
        		apiVersion: "networking.k8s.io/v1beta1"
        	}
        	if !legacyAPI {
        		apiVersion: "networking.k8s.io/v1"
        	}
        	kind: "Ingress"
        	metadata: {
        		name: "\(ingressMetaName)"
        		annotations: {
        			if !parameter.classInSpec {
        				"kubernetes.io/ingress.class": parameter.class
        			}
        			if parameter.gatewayHost != _|_ {
        				"ingress.controller/host": parameter.gatewayHost
        			}
        			if parameter.annotations != _|_ {
        				for key, value in parameter.annotations {
        					"\(key)": "\(value)"
        				}
        			}
        		}
        		labels: {
        			if parameter.labels != _|_ {
        				for key, value in parameter.labels {
        					"\(key)": "\(value)"
        				}
        			}
        		}
        	}
        	spec: {
        		if parameter.classInSpec {
        			ingressClassName: parameter.class
        		}
        		if parameter.secretName != _|_ {
        			tls: [{
        				hosts: [
        					parameter.domain,
        				]
        				secretName: parameter.secretName
        			}]
        		}
        		rules: [{
        			if parameter.domain != _|_ {
        				host: parameter.domain
        			}
        			http: paths: [
        				for k, v in parameter.http {
        					path:     k
        					pathType: parameter.pathType
        					backend: {
        						if legacyAPI {
        							serviceName: serviceMetaName
        							servicePort: v
        						}
        						if !legacyAPI {
        							service: {
        								name: serviceMetaName
        								port: number: v
        							}
        						}
        					}
        				},
        			]
        		}]
        	}
        }

        parameter: {
        	// +usage=Specify the domain you want to expose
        	domain?: string

        	// +usage=Specify the mapping relationship between the http path and the workload port
        	http: [string]: int

        	// +usage=Specify the class of ingress to use
        	class: *"nginx" | string

        	// +usage=Set ingress class in '.spec.ingressClassName' instead of 'kubernetes.io/ingress.class' annotation.
        	classInSpec: *false | bool

        	// +usage=Specify the secret name you want to quote to use tls.
        	secretName?: string

        	// +usage=Specify the host of the ingress gateway, which is used to generate the endpoints when the host is empty.
        	gatewayHost?: string

        	// +usage=Specify a unique name for this gateway, required to support multiple gateway traits on a component
        	name?: string

        	// +usage=Specify a pathType for the ingress rules, defaults to "ImplementationSpecific"
        	pathType: *"ImplementationSpecific" | "Prefix" | "Exact"

        	// +usage=Specify the annotations to be added to the ingress
        	annotations?: [string]: string

        	// +usage=Specify the labels to be added to the ingress
        	labels?: [string]: string

        	// +usage=If specified, use an existing Service rather than creating one
        	existingServiceName?: string
        }
  status:
    customStatus: |-
      let nameSuffix = {
        if parameter.name != _|_ { "-" + parameter.name }
        if parameter.name == _|_ { "" }
      }
      let ingressMetaName = context.name + nameSuffix
      let ig  = [for i in context.outputs if (i.kind == "Ingress") && (i.metadata.name == ingressMetaName) {i}][0]
      igs: *null | string
      if ig != _|_ if ig.status != _|_ if ig.status.loadbalancer != _|_ {
        igs: ig.status.loadbalancer.ingress[0]
      }
      igr: *null | string
      if ig != _|_ if ig.spec != _|_  {
        igr: ig.spec.rules[0]
      }
      if igs == _|_ {
        message: "No loadBalancer found, visiting by using 'vela port-forward " + context.appName + "'\n"
      }
      if igs != _|_ {
        if igs.ip != _|_ {
          if igr.host != _|_ {
            message: "Visiting URL: " + igr.host + ", IP: " + igs.ip + "\n"
          }
          if igr.host == _|_ {
            message: "Host not specified, visit the cluster or load balancer in front of the cluster, IP: " + igs.ip + "\n"
          }
        }
        if igs.ip == _|_ {
          if igr.host != _|_ {
            message: "Visiting URL: " + igr.host + "\n"
          }
          if igs.host == _|_ {
            message: "Host not specified, visit the cluster or load balancer in front of the cluster\n"
          }
        }
      }
    healthPolicy: |-
      let nameSuffix = {
        if parameter.name != _|_ { "-" + parameter.name }
        if parameter.name == _|_ { "" }
      }
      let ingressMetaName = context.name + nameSuffix
      let igstat  = len([for i in context.outputs if (i.kind == "Ingress") && (i.metadata.name == ingressMetaName) {i}]) > 0
      isHealth: igstat
---
# Source: vela-core/templates/defwithtemplate/hostalias.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/hostalias.cue
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Add host aliases on K8s pod for your workload which follows the pod spec in path 'spec.template'.
  name: hostalias
  namespace: default
spec:
  appliesToWorkloads:
    - deployments.apps
    - statefulsets.apps
    - daemonsets.apps
    - jobs.batch
  podDisruptive: false
  schematic:
    cue:
      template: |
        patch: {
        	// +patchKey=ip
        	spec: template: spec: hostAliases: parameter.hostAliases
        }
        parameter: {
        	// +usage=Specify the hostAliases to add
        	hostAliases: [...{
        		ip: string
        		hostnames: [...string]
        	}]
        }
---
# Source: vela-core/templates/defwithtemplate/hpa.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/hpa.cue
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Configure k8s HPA for Deployment or Statefulsets
  name: hpa
  namespace: default
spec:
  appliesToWorkloads:
    - deployments.apps
    - statefulsets.apps
  podDisruptive: false
  schematic:
    cue:
      template: |
        outputs: hpa: {
        	if context.clusterVersion.minor < 23 {
        		apiVersion: "autoscaling/v2beta2"
        	}
        	if context.clusterVersion.minor >= 23 {
        		apiVersion: "autoscaling/v2"
        	}
        	kind: "HorizontalPodAutoscaler"
        	metadata: name: context.name
        	spec: {
        		scaleTargetRef: {
        			apiVersion: parameter.targetAPIVersion
        			kind:       parameter.targetKind
        			name:       context.name
        		}
        		minReplicas: parameter.min
        		maxReplicas: parameter.max
        		metrics: [
        			{
        				type: "Resource"
        				resource: {
        					name: "cpu"
        					target: {
        						type: parameter.cpu.type
        						if parameter.cpu.type == "Utilization" {
        							averageUtilization: parameter.cpu.value
        						}
        						if parameter.cpu.type == "AverageValue" {
        							averageValue: parameter.cpu.value
        						}
        					}
        				}
        			},
        			if parameter.mem != _|_ {
        				{
        					type: "Resource"
        					resource: {
        						name: "memory"
        						target: {
        							type: parameter.mem.type
        							if parameter.mem.type == "Utilization" {
        								averageUtilization: parameter.mem.value
        							}
        							if parameter.mem.type == "AverageValue" {
        								averageValue: parameter.mem.value
        							}
        						}
        					}
        				}
        			},
        			if parameter.podCustomMetrics != _|_ for m in parameter.podCustomMetrics {
        				type: "Pods"
        				pods: {
        					metric: name: m.name
        					target: {
        						type:         "AverageValue"
        						averageValue: m.value
        					}
        				}
        			},
        		]
        	}
        }
        parameter: {
        	// +usage=Specify the minimal number of replicas to which the autoscaler can scale down
        	min: *1 | int
        	// +usage=Specify the maximum number of of replicas to which the autoscaler can scale up
        	max: *10 | int
        	// +usage=Specify the apiVersion of scale target
        	targetAPIVersion: *"apps/v1" | string
        	// +usage=Specify the kind of scale target
        	targetKind: *"Deployment" | string
        	cpu: {
        		// +usage=Specify resource metrics in terms of percentage("Utilization") or direct value("AverageValue")
        		type: *"Utilization" | "AverageValue"
        		// +usage=Specify the value of CPU utilization or averageValue
        		value: *50 | int
        	}
        	mem?: {
        		// +usage=Specify resource metrics in terms of percentage("Utilization") or direct value("AverageValue")
        		type: *"Utilization" | "AverageValue"
        		// +usage=Specify  the value of MEM utilization or averageValue
        		value: *50 | int
        	}
        	// +usage=Specify custom metrics of pod type
        	podCustomMetrics?: [...{
        		// +usage=Specify name of custom metrics
        		name: string
        		// +usage=Specify target value of custom metrics
        		value: string
        	}]
        }
---
# Source: vela-core/templates/defwithtemplate/ingress-1-20.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/deprecated/ingress-1-20.cue
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Enable public web traffic for the component, the ingress API matches K8s v1.20+.
  labels:
    custom.definition.oam.dev/deprecated: "true"
  name: ingress-1-20
  namespace: default
spec:
  podDisruptive: false
  schematic:
    cue:
      template: |
        // trait template can have multiple outputs in one trait
        outputs: service: {
        	apiVersion: "v1"
        	kind:       "Service"
        	metadata: name: context.name
        	spec: {
        		selector: "app.oam.dev/component": context.name
        		ports: [
        			for k, v in parameter.http {
        				port:       v
        				targetPort: v
        			},
        		]
        	}
        }

        outputs: ingress: {
        	apiVersion: "networking.k8s.io/v1"
        	kind:       "Ingress"
        	metadata: {
        		name: context.name
        		annotations: "kubernetes.io/ingress.class": parameter.class
        	}
        	spec: rules: [{
        		host: parameter.domain
        		http: paths: [
        			for k, v in parameter.http {
        				path:     k
        				pathType: "ImplementationSpecific"
        				backend: service: {
        					name: context.name
        					port: number: v
        				}
        			},
        		]
        	}]
        }

        parameter: {
        	// +usage=Specify the domain you want to expose
        	domain: string

        	// +usage=Specify the mapping relationship between the http path and the workload port
        	http: [string]: int

        	// +usage=Specify the class of ingress to use
        	class: *"nginx" | string
        }
  status:
    customStatus: |-
      let igs = context.outputs.ingress.status.loadBalancer.ingress
      if igs == _|_ {
        message: "No loadBalancer found, visiting by using 'vela port-forward " + context.appName + "'\n"
      }
      if len(igs) > 0 {
        if igs[0].ip != _|_ {
      	  message: "Visiting URL: " + context.outputs.ingress.spec.rules[0].host + ", IP: " + igs[0].ip
        }
        if igs[0].ip == _|_ {
      	  message: "Visiting URL: " + context.outputs.ingress.spec.rules[0].host
        }
      }
    healthPolicy: 'isHealth: len(context.outputs.service.spec.clusterIP) > 0'
---
# Source: vela-core/templates/defwithtemplate/ingress.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/deprecated/ingress.cue
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Enable public web traffic for the component.
  labels:
    custom.definition.oam.dev/deprecated: "true"
  name: ingress
  namespace: default
spec:
  podDisruptive: false
  schematic:
    cue:
      template: |
        // trait template can have multiple outputs in one trait
        outputs: service: {
        	apiVersion: "v1"
        	kind:       "Service"
        	metadata: name: context.name
        	spec: {
        		selector: "app.oam.dev/component": context.name
        		ports: [
        			for k, v in parameter.http {
        				port:       v
        				targetPort: v
        			},
        		]
        	}
        }

        outputs: ingress: {
        	apiVersion: "networking.k8s.io/v1beta1"
        	kind:       "Ingress"
        	metadata: name: context.name
        	spec: rules: [{
        		host: parameter.domain
        		http: paths: [
        			for k, v in parameter.http {
        				path: k
        				backend: {
        					serviceName: context.name
        					servicePort: v
        				}
        			},
        		]
        	}]
        }

        parameter: {
        	// +usage=Specify the domain you want to expose
        	domain: string

        	// +usage=Specify the mapping relationship between the http path and the workload port
        	http: [string]: int
        }
  status:
    customStatus: |-
      let igs = context.outputs.ingress.status.loadBalancer.ingress
      if igs == _|_ {
      	message: "No loadBalancer found, visiting by using 'vela port-forward " + context.appName + "'\n"
      }
      if len(igs) > 0 {
      	if igs[0].ip != _|_ {
      		message: "Visiting URL: " + context.outputs.ingress.spec.rules[0].host + ", IP: " + igs[0].ip
      	}
      	if igs[0].ip == _|_ {
      		message: "Visiting URL: " + context.outputs.ingress.spec.rules[0].host
      	}
      }
    healthPolicy: 'isHealth: len(context.outputs.service.spec.clusterIP) > 0'
---
# Source: vela-core/templates/defwithtemplate/init-container.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/init-container.cue
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: add an init container and use shared volume with pod
  name: init-container
  namespace: default
spec:
  appliesToWorkloads:
    - deployments.apps
    - statefulsets.apps
    - daemonsets.apps
    - jobs.batch
  podDisruptive: true
  schematic:
    cue:
      template: |
        patch: spec: template: spec: {
        	// +patchKey=name
        	containers: [{
        		name: context.name
        		// +patchKey=name
        		volumeMounts: [{
        			name:      parameter.mountName
        			mountPath: parameter.appMountPath
        		}]
        	}]
        	// +patchKey=name
        	initContainers: [{
        		name:            parameter.name
        		image:           parameter.image
        		imagePullPolicy: parameter.imagePullPolicy
        		if parameter.cmd != _|_ {
        			command: parameter.cmd
        		}
        		if parameter.args != _|_ {
        			args: parameter.args
        		}
        		if parameter["env"] != _|_ {
        			env: parameter.env
        		}

        		// +patchKey=name
        		volumeMounts: [{
        			name:      parameter.mountName
        			mountPath: parameter.initMountPath
        		}] + parameter.extraVolumeMounts
        	}]
        	// +patchKey=name
        	volumes: [{
        		name: parameter.mountName
        		emptyDir: {}
        	}]
        }
        parameter: {
        	// +usage=Specify the name of init container
        	name: string

        	// +usage=Specify the image of init container
        	image: string

        	// +usage=Specify image pull policy for your service
        	imagePullPolicy: *"IfNotPresent" | "Always" | "Never"

        	// +usage=Specify the commands run in the init container
        	cmd?: [...string]

        	// +usage=Specify the args run in the init container
        	args?: [...string]

        	// +usage=Specify the env run in the init container
        	env?: [...{
        		// +usage=Environment variable name
        		name: string
        		// +usage=The value of the environment variable
        		value?: string
        		// +usage=Specifies a source the value of this var should come from
        		valueFrom?: {
        			// +usage=Selects a key of a secret in the pod's namespace
        			secretKeyRef?: {
        				// +usage=The name of the secret in the pod's namespace to select from
        				name: string
        				// +usage=The key of the secret to select from. Must be a valid secret key
        				key: string
        			}
        			// +usage=Selects a key of a config map in the pod's namespace
        			configMapKeyRef?: {
        				// +usage=The name of the config map in the pod's namespace to select from
        				name: string
        				// +usage=The key of the config map to select from. Must be a valid secret key
        				key: string
        			}
        		}
        	}]

        	// +usage=Specify the mount name of shared volume
        	mountName: *"workdir" | string

        	// +usage=Specify the mount path of app container
        	appMountPath: string

        	// +usage=Specify the mount path of init container
        	initMountPath: string

        	// +usage=Specify the extra volume mounts for the init container
        	extraVolumeMounts: [...{
        		// +usage=The name of the volume to be mounted
        		name: string
        		// +usage=The mountPath for mount in the init container
        		mountPath: string
        	}]
        }
---
# Source: vela-core/templates/defwithtemplate/json-merge-patch.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/json-merge-patch.cue
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Patch the output following Json Merge Patch strategy, following RFC 7396.
  labels:
    custom.definition.oam.dev/ui-hidden: "true"
  name: json-merge-patch
  namespace: default
spec:
  appliesToWorkloads:
    - '*'
  podDisruptive: true
  schematic:
    cue:
      template: |
        parameter: {...}
        // +patchStrategy=jsonMergePatch
        patch: parameter
---
# Source: vela-core/templates/defwithtemplate/json-patch.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/json-patch.cue
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Patch the output following Json Patch strategy, following RFC 6902.
  labels:
    custom.definition.oam.dev/ui-hidden: "true"
  name: json-patch
  namespace: default
spec:
  appliesToWorkloads:
    - '*'
  podDisruptive: true
  schematic:
    cue:
      template: |
        parameter: operations: [...{...}]
        // +patchStrategy=jsonPatch
        patch: parameter
---
# Source: vela-core/templates/defwithtemplate/k8s-update-strategy.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/k8s-update-strategy.cue
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/alias: ""
    definition.oam.dev/description: Set k8s update strategy for Deployment/DaemonSet/StatefulSet
  name: k8s-update-strategy
  namespace: default
spec:
  appliesToWorkloads:
    - deployments.apps
    - statefulsets.apps
    - daemonsets.apps
  conflictsWith: []
  podDisruptive: false
  schematic:
    cue:
      template: |
        patch: spec: {
        	if parameter.targetKind == "Deployment" && parameter.strategy.type != "OnDelete" {
        		// +patchStrategy=retainKeys
        		strategy: {
        			type: parameter.strategy.type
        			if parameter.strategy.type == "RollingUpdate" {
        				rollingUpdate: {
        					maxSurge:       parameter.strategy.rollingStrategy.maxSurge
        					maxUnavailable: parameter.strategy.rollingStrategy.maxUnavailable
        				}
        			}
        		}
        	}

        	if parameter.targetKind == "StatefulSet" && parameter.strategy.type != "Recreate" {
        		// +patchStrategy=retainKeys
        		updateStrategy: {
        			type: parameter.strategy.type
        			if parameter.strategy.type == "RollingUpdate" {
        				rollingUpdate: partition: parameter.strategy.rollingStrategy.partition
        			}
        		}
        	}

        	if parameter.targetKind == "DaemonSet" && parameter.strategy.type != "Recreate" {
        		// +patchStrategy=retainKeys
        		updateStrategy: {
        			type: parameter.strategy.type
        			if parameter.strategy.type == "RollingUpdate" {
        				rollingUpdate: {
        					maxSurge:       parameter.strategy.rollingStrategy.maxSurge
        					maxUnavailable: parameter.strategy.rollingStrategy.maxUnavailable
        				}
        			}
        		}
        	}

        }
        parameter: {
        	// +usage=Specify the apiVersion of target
        	targetAPIVersion: *"apps/v1" | string
        	// +usage=Specify the kind of target
        	targetKind: *"Deployment" | "StatefulSet" | "DaemonSet"
        	// +usage=Specify the strategy of update
        	strategy: {
        		// +usage=Specify the strategy type
        		type: *"RollingUpdate" | "Recreate" | "OnDelete"
        		// +usage=Specify the parameters of rollong update strategy
        		rollingStrategy?: {
        			maxSurge:       *"25%" | string
        			maxUnavailable: *"25%" | string
        			partition:      *0 | int
        		}
        	}
        }
  workloadRefPath: ""
---
# Source: vela-core/templates/defwithtemplate/labels.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/labels.cue
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Add labels on your workload. if it generates pod, add same label for generated pods.
  name: labels
  namespace: default
spec:
  appliesToWorkloads:
    - '*'
  podDisruptive: true
  schematic:
    cue:
      template: |
        // +patchStrategy=jsonMergePatch
        patch: {
        	metadata: labels: {
        		for k, v in parameter {
        			(k): v
        		}
        	}
        	if context.output.spec != _|_ && context.output.spec.template != _|_ {
        		spec: template: metadata: labels: {
        			for k, v in parameter {
        				(k): v
        			}
        		}
        	}
        }
        parameter: [string]: string | null
---
# Source: vela-core/templates/defwithtemplate/lifecycle.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/lifecycle.cue
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Add lifecycle hooks for every container of K8s pod for your workload which follows the pod spec in path 'spec.template'.
  name: lifecycle
  namespace: default
spec:
  appliesToWorkloads:
    - deployments.apps
    - statefulsets.apps
    - daemonsets.apps
    - jobs.batch
  podDisruptive: true
  schematic:
    cue:
      template: |
        patch: spec: template: spec: containers: [...{
        	lifecycle: {
        		if parameter.postStart != _|_ {
        			postStart: parameter.postStart
        		}
        		if parameter.preStop != _|_ {
        			preStop: parameter.preStop
        		}
        	}
        }]
        parameter: {
        	postStart?: #LifeCycleHandler
        	preStop?:   #LifeCycleHandler
        }
        #Port: int & >=1 & <=65535
        #LifeCycleHandler: {
        	exec?: command: [...string]
        	httpGet?: {
        		path?:  string
        		port:   #Port
        		host?:  string
        		scheme: *"HTTP" | "HTTPS"
        		httpHeaders?: [...{
        			name:  string
        			value: string
        		}]
        	}
        	tcpSocket?: {
        		port:  #Port
        		host?: string
        	}
        }
---
# Source: vela-core/templates/defwithtemplate/nocalhost.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/nocalhost.cue
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: nocalhost develop configuration.
  labels:
    custom.definition.oam.dev/ui-hidden: "true"
  name: nocalhost
  namespace: default
spec:
  appliesToWorkloads:
    - deployments.apps
    - statefulsets.apps
    - daemonsets.apps
    - jobs.batch
  podDisruptive: true
  schematic:
    cue:
      template: |
        import (
        	"encoding/json"
        )

        outputs: nocalhostService: {
        	apiVersion: "v1"
        	kind:       "Service"
        	metadata: name: context.name
        	spec: {
        		selector: "app.oam.dev/component": context.name
        		ports: [
        			{
        				port:       parameter.port
        				targetPort: parameter.port
        			},
        		]
        		type: "ClusterIP"
        	}
        }

        patch: metadata: annotations: {
        	"dev.nocalhost/application-name":      context.appName
        	"dev.nocalhost/application-namespace": context.namespace
        	"dev.nocalhost":                       json.Marshal({
        		name:        context.name
        		serviceType: parameter.serviceType
        		containers: [
        			{
        				name: context.name
        				dev: {
        					if parameter.gitUrl != _|_ {
        						gitUrl: parameter.gitUrl
        					}
        					if parameter.image == "go" {
        						image: "nocalhost-docker.pkg.coding.net/nocalhost/dev-images/golang:latest"
        					}
        					if parameter.image == "java" {
        						image: "nocalhost-docker.pkg.coding.net/nocalhost/dev-images/java:latest"
        					}
        					if parameter.image == "python" {
        						image: "nocalhost-docker.pkg.coding.net/nocalhost/dev-images/python:latest"
        					}
        					if parameter.image == "node" {
        						image: "nocalhost-docker.pkg.coding.net/nocalhost/dev-images/node:latest"
        					}
        					if parameter.image == "ruby" {
        						image: "nocalhost-docker.pkg.coding.net/nocalhost/dev-images/ruby:latest"
        					}
        					if parameter.image != "go" && parameter.image != "java" && parameter.image != "python" && parameter.image != "node" && parameter.image != "ruby" {
        						image: parameter.image
        					}
        					shell:   parameter.shell
        					workDir: parameter.workDir
        					if parameter.storageClass != _|_ {
        						storageClass: parameter.storageClass
        					}
        					resources: {
        						limits:   parameter.resources.limits
        						requests: parameter.resources.requests
        					}
        					if parameter.persistentVolumeDirs != _|_ {
        						persistentVolumeDirs: [
        							for v in parameter.persistentVolumeDirs {
        								path:     v.path
        								capacity: v.capacity
        							},
        						]
        					}
        					if parameter.command != _|_ {
        						command: parameter.command
        					}
        					if parameter.debug != _|_ {
        						debug: parameter.debug
        					}
        					hotReload: parameter.hotReload
        					if parameter.sync != _|_ {
        						sync: parameter.sync
        					}
        					if parameter.env != _|_ {
        						env: [
        							for v in parameter.env {
        								name:  v.name
        								value: v.value
        							},
        						]
        					}
        					if parameter.portForward != _|_ {
        						portForward: parameter.portForward
        					}
        					if parameter.portForward == _|_ {
        						portForward: ["\(parameter.port):\(parameter.port)"]
        					}
        				}
        			},
        		]
        	})
        }
        language: "go" | "java" | "python" | "node" | "ruby"
        parameter: {
        	port:          int
        	serviceType:   *"deployment" | string
        	gitUrl?:       string
        	image:         language | string
        	shell:         *"bash" | string
        	workDir:       *"/home/nocalhost-dev" | string
        	storageClass?: string
        	command: {
        		run:   *["sh", "run.sh"] | [...string]
        		debug: *["sh", "debug.sh"] | [...string]
        	}
        	debug?: remoteDebugPort?: int
        	hotReload: *true | bool
        	sync: {
        		type:              *"send" | string
        		filePattern:       *["./"] | [...string]
        		ignoreFilePattern: *[".git", ".vscode", ".idea", ".gradle", "build"] | [...string]
        	}
        	env?: [...{
        		name:  string
        		value: string
        	}]
        	portForward?: [...string]
        	persistentVolumeDirs?: [...{
        		path:     string
        		capacity: string
        	}]
        	resources: {
        		limits: {
        			memory: *"2Gi" | string
        			cpu:    *"2" | string
        		}
        		requests: {
        			memory: *"512Mi" | string
        			cpu:    *"0.5" | string
        		}
        	}
        }
---
# Source: vela-core/templates/defwithtemplate/node-affinity.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/deprecated/node-affinity.cue
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: affinity specify node affinity and toleration on K8s pod for your workload which follows the pod spec in path 'spec.template'.
  labels:
    custom.definition.oam.dev/deprecated: "true"
    custom.definition.oam.dev/ui-hidden: "true"
  name: node-affinity
  namespace: default
spec:
  appliesToWorkloads:
    - '*'
  podDisruptive: true
  schematic:
    cue:
      template: |
        patch: spec: template: spec: {
        	if parameter.affinity != _|_ {
        		affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: [{
        			matchExpressions: [
        				for k, v in parameter.affinity {
        					key:      k
        					operator: "In"
        					values:   v
        				},
        			]}]
        	}
        	if parameter.tolerations != _|_ {
        		tolerations: [
        			for k, v in parameter.tolerations {
        				effect:   "NoSchedule"
        				key:      k
        				operator: "Equal"
        				value:    v
        			}]
        	}
        }
        parameter: {
        	affinity?: [string]: [...string]
        	tolerations?: [string]: string
        }
---
# Source: vela-core/templates/defwithtemplate/pure-ingress.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/pure-ingress.cue
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Enable public web traffic for the component without creating a Service.
  labels:
    custom.definition.oam.dev/deprecated: "true"
    custom.definition.oam.dev/ui-hidden: "true"
  name: pure-ingress
  namespace: default
spec:
  appliesToWorkloads:
    - '*'
  conflictsWith: []
  podDisruptive: false
  schematic:
    cue:
      template: |
        outputs: ingress: {
        	apiVersion: "networking.k8s.io/v1beta1"
        	kind:       "Ingress"
        	metadata: name: context.name
        	spec: rules: [{
        		host: parameter.domain
        		http: paths: [
        			for k, v in parameter.http {
        				path: k
        				backend: {
        					serviceName: context.name
        					servicePort: v
        				}
        			},
        		]
        	}]
        }
        parameter: {
        	// +usage=Specify the domain you want to expose
        	domain: string

        	// +usage=Specify the mapping relationship between the http path and the workload port
        	http: [string]: int
        }
  status:
    customStatus: |-
      let igs = context.outputs.ingress.status.loadBalancer.ingress
      if igs == _|_ {
      	message: "No loadBalancer found, visiting by using 'vela port-forward " + context.appName + " --route'\n"
      }
      if len(igs) > 0 {
      	if igs[0].ip != _|_ {
      		message: "Visiting URL: " + context.outputs.ingress.spec.rules[0].host + ", IP: " + igs[0].ip
      	}
      	if igs[0].ip == _|_ {
      		message: "Visiting URL: " + context.outputs.ingress.spec.rules[0].host
      	}
      }
  workloadRefPath: ""
---
# Source: vela-core/templates/defwithtemplate/pvc.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/deprecated/pvc.cue
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Create a Persistent Volume Claim and mount the PVC as volume to the  first container in the pod. This definition is DEPRECATED, please specify pvc in 'storage' instead.
  labels:
    custom.definition.oam.dev/deprecated: "true"
  name: pvc
  namespace: default
spec:
  appliesToWorkloads:
    - deployments.apps
  podDisruptive: true
  schematic:
    cue:
      template: |
        patch: spec: template: spec: {
        	containers: [{
        		if parameter.volumeMode == "Block" {
        			// +patchKey=name
        			volumeDevices: [
        				for v in parameter.volumesToMount {
        					{
        						name:       v.name
        						devicePath: v.devicePath
        					}
        				},
        			]
        		}
        		if parameter.volumeMode == "Filesystem" {
        			// +patchKey=name
        			volumeMounts: [
        				for v in parameter.volumesToMount {
        					{
        						name:      v.name
        						mountPath: v.mountPath
        					}
        				},
        			]
        		}
        	}]

        	// +patchKey=name
        	volumes: [
        		for v in parameter.volumesToMount {
        			{
        				name: v.name
        				persistentVolumeClaim: claimName: parameter.claimName
        			}
        		},
        	]
        }
        outputs: claim: {
        	apiVersion: "v1"
        	kind:       "PersistentVolumeClaim"
        	metadata: name: parameter.claimName
        	spec: {
        		accessModes: parameter.accessModes
        		volumeMode:  parameter.volumeMode
        		if parameter.volumeName != _|_ {
        			volumeName: parameter.volumeName
        		}

        		if parameter.storageClassName != _|_ {
        			storageClassName: parameter.storageClassName
        		}
        		resources: requests: storage: parameter.resources.requests.storage
        		if parameter.resources.limits != _|_ {
        			resources: limits: storage: parameter.resources.limits.storage
        		}
        		if parameter.dataSourceRef != _|_ {
        			dataSourceRef: parameter.dataSourceRef
        		}
        		if parameter.dataSource != _|_ {
        			dataSource: parameter.dataSource
        		}
        		if parameter.selector != _|_ {
        			dataSource: parameter.selector
        		}
        	}
        }
        parameter: {
        	claimName:   string
        	volumeMode:  *"Filesystem" | string
        	volumeName?: string
        	accessModes: [...string]
        	storageClassName?: string
        	resources: {
        		requests: storage: =~"^([1-9][0-9]{0,63})(E|P|T|G|M|K|Ei|Pi|Ti|Gi|Mi|Ki)$"
        		limits?: storage:  =~"^([1-9][0-9]{0,63})(E|P|T|G|M|K|Ei|Pi|Ti|Gi|Mi|Ki)$"
        	}
        	dataSourceRef?: {
        		name:     string
        		kind:     string
        		apiGroup: string
        	}
        	dataSource?: {
        		name:     string
        		kind:     string
        		apiGroup: string
        	}
        	selector?: {
        		matchLabels?: [string]: string
        		matchExpressions?: {
        			key: string
        			values: [...string]
        			operator: string
        		}
        	}
        	volumesToMount: [...{
        		name: string
        		if volumeMode == "Block" {
        			devicePath: string
        		}
        		if volumeMode == "Filesystem" {
        			mountPath: string
        		}
        	}]
        }
---
# Source: vela-core/templates/defwithtemplate/resource.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/resource.cue
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Add resource requests and limits on K8s pod for your workload which follows the pod spec in path 'spec.template.'
  name: resource
  namespace: default
spec:
  appliesToWorkloads:
    - deployments.apps
    - statefulsets.apps
    - daemonsets.apps
    - jobs.batch
    - cronjobs.batch
  podDisruptive: true
  schematic:
    cue:
      template: |2
        let resourceContent = {
        	resources: {
        		if parameter.cpu != _|_ if parameter.memory != _|_ if parameter.requests == _|_ if parameter.limits == _|_ {
        			// +patchStrategy=retainKeys
        			requests: {
        				cpu:    parameter.cpu
        				memory: parameter.memory
        			}
        			// +patchStrategy=retainKeys
        			limits: {
        				cpu:    parameter.cpu
        				memory: parameter.memory
        			}
        		}

        		if parameter.requests != _|_ {
        			// +patchStrategy=retainKeys
        			requests: {
        				cpu:    parameter.requests.cpu
        				memory: parameter.requests.memory
        			}
        		}
        		if parameter.limits != _|_ {
        			// +patchStrategy=retainKeys
        			limits: {
        				cpu:    parameter.limits.cpu
        				memory: parameter.limits.memory
        			}
        		}
        	}
        }

        if context.output.spec != _|_ if context.output.spec.template != _|_ {
        	patch: spec: template: spec: {
        		// +patchKey=name
        		containers: [resourceContent]
        	}
        }
        if context.output.spec != _|_ if context.output.spec.jobTemplate != _|_ {
        	patch: spec: jobTemplate: spec: template: spec: {
        		// +patchKey=name
        		containers: [resourceContent]
        	}
        }

        parameter: {
        	// +usage=Specify the amount of cpu for requests and limits
        	cpu?: *1 | number | string
        	// +usage=Specify the amount of memory for requests and limits
        	memory?: *"2048Mi" | =~"^([1-9][0-9]{0,63})(E|P|T|G|M|K|Ei|Pi|Ti|Gi|Mi|Ki)$"
        	// +usage=Specify the resources in requests
        	requests?: {
        		// +usage=Specify the amount of cpu for requests
        		cpu: *1 | number | string
        		// +usage=Specify the amount of memory for requests
        		memory: *"2048Mi" | =~"^([1-9][0-9]{0,63})(E|P|T|G|M|K|Ei|Pi|Ti|Gi|Mi|Ki)$"
        	}
        	// +usage=Specify the resources in limits
        	limits?: {
        		// +usage=Specify the amount of cpu for limits
        		cpu: *1 | number | string
        		// +usage=Specify the amount of memory for limits
        		memory: *"2048Mi" | =~"^([1-9][0-9]{0,63})(E|P|T|G|M|K|Ei|Pi|Ti|Gi|Mi|Ki)$"
        	}
        }
---
# Source: vela-core/templates/defwithtemplate/scaler.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/scaler.cue
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Manually scale K8s pod for your workload which follows the pod spec in path 'spec.template'.
  name: scaler
  namespace: default
spec:
  appliesToWorkloads:
    - deployments.apps
    - statefulsets.apps
  podDisruptive: false
  schematic:
    cue:
      template: |
        parameter: {
        	// +usage=Specify the number of workload
        	replicas: *1 | int
        }
        // +patchStrategy=retainKeys
        patch: spec: replicas: parameter.replicas
---
# Source: vela-core/templates/defwithtemplate/service-account.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/service-account.cue
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Specify serviceAccount for your workload which follows the pod spec in path 'spec.template'.
  name: service-account
  namespace: default
spec:
  appliesToWorkloads:
    - deployments.apps
    - statefulsets.apps
    - daemonsets.apps
    - jobs.batch
  podDisruptive: false
  schematic:
    cue:
      template: |
        #Privileges: {
        	// +usage=Specify the verbs to be allowed for the resource
        	verbs: [...string]
        	// +usage=Specify the apiGroups of the resource
        	apiGroups?: [...string]
        	// +usage=Specify the resources to be allowed
        	resources?: [...string]
        	// +usage=Specify the resourceNames to be allowed
        	resourceNames?: [...string]
        	// +usage=Specify the resource url to be allowed
        	nonResourceURLs?: [...string]
        	// +usage=Specify the scope of the privileges, default to be namespace scope
        	scope: *"namespace" | "cluster"
        }
        parameter: {
        	// +usage=Specify the name of ServiceAccount
        	name: string
        	// +usage=Specify whether to create new ServiceAccount or not
        	create: *false | bool
        	// +usage=Specify the privileges of the ServiceAccount, if not empty, RoleBindings(ClusterRoleBindings) will be created
        	privileges?: [...#Privileges]
        }
        // +patchStrategy=retainKeys
        patch: spec: template: spec: serviceAccountName: parameter.name

        _clusterPrivileges: [ if parameter.privileges != _|_ for p in parameter.privileges if p.scope == "cluster" {p}]
        _namespacePrivileges: [ if parameter.privileges != _|_ for p in parameter.privileges if p.scope == "namespace" {p}]
        outputs: {
        	if parameter.create {
        		"service-account": {
        			apiVersion: "v1"
        			kind:       "ServiceAccount"
        			metadata: name: parameter.name
        		}
        	}
        	if parameter.privileges != _|_ {
        		if len(_clusterPrivileges) > 0 {
        			"cluster-role": {
        				apiVersion: "rbac.authorization.k8s.io/v1"
        				kind:       "ClusterRole"
        				metadata: name: "\(context.namespace):\(parameter.name)"
        				rules: [ for p in _clusterPrivileges {
        					verbs: p.verbs
        					if p.apiGroups != _|_ {
        						apiGroups: p.apiGroups
        					}
        					if p.resources != _|_ {
        						resources: p.resources
        					}
        					if p.resourceNames != _|_ {
        						resourceNames: p.resourceNames
        					}
        					if p.nonResourceURLs != _|_ {
        						nonResourceURLs: p.nonResourceURLs
        					}
        				}]
        			}
        			"cluster-role-binding": {
        				apiVersion: "rbac.authorization.k8s.io/v1"
        				kind:       "ClusterRoleBinding"
        				metadata: name: "\(context.namespace):\(parameter.name)"
        				roleRef: {
        					apiGroup: "rbac.authorization.k8s.io"
        					kind:     "ClusterRole"
        					name:     "\(context.namespace):\(parameter.name)"
        				}
        				subjects: [{
        					kind:      "ServiceAccount"
        					name:      parameter.name
        					namespace: (context.namespace)
        				}]
        			}
        		}
        		if len(_namespacePrivileges) > 0 {
        			role: {
        				apiVersion: "rbac.authorization.k8s.io/v1"
        				kind:       "Role"
        				metadata: name: parameter.name
        				rules: [ for p in _namespacePrivileges {
        					verbs: p.verbs
        					if p.apiGroups != _|_ {
        						apiGroups: p.apiGroups
        					}
        					if p.resources != _|_ {
        						resources: p.resources
        					}
        					if p.resourceNames != _|_ {
        						resourceNames: p.resourceNames
        					}
        					if p.nonResourceURLs != _|_ {
        						nonResourceURLs: p.nonResourceURLs
        					}
        				}]
        			}
        			"role-binding": {
        				apiVersion: "rbac.authorization.k8s.io/v1"
        				kind:       "RoleBinding"
        				metadata: name: parameter.name
        				roleRef: {
        					apiGroup: "rbac.authorization.k8s.io"
        					kind:     "Role"
        					name:     parameter.name
        				}
        				subjects: [{
        					kind: "ServiceAccount"
        					name: parameter.name
        				}]
        			}
        		}
        	}
        }
---
# Source: vela-core/templates/defwithtemplate/service-binding.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/service-binding.cue
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Binding secrets of cloud resources to component env. This definition is DEPRECATED, please use 'storage' instead.
  labels:
    custom.definition.oam.dev/ui-hidden: "true"
  name: service-binding
  namespace: default
spec:
  appliesToWorkloads:
    - deployments.apps
    - statefulsets.apps
    - daemonsets.apps
    - jobs.batch
  schematic:
    cue:
      template: |
        patch: spec: template: spec: {
        	// +patchKey=name
        	containers: [{
        		name: context.name
        		// +patchKey=name
        		env: [
        			for envName, v in parameter.envMappings {
        				name: envName
        				valueFrom: secretKeyRef: {
        					name: v.secret
        					if v["key"] != _|_ {
        						key: v.key
        					}
        					if v["key"] == _|_ {
        						key: envName
        					}
        				}
        			},
        		]
        	}]
        }

        parameter: {
        	// +usage=The mapping of environment variables to secret
        	envMappings: [string]: #KeySecret
        }
        #KeySecret: {
        	key?:   string
        	secret: string
        }
---
# Source: vela-core/templates/defwithtemplate/sidecar.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/sidecar.cue
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Inject a sidecar container to K8s pod for your workload which follows the pod spec in path 'spec.template'.
  name: sidecar
  namespace: default
spec:
  appliesToWorkloads:
    - deployments.apps
    - statefulsets.apps
    - daemonsets.apps
    - jobs.batch
  podDisruptive: true
  schematic:
    cue:
      template: |
        patch: {
        	// +patchKey=name
        	spec: template: spec: containers: [{
        		name:  parameter.name
        		image: parameter.image
        		if parameter.cmd != _|_ {
        			command: parameter.cmd
        		}
        		if parameter.args != _|_ {
        			args: parameter.args
        		}
        		if parameter["env"] != _|_ {
        			env: parameter.env
        		}
        		if parameter["volumes"] != _|_ {
        			volumeMounts: [ for v in parameter.volumes {
        				{
        					mountPath: v.path
        					name:      v.name
        				}
        			}]
        		}
        		if parameter["livenessProbe"] != _|_ {
        			livenessProbe: parameter.livenessProbe
        		}

        		if parameter["readinessProbe"] != _|_ {
        			readinessProbe: parameter.readinessProbe
        		}
        	}]
        }
        parameter: {
        	// +usage=Specify the name of sidecar container
        	name: string

        	// +usage=Specify the image of sidecar container
        	image: string

        	// +usage=Specify the commands run in the sidecar
        	cmd?: [...string]

        	// +usage=Specify the args in the sidecar
        	args?: [...string]

        	// +usage=Specify the env in the sidecar
        	env?: [...{
        		// +usage=Environment variable name
        		name: string
        		// +usage=The value of the environment variable
        		value?: string
        		// +usage=Specifies a source the value of this var should come from
        		valueFrom?: {
        			// +usage=Selects a key of a secret in the pod's namespace
        			secretKeyRef?: {
        				// +usage=The name of the secret in the pod's namespace to select from
        				name: string
        				// +usage=The key of the secret to select from. Must be a valid secret key
        				key: string
        			}
        			// +usage=Selects a key of a config map in the pod's namespace
        			configMapKeyRef?: {
        				// +usage=The name of the config map in the pod's namespace to select from
        				name: string
        				// +usage=The key of the config map to select from. Must be a valid secret key
        				key: string
        			}
        			// +usage=Specify the field reference for env
        			fieldRef?: {
        				// +usage=Specify the field path for env
        				fieldPath: string
        			}
        		}
        	}]

        	// +usage=Specify the shared volume path
        	volumes?: [...{
        		name: string
        		path: string
        	}]

        	// +usage=Instructions for assessing whether the container is alive.
        	livenessProbe?: #HealthProbe

        	// +usage=Instructions for assessing whether the container is in a suitable state to serve traffic.
        	readinessProbe?: #HealthProbe
        }

        #HealthProbe: {

        	// +usage=Instructions for assessing container health by executing a command. Either this attribute or the httpGet attribute or the tcpSocket attribute MUST be specified. This attribute is mutually exclusive with both the httpGet attribute and the tcpSocket attribute.
        	exec?: {
        		// +usage=A command to be executed inside the container to assess its health. Each space delimited token of the command is a separate array element. Commands exiting 0 are considered to be successful probes, whilst all other exit codes are considered failures.
        		command: [...string]
        	}

        	// +usage=Instructions for assessing container health by executing an HTTP GET request. Either this attribute or the exec attribute or the tcpSocket attribute MUST be specified. This attribute is mutually exclusive with both the exec attribute and the tcpSocket attribute.
        	httpGet?: {
        		// +usage=The endpoint, relative to the port, to which the HTTP GET request should be directed.
        		path: string
        		// +usage=The TCP socket within the container to which the HTTP GET request should be directed.
        		port: int
        		httpHeaders?: [...{
        			name:  string
        			value: string
        		}]
        	}

        	// +usage=Instructions for assessing container health by probing a TCP socket. Either this attribute or the exec attribute or the httpGet attribute MUST be specified. This attribute is mutually exclusive with both the exec attribute and the httpGet attribute.
        	tcpSocket?: {
        		// +usage=The TCP socket within the container that should be probed to assess container health.
        		port: int
        	}

        	// +usage=Number of seconds after the container is started before the first probe is initiated.
        	initialDelaySeconds: *0 | int

        	// +usage=How often, in seconds, to execute the probe.
        	periodSeconds: *10 | int

        	// +usage=Number of seconds after which the probe times out.
        	timeoutSeconds: *1 | int

        	// +usage=Minimum consecutive successes for the probe to be considered successful after having failed.
        	successThreshold: *1 | int

        	// +usage=Number of consecutive failures required to determine the container is not alive (liveness probe) or not ready (readiness probe).
        	failureThreshold: *3 | int
        }
---
# Source: vela-core/templates/defwithtemplate/startup-probe.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/startup-probe.cue
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Add startup probe hooks for the specified container of K8s pod for your workload which follows the pod spec in path 'spec.template'.
  name: startup-probe
  namespace: default
spec:
  appliesToWorkloads:
    - deployments.apps
    - statefulsets.apps
    - daemonsets.apps
    - jobs.batch
  podDisruptive: true
  schematic:
    cue:
      template: |
        #StartupProbeParams: {
        	// +usage=Specify the name of the target container, if not set, use the component name
        	containerName: *"" | string
        	// +usage=Number of seconds after the container has started before liveness probes are initiated. Minimum value is 0.
        	initialDelaySeconds: *0 | int
        	// +usage=How often, in seconds, to execute the probe. Minimum value is 1.
        	periodSeconds: *10 | int
        	// +usage=Number of seconds after which the probe times out. Minimum value is 1.
        	timeoutSeconds: *1 | int
        	// +usage=Minimum consecutive successes for the probe to be considered successful after having failed.  Minimum value is 1.
        	successThreshold: *1 | int
        	// +usage=Minimum consecutive failures for the probe to be considered failed after having succeeded. Minimum value is 1.
        	failureThreshold: *3 | int
        	// +usage=Optional duration in seconds the pod needs to terminate gracefully upon probe failure. Set this value longer than the expected cleanup time for your process.
        	terminationGracePeriodSeconds?: int
        	// +usage=Instructions for assessing container startup status by executing a command. Either this attribute or the httpGet attribute or the grpc attribute or the tcpSocket attribute MUST be specified. This attribute is mutually exclusive with the httpGet attribute and the tcpSocket attribute and the gRPC attribute.
        	exec?: {
        		// +usage=A command to be executed inside the container to assess its health. Each space delimited token of the command is a separate array element. Commands exiting 0 are considered to be successful probes, whilst all other exit codes are considered failures.
        		command: [...string]
        	}
        	// +usage=Instructions for assessing container startup status by executing an HTTP GET request. Either this attribute or the exec attribute or the grpc attribute or the tcpSocket attribute MUST be specified. This attribute is mutually exclusive with the exec attribute and the tcpSocket attribute and the gRPC attribute.
        	httpGet?: {
        		// +usage=The endpoint, relative to the port, to which the HTTP GET request should be directed.
        		path?: string
        		// +usage=The port numer to access on the host or container.
        		port: int
        		// +usage=The hostname to connect to, defaults to the pod IP. You probably want to set "Host" in httpHeaders instead.
        		host?: string
        		// +usage=The Scheme to use for connecting to the host.
        		scheme?: *"HTTP" | "HTTPS"
        		// +usage=Custom headers to set in the request. HTTP allows repeated headers.
        		httpHeaders?: [...{
        			// +usage=The header field name
        			name: string
        			//+usage=The header field value
        			value: string
        		}]
        	}
        	// +usage=Instructions for assessing container startup status by probing a gRPC service. Either this attribute or the exec attribute or the grpc attribute or the httpGet attribute MUST be specified. This attribute is mutually exclusive with the exec attribute and the httpGet attribute and the tcpSocket attribute.
        	grpc?: {
        		// +usage=The port number of the gRPC service.
        		port: int
        		// +usage=The name of the service to place in the gRPC HealthCheckRequest
        		service?: string
        	}
        	// +usage=Instructions for assessing container startup status by probing a TCP socket. Either this attribute or the exec attribute or the tcpSocket attribute or the httpGet attribute MUST be specified. This attribute is mutually exclusive with the exec attribute and the httpGet attribute and the gRPC attribute.
        	tcpSocket?: {
        		// +usage=Number or name of the port to access on the container.
        		port: int
        		// +usage=Host name to connect to, defaults to the pod IP.
        		host?: string
        	}
        }
        PatchContainer: {
        	_params:         #StartupProbeParams
        	name:            _params.containerName
        	_baseContainers: context.output.spec.template.spec.containers
        	_matchContainers_: [ for _container_ in _baseContainers if _container_.name == name {_container_}]
        	if len(_matchContainers_) == 0 {
        		err: "container \(name) not found"
        	}
        	if len(_matchContainers_) > 0 {
        		startupProbe: {
        			if _params.exec != _|_ {
        				exec: _params.exec
        			}
        			if _params.httpGet != _|_ {
        				httpGet: _params.httpGet
        			}
        			if _params.grpc != _|_ {
        				grpc: _params.grpc
        			}
        			if _params.tcpSocket != _|_ {
        				tcpSocket: _params.tcpSocket
        			}
        			if _params.initialDelaySeconds != _|_ {
        				initialDelaySeconds: _params.initialDelaySeconds
        			}
        			if _params.periodSeconds != _|_ {
        				periodSeconds: _params.periodSeconds
        			}
        			if _params.tcpSocket != _|_ {
        				tcpSocket: _params.tcpSocket
        			}
        			if _params.timeoutSeconds != _|_ {
        				timeoutSeconds: _params.timeoutSeconds
        			}
        			if _params.successThreshold != _|_ {
        				successThreshold: _params.successThreshold
        			}
        			if _params.failureThreshold != _|_ {
        				failureThreshold: _params.failureThreshold
        			}
        			if _params.terminationGracePeriodSeconds != _|_ {
        				terminationGracePeriodSeconds: _params.terminationGracePeriodSeconds
        			}
        		}
        	}
        }

        patch: spec: template: spec: {
        	if parameter.probes == _|_ {
        		// +patchKey=name
        		containers: [{
        			PatchContainer & {_params: {
        				if parameter.containerName == "" {
        					containerName: context.name
        				}
        				if parameter.containerName != "" {
        					containerName: parameter.containerName
        				}
        				periodSeconds:                 parameter.periodSeconds
        				initialDelaySeconds:           parameter.initialDelaySeconds
        				timeoutSeconds:                parameter.timeoutSeconds
        				successThreshold:              parameter.successThreshold
        				failureThreshold:              parameter.failureThreshold
        				terminationGracePeriodSeconds: parameter.terminationGracePeriodSeconds
        				if parameter.exec != _|_ {
        					exec: parameter.exec
        				}
        				if parameter.httpGet != _|_ {
        					httpGet: parameter.httpGet
        				}
        				if parameter.grpc != _|_ {
        					grpc: parameter.grpc
        				}
        				if parameter.tcpSocket != _|_ {
        					tcpSocket: parameter.tcpSocket
        				}
        			}}
        		}]
        	}
        	if parameter.probes != _|_ {
        		// +patchKey=name
        		containers: [ for c in parameter.probes {
        			if c.name == "" {
        				err: "containerName must be set when specifying startup probe for multiple containers"
        			}
        			if c.name != "" {
        				PatchContainer & {_params: c}
        			}
        		}]
        	}
        }

        parameter: *#StartupProbeParams | close({
        	// +usage=Specify the startup probe for multiple containers
        	probes: [...#StartupProbeParams]
        })

        errs: [ for c in patch.spec.template.spec.containers if c.err != _|_ {c.err}]
---
# Source: vela-core/templates/defwithtemplate/storage.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/storage.cue
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Add storages on K8s pod for your workload which follows the pod spec in path 'spec.template'.
  name: storage
  namespace: default
spec:
  appliesToWorkloads:
    - deployments.apps
    - statefulsets.apps
    - daemonsets.apps
    - jobs.batch
  podDisruptive: true
  schematic:
    cue:
      template: |
        volumesList: [
        	if parameter.pvc != _|_ for v in parameter.pvc {
        		{
        			name: "pvc-" + v.name
        			persistentVolumeClaim: claimName: v.name
        		}
        	},
        	if parameter.configMap != _|_ for v in parameter.configMap if v.mountPath != _|_ {
        		{
        			name: "configmap-" + v.name
        			configMap: {
        				defaultMode: v.defaultMode
        				name:        v.name
        				if v.items != _|_ {
        					items: v.items
        				}
        			}
        		}
        	},
        	if parameter.secret != _|_ for v in parameter.secret if v.mountPath != _|_ {
        		{
        			name: "secret-" + v.name
        			secret: {
        				defaultMode: v.defaultMode
        				secretName:  v.name
        				if v.items != _|_ {
        					items: v.items
        				}
        			}
        		}
        	},
        	if parameter.emptyDir != _|_ for v in parameter.emptyDir {
        		{
        			name: "emptydir-" + v.name
        			emptyDir: medium: v.medium
        		}
        	},
        	if parameter.hostPath != _|_ for v in parameter.hostPath {
        		{
        			name: "hostpath-" + v.name
        			path: v.path
        		}
        	},
        ]

        volumeMountsList: [
        	if parameter.pvc != _|_ for v in parameter.pvc {
        		if v.volumeMode == "Filesystem" {
        			{
        				name:      "pvc-" + v.name
        				mountPath: v.mountPath
        				if v.subPath != _|_ {
        					subPath: v.subPath
        				}
        			}
        		}
        	},
        	if parameter.configMap != _|_ for v in parameter.configMap if v.mountPath != _|_ {
        		{
        			name:      "configmap-" + v.name
        			mountPath: v.mountPath
        			if v.subPath != _|_ {
        				subPath: v.subPath
        			}
        		}
        	},
        	if parameter.secret != _|_ for v in parameter.secret if v.mountPath != _|_ {
        		{
        			name:      "secret-" + v.name
        			mountPath: v.mountPath
        			if v.subPath != _|_ {
        				subPath: v.subPath
        			}
        		}
        	},
        	if parameter.emptyDir != _|_ for v in parameter.emptyDir {
        		{
        			name:      "emptydir-" + v.name
        			mountPath: v.mountPath
        			if v.subPath != _|_ {
        				subPath: v.subPath
        			}
        		}
        	},
        	if parameter.hostPath != _|_ for v in parameter.hostPath {
        		{
        			name:      "hostpath-" + v.name
        			mountPath: v.mountPath
        		}
        	},
        ]

        envList: [
        	if parameter.configMap != _|_ for v in parameter.configMap if v.mountToEnv != _|_ {
        		{
        			name: v.mountToEnv.envName
        			valueFrom: configMapKeyRef: {
        				name: v.name
        				key:  v.mountToEnv.configMapKey
        			}
        		}
        	},
        	if parameter.configMap != _|_ for v in parameter.configMap if v.mountToEnvs != _|_ for k in v.mountToEnvs {
        		{
        			name: k.envName
        			valueFrom: configMapKeyRef: {
        				name: v.name
        				key:  k.configMapKey
        			}
        		}
        	},
        	if parameter.secret != _|_ for v in parameter.secret if v.mountToEnv != _|_ {
        		{
        			name: v.mountToEnv.envName
        			valueFrom: secretKeyRef: {
        				name: v.name
        				key:  v.mountToEnv.secretKey
        			}
        		}
        	},
        	if parameter.secret != _|_ for v in parameter.secret if v.mountToEnvs != _|_ for k in v.mountToEnvs {
        		{
        			name: k.envName
        			valueFrom: secretKeyRef: {
        				name: v.name
        				key:  k.secretKey
        			}
        		}
        	},
        ]

        volumeDevicesList: *[
        			for v in parameter.pvc if v.volumeMode == "Block" {
        		{
        			name:       "pvc-" + v.name
        			devicePath: v.mountPath
        			if v.subPath != _|_ {
        				subPath: v.subPath
        			}
        		}
        	},
        ] | []

        deDupVolumesArray: [
        	for val in [
        		for i, vi in volumesList {
        			for j, vj in volumesList if j < i && vi.name == vj.name {
        				_ignore: true
        			}
        			vi
        		},
        	] if val._ignore == _|_ {
        		val
        	},
        ]

        patch: spec: template: spec: {
        	// +patchKey=name
        	volumes: deDupVolumesArray

        	containers: [{
        		// +patchKey=name
        		env: envList
        		// +patchKey=name
        		volumeDevices: volumeDevicesList
        		// +patchKey=name
        		volumeMounts: volumeMountsList
        	}, ...]

        }

        outputs: {
        	for v in parameter.pvc {
        		if v.mountOnly == false {
        			"pvc-\(v.name)": {
        				apiVersion: "v1"
        				kind:       "PersistentVolumeClaim"
        				metadata: name: v.name
        				spec: {
        					accessModes: v.accessModes
        					volumeMode:  v.volumeMode
        					if v.volumeName != _|_ {
        						volumeName: v.volumeName
        					}
        					if v.storageClassName != _|_ {
        						storageClassName: v.storageClassName
        					}

        					if v.resources.requests.storage == _|_ {
        						resources: requests: storage: "8Gi"
        					}
        					if v.resources.requests.storage != _|_ {
        						resources: requests: storage: v.resources.requests.storage
        					}
        					if v.resources.limits.storage != _|_ {
        						resources: limits: storage: v.resources.limits.storage
        					}
        					if v.dataSourceRef != _|_ {
        						dataSourceRef: v.dataSourceRef
        					}
        					if v.dataSource != _|_ {
        						dataSource: v.dataSource
        					}
        					if v.selector != _|_ {
        						dataSource: v.selector
        					}
        				}
        			}
        		}
        	}

        	for v in parameter.configMap {
        		if v.mountOnly == false {
        			"configmap-\(v.name)": {
        				apiVersion: "v1"
        				kind:       "ConfigMap"
        				metadata: name: v.name
        				if v.data != _|_ {
        					data: v.data
        				}
        			}
        		}
        	}

        	for v in parameter.secret {
        		if v.mountOnly == false {
        			"secret-\(v.name)": {
        				apiVersion: "v1"
        				kind:       "Secret"
        				metadata: name: v.name
        				if v.data != _|_ {
        					data: v.data
        				}
        				if v.stringData != _|_ {
        					stringData: v.stringData
        				}
        			}
        		}
        	}

        }

        parameter: {
        	// +usage=Declare pvc type storage
        	pvc?: [...{
        		name:              string
        		mountOnly:         *false | bool
        		mountPath:         string
        		subPath?:          string
        		volumeMode:        *"Filesystem" | string
        		volumeName?:       string
        		accessModes:       *["ReadWriteOnce"] | [...string]
        		storageClassName?: string
        		resources?: {
        			requests: storage: =~"^([1-9][0-9]{0,63})(E|P|T|G|M|K|Ei|Pi|Ti|Gi|Mi|Ki)$"
        			limits?: storage:  =~"^([1-9][0-9]{0,63})(E|P|T|G|M|K|Ei|Pi|Ti|Gi|Mi|Ki)$"
        		}
        		dataSourceRef?: {
        			name:     string
        			kind:     string
        			apiGroup: string
        		}
        		dataSource?: {
        			name:     string
        			kind:     string
        			apiGroup: string
        		}
        		selector?: {
        			matchLabels?: [string]: string
        			matchExpressions?: {
        				key: string
        				values: [...string]
        				operator: string
        			}
        		}
        	}]

        	// +usage=Declare config map type storage
        	configMap?: [...{
        		name:      string
        		mountOnly: *false | bool
        		mountToEnv?: {
        			envName:      string
        			configMapKey: string
        		}
        		mountToEnvs?: [...{
        			envName:      string
        			configMapKey: string
        		}]
        		mountPath?:  string
        		subPath?:    string
        		defaultMode: *420 | int
        		readOnly:    *false | bool
        		data?: {...}
        		items?: [...{
        			key:  string
        			path: string
        			mode: *511 | int
        		}]
        	}]

        	// +usage=Declare secret type storage
        	secret?: [...{
        		name:      string
        		mountOnly: *false | bool
        		mountToEnv?: {
        			envName:   string
        			secretKey: string
        		}
        		mountToEnvs?: [...{
        			envName:   string
        			secretKey: string
        		}]
        		mountPath:   string
        		subPath?:    string
        		defaultMode: *420 | int
        		readOnly:    *false | bool
        		stringData?: {...}
        		data?: {...}
        		items?: [...{
        			key:  string
        			path: string
        			mode: *511 | int
        		}]
        	}]

        	// +usage=Declare empty dir type storage
        	emptyDir?: [...{
        		name:      string
        		mountPath: string
        		subPath?:  string
        		medium:    *"" | "Memory"
        	}]

        	// +usage=Declare host path type storage
        	hostPath?: [...{
        		name:      string
        		path:      string
        		mountPath: string
        		type:      *"Directory" | "DirectoryOrCreate" | "FileOrCreate" | "File" | "Socket" | "CharDevice" | "BlockDevice"
        	}]
        }
---
# Source: vela-core/templates/defwithtemplate/topologyspreadconstraints.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/topologyspreadconstraints.cue
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Add topology spread constraints hooks for every container of K8s pod for your workload which follows the pod spec in path 'spec.template'.
  name: topologyspreadconstraints
  namespace: default
spec:
  appliesToWorkloads:
    - deployments.apps
    - statefulsets.apps
    - daemonsets.apps
    - jobs.batch
  podDisruptive: true
  schematic:
    cue:
      template: |
        constraintsArray: [
        	for v in parameter.constraints {
        		maxSkew:           v.maxSkew
        		topologyKey:       v.topologyKey
        		whenUnsatisfiable: v.whenUnsatisfiable
        		labelSelector:     v.labelSelector
        		if v.nodeAffinityPolicy != _|_ {
        			nodeAffinityPolicy: v.nodeAffinityPolicy
        		}
        		if v.nodeTaintsPolicy != _|_ {
        			nodeTaintsPolicy: v.nodeTaintsPolicy
        		}
        		if v.minDomains != _|_ {
        			minDomains: v.minDomains
        		}
        		if v.matchLabelKeys != _|_ {
        			matchLabelKeys: v.matchLabelKeys
        		}
        	},
        ]
        patch: spec: template: spec: topologySpreadConstraints: constraintsArray
        #labSelector: {
        	matchLabels?: [string]: string
        	matchExpressions?: [...{
        		key:      string
        		operator: *"In" | "NotIn" | "Exists" | "DoesNotExist"
        		values?: [...string]
        	}]
        }
        parameter: constraints: [...{
        	// +usage=Describe the degree to which Pods may be unevenly distributed
        	maxSkew: int
        	// +usage=Specify the key of node labels
        	topologyKey: string
        	// +usage=Indicate how to deal with a Pod if it doesn't satisfy the spread constraint
        	whenUnsatisfiable: *"DoNotSchedule" | "ScheduleAnyway"
        	// +usage=labelSelector to find matching Pods
        	labelSelector: #labSelector
        	// +usage=Indicate a minimum number of eligible domains
        	minDomains?: int
        	// +usage=A list of pod label keys to select the pods over which spreading will be calculated
        	matchLabelKeys?: [...string]
        	// +usage=Indicate how we will treat Pod's nodeAffinity/nodeSelector when calculating pod topology spread skew
        	nodeAffinityPolicy?: *"Honor" | "Ignore"
        	// +usage=Indicate how we will treat node taints when calculating pod topology spread skew
        	nodeTaintsPolicy?: *"Honor" | "Ignore"
        }]
---
# Source: vela-core/templates/defwithtemplate/volumes.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/deprecated/volumes.cue
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Add volumes on K8s pod for your workload which follows the pod spec in path 'spec.template'. This definition is DEPRECATED, please use 'storage' instead.
  labels:
    custom.definition.oam.dev/deprecated: "true"
  name: volumes
  namespace: default
spec:
  appliesToWorkloads:
    - deployments.apps
  podDisruptive: true
  schematic:
    cue:
      template: |
        patch: {
        	// +patchKey=name
        	spec: template: spec: volumes: [
        		if parameter.volumes != _|_ for v in parameter.volumes {
        			{
        				name: v.name
        				if v.type == "pvc" {
        					persistentVolumeClaim: claimName: v.claimName
        				}
        				if v.type == "configMap" {
        					configMap: {
        						defaultMode: v.defaultMode
        						name:        v.cmName
        						if v.items != _|_ {
        							items: v.items
        						}
        					}
        				}
        				if v.type == "secret" {
        					secret: {
        						defaultMode: v.defaultMode
        						secretName:  v.secretName
        						if v.items != _|_ {
        							items: v.items
        						}
        					}
        				}
        				if v.type == "emptyDir" {
        					emptyDir: medium: v.medium
        				}
        			}
        		},
        	]
        }

        parameter: {
        	// +usage=Declare volumes and volumeMounts
        	volumes?: [...{
        		name: string
        		// +usage=Specify volume type, options: "pvc","configMap","secret","emptyDir", default to emptyDir
        		type: *"emptyDir" | "pvc" | "configMap" | "secret"
        		if type == "pvc" {
        			claimName: string
        		}
        		if type == "configMap" {
        			defaultMode: *420 | int
        			cmName:      string
        			items?: [...{
        				key:  string
        				path: string
        				mode: *511 | int
        			}]
        		}
        		if type == "secret" {
        			defaultMode: *420 | int
        			secretName:  string
        			items?: [...{
        				key:  string
        				path: string
        				mode: *511 | int
        			}]
        		}
        		if type == "emptyDir" {
        			medium: *"" | "Memory"
        		}
        	}]
        }
---
# Source: vela-core/templates/admission-webhooks/validatingWebhookConfiguration.yaml
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  name: my-release-vela-core-admission
  namespace: default
webhooks:
  - clientConfig:
      caBundle: Cg==
      service:
        name: vela-core-webhook
        namespace: default
        path: /validating-core-oam-dev-v1alpha2-traitdefinitions
    failurePolicy: Ignore
    name: validating.core.oam.dev.v1alpha2.traitdefinitions
    sideEffects: None
    admissionReviewVersions:
      - v1beta1
      - v1
    rules:
      - apiGroups:
          - core.oam.dev
        apiVersions:
          - v1beta1
        operations:
          - CREATE
          - UPDATE
        resources:
          - traitdefinitions
        scope: Cluster
    timeoutSeconds: 5
  - clientConfig:
      caBundle: Cg==
      service:
        name: vela-core-webhook
        namespace: default
        path: /validating-core-oam-dev-v1beta1-applications
    failurePolicy: Ignore
    name: validating.core.oam.dev.v1beta1.applications
    admissionReviewVersions:
      - v1beta1
      - v1
    sideEffects: None
    rules:
      - apiGroups:
          - core.oam.dev
        apiVersions:
          - v1beta1
        operations:
          - CREATE
          - UPDATE
        resources:
          - applications
  - clientConfig:
      caBundle: Cg==
      service:
        name: vela-core-webhook
        namespace: default
        path: /validating-core-oam-dev-v1beta1-componentdefinitions
    failurePolicy: Ignore
    name: validating.core.oam-dev.v1beta1.componentdefinitions
    sideEffects: None
    admissionReviewVersions:
      - v1beta1
      - v1
    rules:
      - apiGroups:
          - core.oam.dev
        apiVersions:
          - v1beta1
        operations:
          - CREATE
          - UPDATE
        resources:
          - componentdefinitions
  - clientConfig:
      caBundle: Cg==
      service:
        name: vela-core-webhook
        namespace: default
        path: /validating-core-oam-dev-v1beta1-policydefinitions
    failurePolicy: Ignore
    name: validating.core.oam-dev.v1beta1.policydefinitions
    sideEffects: None
    admissionReviewVersions:
      - v1beta1
      - v1
    rules:
      - apiGroups:
          - core.oam.dev
        apiVersions:
          - v1beta1
        operations:
          - CREATE
          - UPDATE
        resources:
          - policydefinitions
---
# Source: vela-core/templates/defwithtemplate/apply-application-in-parallel.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/deprecated/apply-application-in-parallel.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/description: Apply components of an application in parallel for your workflow steps
  labels:
    custom.definition.oam.dev/deprecated: "true"
    custom.definition.oam.dev/scope: Application
    custom.definition.oam.dev/ui-hidden: "true"
  name: apply-application-in-parallel
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"vela/op"
        )

        output: op.#ApplyApplicationInParallel & {}
---
# Source: vela-core/templates/defwithtemplate/apply-application.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/deprecated/apply-application.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    custom.definition.oam.dev/category: Application Delivery
    definition.oam.dev/description: Apply application for your workflow steps, it has no arguments, should be used for custom steps before or after application applied.
  labels:
    custom.definition.oam.dev/deprecated: "true"
    custom.definition.oam.dev/scope: Application
    custom.definition.oam.dev/ui-hidden: "true"
  name: apply-application
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"vela/op"
        )

        // apply application
        output: op.#ApplyApplication & {}
---
# Source: vela-core/templates/defwithtemplate/apply-component.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/apply-component.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    custom.definition.oam.dev/category: Application Delivery
    definition.oam.dev/description: Apply a specific component and its corresponding traits in application
  labels:
    custom.definition.oam.dev/scope: Application
  name: apply-component
  namespace: default
spec:
  schematic:
    cue:
      template: |
        parameter: {
        	// +usage=Specify the component name to apply
        	component: string
        	// +usage=Specify the cluster
        	cluster: *"" | string
        	// +usage=Specify the namespace
        	namespace: *"" | string
        }
---
# Source: vela-core/templates/defwithtemplate/apply-deployment.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/apply-deployment.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    custom.definition.oam.dev/category: Resource Management
    definition.oam.dev/alias: ""
    definition.oam.dev/description: Apply deployment with specified image and cmd.
  name: apply-deployment
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"strconv"
        	"strings"
        	"vela/op"
        )

        output: op.#Apply & {
        	cluster: parameter.cluster
        	value: {
        		apiVersion: "apps/v1"
        		kind:       "Deployment"
        		metadata: {
        			name:      context.stepName
        			namespace: context.namespace
        		}
        		spec: {
        			selector: matchLabels: "workflow.oam.dev/step-name": "\(context.name)-\(context.stepName)"
        			replicas: parameter.replicas
        			template: {
        				metadata: labels: "workflow.oam.dev/step-name": "\(context.name)-\(context.stepName)"
        				spec: containers: [{
        					name:  context.stepName
        					image: parameter.image
        					if parameter["cmd"] != _|_ {
        						command: parameter.cmd
        					}
        				}]
        			}
        		}
        	}
        }
        wait: op.#ConditionalWait & {
        	continue: output.value.status.readyReplicas == parameter.replicas
        }
        parameter: {
        	image:    string
        	replicas: *1 | int
        	cluster:  *"" | string
        	cmd?: [...string]
        }
---
# Source: vela-core/templates/defwithtemplate/apply-object.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/apply-object.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    custom.definition.oam.dev/category: Resource Management
    definition.oam.dev/description: Apply raw kubernetes objects for your workflow steps
  name: apply-object
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"vela/op"
        )

        apply: op.#Apply & {
        	value:   parameter.value
        	cluster: parameter.cluster
        }
        parameter: {
        	// +usage=Specify Kubernetes native resource object to be applied
        	value: {...}
        	// +usage=The cluster you want to apply the resource to, default is the current control plane cluster
        	cluster: *"" | string
        }
---
# Source: vela-core/templates/defwithtemplate/apply-remaining.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/deprecated/apply-remaining.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    custom.definition.oam.dev/category: Application Delivery
    definition.oam.dev/description: Apply remaining components and traits
  labels:
    custom.definition.oam.dev/deprecated: "true"
    custom.definition.oam.dev/scope: Application
    custom.definition.oam.dev/ui-hidden: "true"
  name: apply-remaining
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"vela/op"
        )

        // apply remaining components and traits
        apply: op.#ApplyRemaining & {
        	parameter
        }

        parameter: {
        	// +usage=Declare the name of the component
        	exceptions?: [...string]
        }
---
# Source: vela-core/templates/defwithtemplate/apply-terraform-config.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/apply-terraform-config.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    custom.definition.oam.dev/category: Terraform
    definition.oam.dev/alias: ""
    definition.oam.dev/description: Apply terraform configuration in the step
  name: apply-terraform-config
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"vela/op"
        )

        apply: op.#Apply & {
        	value: {
        		apiVersion: "terraform.core.oam.dev/v1beta2"
        		kind:       "Configuration"
        		metadata: {
        			name:      "\(context.name)-\(context.stepName)"
        			namespace: context.namespace
        		}
        		spec: {
        			deleteResource: parameter.deleteResource
        			variable:       parameter.variable
        			forceDelete:    parameter.forceDelete
        			if parameter.source.path != _|_ {
        				path: parameter.source.path
        			}
        			if parameter.source.remote != _|_ {
        				remote: parameter.source.remote
        			}
        			if parameter.source.hcl != _|_ {
        				hcl: parameter.source.hcl
        			}
        			if parameter.providerRef != _|_ {
        				providerRef: parameter.providerRef
        			}
        			if parameter.jobEnv != _|_ {
        				jobEnv: parameter.jobEnv
        			}
        			if parameter.writeConnectionSecretToRef != _|_ {
        				writeConnectionSecretToRef: parameter.writeConnectionSecretToRef
        			}
        			if parameter.region != _|_ {
        				region: parameter.region
        			}
        		}
        	}
        }
        check: op.#ConditionalWait & {
        	continue: apply.value.status != _|_ && apply.value.status.apply != _|_ && apply.value.status.apply.state == "Available"
        }
        parameter: {
        	// +usage=specify the source of the terraform configuration
        	source: close({
        		// +usage=directly specify the hcl of the terraform configuration
        		hcl: string
        	}) | close({
        		// +usage=specify the remote url of the terraform configuration
        		remote: *"https://github.com/kubevela-contrib/terraform-modules.git" | string
        		// +usage=specify the path of the terraform configuration
        		path?: string
        	})
        	// +usage=whether to delete resource
        	deleteResource: *true | bool
        	// +usage=the variable in the configuration
        	variable: {...}
        	// +usage=this specifies the namespace and name of a secret to which any connection details for this managed resource should be written.
        	writeConnectionSecretToRef?: {
        		name:      string
        		namespace: *context.namespace | string
        	}
        	// +usage=providerRef specifies the reference to Provider
        	providerRef?: {
        		name:      string
        		namespace: *context.namespace | string
        	}
        	// +usage=region is cloud provider's region. It will override the region in the region field of providerRef
        	region?: string
        	// +usage=the envs for job
        	jobEnv?: {...}
        	// +usage=forceDelete will force delete Configuration no matter which state it is or whether it has provisioned some resources
        	forceDelete: *false | bool
        }
---
# Source: vela-core/templates/defwithtemplate/apply-terraform-provider.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/apply-terraform-provider.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    custom.definition.oam.dev/category: Terraform
    definition.oam.dev/alias: ""
    definition.oam.dev/description: Apply terraform provider config
  name: apply-terraform-provider
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"vela/op"
        	"strings"
        )

        config: op.#CreateConfig & {
        	name:      "\(context.name)-\(context.stepName)"
        	namespace: context.namespace
        	template:  "terraform-\(parameter.type)"
        	config: {
        		name: parameter.name
        		if parameter.type == "alibaba" {
        			ALICLOUD_ACCESS_KEY: parameter.accessKey
        			ALICLOUD_SECRET_KEY: parameter.secretKey
        			ALICLOUD_REGION:     parameter.region
        		}
        		if parameter.type == "aws" {
        			AWS_ACCESS_KEY_ID:     parameter.accessKey
        			AWS_SECRET_ACCESS_KEY: parameter.secretKey
        			AWS_DEFAULT_REGION:    parameter.region
        			AWS_SESSION_TOKEN:     parameter.token
        		}
        		if parameter.type == "azure" {
        			ARM_CLIENT_ID:       parameter.clientID
        			ARM_CLIENT_SECRET:   parameter.clientSecret
        			ARM_SUBSCRIPTION_ID: parameter.subscriptionID
        			ARM_TENANT_ID:       parameter.tenantID
        		}
        		if parameter.type == "baidu" {
        			BAIDUCLOUD_ACCESS_KEY: parameter.accessKey
        			BAIDUCLOUD_SECRET_KEY: parameter.secretKey
        			BAIDUCLOUD_REGION:     parameter.region
        		}
        		if parameter.type == "ec" {
        			EC_API_KEY: parameter.apiKey
        		}
        		if parameter.type == "gcp" {
        			GOOGLE_CREDENTIALS: parameter.credentials
        			GOOGLE_REGION:      parameter.region
        			GOOGLE_PROJECT:     parameter.project
        		}
        		if parameter.type == "tencent" {
        			TENCENTCLOUD_SECRET_ID:  parameter.secretID
        			TENCENTCLOUD_SECRET_KEY: parameter.secretKey
        			TENCENTCLOUD_REGION:     parameter.region
        		}
        		if parameter.type == "ucloud" {
        			UCLOUD_PRIVATE_KEY: parameter.privateKey
        			UCLOUD_PUBLIC_KEY:  parameter.publicKey
        			UCLOUD_PROJECT_ID:  parameter.projectID
        			UCLOUD_REGION:      parameter.region
        		}
        	}
        }
        read: op.#Read & {
        	value: {
        		apiVersion: "terraform.core.oam.dev/v1beta1"
        		kind:       "Provider"
        		metadata: {
        			name:      parameter.name
        			namespace: context.namespace
        		}
        	}
        }
        check: op.#ConditionalWait & {
        	if read.value.status != _|_ {
        		continue: read.value.status.state == "ready"
        	}
        	if read.value.status == _|_ {
        		continue: false
        	}
        }
        providerBasic: {
        	accessKey: string
        	secretKey: string
        	region:    string
        }
        #AlibabaProvider: {
        	providerBasic
        	type: "alibaba"
        	name: *"alibaba-provider" | string
        }
        #AWSProvider: {
        	providerBasic
        	token: *"" | string
        	type:  "aws"
        	name:  *"aws-provider" | string
        }
        #AzureProvider: {
        	subscriptionID: string
        	tenantID:       string
        	clientID:       string
        	clientSecret:   string
        	name:           *"azure-provider" | string
        }
        #BaiduProvider: {
        	providerBasic
        	type: "baidu"
        	name: *"baidu-provider" | string
        }
        #ECProvider: {
        	type:   "ec"
        	apiKey: *"" | string
        	name:   *"ec-provider" | string
        }
        #GCPProvider: {
        	credentials: string
        	region:      string
        	project:     string
        	type:        "gcp"
        	name:        *"gcp-provider" | string
        }
        #TencentProvider: {
        	secretID:  string
        	secretKey: string
        	region:    string
        	type:      "tencent"
        	name:      *"tencent-provider" | string
        }
        #UCloudProvider: {
        	publicKey:  string
        	privateKey: string
        	projectID:  string
        	region:     string
        	type:       "ucloud"
        	name:       *"ucloud-provider" | string
        }
        parameter: *#AlibabaProvider | #AWSProvider | #AzureProvider | #BaiduProvider | #ECProvider | #GCPProvider | #TencentProvider | #UCloudProvider
---
# Source: vela-core/templates/defwithtemplate/build-push-image.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/build-push-image.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    custom.definition.oam.dev/category: CI Integration
    definition.oam.dev/alias: ""
    definition.oam.dev/description: Build and push image from git url
  name: build-push-image
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"vela/op"
        	"encoding/json"
        	"strings"
        )

        url: {
        	if parameter.context.git != _|_ {
        		address: strings.TrimPrefix(parameter.context.git, "git://")
        		value:   "git://\(address)#refs/heads/\(parameter.context.branch)"
        	}
        	if parameter.context.git == _|_ {
        		value: parameter.context
        	}
        }
        kaniko: op.#Apply & {
        	value: {
        		apiVersion: "v1"
        		kind:       "Pod"
        		metadata: {
        			name:      "\(context.name)-\(context.stepSessionID)-kaniko"
        			namespace: context.namespace
        		}
        		spec: {
        			containers: [
        				{
        					args: [
        						"--dockerfile=\(parameter.dockerfile)",
        						"--context=\(url.value)",
        						"--destination=\(parameter.image)",
        						"--verbosity=\(parameter.verbosity)",
        						if parameter.platform != _|_ {
        							"--customPlatform=\(parameter.platform)"
        						},
        						if parameter.buildArgs != _|_ for arg in parameter.buildArgs {
        							"--build-arg=\(arg)"
        						},
        					]
        					image: parameter.kanikoExecutor
        					name:  "kaniko"
        					if parameter.credentials != _|_ && parameter.credentials.image != _|_ {
        						volumeMounts: [
        							{
        								mountPath: "/kaniko/.docker/"
        								name:      parameter.credentials.image.name
        							},
        						]
        					}
        					if parameter.credentials != _|_ && parameter.credentials.git != _|_ {
        						env: [
        							{
        								name: "GIT_TOKEN"
        								valueFrom: secretKeyRef: {
        									key:  parameter.credentials.git.key
        									name: parameter.credentials.git.name
        								}
        							},
        						]
        					}
        				},
        			]
        			if parameter.credentials != _|_ && parameter.credentials.image != _|_ {
        				volumes: [
        					{
        						name: parameter.credentials.image.name
        						secret: {
        							defaultMode: 420
        							items: [
        								{
        									key:  parameter.credentials.image.key
        									path: "config.json"
        								},
        							]
        							secretName: parameter.credentials.image.name
        						}
        					},
        				]
        			}
        			restartPolicy: "Never"
        		}
        	}
        }
        log: op.#Log & {
        	source: resources: [{
        		name:      "\(context.name)-\(context.stepSessionID)-kaniko"
        		namespace: context.namespace
        	}]
        }
        read: op.#Read & {
        	value: {
        		apiVersion: "v1"
        		kind:       "Pod"
        		metadata: {
        			name:      "\(context.name)-\(context.stepSessionID)-kaniko"
        			namespace: context.namespace
        		}
        	}
        }
        wait: op.#ConditionalWait & {
        	continue: read.value.status != _|_ && read.value.status.phase == "Succeeded"
        }
        #secret: {
        	name: string
        	key:  string
        }
        #git: {
        	git:    string
        	branch: *"master" | string
        }
        parameter: {
        	// +usage=Specify the kaniko executor image, default to oamdev/kaniko-executor:v1.9.1
        	kanikoExecutor: *"oamdev/kaniko-executor:v1.9.1" | string
        	// +usage=Specify the context to build image, you can use context with git and branch or directly specify the context, please refer to https://github.com/GoogleContainerTools/kaniko#kaniko-build-contexts
        	context: #git | string
        	// +usage=Specify the dockerfile
        	dockerfile: *"./Dockerfile" | string
        	// +usage=Specify the image
        	image: string
        	// +usage=Specify the platform to build
        	platform?: string
        	// +usage=Specify the build args
        	buildArgs?: [...string]
        	// +usage=Specify the credentials to access git and image registry
        	credentials?: {
        		// +usage=Specify the credentials to access git
        		git?: {
        			// +usage=Specify the secret name
        			name: string
        			// +usage=Specify the secret key
        			key: string
        		}
        		// +usage=Specify the credentials to access image registry
        		image?: {
        			// +usage=Specify the secret name
        			name: string
        			// +usage=Specify the secret key
        			key: *".dockerconfigjson" | string
        		}
        	}
        	// +usage=Specify the verbosity level
        	verbosity: *"info" | "panic" | "fatal" | "error" | "warn" | "debug" | "trace"
        }
---
# Source: vela-core/templates/defwithtemplate/check-metrics.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/check-metrics.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    custom.definition.oam.dev/category: Application Delivery
    definition.oam.dev/description: Verify application's metrics
  labels:
    custom.definition.oam.dev/catalog: Delivery
  name: check-metrics
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"vela/op"
        )

        check: op.#PromCheck & {
        	query:          parameter.query
        	metricEndpoint: parameter.metricEndpoint
        	condition:      parameter.condition
        	stepID:         context.stepSessionID
        	duration:       parameter.duration
        	failDuration:   parameter.failDuration
        }

        fail: op.#Steps & {
        	if check.failed != _|_ {
        		if check.failed == true {
        			breakWorkflow: op.#Fail & {
        				message: check.message
        			}
        		}
        	}
        }

        wait: op.#ConditionalWait & {
        	continue: check.result
        	if check.message != _|_ {
        		message: check.message
        	}
        }

        parameter: {
        	// +usage=Query is a raw prometheus query to perform
        	query: string
        	// +usage=The HTTP address and port of the prometheus server
        	metricEndpoint?: "http://prometheus-server.o11y-system.svc:9090" | string
        	// +usage=Condition is an expression which determines if a measurement is considered successful. eg: >=0.95
        	condition: string
        	// +usage=Duration defines the duration of time required for this step to be considered successful.
        	duration?: *"5m" | string
        	// +usage=FailDuration is the duration of time that, if the check fails, will result in the step being marked as failed.
        	failDuration?: *"2m" | string
        }
---
# Source: vela-core/templates/defwithtemplate/clean-jobs.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/clean-jobs.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    custom.definition.oam.dev/category: Resource Management
    definition.oam.dev/description: clean applied jobs in the cluster
  name: clean-jobs
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"vela/op"
        )

        parameter: {
        	labelselector?: {...}
        	namespace: *context.namespace | string
        }

        cleanJobs: op.#Delete & {
        	value: {
        		apiVersion: "batch/v1"
        		kind:       "Job"
        		metadata: {
        			name:      context.name
        			namespace: parameter.namespace
        		}
        	}
        	filter: {
        		namespace: parameter.namespace
        		if parameter.labelselector != _|_ {
        			matchingLabels: parameter.labelselector
        		}
        		if parameter.labelselector == _|_ {
        			matchingLabels: "workflow.oam.dev/name": context.name
        		}
        	}
        }

        cleanPods: op.#Delete & {
        	value: {
        		apiVersion: "v1"
        		kind:       "pod"
        		metadata: {
        			name:      context.name
        			namespace: parameter.namespace
        		}
        	}
        	filter: {
        		namespace: parameter.namespace
        		if parameter.labelselector != _|_ {
        			matchingLabels: parameter.labelselector
        		}
        		if parameter.labelselector == _|_ {
        			matchingLabels: "workflow.oam.dev/name": context.name
        		}
        	}
        }
---
# Source: vela-core/templates/defwithtemplate/collect-service-endpoints.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/collect-service-endpoints.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    custom.definition.oam.dev/category: Application Delivery
    definition.oam.dev/description: Collect service endpoints for the application.
  name: collect-service-endpoints
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"vela/op"
        	"vela/ql"
        	"strconv"
        )

        collect: ql.#CollectServiceEndpoints & {
        	app: {
        		name:      *context.name | string
        		namespace: *context.namespace | string
        		if parameter.name != _|_ {
        			name: parameter.name
        		}
        		if parameter.namespace != _|_ {
        			namespace: parameter.namespace
        		}
        		filter: {
        			if parameter.components != _|_ {
        				components: parameter.components
        			}
        		}
        	}
        } @step(1)

        outputs: {
        	eps_port_name_filtered: *[] | [...]
        	if parameter.portName == _|_ {
        		eps_port_name_filtered: collect.list
        	}
        	if parameter.portName != _|_ {
        		eps_port_name_filtered: [ for ep in collect.list if parameter.portName == ep.endpoint.portName {ep}]
        	}

        	eps_port_filtered: *[] | [...]
        	if parameter.port == _|_ {
        		eps_port_filtered: eps_port_name_filtered
        	}
        	if parameter.port != _|_ {
        		eps_port_filtered: [ for ep in eps_port_name_filtered if parameter.port == ep.endpoint.port {ep}]
        	}
        	eps:       eps_port_filtered
        	endpoints: *[] | [...]
        	if parameter.outer != _|_ {
        		tmps: [ for ep in eps {
        			ep
        			if ep.endpoint.inner == _|_ {
        				outer: true
        			}
        			if ep.endpoint.inner != _|_ {
        				outer: !ep.endpoint.inner
        			}
        		}]
        		endpoints: [ for ep in tmps if (!parameter.outer || ep.outer) {ep}]
        	}
        	if parameter.outer == _|_ {
        		endpoints: eps_port_filtered
        	}
        }

        wait: op.#ConditionalWait & {
        	continue: len(outputs.endpoints) > 0
        } @step(2)

        value: {
        	if len(outputs.endpoints) > 0 {
        		endpoint: outputs.endpoints[0].endpoint
        		_portStr: strconv.FormatInt(endpoint.port, 10)
        		url:      "\(parameter.protocal)://\(endpoint.host):\(_portStr)"
        	}
        }

        parameter: {
        	// +usage=Specify the name of the application
        	name?: string
        	// +usage=Specify the namespace of the application
        	namespace?: string
        	// +usage=Filter the component of the endpoints
        	components?: [...string]
        	// +usage=Filter the port of the endpoints
        	port?: int
        	// +usage=Filter the port name of the endpoints
        	portName?: string
        	// +usage=Filter the endpoint that are only outer
        	outer?: bool
        	// +usage=The protocal of endpoint url
        	protocal: *"http" | "https"
        }
---
# Source: vela-core/templates/defwithtemplate/create-config.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/create-config.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    custom.definition.oam.dev/category: Config Management
    definition.oam.dev/description: Create or update a config
  name: create-config
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"vela/op"
        )

        deploy: op.#CreateConfig & {
        	name: parameter.name
        	if parameter.namespace != _|_ {
        		namespace: parameter.namespace
        	}
        	if parameter.namespace == _|_ {
        		namespace: context.namespace
        	}
        	if parameter.template != _|_ {
        		template: parameter.template
        	}
        	config: parameter.config
        }
        parameter: {
        	//+usage=Specify the name of the config.
        	name: string

        	//+usage=Specify the namespace of the config.
        	namespace?: string

        	//+usage=Specify the template of the config.
        	template?: string

        	//+usage=Specify the content of the config.
        	config: {...}
        }
---
# Source: vela-core/templates/defwithtemplate/delete-config.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/delete-config.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    custom.definition.oam.dev/category: Config Management
    definition.oam.dev/description: Delete a config
  name: delete-config
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"vela/op"
        )

        deploy: op.#DeleteConfig & {
        	name: parameter.name
        	if parameter.namespace != _|_ {
        		namespace: parameter.namespace
        	}
        	if parameter.namespace == _|_ {
        		namespace: context.namespace
        	}
        }
        parameter: {
        	//+usage=Specify the name of the config.
        	name: string

        	//+usage=Specify the namespace of the config.
        	namespace?: string
        }
---
# Source: vela-core/templates/defwithtemplate/depends-on-app.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/depends-on-app.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    custom.definition.oam.dev/category: Application Delivery
    definition.oam.dev/description: Wait for the specified Application to complete.
  name: depends-on-app
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"vela/op"
        	"encoding/yaml"
        )

        dependsOn: op.#Read & {
        	value: {
        		apiVersion: "core.oam.dev/v1beta1"
        		kind:       "Application"
        		metadata: {
        			name:      parameter.name
        			namespace: parameter.namespace
        		}
        	}
        }
        load: op.#Steps & {
        	if dependsOn.err != _|_ {
        		configMap: op.#Read & {
        			value: {
        				apiVersion: "v1"
        				kind:       "ConfigMap"
        				metadata: {
        					name:      parameter.name
        					namespace: parameter.namespace
        				}
        			}
        		}         @step(1)
        		template: configMap.value.data["application"]
        		apply:    op.#Apply & {
        			value: yaml.Unmarshal(template)
        		}     @step(2)
        		wait: op.#ConditionalWait & {
        			continue: apply.value.status.status == "running"
        		} @step(3)
        	}

        	if dependsOn.err == _|_ {
        		wait: op.#ConditionalWait & {
        			continue: dependsOn.value.status.status == "running"
        		}
        	}
        }
        parameter: {
        	// +usage=Specify the name of the dependent Application
        	name: string
        	// +usage=Specify the namespace of the dependent Application
        	namespace: string
        }
---
# Source: vela-core/templates/defwithtemplate/deploy-cloud-resource.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/deploy-cloud-resource.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    custom.definition.oam.dev/category: Application Delivery
    definition.oam.dev/description: Deploy cloud resource and deliver secret to multi clusters.
  labels:
    custom.definition.oam.dev/scope: Application
  name: deploy-cloud-resource
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"vela/op"
        )

        app: op.#DeployCloudResource & {
        	env:    parameter.env
        	policy: parameter.policy
        	// context.namespace indicates the namespace of the app
        	namespace: context.namespace
        	// context.namespace indicates the name of the app
        	name: context.name
        }

        parameter: {
        	// +usage=Declare the name of the env-binding policy, if empty, the first env-binding policy will be used
        	policy: *"" | string
        	// +usage=Declare the name of the env in policy
        	env: string
        }
---
# Source: vela-core/templates/defwithtemplate/deploy.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/deploy.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    custom.definition.oam.dev/category: Application Delivery
    definition.oam.dev/description: A powerful and unified deploy step for components multi-cluster delivery with policies.
  labels:
    custom.definition.oam.dev/scope: Application
  name: deploy
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"vela/op"
        )

        if parameter.auto == false {
        	suspend: op.#Suspend & {message: "Waiting approval to the deploy step \"\(context.stepName)\""}
        }
        deploy: op.#Deploy & {
        	policies:                 parameter.policies
        	parallelism:              parameter.parallelism
        	ignoreTerraformComponent: parameter.ignoreTerraformComponent
        }
        parameter: {
        	//+usage=If set to false, the workflow will suspend automatically before this step, default to be true.
        	auto: *true | bool
        	//+usage=Declare the policies that used for this deployment. If not specified, the components will be deployed to the hub cluster.
        	policies: *[] | [...string]
        	//+usage=Maximum number of concurrent delivered components.
        	parallelism: *5 | int
        	//+usage=If set false, this step will apply the components with the terraform workload.
        	ignoreTerraformComponent: *true | bool
        }
---
# Source: vela-core/templates/defwithtemplate/deploy2env.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/deprecated/deploy2env.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/description: Deploy env binding component to target env
  labels:
    custom.definition.oam.dev/deprecated: "true"
    custom.definition.oam.dev/scope: Application
    custom.definition.oam.dev/ui-hidden: "true"
  name: deploy2env
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"vela/op"
        )

        app: op.#ApplyEnvBindApp & {
        	env:      parameter.env
        	policy:   parameter.policy
        	parallel: parameter.parallel
        	app:      context.name
        	// context.namespace indicates the namespace of the app
        	namespace: context.namespace
        }

        parameter: {
        	// +usage=Declare the name of the env-binding policy, if empty, the first env-binding policy will be used
        	policy: *"" | string
        	// +usage=Declare the name of the env in policy
        	env: string
        	// +usage=components are applied in parallel
        	parallel: *false | bool
        }
---
# Source: vela-core/templates/defwithtemplate/deploy2runtime.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/deprecated/deploy2runtime.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/description: Deploy application to runtime clusters
  labels:
    custom.definition.oam.dev/deprecated: "true"
    custom.definition.oam.dev/scope: Application
    custom.definition.oam.dev/ui-hidden: "true"
  name: deploy2runtime
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"vela/op"
        )

        app: op.#Steps & {
        	load: op.#Load
        	clusters: [...string]
        	if parameter.clusters == _|_ {
        		listClusters: op.#ListClusters
        		clusters:     listClusters.outputs.clusters
        	}
        	if parameter.clusters != _|_ {
        		clusters: parameter.clusters
        	}

        	apply: op.#Steps & {
        		for _, cluster_ in clusters {
        			for name, c in load.value {
        				"\(cluster_)-\(name)": op.#ApplyComponent & {
        					value:   c
        					cluster: cluster_
        				}
        			}
        		}
        	}
        }

        parameter: {
        	// +usage=Declare the runtime clusters to apply, if empty, all runtime clusters will be used
        	clusters?: [...string]
        }
---
# Source: vela-core/templates/defwithtemplate/export-data.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/export-data.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    custom.definition.oam.dev/category: Application Delivery
    definition.oam.dev/description: Export data to clusters specified by topology.
  labels:
    custom.definition.oam.dev/scope: Application
  name: export-data
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"vela/op"
        )

        object: {
        	apiVersion: "v1"
        	kind:       parameter.kind
        	metadata: {
        		name:      *context.name | string
        		namespace: *context.namespace | string
        		if parameter.name != _|_ {
        			name: parameter.name
        		}
        		if parameter.namespace != _|_ {
        			namespace: parameter.namespace
        		}
        	}
        	if parameter.kind == "ConfigMap" {
        		data: parameter.data
        	}
        	if parameter.kind == "Secret" {
        		stringData: parameter.data
        	}
        } @step(1)

        getPlacements: op.#GetPlacementsFromTopologyPolicies & {
        	policies: *[] | [...string]
        	if parameter.topology != _|_ {
        		policies: [parameter.topology]
        	}
        } @step(2)

        apply: op.#Steps & {
        	for p in getPlacements.placements {
        		(p.cluster): op.#Apply & {
        			value:   object
        			cluster: p.cluster
        		}
        	}
        } @step(3)

        parameter: {
        	// +usage=Specify the name of the export destination
        	name?: string
        	// +usage=Specify the namespace of the export destination
        	namespace?: string
        	// +usage=Specify the kind of the export destination
        	kind: *"ConfigMap" | "Secret"
        	// +usage=Specify the data to export
        	data: {}
        	// +usage=Specify the topology to export
        	topology?: string
        }
---
# Source: vela-core/templates/defwithtemplate/export-service.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/export-service.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    custom.definition.oam.dev/category: Application Delivery
    definition.oam.dev/description: Export service to clusters specified by topology.
  labels:
    custom.definition.oam.dev/scope: Application
  name: export-service
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"vela/op"
        )

        meta: {
        	name:      *context.name | string
        	namespace: *context.namespace | string
        	if parameter.name != _|_ {
        		name: parameter.name
        	}
        	if parameter.namespace != _|_ {
        		namespace: parameter.namespace
        	}
        }
        objects: [{
        	apiVersion: "v1"
        	kind:       "Service"
        	metadata:   meta
        	spec: {
        		type: "ClusterIP"
        		ports: [{
        			protocol:   "TCP"
        			port:       parameter.port
        			targetPort: parameter.targetPort
        		}]
        	}
        }, {
        	apiVersion: "v1"
        	kind:       "Endpoints"
        	metadata:   meta
        	subsets: [{
        		addresses: [{ip: parameter.ip}]
        		ports: [{port: parameter.targetPort}]
        	}]
        }] @step(1)

        getPlacements: op.#GetPlacementsFromTopologyPolicies & {
        	policies: *[] | [...string]
        	if parameter.topology != _|_ {
        		policies: [parameter.topology]
        	}
        } @step(2)

        apply: op.#Steps & {
        	for p in getPlacements.placements {
        		for o in objects {
        			"\(p.cluster)-\(o.kind)": op.#Apply & {
        				value:   o
        				cluster: p.cluster
        			}
        		}
        	}
        } @step(3)

        parameter: {
        	// +usage=Specify the name of the export destination
        	name?: string
        	// +usage=Specify the namespace of the export destination
        	namespace?: string
        	// +usage=Specify the ip to be export
        	ip: string
        	// +usage=Specify the port to be used in service
        	port: int
        	// +usage=Specify the port to be export
        	targetPort: int
        	// +usage=Specify the topology to export
        	topology?: string
        }
---
# Source: vela-core/templates/defwithtemplate/export2config.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/export2config.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    custom.definition.oam.dev/category: Resource Management
    definition.oam.dev/description: Export data to specified Kubernetes ConfigMap in your workflow.
  name: export2config
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"vela/op"
        )

        apply: op.#Apply & {
        	value: {
        		apiVersion: "v1"
        		kind:       "ConfigMap"
        		metadata: {
        			name: parameter.configName
        			if parameter.namespace != _|_ {
        				namespace: parameter.namespace
        			}
        			if parameter.namespace == _|_ {
        				namespace: context.namespace
        			}
        		}
        		data: parameter.data
        	}
        	cluster: parameter.cluster
        }
        parameter: {
        	// +usage=Specify the name of the config map
        	configName: string
        	// +usage=Specify the namespace of the config map
        	namespace?: string
        	// +usage=Specify the data of config map
        	data: {}
        	// +usage=Specify the cluster of the config map
        	cluster: *"" | string
        }
---
# Source: vela-core/templates/defwithtemplate/export2secret.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/export2secret.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    custom.definition.oam.dev/category: Resource Management
    definition.oam.dev/description: Export data to Kubernetes Secret in your workflow.
  name: export2secret
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"vela/op"
        	"encoding/base64"
        	"encoding/json"
        )

        secret: op.#Steps & {
        	data: *parameter.data | {}
        	if parameter.kind == "docker-registry" && parameter.dockerRegistry != _|_ {
        		registryData: auths: "\(parameter.dockerRegistry.server)": {
        			username: parameter.dockerRegistry.username
        			password: parameter.dockerRegistry.password
        			auth:     base64.Encode(null, "\(parameter.dockerRegistry.username):\(parameter.dockerRegistry.password)")
        		}
        		data: ".dockerconfigjson": json.Marshal(registryData)
        	}
        	apply: op.#Apply & {
        		value: {
        			apiVersion: "v1"
        			kind:       "Secret"
        			if parameter.type == _|_ && parameter.kind == "docker-registry" {
        				type: "kubernetes.io/dockerconfigjson"
        			}
        			if parameter.type != _|_ {
        				type: parameter.type
        			}
        			metadata: {
        				name: parameter.secretName
        				if parameter.namespace != _|_ {
        					namespace: parameter.namespace
        				}
        				if parameter.namespace == _|_ {
        					namespace: context.namespace
        				}
        			}
        			stringData: data
        		}
        		cluster: parameter.cluster
        	}
        }
        parameter: {
        	// +usage=Specify the name of the secret
        	secretName: string
        	// +usage=Specify the namespace of the secret
        	namespace?: string
        	// +usage=Specify the type of the secret
        	type?: string
        	// +usage=Specify the data of secret
        	data: {}
        	// +usage=Specify the cluster of the secret
        	cluster: *"" | string
        	// +usage=Specify the kind of the secret
        	kind: *"generic" | "docker-registry"
        	// +usage=Specify the docker data
        	dockerRegistry?: {
        		// +usage=Specify the username of the docker registry
        		username: string
        		// +usage=Specify the password of the docker registry
        		password: string
        		// +usage=Specify the server of the docker registry
        		server: *"https://index.docker.io/v1/" | string
        	}
        }
---
# Source: vela-core/templates/defwithtemplate/generate-jdbc-connection.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/generate-jdbc-connection.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    custom.definition.oam.dev/category: Terraform
    definition.oam.dev/description: Generate a JDBC connection based on Component of alibaba-rds
  name: generate-jdbc-connection
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"vela/op"
        	"encoding/base64"
        )

        output: op.#Read & {
        	value: {
        		apiVersion: "v1"
        		kind:       "Secret"
        		metadata: {
        			name: parameter.name
        			if parameter.namespace != _|_ {
        				namespace: parameter.namespace
        			}
        		}
        	}
        }
        dbHost:   op.#ConvertString & {bt: base64.Decode(null, output.value.data["DB_HOST"])}
        dbPort:   op.#ConvertString & {bt: base64.Decode(null, output.value.data["DB_PORT"])}
        dbName:   op.#ConvertString & {bt: base64.Decode(null, output.value.data["DB_NAME"])}
        username: op.#ConvertString & {bt: base64.Decode(null, output.value.data["DB_USER"])}
        password: op.#ConvertString & {bt: base64.Decode(null, output.value.data["DB_PASSWORD"])}

        env: [
        	{name: "url", value:      "jdbc://" + dbHost.str + ":" + dbPort.str + "/" + dbName.str + "?characterEncoding=utf8&useSSL=false"},
        	{name: "username", value: username.str},
        	{name: "password", value: password.str},
        ]

        parameter: {
        	// +usage=Specify the name of the secret generated by database component
        	name: string
        	// +usage=Specify the namespace of the secret generated by database component
        	namespace?: string
        }
---
# Source: vela-core/templates/defwithtemplate/list-config.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/list-config.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    custom.definition.oam.dev/category: Config Management
    definition.oam.dev/description: List the configs
  name: list-config
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"vela/op"
        )

        output: op.#ListConfig & {
        	if parameter.namespace != _|_ {
        		namespace: parameter.namespace
        	}
        	if parameter.namespace == _|_ {
        		namespace: context.namespace
        	}
        	template: parameter.template
        }
        parameter: {
        	//+usage=Specify the template of the config.
        	template: string
        	//+usage=Specify the namespace of the config.
        	namespace?: string
        }
---
# Source: vela-core/templates/defwithtemplate/notification.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/notification.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    custom.definition.oam.dev/category: External Integration
    definition.oam.dev/description: Send notifications to Email, DingTalk, Slack, Lark or webhook in your workflow.
  name: notification
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"vela/op"
        	"encoding/base64"
        )

        parameter: {
        	// +usage=Please fulfill its url and message if you want to send Lark messages
        	lark?: {
        		// +usage=Specify the the lark url, you can either sepcify it in value or use secretRef
        		url: close({
        			// +usage=the url address content in string
        			value: string
        		}) | close({
        			secretRef: {
        				// +usage=name is the name of the secret
        				name: string
        				// +usage=key is the key in the secret
        				key: string
        			}
        		})
        		// +usage=Specify the message that you want to sent, refer to [Lark messaging](https://open.feishu.cn/document/ukTMukTMukTM/ucTM5YjL3ETO24yNxkjN#8b0f2a1b).
        		message: {
        			// +usage=msg_type can be text, post, image, interactive, share_chat, share_user, audio, media, file, sticker
        			msg_type: string
        			// +usage=content should be json encode string
        			content: string
        		}
        	}
        	// +usage=Please fulfill its url and message if you want to send DingTalk messages
        	dingding?: {
        		// +usage=Specify the the dingding url, you can either sepcify it in value or use secretRef
        		url: close({
        			// +usage=the url address content in string
        			value: string
        		}) | close({
        			secretRef: {
        				// +usage=name is the name of the secret
        				name: string
        				// +usage=key is the key in the secret
        				key: string
        			}
        		})
        		// +usage=Specify the message that you want to sent, refer to [dingtalk messaging](https://developers.dingtalk.com/document/robots/custom-robot-access/title-72m-8ag-pqw)
        		message: {
        			// +usage=Specify the message content of dingtalk notification
        			text?: close({
        				content: string
        			})
        			// +usage=msgType can be text, link, mardown, actionCard, feedCard
        			msgtype: *"text" | "link" | "markdown" | "actionCard" | "feedCard"
        			#link: {
        				text?:       string
        				title?:      string
        				messageUrl?: string
        				picUrl?:     string
        			}

        			link?:     #link
        			markdown?: close({
        				text:  string
        				title: string
        			})
        			at?: close({
        				atMobiles?: [...string]
        				isAtAll?: bool
        			})
        			actionCard?: close({
        				text:           string
        				title:          string
        				hideAvatar:     string
        				btnOrientation: string
        				singleTitle:    string
        				singleURL:      string
        				btns?: [...close({
        					title:     string
        					actionURL: string
        				})]
        			})
        			feedCard?: close({
        				links: [...#link]
        			})
        		}
        	}
        	// +usage=Please fulfill its url and message if you want to send Slack messages
        	slack?: {
        		// +usage=Specify the the slack url, you can either sepcify it in value or use secretRef
        		url: close({
        			// +usage=the url address content in string
        			value: string
        		}) | close({
        			secretRef: {
        				// +usage=name is the name of the secret
        				name: string
        				// +usage=key is the key in the secret
        				key: string
        			}
        		})
        		// +usage=Specify the message that you want to sent, refer to [slack messaging](https://api.slack.com/reference/messaging/payload)
        		message: {
        			// +usage=Specify the message text for slack notification
        			text: string
        			blocks?: [...block]
        			attachments?: close({
        				blocks?: [...block]
        				color?: string
        			})
        			thread_ts?: string
        			// +usage=Specify the message text format in markdown for slack notification
        			mrkdwn?: *true | bool
        		}
        	}
        	// +usage=Please fulfill its from, to and content if you want to send email
        	email?: {
        		// +usage=Specify the email info that you want to send from
        		from: {
        			// +usage=Specify the email address that you want to send from
        			address: string
        			// +usage=The alias is the email alias to show after sending the email
        			alias?: string
        			// +usage=Specify the password of the email, you can either sepcify it in value or use secretRef
        			password: close({
        				// +usage=the password content in string
        				value: string
        			}) | close({
        				secretRef: {
        					// +usage=name is the name of the secret
        					name: string
        					// +usage=key is the key in the secret
        					key: string
        				}
        			})
        			// +usage=Specify the host of your email
        			host: string
        			// +usage=Specify the port of the email host, default to 587
        			port: *587 | int
        		}
        		// +usage=Specify the email address that you want to send to
        		to: [...string]
        		// +usage=Specify the content of the email
        		content: {
        			// +usage=Specify the subject of the email
        			subject: string
        			// +usage=Specify the context body of the email
        			body: string
        		}
        	}
        }

        block: {
        	type:      string
        	block_id?: string
        	elements?: [...{
        		type:       string
        		action_id?: string
        		url?:       string
        		value?:     string
        		style?:     string
        		text?:      textType
        		confirm?: {
        			title:   textType
        			text:    textType
        			confirm: textType
        			deny:    textType
        			style?:  string
        		}
        		options?: [...option]
        		initial_options?: [...option]
        		placeholder?:  textType
        		initial_date?: string
        		image_url?:    string
        		alt_text?:     string
        		option_groups?: [...option]
        		max_selected_items?: int
        		initial_value?:      string
        		multiline?:          bool
        		min_length?:         int
        		max_length?:         int
        		dispatch_action_config?: trigger_actions_on?: [...string]
        		initial_time?: string
        	}]
        }

        textType: {
        	type:      string
        	text:      string
        	emoji?:    bool
        	verbatim?: bool
        }

        option: {
        	text:         textType
        	value:        string
        	description?: textType
        	url?:         string
        }

        // send webhook notification
        ding: op.#Steps & {
        	if parameter.dingding != _|_ {
        		if parameter.dingding.url.value != _|_ {
        			ding1: op.#DingTalk & {
        				message: parameter.dingding.message
        				dingUrl: parameter.dingding.url.value
        			}
        		}
        		if parameter.dingding.url.secretRef != _|_ && parameter.dingding.url.value == _|_ {
        			read: op.#Read & {
        				value: {
        					apiVersion: "v1"
        					kind:       "Secret"
        					metadata: {
        						name:      parameter.dingding.url.secretRef.name
        						namespace: context.namespace
        					}
        				}
        			}

        			stringValue: op.#ConvertString & {bt: base64.Decode(null, read.value.data[parameter.dingding.url.secretRef.key])}
        			ding2:       op.#DingTalk & {
        				message: parameter.dingding.message
        				dingUrl: stringValue.str
        			}
        		}
        	}
        }

        lark: op.#Steps & {
        	if parameter.lark != _|_ {
        		if parameter.lark.url.value != _|_ {
        			lark1: op.#Lark & {
        				message: parameter.lark.message
        				larkUrl: parameter.lark.url.value
        			}
        		}
        		if parameter.lark.url.secretRef != _|_ && parameter.lark.url.value == _|_ {
        			read: op.#Read & {
        				value: {
        					apiVersion: "v1"
        					kind:       "Secret"
        					metadata: {
        						name:      parameter.lark.url.secretRef.name
        						namespace: context.namespace
        					}
        				}
        			}

        			stringValue: op.#ConvertString & {bt: base64.Decode(null, read.value.data[parameter.lark.url.secretRef.key])}
        			lark2:       op.#Lark & {
        				message: parameter.lark.message
        				larkUrl: stringValue.str
        			}
        		}
        	}
        }

        slack: op.#Steps & {
        	if parameter.slack != _|_ {
        		if parameter.slack.url.value != _|_ {
        			slack1: op.#Slack & {
        				message:  parameter.slack.message
        				slackUrl: parameter.slack.url.value
        			}
        		}
        		if parameter.slack.url.secretRef != _|_ && parameter.slack.url.value == _|_ {
        			read: op.#Read & {
        				value: {
        					kind:       "Secret"
        					apiVersion: "v1"
        					metadata: {
        						name:      parameter.slack.url.secretRef.name
        						namespace: context.namespace
        					}
        				}
        			}

        			stringValue: op.#ConvertString & {bt: base64.Decode(null, read.value.data[parameter.slack.url.secretRef.key])}
        			slack2:      op.#Slack & {
        				message:  parameter.slack.message
        				slackUrl: stringValue.str
        			}
        		}
        	}
        }

        email: op.#Steps & {
        	if parameter.email != _|_ {
        		if parameter.email.from.password.value != _|_ {
        			email1: op.#SendEmail & {
        				from: {
        					address: parameter.email.from.address
        					if parameter.email.from.alias != _|_ {
        						alias: parameter.email.from.alias
        					}
        					password: parameter.email.from.password.value
        					host:     parameter.email.from.host
        					port:     parameter.email.from.port
        				}
        				to:      parameter.email.to
        				content: parameter.email.content
        			}
        		}

        		if parameter.email.from.password.secretRef != _|_ && parameter.email.from.password.value == _|_ {
        			read: op.#Read & {
        				value: {
        					kind:       "Secret"
        					apiVersion: "v1"
        					metadata: {
        						name:      parameter.email.from.password.secretRef.name
        						namespace: context.namespace
        					}
        				}
        			}

        			stringValue: op.#ConvertString & {bt: base64.Decode(null, read.value.data[parameter.email.from.password.secretRef.key])}
        			email2:      op.#SendEmail & {
        				from: {
        					address: parameter.email.from.address
        					if parameter.email.from.alias != _|_ {
        						alias: parameter.email.from.alias
        					}
        					password: stringValue.str
        					host:     parameter.email.from.host
        					port:     parameter.email.from.port
        				}
        				to:      parameter.email.to
        				content: parameter.email.content
        			}
        		}
        	}
        }
---
# Source: vela-core/templates/defwithtemplate/print-message-in-status.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/print-message-in-status.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    custom.definition.oam.dev/category: Process Control
    definition.oam.dev/description: print message in workflow step status
  name: print-message-in-status
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"vela/op"
        )

        parameter: message: string

        msg: op.#Message & {
        	message: parameter.message
        }
---
# Source: vela-core/templates/defwithtemplate/read-config.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/read-config.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    custom.definition.oam.dev/category: Config Management
    definition.oam.dev/description: Read a config
  name: read-config
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"vela/op"
        )

        output: op.#ReadConfig & {
        	name: parameter.name
        	if parameter.namespace != _|_ {
        		namespace: parameter.namespace
        	}
        	if parameter.namespace == _|_ {
        		namespace: context.namespace
        	}
        }
        parameter: {
        	//+usage=Specify the name of the config.
        	name: string

        	//+usage=Specify the namespace of the config.
        	namespace?: string
        }
---
# Source: vela-core/templates/defwithtemplate/read-object.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/read-object.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    custom.definition.oam.dev/category: Resource Management
    definition.oam.dev/description: Read Kubernetes objects from cluster for your workflow steps
  name: read-object
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"vela/op"
        )

        output: {
        	if parameter.apiVersion == _|_ && parameter.kind == _|_ {
        		op.#Read & {
        			value: {
        				apiVersion: "core.oam.dev/v1beta1"
        				kind:       "Application"
        				metadata: {
        					name: parameter.name
        					if parameter.namespace != _|_ {
        						namespace: parameter.namespace
        					}
        				}
        			}
        			cluster: parameter.cluster
        		}
        	}
        	if parameter.apiVersion != _|_ || parameter.kind != _|_ {
        		op.#Read & {
        			value: {
        				apiVersion: parameter.apiVersion
        				kind:       parameter.kind
        				metadata: {
        					name: parameter.name
        					if parameter.namespace != _|_ {
        						namespace: parameter.namespace
        					}
        				}
        			}
        			cluster: parameter.cluster
        		}
        	}
        }
        parameter: {
        	// +usage=Specify the apiVersion of the object, defaults to 'core.oam.dev/v1beta1'
        	apiVersion?: string
        	// +usage=Specify the kind of the object, defaults to Application
        	kind?: string
        	// +usage=Specify the name of the object
        	name: string
        	// +usage=The namespace of the resource you want to read
        	namespace?: *"default" | string
        	// +usage=The cluster you want to apply the resource to, default is the current control plane cluster
        	cluster: *"" | string
        }
---
# Source: vela-core/templates/defwithtemplate/request.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/request.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    custom.definition.oam.dev/category: External Intergration
    definition.oam.dev/alias: ""
    definition.oam.dev/description: Send request to the url
  name: request
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"vela/op"
        	"encoding/json"
        )

        http: op.#HTTPDo & {
        	method: parameter.method
        	url:    parameter.url
        	request: {
        		if parameter.body != _|_ {
        			body: json.Marshal(parameter.body)
        		}
        		if parameter.header != _|_ {
        			header: parameter.header
        		}
        	}
        }
        fail: op.#Steps & {
        	if http.response.statusCode > 400 {
        		requestFail: op.#Fail & {
        			message: "request of \(parameter.url) is fail: \(http.response.statusCode)"
        		}
        	}
        }
        response: json.Unmarshal(http.response.body)
        parameter: {
        	url:    string
        	method: *"GET" | "POST" | "PUT" | "DELETE"
        	body?: {...}
        	header?: [string]: string
        }
---
# Source: vela-core/templates/defwithtemplate/share-cloud-resource.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/share-cloud-resource.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    custom.definition.oam.dev/category: Application Delivery
    definition.oam.dev/description: Sync secrets created by terraform component to runtime clusters so that runtime clusters can share the created cloud resource.
  labels:
    custom.definition.oam.dev/scope: Application
  name: share-cloud-resource
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"vela/op"
        )

        app: op.#ShareCloudResource & {
        	env:        parameter.env
        	policy:     parameter.policy
        	placements: parameter.placements
        	// context.namespace indicates the namespace of the app
        	namespace: context.namespace
        	// context.namespace indicates the name of the app
        	name: context.name
        }

        parameter: {
        	// +usage=Declare the location to bind
        	placements: [...{
        		namespace?: string
        		cluster?:   string
        	}]
        	// +usage=Declare the name of the env-binding policy, if empty, the first env-binding policy will be used
        	policy: *"" | string
        	// +usage=Declare the name of the env in policy
        	env: string
        }
---
# Source: vela-core/templates/defwithtemplate/step-group.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/step-group.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    custom.definition.oam.dev/category: Process Control
    definition.oam.dev/description: A special step that you can declare 'subSteps' in it, 'subSteps' is an array containing any step type whose valid parameters do not include the `step-group` step type itself. The sub steps were executed in parallel.
  name: step-group
  namespace: default
spec:
  schematic:
    cue:
      template: |
        // no parameters, the nop only to make the template not empty or it's invalid
        nop: {}
---
# Source: vela-core/templates/defwithtemplate/suspend.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/suspend.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    custom.definition.oam.dev/category: Process Control
    definition.oam.dev/description: Suspend the current workflow, it can be resumed by 'vela workflow resume' command.
  name: suspend
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"vela/op"
        )

        suspend: op.#Suspend & {
        	if parameter.duration != _|_ {
        		duration: parameter.duration
        	}
        	if parameter.message != _|_ {
        		message: parameter.message
        	}
        }

        parameter: {
        	// +usage=Specify the wait duration time to resume workflow such as "30s", "1min" or "2m15s"
        	duration?: string
        	// +usage=The suspend message to show
        	message?: string
        }
---
# Source: vela-core/templates/defwithtemplate/vela-cli.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/vela-cli.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    custom.definition.oam.dev/category: Scripts & Commands
    definition.oam.dev/description: Run a vela command
  name: vela-cli
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"vela/op"
        )

        mountsArray: [
        	if parameter.storage != _|_ && parameter.storage.secret != _|_ for v in parameter.storage.secret {
        		{
        			name:      "secret-" + v.name
        			mountPath: v.mountPath
        			if v.subPath != _|_ {
        				subPath: v.subPath
        			}
        		}
        	},
        	if parameter.storage != _|_ && parameter.storage.hostPath != _|_ for v in parameter.storage.hostPath {
        		{
        			name:      "hostpath-" + v.name
        			mountPath: v.mountPath
        		}
        	},
        ]

        volumesList: [
        	if parameter.storage != _|_ && parameter.storage.secret != _|_ for v in parameter.storage.secret {
        		{
        			name: "secret-" + v.name
        			secret: {
        				defaultMode: v.defaultMode
        				secretName:  v.secretName
        				if v.items != _|_ {
        					items: v.items
        				}
        			}
        		}
        		if parameter.storage != _|_ && parameter.storage.hostPath != _|_ for v in parameter.storage.hostPath {
        			{
        				name: "hostpath-" + v.name
        				path: v.path
        			}
        		}
        	},
        ]

        deDupVolumesArray: [
        	for val in [
        		for i, vi in volumesList {
        			for j, vj in volumesList if j < i && vi.name == vj.name {
        				_ignore: true
        			}
        			vi
        		},
        	] if val._ignore == _|_ {
        		val
        	},
        ]

        job: op.#Apply & {
        	value: {
        		apiVersion: "batch/v1"
        		kind:       "Job"
        		metadata: {
        			name: "\(context.name)-\(context.stepName)-\(context.stepSessionID)"
        			if parameter.serviceAccountName == "kubevela-vela-core" {
        				namespace: "vela-system"
        			}
        			if parameter.serviceAccountName != "kubevela-vela-core" {
        				namespace: context.namespace
        			}
        		}
        		spec: {
        			backoffLimit: 3
        			template: {
        				metadata: labels: "workflow.oam.dev/step-name": "\(context.name)-\(context.stepName)"
        				spec: {
        					containers: [
        						{
        							name:         "\(context.name)-\(context.stepName)-\(context.stepSessionID)-job"
        							image:        parameter.image
        							command:      parameter.command
        							volumeMounts: mountsArray
        						},
        					]
        					restartPolicy:  "Never"
        					serviceAccount: parameter.serviceAccountName
        					volumes:        deDupVolumesArray
        				}
        			}
        		}
        	}
        }

        log: op.#Log & {
        	source: resources: [{labelSelector: "workflow.oam.dev/step-name": "\(context.name)-\(context.stepName)"}]
        }

        fail: op.#Steps & {
        	if job.value.status.failed != _|_ {
        		if job.value.status.failed > 2 {
        			breakWorkflow: op.#Fail & {
        				message: "failed to execute vela command"
        			}
        		}
        	}
        }

        wait: op.#ConditionalWait & {
        	continue: job.value.status.succeeded != _|_ && job.value.status.succeeded > 0
        }

        parameter: {
        	// +usage=Specify the name of the addon.
        	addonName: string
        	// +usage=Specify the vela command
        	command: [...string]
        	// +usage=Specify the image
        	image: *"oamdev/vela-cli:v1.6.4" | string
        	// +usage=specify serviceAccountName want to use
        	serviceAccountName: *"kubevela-vela-core" | string
        	storage?: {
        		// +usage=Mount Secret type storage
        		secret?: [...{
        			name:        string
        			mountPath:   string
        			subPath?:    string
        			defaultMode: *420 | int
        			secretName:  string
        			items?: [...{
        				key:  string
        				path: string
        				mode: *511 | int
        			}]
        		}]
        		// +usage=Declare host path type storage
        		hostPath?: [...{
        			name:      string
        			path:      string
        			mountPath: string
        			type:      *"Directory" | "DirectoryOrCreate" | "FileOrCreate" | "File" | "Socket" | "CharDevice" | "BlockDevice"
        		}]
        	}
        }
---
# Source: vela-core/templates/defwithtemplate/webhook.yaml
# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/webhook.cue
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    custom.definition.oam.dev/category: External Intergration
    definition.oam.dev/description: Send a POST request to the specified Webhook URL. If no request body is specified, the current Application body will be sent by default.
  name: webhook
  namespace: default
spec:
  schematic:
    cue:
      template: |
        import (
        	"vela/op"
        	"encoding/json"
        	"encoding/base64"
        )

        data: op.#Steps & {
        	if parameter.data == _|_ {
        		read: op.#Read & {
        			value: {
        				apiVersion: "core.oam.dev/v1beta1"
        				kind:       "Application"
        				metadata: {
        					name:      context.name
        					namespace: context.namespace
        				}
        			}
        		}      @step(1)
        		value: json.Marshal(read.value) @step(2)
        	}
        	if parameter.data != _|_ {
        		value: json.Marshal(parameter.data) @step(3)
        	}
        }
        webhook: op.#Steps & {
        	if parameter.url.value != _|_ {
        		http: op.#HTTPPost & {
        			url: parameter.url.value
        			request: {
        				body: data.value
        				header: "Content-Type": "application/json"
        			}
        		} @step(4)
        	}
        	if parameter.url.secretRef != _|_ && parameter.url.value == _|_ {
        		read: op.#Read & {
        			value: {
        				apiVersion: "v1"
        				kind:       "Secret"
        				metadata: {
        					name:      parameter.url.secretRef.name
        					namespace: context.namespace
        				}
        			}
        		} @step(5)

        		stringValue: op.#ConvertString & {bt: base64.Decode(null, read.value.data[parameter.url.secretRef.key])} @step(6)
        		http:        op.#HTTPPost & {
        			url: stringValue.str
        			request: {
        				body: data.value
        				header: "Content-Type": "application/json"
        			}
        		} @step(7)
        	}
        }

        parameter: {
        	// +usage=Specify the webhook url
        	url: close({
        		value: string
        	}) | close({
        		secretRef: {
        			// +usage=name is the name of the secret
        			name: string
        			// +usage=key is the key in the secret
        			key: string
        		}
        	})
        	// +usage=Specify the data you want to send
        	data?: {...}
        }
---
# Source: vela-core/templates/admission-webhooks/job-patch/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name:  my-release-vela-core-admission
  namespace: default
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade,post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: vela-core-admission
    helm.sh/chart: vela-core-1.9.11
    app.kubernetes.io/name: vela-core
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.9.11"
    app.kubernetes.io/managed-by: Helm
---
# Source: vela-core/templates/cluster-gateway/job-patch.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-vela-core-cluster-gateway-admission
  namespace: default
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade,post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: vela-core-cluster-gateway-admission
    helm.sh/chart: vela-core-1.9.11
    app.kubernetes.io/name: vela-core
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.9.11"
    app.kubernetes.io/managed-by: Helm
---
# Source: vela-core/templates/admission-webhooks/job-patch/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name:  my-release-vela-core-admission
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade,post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: vela-core-admission
    helm.sh/chart: vela-core-1.9.11
    app.kubernetes.io/name: vela-core
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.9.11"
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - admissionregistration.k8s.io
    resources:
      - validatingwebhookconfigurations
      - mutatingwebhookconfigurations
    verbs:
      - get
      - update
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - get
      - update
---
# Source: vela-core/templates/admission-webhooks/job-patch/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name:  my-release-vela-core-admission
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade,post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: vela-core-admission
    helm.sh/chart: vela-core-1.9.11
    app.kubernetes.io/name: vela-core
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.9.11"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: my-release-vela-core-admission
subjects:
  - kind: ServiceAccount
    name: my-release-vela-core-admission
    namespace: default
---
# Source: vela-core/templates/admission-webhooks/job-patch/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name:  my-release-vela-core-admission
  namespace: default
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade,post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: vela-core-admission
    helm.sh/chart: vela-core-1.9.11
    app.kubernetes.io/name: vela-core
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.9.11"
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - create
---
# Source: vela-core/templates/cluster-gateway/job-patch.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: my-release-vela-core-cluster-gateway-admission
  namespace: default
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade,post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: vela-core-cluster-gateway-admission
    helm.sh/chart: vela-core-1.9.11
    app.kubernetes.io/name: vela-core
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.9.11"
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - create
---
# Source: vela-core/templates/admission-webhooks/job-patch/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name:  my-release-vela-core-admission
  namespace: default
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade,post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: vela-core-admission
    helm.sh/chart: vela-core-1.9.11
    app.kubernetes.io/name: vela-core
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.9.11"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: my-release-vela-core-admission
subjects:
  - kind: ServiceAccount
    name: my-release-vela-core-admission
    namespace: default
---
# Source: vela-core/templates/cluster-gateway/job-patch.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: my-release-vela-core-cluster-gateway-admission
  namespace: default
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade,post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: vela-core-cluster-gateway-admission
    helm.sh/chart: vela-core-1.9.11
    app.kubernetes.io/name: vela-core
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.9.11"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: my-release-vela-core-cluster-gateway-admission
subjects:
  - kind: ServiceAccount
    name: my-release-vela-core-cluster-gateway-admission
    namespace: default
---
# Source: vela-core/templates/test/test-application.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "my-release-application-test"
  namespace: default
  annotations:
    "helm.sh/hook": test
    helm.sh/hook-delete-policy: hook-succeeded
spec:
  serviceAccountName: my-release-vela-core
  containers:
    - name: my-release-application-test
      image: oamdev/alpine-k8s:1.18.2
      imagePullPolicy: IfNotPresent
      command:
        - /bin/bash
        - -ec
        - |

          set -e

          echo "Waiting application is ready..."

          echo "waiting for application being Ready"
          kubectl -n default wait --for=condition=Ready applications.core.oam.dev helm-test-vela-app --timeout=3m
          echo "application is Ready"

          # wait for deploy being created
          echo "waiting for deployment being available"
          kubectl -n default wait --for=condition=available deployments helm-test-express-server --timeout 3m
          echo "deployment being available"

          echo "Application and its components are created"
  restartPolicy: Never
---
# Source: vela-core/templates/admission-webhooks/job-patch/job-createSecret.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name:  my-release-vela-core-admission-create
  namespace: default
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: vela-core-admission-create
    helm.sh/chart: vela-core-1.9.11
    app.kubernetes.io/name: vela-core
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.9.11"
    app.kubernetes.io/managed-by: Helm
spec:
  template:
    metadata:
      name:  my-release-vela-core-admission-create
      labels:
        app: vela-core-admission-create
        helm.sh/chart: vela-core-1.9.11
        app.kubernetes.io/name: vela-core
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/version: "1.9.11"
        app.kubernetes.io/managed-by: Helm
    spec:
      containers:
        - name: create
          image: oamdev/kube-webhook-certgen:v2.4.1
          imagePullPolicy: IfNotPresent
          args:
            - create
            - --host=vela-core-webhook,vela-core-webhook.default.svc
            - --namespace=default
            - --secret-name=my-release-vela-core-admission
            - --key-name=tls.key
            - --cert-name=tls.crt
      restartPolicy: OnFailure
      serviceAccountName: my-release-vela-core-admission
      securityContext:
        runAsGroup: 2000
        runAsNonRoot: true
        runAsUser: 2000
---
# Source: vela-core/templates/admission-webhooks/job-patch/job-patchWebhook.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name:  my-release-vela-core-admission-patch
  namespace: default
  annotations:
    "helm.sh/hook": post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: vela-core-admission-patch
    helm.sh/chart: vela-core-1.9.11
    app.kubernetes.io/name: vela-core
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.9.11"
    app.kubernetes.io/managed-by: Helm
spec:
  template:
    metadata:
      name:  my-release-vela-core-admission-patch
      labels:
        app: vela-core-admission-patch
        helm.sh/chart: vela-core-1.9.11
        app.kubernetes.io/name: vela-core
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/version: "1.9.11"
        app.kubernetes.io/managed-by: Helm
    spec:
      containers:
        - name: patch
          image: oamdev/kube-webhook-certgen:v2.4.1
          imagePullPolicy: IfNotPresent
          args:
            - patch
            - --webhook-name=my-release-vela-core-admission
            - --namespace=default
            - --secret-name=my-release-vela-core-admission
            - --patch-failure-policy=Fail
      restartPolicy: OnFailure
      serviceAccountName: my-release-vela-core-admission
      securityContext:
        runAsGroup: 2000
        runAsNonRoot: true
        runAsUser: 2000
---
# Source: vela-core/templates/cluster-gateway/job-patch.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: my-release-vela-core-cluster-gateway-tls-secret-create
  namespace: default
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: my-release-vela-core-cluster-gateway-tls-secret-create
    helm.sh/chart: vela-core-1.9.11
    app.kubernetes.io/name: vela-core
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.9.11"
    app.kubernetes.io/managed-by: Helm
spec:
  template:
    metadata:
      name: my-release-vela-core-cluster-gateway-tls-secret-create
      labels:
        app: my-release-vela-core-cluster-gateway-tls-secret-create
        helm.sh/chart: vela-core-1.9.11
        app.kubernetes.io/name: vela-core
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/version: "1.9.11"
        app.kubernetes.io/managed-by: Helm
    spec:
      containers:
      - name: create
        image: oamdev/kube-webhook-certgen:v2.4.1
        imagePullPolicy: IfNotPresent
        args:
          - create
          - --host=my-release-cluster-gateway-service,my-release-cluster-gateway-service.default.svc
          - --namespace=default
          - --secret-name=my-release-vela-core-cluster-gateway-tls-v2
          - --cert-name=tls.crt
          - --key-name=tls.key
      restartPolicy: OnFailure
      serviceAccountName: my-release-vela-core-cluster-gateway-admission
      securityContext:
        runAsGroup: 2000
        runAsNonRoot: true
        runAsUser: 2000
---
# Source: vela-core/templates/cluster-gateway/job-patch.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: my-release-vela-core-cluster-gateway-tls-secret-patch
  namespace: default
  annotations:
    "helm.sh/hook": post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app: my-release-vela-core-cluster-gateway-tls-secret-patch
    helm.sh/chart: vela-core-1.9.11
    app.kubernetes.io/name: vela-core
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.9.11"
    app.kubernetes.io/managed-by: Helm
spec:
  template:
    metadata:
      name: my-release-vela-core-cluster-gateway-tls-secret-patch
      labels:
        app: my-release-vela-core-cluster-gateway-tls-secret-patch
        helm.sh/chart: vela-core-1.9.11
        app.kubernetes.io/name: vela-core
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/version: "1.9.11"
        app.kubernetes.io/managed-by: Helm
    spec:
      containers:
      - name: patch
        image: oamdev/cluster-gateway:v1.9.0-alpha.2
        imagePullPolicy: IfNotPresent
        command:
          - /patch
        args:
          - --secret-namespace=default
          - --secret-name=my-release-vela-core-cluster-gateway-tls-v2
      restartPolicy: OnFailure
      serviceAccountName: my-release-vela-core
      securityContext:
        runAsGroup: 2000
        runAsNonRoot: true
        runAsUser: 2000
---
# Source: vela-core/templates/test/test-application.yaml
apiVersion: core.oam.dev/v1beta1
kind: Application
metadata:
  annotations:
    helm.sh/hook: test-success
    helm.sh/hook-delete-policy: hook-succeeded
  name: helm-test-vela-app
  namespace: default
spec:
  components:
    - name: helm-test-express-server
      type: webservice
      properties:
        image: oamdev/hello-world:v1
        port: 8000
