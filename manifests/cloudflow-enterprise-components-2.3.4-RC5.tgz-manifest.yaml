---
# Source: cloudflow-enterprise-components/charts/enterprise-suite/templates/backend-prometheus-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus-server
  labels:
    app.kubernetes.io/name: lightbend-console
    helm.sh/chart: enterprise-suite-1.2.12
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    com.lightbend.cloudflow/instance-id: my-release
    app.kubernetes.io/component: console-backend
---
# Source: cloudflow-enterprise-components/charts/enterprise-suite/templates/kube-state-metrics-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus-kube-state-metrics
  labels:
    app.kubernetes.io/name: lightbend-console
    helm.sh/chart: enterprise-suite-1.2.12
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    com.lightbend.cloudflow/instance-id: my-release
    app.kubernetes.io/component: kube-state-metrics
---
# Source: cloudflow-enterprise-components/templates/03-cloudflow-enterprise-operator-serviceaccount.yaml
kind: ServiceAccount
apiVersion: v1
metadata:
  name: cloudflow-enterprise-operator
  labels: 
    app.kubernetes.io/name: my-release
    app.kubernetes.io/component: cloudflow
automountServiceAccountToken: true
---
# Source: cloudflow-enterprise-components/charts/enterprise-suite/templates/commercial-credentials.yaml
apiVersion: v1
kind: Secret
metadata:
  name: commercial-credentials
  labels:
    app.kubernetes.io/name: lightbend-console
    helm.sh/chart: enterprise-suite-1.2.12
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    com.lightbend.cloudflow/instance-id: my-release
    app.kubernetes.io/component: credentials
type: "kubernetes.io/dockerconfigjson"
data:
  .dockerconfigjson: eyJhdXRocyI6IHsiY29tbWVyY2lhbC1yZWdpc3RyeS5saWdodGJlbmQuY29tIjogeyJhdXRoIjogIkpTRnpLRHh1YVd3K0tUb2xJWE1vUEc1cGJENHAifX19
---
# Source: cloudflow-enterprise-components/charts/enterprise-suite/templates/backend-configmap-api-default-monitors.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/name: lightbend-console
    helm.sh/chart: enterprise-suite-1.2.12
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    com.lightbend.cloudflow/instance-id: my-release
    app.kubernetes.io/component: console-backend
  name: console-api-default-monitors
data:
  default-monitors.json: |
    {
      "monitors": {
        "_": {
          "kube_container_restarting": {
            "monitorVersion": "1",
            "model": "threshold",
            "parameters": {
              "metric": "kube_pod_container_restarts_rate",
              "window": "15m",
              "confidence": "0.25",
              "severity": {
                "warning": {
                  "comparator": ">",
                  "threshold": "0"
                }
              },
              "alertSummary": "container restarting rapidly",
              "alertDescription": "container {{$labels.container}} in pod {{$labels.pod}} of {{$labels.es_workload}} restarting rapidly",
              "monitorDescription": "Detects if a container in a pod is continuously restarting. Can be due to getting OOM killed by OS or due to a bug that causes a crash shortly after starting.",
              "metricUnits": "restarts/s",
              "docLink": "https://developer.lightbend.com/docs/console/current/monitors/kubernetes.html#kube-container-restarting"
            }
          },
          "kube_pod_not_ready": {
            "monitorVersion": "1",
            "model": "threshold",
            "parameters": {
              "metric": "kube_pod_ready",
              "window": "1m",
              "confidence": "5e-324",
              "severity": {
                "warning": {
                  "comparator": "<",
                  "threshold": "1"
                }
              },
              "alertSummary": "pod not ready",
              "alertDescription": "pod {{$labels.pod}} on {{$labels.es_workload}} not ready",
              "monitorDescription": "Detects if the pod's readiness probe failed. Use Kubernetes to get more information on the pod's status.",
              "metricUnits": "boolean",
              "docLink": "https://developer.lightbend.com/docs/console/current/monitors/kubernetes.html#kube-pod-not-ready"
            }
          },
          "scrape_time": {
            "monitorVersion": "1",
            "model": "sma",
            "parameters": {
              "metric": "scrape_duration_seconds",
              "period": "15m",
              "minval": "3",
              "window": "15m",
              "confidence": "0.5",
              "severity": {
                "warning": {
                  "numsigma": "3"
                }
              },
              "alertSummary": "scrape time anomalous",
              "alertDescription": "{{$labels.instance}} has anomalous scrape_duration_seconds"
            }
          },
          "lightbend_monitor_api_errors": {
            "monitorVersion": "1",
            "model": "threshold",
            "parameters": {
              "metric": "es_monitor_api_last_error",
              "window": "1m",
              "confidence": "1",
              "severity": {
                "warning": {
                  "comparator": ">",
                  "threshold": "0"
                }
              },
              "alertSummary": "lightbend console monitor errors",
              "alertDescription": "Lightbend console api on {{$labels.instance}} has monitor errors"
            }
          },
          "akka_inbox_queue_time": {
            "monitorVersion": "1",
            "model": "growth",
            "parameters": {
              "metric": "akka_actor_mailbox_time_ms",
              "filters": {
                "quantile": "0.99"
              },
              "period": "5m",
              "minslope": "0.2",
              "confidence": "0.5",
              "severity": {
                "warning": {
                  "window": "1m"
                }
              },
              "alertSummary": "actor inbox time growing",
              "alertDescription": "actor {{$labels.actor}} in {{$labels.app}} on {{$labels.instance}} has a rapidly growing mailbox time",
              "monitorDescription": "Detects if amount of time messages spend in an inbox is growing.",
              "metricUnits": "ms",
              "docLink": "https://developer.lightbend.com/docs/console/current/monitors/akka.html#akka-inbox-queue-time"
            }
          },
          "akka_processing_time": {
            "monitorVersion": "1",
            "model": "growth",
            "parameters": {
              "metric": "akka_actor_processing_time_ms",
              "filters": {
                "quantile": "0.99"
              },
              "period": "5m",
              "minslope": "0.2",
              "confidence": "0.5",
              "severity": {
                "warning": {
                  "window": "1m"
                }
              },
              "alertSummary": "actor processing time is growing",
              "alertDescription": "actor {{$labels.actor}} in {{$labels.app}} on {{$labels.instance}} has growing processing time",
              "monitorDescription": "Detects if message processing time is growing.",
              "metricUnits": "ms",
              "docLink": "https://developer.lightbend.com/docs/console/current/monitors/akka.html#akka-processing-time"
            }
          },
          "prometheus_rule_evaluation_failures": {
            "monitorVersion": "1",
            "model": "threshold",
            "parameters": {
              "metric": "prometheus_rule_evaluation_failures_rate",
              "window": "5m",
              "confidence": "1",
              "severity": {
                "warning": {
                  "comparator": ">",
                  "threshold": "0"
                }
              },
              "alertSummary": "Prometheus rule failures",
              "alertDescription": "Prometheus has {{$value}} rules failing"
            }
          },
          "prometheus_target_too_many_metrics": {
            "monitorVersion": "1",
            "model": "threshold",
            "parameters": {
              "metric": "prometheus_target_scrapes_exceeded_sample_limit_rate",
              "window": "5m",
              "confidence": "1",
              "severity": {
                "warning": {
                  "comparator": ">",
                  "threshold": "0"
                }
              },
              "alertSummary": "Prometheus target over limit",
              "alertDescription": "Prometheus target at {{$labels.instance}} has too many metrics"
            }
          },
          "prometheus_tsdb_reloads_failures": {
            "monitorVersion": "1",
            "model": "threshold",
            "parameters": {
              "metric": "prometheus_tsdb_reloads_failures_rate",
              "window": "5m",
              "confidence": "1",
              "severity": {
                "critical": {
                  "comparator": ">=",
                  "threshold": "1"
                }
              },
              "alertSummary": "Prometheus tsdb reload failing",
              "alertDescription": "Prometheus had {{$value}} reload failures"
            }
          },
          "prometheus_target_down": {
            "monitorVersion": "1",
            "model": "threshold",
            "parameters": {
              "metric": "up",
              "window": "5m",
              "confidence": "1",
              "severity": {
                "warning": {
                  "comparator": "!=",
                  "threshold": "1"
                }
              },
              "alertSummary": "metrics target down",
              "alertDescription": "cannot connect to {{$labels.instance}} metrics endpoint for {{$labels.job}} data"
            }
          },
          "prometheus_config_reload_failed": {
            "monitorVersion": "1",
            "model": "threshold",
            "parameters": {
              "metric": "prometheus_config_last_reload_successful",
              "window": "5m",
              "confidence": "1",
              "severity": {
                "critical": {
                  "comparator": "!=",
                  "threshold": "1"
                }
              },
              "alertSummary": "prometheus bad config",
              "alertDescription": "current config for prometheus has errors, will prevent restarts"
            }
          },
          "prometheus_scrape_time": {
            "monitorVersion": "1",
            "model": "threshold",
            "parameters": {
              "metric": "prometheus_target_sync_percent",
              "window": "5m",
              "confidence": "1",
              "severity": {
                "warning": {
                  "comparator": ">",
                  "threshold": "75"
                }
              },
              "alertSummary": "prometheus has long scrape times",
              "alertDescription": "prometheus is taking {{$value}}% of the {{$labels.interval}} interval to get {{$labels.scrape_job}} metrics from {{$labels.instance}}"
            }
          },
          "akka_http_server_response_time": {
            "monitorVersion": "1",
            "model": "growth",
            "parameters": {
              "metric": "akka_http_server_response_time_ms",
              "filters": {
                "quantile": "0.99"
              },
              "period": "5m",
              "minslope": "0.1",
              "confidence": "1",
              "severity": {
                "warning": {
                  "window": "1m"
                }
              },
              "alertSummary": "HTTP server response time is anomalous",
              "alertDescription": "{{$labels.app}} on {{$labels.instance}} has unusual response time",
              "monitorDescription": "Detects if the HTTP server response times have been increasing.",
              "metricUnits": "ms",
              "docLink": "https://developer.lightbend.com/docs/console/current/monitors/akka.html#akka-http-server-response-time"
            }
          },
          "akka_http_client_response_time": {
            "monitorVersion": "1",
            "model": "growth",
            "parameters": {
              "metric": "akka_http_client_service_response_time_ms",
              "filters": {
                "quantile": "0.99"
              },
              "period": "5m",
              "minslope": "0.1",
              "confidence": "1",
              "severity": {
                "warning": {
                  "window": "1m"
                }
              },
              "alertSummary": "HTTP client response time is anomalous",
              "alertDescription": "{{$labels.app}} on {{$labels.instance}} has a growing response time",
              "monitorDescription": "Detects if the Akka HTTP client request times have been increasing.",
              "metricUnits": "ms",
              "docLink": "https://developer.lightbend.com/docs/console/current/monitors/akka.html#akka-http-client-response-time"
            }
          },
          "akka_http_server_5xx": {
            "monitorVersion": "1",
            "model": "threshold",
            "parameters": {
              "metric": "akka_http_http_server_responses_5xx_rate",
              "window": "1m",
              "confidence": "5e-324",
              "severity": {
                "warning": {
                  "comparator": ">",
                  "threshold": "0"
                }
              },
              "alertSummary": "HTTP 5xx errors",
              "alertDescription": "HTTP server at {{$labels.instance}} has 5xx errors",
              "monitorDescription": "Detects if Akka HTTP server is producing 5xx errors.",
              "metricUnits": "errors/s",
              "docLink": "https://developer.lightbend.com/docs/console/current/monitors/akka.html#akka-http-server-5xx"
            }
          },
          "play_http_client_response_time": {
            "monitorVersion": "1",
            "model": "growth",
            "parameters": {
              "metric": "play_http_client_service_response_time_ms",
              "filters": {
                "quantile": "0.99"
              },
              "period": "5m",
              "minslope": "0.1",
              "confidence": "1",
              "severity": {
                "warning": {
                  "window": "1m"
                }
              },
              "alertSummary": "HTTP client response time is growing",
              "alertDescription": "{{$labels.app}} on {{$labels.instance}} has a growing response time",
              "monitorDescription": "Detects if the Play HTTP client request times have been increasing.",
              "metricUnits": "ms"
            }
          },
          "lagom_circuit_breaker_state": {
            "monitorVersion": "1",
            "model": "threshold",
            "parameters": {
              "metric": "lagom_circuit_breaker_state",
              "window": "1m",
              "confidence": "5e-324",
              "severity": {
                "warning": {
                  "comparator": "==",
                  "threshold": "2"
                },
                "critical": {
                  "comparator": "==",
                  "threshold": "1"
                }
              },
              "alertSummary": "Circuit breaker tripped",
              "alertDescription": "Circuit breaker {{$labels.circuit_breaker}} tripped on {{$labels.instance}}",
              "monitorDescription": "Detects if a circuit breaker in a Lagom application has tripped."
            }
          },
          "pipelines_kafka_consumer_throughput": {
            "monitorVersion": "1",
            "model": "sma",
            "parameters": {
              "metric": "kafka_consumer_topic_consumed_rate",
              "period": "15m",
              "minval": "1000",
              "window": "15m",
              "confidence": "1",
              "severity": {
                "warning": {
                  "numsigma": "3"
                }
              },
              "alertSummary": "Kafka consumer throughput is anomalous",
              "alertDescription": "{{$labels.es_workload}} has unusual throughput on {{$labels.topic}}"
            }
          },
          "pipelines_kafka_producer_throughput": {
            "monitorVersion": "1",
            "model": "sma",
            "parameters": {
              "metric": "kafka_producer_topic_send_rate",
              "period": "15m",
              "minval": "1000",
              "window": "15m",
              "confidence": "1",
              "severity": {
                "warning": {
                  "numsigma": "3"
                }
              },
              "alertSummary": "Kafka producer throughput is anomalous",
              "alertDescription": "{{$labels.es_workload}} has unusual throughput on {{$labels.topic}}"
            }
          },
          "pipelines_kafka_consumer_lag": {
            "monitorVersion": "1",
            "model": "growth",
            "parameters": {
              "metric": "kafka_consumer_topic_lag",
              "period": "5m",
              "minslope": "0.1",
              "confidence": "1",
              "severity": {
                "warning": {
                  "window": "1m"
                }
              },
              "alertSummary": "consumergroup falling behind",
              "alertDescription": "{{$labels.es_workload}} has lag on {{$labels.topic}}",
              "monitorDescription": "Detects if consumer topic lag is increasing.",
              "metricUnits": "records"
            }
          },
  
          "kube_container_restarts": null,
          "kube_pod_not_running": null,
          "akka_inbox_growth": null,
          "zookeeper_latency": null,
          "zookeeper_connections": null,
          "zookeeper_pending_syncs": null,
          "zookeeper_open_file_descriptor": null,
          "cassandra_write_latency": null,
          "cassandra_read_latency": null,
          "redis_keyspace_miss": null,
          "redis_evictions": null,
          "redis_commands_processed": null,
          "redis_connections": null,
          "kafka_under_replicated_partitions": null,
          "memcached_miss_ratio": null,
          "memcached_current_connections": null,
          "memcached_evictions": null,
          "kafka_consumer_throughput": null,
          "kafka_producer_throughput": null,
          "kafka_consumer_lag": null,
  
          "kafka_consumergroup_lag": null,
          "kafka_incoming_messages": null,
          "kafka_offline_partition": null,
          "kube_workload_generation_lag": null
        }
      }
    }
---
# Source: cloudflow-enterprise-components/charts/enterprise-suite/templates/backend-configmap-api-prometheus.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/name: lightbend-console
    helm.sh/chart: enterprise-suite-1.2.12
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    com.lightbend.cloudflow/instance-id: my-release
    app.kubernetes.io/component: console-backend
  name: console-api-prometheus
data:
  prometheus.yml: |
    {{/*
       * Note to Helm devs: this is NOT a Helm template, but rather a plain go template used
       * in console-api to generate the final prometheus.yml.
       * sanitizeForPrometheusLabel is a function in console-api that replaces [.-] with "_"
       */}}
    global:
      # dev mode
      scrape_interval: 10s
      evaluation_interval: 10s
  
    rule_files:
      - /etc/config/rules/*.yaml
  
    {{ if .AlertmanagerAddresses }}
    alerting:
      alertmanagers:
      - static_configs:
        - targets:
          {{ range $index, $address := .AlertmanagerAddresses }}
          - {{ $address }}
          {{ end }}
    {{ end }}
  
    scrape_configs:
  
      # Container metrics from kubelet cadvisor endpoint
  
      - job_name: 'kubernetes-cadvisor'
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
  
        kubernetes_sd_configs:
          - role: node
  
        honor_labels: true
  
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - target_label: __address__
            replacement: kubernetes.default.svc:443
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
          - source_labels: [__meta_kubernetes_node_name]
            target_label: node_name
          - source_labels: [__meta_kubernetes_node_address_InternalIP]
            target_label: node_ip
  
        # These are metric_relabel_configs since the source labels come from the scraped `/metrics`.
        metric_relabel_configs:
          # 'pod_name' for k8s < 1.16, 'pod' for k8s > 1.13 (overlap for 1.14, 1.15)
          - source_labels: [pod_name]
            target_label: kubernetes_pod_name
          - source_labels: [pod]
            target_label: kubernetes_pod_name
          # pause containers have all the network stats for a pod
          # 'container_name' for k8s < 1.16, 'container' for k8s > 1.13 (overlap for 1.14, 1.15)
          - source_labels: [container_name, __name__]
            regex: POD;container_(network).*
            target_label: container_name
          - source_labels: [container, __name__]
            regex: POD;container_(network).*
            target_label: container
          # drop all other pause container stats
          - source_labels: [container_name]
            regex: POD
            action: drop
          - source_labels: [container]
            regex: POD
            action: drop
          # drop system containers with no name
          # Need both labels to be empty or we'll drop everything
          - source_labels: [container_name, container]
            regex: ^;$
            action: drop
          # drop high cardinality debug metrics
          - source_labels: [__name__]
            regex: container_(network_tcp_usage_total|tasks_state|cpu_usage_seconds_total|memory_failures_total|network_udp_usage_total)
            action: drop
          # extract workload from workload-hash-rand
          - source_labels: [kubernetes_pod_name, es_workload]
            regex: (.*)-[456789bcdf]{4,10}-[bcdfghjklmnpqrstvwxz2456789]{5};
            target_label: es_workload
          # extract workload from workload-rand
          - source_labels: [kubernetes_pod_name, es_workload]
            regex: (.*)-[bcdfghjklmnpqrstvwxz2456789]{5};
            target_label: es_workload
          # spark workload
          - source_labels: [kubernetes_pod_name, es_workload]
            regex: (.*)-[0-9]{13}-(exec|driver)-[0-9]+;
            replacement: ${1}-${2}
            target_label: es_workload
          # extract workload from workload-num
          - source_labels: [kubernetes_pod_name, es_workload]
            regex: (.*)-[0-9]{1,4};
            target_label: es_workload
          # copy pod name to workload
          - source_labels: [kubernetes_pod_name, es_workload]
            regex: (.+);
            target_label: es_workload
  
    {{ .MonitorTypeRules | indent 6 }}
  
      # kubelet metrics (not for monitoring, just grafana data)
      - job_name: 'kubelet'
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
  
        kubernetes_sd_configs:
          - role: node
  
        honor_labels: true
  
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - target_label: __address__
            replacement: kubernetes.default.svc:443
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/${1}/proxy/metrics
          - source_labels: [__meta_kubernetes_node_name]
            target_label: node_name
          - source_labels: [__meta_kubernetes_node_address_InternalIP]
            target_label: node_ip
  
  
      # kube-state-metrics
  
      - job_name: 'kube-state-metrics'
  
        kubernetes_sd_configs:
          - role: endpoints
  
        honor_labels: true
  
        relabel_configs:
          # Only scrape kube-state-metrics.
          - source_labels: [__meta_kubernetes_endpoints_name]
            regex: prometheus-kube-state-metrics
            action: keep
  
          # Only scrape the kube-state-metrics in the local Console namespace.
          - source_labels: [__meta_kubernetes_namespace]
            regex: {{ .Namespace }}
            action: keep
  
          # Preserve labels on service.
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
  
          # Map service name to kubernetes_name label.
          - source_labels: [__meta_kubernetes_service_name]
            action: replace
            target_label: kubernetes_name
  
        # es_workload labels:
        # These are metric_relabel_configs since the source labels come from the scraped `/metrics`.
        metric_relabel_configs:
          - source_labels: [created_by_kind, pod, es_workload]
            regex: 'ReplicaSet;(.+)-[^-]+-[^-]+;'
            target_label: es_workload
          - source_labels: [created_by_kind, pod, es_workload]
            regex: 'StatefulSet;(.+)-[^-]+;'
            target_label: es_workload
          - source_labels: [created_by_kind, pod, es_workload]
            regex: 'DaemonSet;(.+)-[^-]+;'
            target_label: es_workload
          - source_labels: [created_by_kind, pod, es_workload]
            regex: 'Job;(.+)-[^-]+;'
            target_label: es_workload
          - source_labels: [created_by_kind, pod, es_workload]
            regex: 'ReplicationController;(.+)-[^-]+;'
            target_label: es_workload
          - source_labels: [created_by_kind, pod, es_workload]
            regex: '.none.;(.+);'
            target_label: es_workload
          - source_labels: [deployment, es_workload]
            regex: '(.+);'
            target_label: es_workload
          - source_labels: [daemonset, es_workload]
            regex: '(.+);'
            target_label: es_workload
          # extract workload from workload-hash-rand
          - source_labels: [pod, es_workload]
            regex: (.*)-[456789bcdf]{4,10}-[bcdfghjklmnpqrstvwxz2456789]{5};
            target_label: es_workload
          # extract workload from workload-rand
          - source_labels: [pod, es_workload]
            regex: (.*)-[bcdfghjklmnpqrstvwxz2456789]{5};
            target_label: es_workload
          # spark workload
          - source_labels: [pod, es_workload]
            regex: (.*)-[0-9]{13}-(exec|driver)-[0-9]+;
            replacement: ${1}-${2}
            target_label: es_workload
          # extract workload from workload-num
          - source_labels: [pod, es_workload]
            regex: (.*)-[0-9]{1,4};
            target_label: es_workload
          # copy pod name to workload
          - source_labels: [pod, es_workload]
            regex: (.+);
            target_label: es_workload
          - source_labels: [node]
            target_label: node_name
  
          # Copy pod name to kubernetes_pod_name, to match all the other service discovery sections.
          # The UI uses kubernetes_pod_name for displaying pod health.
          - source_labels: [pod]
            target_label: kubernetes_pod_name
  
  
    {{ .MonitorTypeRules | indent 6 }}
  
      # Scrape config for service endpoints.
      #
      # The relabeling allows the actual service scrape endpoint to be configured
      # via the following annotations:
      #
      # * `{{ .PrometheusDomain }}/scrape`: Only scrape services that have a value of `true`
      # * `{{ .PrometheusDomain }}/scheme`: If the metrics endpoint is secured then you will need
      # to set this to `https` & most likely set the `tls_config` of the scrape config.
      # * `{{ .PrometheusDomain }}/path`: If the metrics path is not `/metrics` override this.
      # * `{{ .PrometheusDomain }}/port`: If the metrics are exposed on a different port to the
      # service then set this appropriately.
  
      - job_name: 'kubernetes-services'
        kubernetes_sd_configs:
          - role: endpoints
  
        honor_labels: true
  
        relabel_configs:
          # these have dedicated scrape sections:
          - source_labels: [__meta_kubernetes_endpoints_name]
            regex: prometheus-node-exporter
            action: drop
          - source_labels: [__meta_kubernetes_endpoints_name]
            regex:  prometheus-kube-state-metrics
            action: drop
  
          # _scrape _scheme _path _port annotation triggers:
          - source_labels: [__meta_kubernetes_service_annotation_{{ sanitizeForPrometheusLabel .PrometheusDomain }}_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_service_annotation_{{ sanitizeForPrometheusLabel .PrometheusDomain }}_scheme]
            action: replace
            target_label: __scheme__
            regex: (https?)
          - source_labels: [__meta_kubernetes_service_annotation_{{ sanitizeForPrometheusLabel .PrometheusDomain }}_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
  
          # Keep only the ports ending in the suffix `metrics` if prometheus.io/port is unspecified.
          - source_labels: [__meta_kubernetes_service_annotation_{{ sanitizeForPrometheusLabel .PrometheusDomain }}_port, __meta_kubernetes_endpoint_port_name]
            action: keep
            regex: ^(.+;.*)|(;.*metrics)$
  
          # Set port from prometheus.io/port if it's set.
          - source_labels: [__address__, __meta_kubernetes_service_annotation_{{ sanitizeForPrometheusLabel .PrometheusDomain }}_port]
            action: replace
            # Extract IP from address. Address might not contain a port in some cases.
            regex: ([^:]+):?(?:\d*);(\d+)
            replacement: ${1}:${2}
            target_label: __address__
  
          # Copy all labels from the service to the scraped metrics.
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
  
          # Set the namespace label from metadata.
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
  
          # Set the name label from the service name.
          - source_labels: [__meta_kubernetes_service_name]
            action: replace
            target_label: kubernetes_name
  
          # Set the node_name label from metadata.
          - source_labels: [__meta_kubernetes_pod_node_name]
            action: replace
            target_label: node_name
  
          # Set the node_ip label from metadata.
          - source_labels: [__meta_kubernetes_pod_host_ip]
            action: replace
            target_label: node_ip
  
          # Copy all labels from the pod to the scraped metrics.
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
  
          # Set the kubernetes_pod_name label from metadata.
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: kubernetes_pod_name
  
          # Set the pod label from metadata. To match the format of https://github.com/coreos/prometheus-operator.
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
  
        # Set the workload label from the ReplicaSet name (assumed to be managed by a Deployment).
          - source_labels: [__meta_kubernetes_pod_controller_kind, __meta_kubernetes_pod_controller_name, es_workload]
            action: replace
            regex: 'ReplicaSet;(.*)-[^-]+;'
            target_label: es_workload
  
        # Set the workload label from the ReplicationController name.
          - source_labels: [__meta_kubernetes_pod_controller_kind, __meta_kubernetes_pod_controller_name, es_workload]
            action: replace
            regex: 'ReplicationController;(.*);'
            target_label: es_workload
  
          # spark workload
          - source_labels: [kubernetes_pod_name, es_workload]
            regex: (.*)-[0-9]{13}-(exec|driver)-[0-9]+;
            replacement: ${1}-${2}
            target_label: es_workload
  
        # If all else fails, use the pod name.
          - source_labels: [__meta_kubernetes_pod_name, es_workload]
            action: replace
            regex: '(.*);'
            target_label: es_workload
  
    {{ .MonitorTypeRules | indent 6 }}
  
  
      # Scrape config for pods
      #
      # The relabeling allows the actual pod scrape endpoint to be configured via the
      # following annotations:
      #
      # * `{{ .PrometheusDomain }}/scrape`: Only scrape pods that have a value of `true`
      # * `{{ .PrometheusDomain }}/path`: If the metrics path is not `/metrics` override this.
      # * `{{ .PrometheusDomain }}/port`: Scrape the pod on the indicated port instead of all `metrics` ports.
      - job_name: 'kubernetes-pods'
  
        kubernetes_sd_configs:
          - role: pod
  
        honor_labels: true
  
        relabel_configs:
  
          # Only scrape pods with prometheus.io/scrape=true.
          - source_labels: [__meta_kubernetes_pod_annotation_{{ sanitizeForPrometheusLabel .PrometheusDomain }}_scrape]
            action: keep
            regex: true
  
          # Set scrape path from prometheus.io/path.
          - source_labels: [__meta_kubernetes_pod_annotation_{{ sanitizeForPrometheusLabel .PrometheusDomain }}_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
  
          # Keep only the ports ending in the suffix `metrics` if prometheus.io/port is unspecified.
          - source_labels: [__meta_kubernetes_pod_annotation_{{ sanitizeForPrometheusLabel .PrometheusDomain }}_port, __meta_kubernetes_pod_container_port_name]
            action: keep
            regex: ^(.+;.*)|(;.*metrics)$
  
          # Set port from prometheus.io/port if it's set.
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_{{ sanitizeForPrometheusLabel .PrometheusDomain }}_port]
            action: replace
            # Extract IP from address. Address might not contain a port in some cases.
            regex: ([^:]+):?(?:\d*);(\d+)
            replacement: ${1}:${2}
            target_label: __address__
  
          # Copy all labels from the pod to the scraped metrics.
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
  
          # Set the namespace label from metadata.
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
  
          # Set the kubernetes_pod_name label from metadata.
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: kubernetes_pod_name
  
          # Set the pod label from metadata. To match the format of https://github.com/coreos/prometheus-operator.
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
  
          # Set the node_name label from metadata.
          - source_labels: [__meta_kubernetes_pod_node_name]
            target_label: node_name
  
          # Set the node_ip label from metadata.
          - source_labels: [__meta_kubernetes_pod_host_ip]
            target_label: node_ip
  
          # Set the workload label from the ReplicaSet name (assumed to be managed by a Deployment).
          - source_labels: [__meta_kubernetes_pod_controller_kind, __meta_kubernetes_pod_controller_name, es_workload]
            action: replace
            regex: 'ReplicaSet;(.*)-[^-]+;'
            target_label: es_workload
  
          # Set the workload label from the ReplicationController name.
          - source_labels: [__meta_kubernetes_pod_controller_kind, __meta_kubernetes_pod_controller_name, es_workload]
            action: replace
            regex: 'ReplicationController;(.*);'
            target_label: es_workload
  
          # Set the workload label from statefulset if pod name was unable to set it.
          - source_labels: [__meta_kubernetes_pod_label_statefulset_kubernetes_io_pod_name, es_workload]
            action: replace
            regex: '(.*)-[0-9]+;'
            target_label: es_workload
  
          # spark workload
          - source_labels: [kubernetes_pod_name, es_workload]
            regex: (.*)-[0-9]{13}-(exec|driver)-[0-9]+;
            replacement: ${1}-${2}
            target_label: es_workload
  
        # If all else fails, use the pod name.
          - source_labels: [__meta_kubernetes_pod_name, es_workload]
            action: replace
            regex: '(.*);'
            target_label: es_workload
  
    {{ .MonitorTypeRules | indent 6 }}
---
# Source: cloudflow-enterprise-components/charts/enterprise-suite/templates/backend-configmap-api-static-rules.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/name: lightbend-console
    helm.sh/chart: enterprise-suite-1.2.12
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    com.lightbend.cloudflow/instance-id: my-release
    app.kubernetes.io/component: console-backend
  name: console-api-static-rules
data:
  static-rules.yml: |
    #  health aggregation of instances in a workload - used by the UI for showing aggregate workload health bars.
    - record: workload:health:max
      expr: max by (es_workload, namespace) (health{es_workload != "", namespace != ""})
  
    # health aggregation of a single pod - used by the UI to show aggregate health for a pod.
    - record: pod:health:max
      expr: max by (pod, pod_name, namespace) (health)
  
    - record: kube_node_pressure
      expr: avg by (es_workload, condition, node_ip, node_name) (kube_node_status_condition{condition!="Ready", status="true"})
  
    # kube_pod_ready: exclude pods where 100% of containers have terminated (eg. Jobs)
    - record: kube_pod_ready
      expr: kube_pod_status_ready{condition="true"} unless on (pod) avg by (pod) (kube_pod_container_status_terminated) == 1
  
    - record: kube_pod_container_restarts_rate
      expr: rate(kube_pod_container_status_restarts_total[1m])>=0 unless on (pod) avg by (pod) (kube_pod_container_status_terminated) == 1
  
    - record: kube_pod_failed
      expr: kube_pod_status_phase{phase="Failed"}
  
    - record: kube_pod_not_running
      expr: 1 - kube_pod_status_phase{phase="Running"} and ignoring(phase) kube_pod_status_phase{phase="Failed"}==0 and ignoring(phase) kube_pod_status_phase{phase="Succeeded"}==0
  
    - record: kube_workload_generation_lag
      expr: avg by (es_workload, namespace, es_monitor_type) (kube_deployment_metadata_generation - kube_deployment_status_observed_generation)
  
    - record: kube_workload_generation_lag
      expr: avg by (es_workload, namespace, es_monitor_type) (kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_number_ready)
  
    - record: prometheus_notification_queue_percent
      expr: 100 * prometheus_notifications_queue_length / prometheus_notifications_queue_capacity
  
    - record: prometheus_target_sync_percent
      expr: 100 * prometheus_target_sync_length_seconds{quantile="0.99"} / on (job,instance) group_left(interval) prometheus_target_interval_length_seconds{quantile="0.01"}
  
    - record: prometheus_notifications_dropped_rate
      expr: irate(prometheus_notifications_dropped_total[5m])
  
    - record: prometheus_rule_evaluation_failures_rate
      expr: irate(prometheus_rule_evaluation_failures_total[5m])
  
    - record: prometheus_target_scrapes_exceeded_sample_limit_rate
      expr: irate(prometheus_target_scrapes_exceeded_sample_limit_total[5m])
  
    - record: prometheus_tsdb_reloads_failures_rate
      expr: irate(prometheus_tsdb_reloads_failures_total[5m])
  
    - record: akka_actor_processing_time_ms
      expr: akka_actor_processing_time_ns / 1000000
  
    - record: zk_open_file_ratio
      expr: (zk_open_file_descriptor_count/zk_max_file_descriptor_count) * 100
  
    - record: redis_keyspace_miss_ratio
      expr: (redis_keyspace_misses_total/redis_keyspace_hits_total) * 100
  
    - record: kafka_incoming_messages_rate
      expr: sum without (instance) (irate(kafka_server_brokertopicmetrics_messagesin_total[5m]))
  
    - record: kafka_active_controllers
      expr: sum by (namespace, es_workload, es_monitor_type) (kafka_controller_kafkacontroller_activecontrollercount)
  
    - record: memcached_miss_ratio
      expr: (sum without(command, status) (memcached_commands_total{status="miss"})/sum without(command, status) (memcached_commands_total)) * 100
  
    - record: memcached_evictions_rate
      expr: irate(memcached_items_evicted_total[5m])
  
    - record: akka_http_http_server_responses_5xx_rate
      expr: irate(akka_http_http_server_responses_5xx[5m]) and akka_http_http_server_responses_5xx offset 1m
  
    - record: akka_http_server_response_time_ms
      expr: akka_http_http_server_response_time_ns / 1000000
  
    - record: akka_http_client_service_response_time_ms
      expr: akka_http_http_client_http_client_service_response_time_ns / 1000000
  
    - record: akka_actor_mailbox_time_ms
      expr: akka_actor_mailbox_time_ns / 1000000
  
    - record: play_http_client_service_response_time_ms
      expr: play_http_client_play_client_service_response_time_ns / 1000000
  
    - record: kafka_consumer_topic_consumed_rate
      expr: sum by (app_kubernetes_io_component, app_kubernetes_io_managed_by, app_kubernetes_io_name, app_kubernetes_io_part_of, es_monitor_type, es_workload, namespace, topic) (kafka_consumer_consumer_fetch_manager_metrics_records_consumed_rate)
  
    - record: kafka_producer_topic_send_rate
      expr: sum by (app_kubernetes_io_component, app_kubernetes_io_managed_by, app_kubernetes_io_name, app_kubernetes_io_part_of, es_monitor_type, es_workload, namespace, topic) (kafka_producer_producer_metrics_record_send_rate)
  
    - record: kafka_consumer_topic_lag
      expr: sum by (app_kubernetes_io_component, app_kubernetes_io_managed_by, app_kubernetes_io_name, app_kubernetes_io_part_of, es_monitor_type, es_workload, namespace, topic) (kafka_consumer_consumer_fetch_manager_metrics_records_lag)
---
# Source: cloudflow-enterprise-components/charts/enterprise-suite/templates/backend-configmap-bare-prometheus.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/name: lightbend-console
    helm.sh/chart: enterprise-suite-1.2.12
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    com.lightbend.cloudflow/instance-id: my-release
    app.kubernetes.io/component: console-backend
  name: bare-prometheus
data:
  prometheus.yml: |-
    # this is a placeholder to keep prometheus running while it awaits the real config
    global:
      scrape_interval:     99m
      evaluation_interval: 99m
    scrape_configs:
      - job_name: wait
        static_configs:
          - targets: ['localhost:9090']
        metric_relabel_configs:
        - source_labels: [__name__]
          regex: .+
          action: drop
---
# Source: cloudflow-enterprise-components/charts/enterprise-suite/templates/discovery-elasticsearch-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/name: lightbend-console
    helm.sh/chart: enterprise-suite-1.2.12
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    com.lightbend.cloudflow/instance-id: my-release
    app.kubernetes.io/component: console-backend
  name: discovery-elasticsearch
data:
  address: console-api:9200
---
# Source: cloudflow-enterprise-components/charts/enterprise-suite/templates/frontend-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: console-frontend
  labels:
    app.kubernetes.io/name: lightbend-console
    helm.sh/chart: enterprise-suite-1.2.12
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    com.lightbend.cloudflow/instance-id: my-release
    app.kubernetes.io/component: console-frontend
data:
  config.js: |
    window.installedConfig = {"isMonitorEditEnabled":false,"logo":""}
  version.js: |
    {
        "description": "Lightbend Console",
        "version": "1.2.12",
        "alpineImage": "alpine",
        "alpineVersion": "3.12",
        "apiGroupVersion": "rbac.authorization.k8s.io",
        "busyboxImage": "busybox",
        "busyboxVersion": "1.32",
        "configMapReloadImage": "jimmidyson/configmap-reload",
        "configMapReloadVersion": "v0.4.0",
        "consoleAPI": "map[defaultMonitorWarmup:1m defaultMonitorsConfigMap:console-api-default-monitors staticRulesConfigMap:console-api-static-rules]",
        "consoleUIConfig": "map[isMonitorEditEnabled:false logo:]",
        "daemonSetApiVersion": "apps/v1beta2",
        "defaultCPURequest": "100m",
        "defaultMemoryRequest": "50Mi",
        "deploymentApiVersion": "apps/v1",
        "elasticsearchImage": "elasticsearch",
        "elasticsearchMemoryRequest": "510Mi",
        "elasticsearchVersion": "7.3.2",
        "esConsoleExposePort": "30080",
        "esConsoleImage": "commercial-registry.lightbend.com/enterprise-suite-es-console",
        "esConsoleVersion": "v1.4.10",
        "esGrafanaImage": "commercial-registry.lightbend.com/enterprise-suite-es-grafana",
        "esGrafanaVersion": "v0.6.0",
        "esGrafanaVolumeSize": "32Gi",
        "esMonitorImage": "commercial-registry.lightbend.com/enterprise-suite-console-api",
        "esMonitorVersion": "v1.2.6",
        "goDnsmasqImage": "lightbend-docker-registry.bintray.io/lightbend/go-dnsmasq",
        "goDnsmasqVersion": "v0.1.7-1",
        "imageCredentials": { "registry": "commercial-registry.lightbend.com" },
        "imagePullPolicy": "IfNotPresent",
        "kibanaImage": "kibana",
        "kubeStateMetricsImage": "quay.io/coreos/kube-state-metrics",
        "kubeStateMetricsVersion": "v1.9.7",
        "logstashImage": "logstash",
        "prometheusDomain": "prometheus.io",
        "prometheusImage": "prom/prometheus",
        "prometheusMemoryRequest": "250Mi",
        "prometheusVersion": "v2.21.0",
        "prometheusVolumeSize": "256Gi",
        "rbacApiVersion": "rbac.authorization.k8s.io/v1",
        "usePersistentVolumes": "true",
        "name": "enterprise-suite"
    }
---
# Source: cloudflow-enterprise-components/charts/enterprise-suite/templates/backend-pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: prometheus-storage
  labels:
    app.kubernetes.io/name: lightbend-console
    helm.sh/chart: enterprise-suite-1.2.12
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    com.lightbend.cloudflow/instance-id: my-release
    app.kubernetes.io/component: console-backend
spec:
  
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 256Gi
---
# Source: cloudflow-enterprise-components/charts/enterprise-suite/templates/grafana-pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: es-grafana-storage
  labels:
    app.kubernetes.io/name: lightbend-console
    helm.sh/chart: enterprise-suite-1.2.12
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    com.lightbend.cloudflow/instance-id: my-release
    app.kubernetes.io/component: grafana
spec:
  
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 32Gi
---
# Source: cloudflow-enterprise-components/charts/enterprise-suite/templates/backend-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: default:prometheus-server
  labels:
    app.kubernetes.io/name: lightbend-console
    helm.sh/chart: enterprise-suite-1.2.12
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    com.lightbend.cloudflow/instance-id: my-release
    app.kubernetes.io/component: console-backend
rules:
- apiGroups: [""]
  resources:
  - nodes
  - nodes/proxy
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups:
  - extensions
  resources:
  - ingresses
  verbs: ["get", "list", "watch"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
---
# Source: cloudflow-enterprise-components/charts/enterprise-suite/templates/kube-state-metrics-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: default:kube-state-metrics
  labels:
    app.kubernetes.io/name: lightbend-console
    helm.sh/chart: enterprise-suite-1.2.12
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    com.lightbend.cloudflow/instance-id: my-release
    app.kubernetes.io/component: kube-state-metrics
rules:
- apiGroups: [""]
  resources:
  - configmaps
  - secrets
  - nodes
  - nodes/proxy
  - pods
  - services
  - resourcequotas
  - replicationcontrollers
  - limitranges
  - persistentvolumeclaims
  - persistentvolumes
  - namespaces
  - endpoints
  verbs: ["get", "list", "watch"]
- apiGroups: ["extensions"]
  resources:
  - daemonsets
  - deployments
  - replicasets
  - ingresses
  verbs: ["list", "watch"]
- apiGroups: ["apps"]
  resources:
  - statefulsets
  - daemonsets
  - deployments
  - replicasets
  verbs: ["list", "watch"]
- apiGroups: ["batch"]
  resources:
  - cronjobs
  - jobs
  verbs: ["list", "watch"]
- apiGroups: ["autoscaling"]
  resources:
  - horizontalpodautoscalers
  verbs: ["list", "watch"]
- apiGroups: ["policy"]
  resources:
  - poddisruptionbudgets
  verbs: ["list", "watch"]
- apiGroups: ["admissionregistration.k8s.io"]
  resources:
  - mutatingwebhookconfigurations
  verbs: ["list", "watch"]
- apiGroups: ["certificates.k8s.io"]
  resources:
  - certificatesigningrequests
  verbs: ["list", "watch"]
- apiGroups: ["storage.k8s.io"]
  resources:
  - storageclasses
  - volumeattachments
  verbs: ["list", "watch"]
- apiGroups: ["networking.k8s.io"]
  resources:
  - networkpolicies
  verbs: ["list", "watch"]
- apiGroups: ["admissionregistration.k8s.io"]
  resources:
  - mutatingwebhookconfigurations
  - validatingwebhookconfigurations
  verbs: ["list", "watch"]
---
# Source: cloudflow-enterprise-components/charts/enterprise-suite/templates/backend-rolebind.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: default:prometheus-server
  labels:
    app.kubernetes.io/name: lightbend-console
    helm.sh/chart: enterprise-suite-1.2.12
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    com.lightbend.cloudflow/instance-id: my-release
    app.kubernetes.io/component: console-backend
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: default:prometheus-server
subjects:
- kind: ServiceAccount
  name: prometheus-server
  namespace: default
---
# Source: cloudflow-enterprise-components/charts/enterprise-suite/templates/kube-state-metrics-rolebind.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: default:kube-state-metrics
  labels:
    app.kubernetes.io/name: lightbend-console
    helm.sh/chart: enterprise-suite-1.2.12
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    com.lightbend.cloudflow/instance-id: my-release
    app.kubernetes.io/component: kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: default:kube-state-metrics
subjects:
- kind: ServiceAccount
  name: prometheus-kube-state-metrics
  namespace: default
---
# Source: cloudflow-enterprise-components/templates/02-cloudflow-enterprise-operator-clusterrolebinding.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: cloudflow-enterprise-operator-bindings
  labels: 
    app.kubernetes.io/name: my-release
    app.kubernetes.io/component: cloudflow
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: cloudflow-enterprise-operator
    namespace: default
---
# Source: cloudflow-enterprise-components/charts/enterprise-suite/templates/backend-service-api.yaml
apiVersion: v1
kind: Service
metadata:
  name: console-api
  labels:
    app.kubernetes.io/name: lightbend-console
    helm.sh/chart: enterprise-suite-1.2.12
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    com.lightbend.cloudflow/instance-id: my-release
    app.kubernetes.io/component: console-backend
spec:
  ports:
    - name: http
      port: 80
      targetPort: 8180
    - name: elasticsearch
      port: 9200
      targetPort: 9200
    - name: kibana
      port: 5601
      targetPort: 5601
    - name: logstash-tcp
      port: 5000
      targetPort: 5000
      protocol: TCP
    - name: logstash-udp
      port: 5000
      targetPort: 5000
      protocol: UDP
  selector:
    app.kubernetes.io/name: lightbend-console
    app.kubernetes.io/component: console-backend
---
# Source: cloudflow-enterprise-components/charts/enterprise-suite/templates/backend-service-prom.yaml
apiVersion: v1
kind: Service
metadata:
  name: prometheus-server
  labels:
    app.kubernetes.io/name: lightbend-console
    helm.sh/chart: enterprise-suite-1.2.12
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    com.lightbend.cloudflow/instance-id: my-release
    app.kubernetes.io/component: console-backend
spec:
  ports:
    - name: http
      port: 80
      targetPort: 9090
  selector:
    app.kubernetes.io/name: lightbend-console
    app.kubernetes.io/component: console-backend
---
# Source: cloudflow-enterprise-components/charts/enterprise-suite/templates/frontend-service.yaml
apiVersion: v1
kind: Service
metadata:
  # Changing this name will break backwards compatibility.
  name: console-server
  labels:
    app.kubernetes.io/name: lightbend-console
    helm.sh/chart: enterprise-suite-1.2.12
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    com.lightbend.cloudflow/instance-id: my-release
    app.kubernetes.io/component: console-frontend
spec:
  ports:
    - name: http
      port: 80
      targetPort: 8080
  selector:
    app.kubernetes.io/name: lightbend-console
    app.kubernetes.io/component: console-frontend
  type: ClusterIP
---
# Source: cloudflow-enterprise-components/charts/enterprise-suite/templates/grafana-service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/name: lightbend-console
    helm.sh/chart: enterprise-suite-1.2.12
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    com.lightbend.cloudflow/instance-id: my-release
    app.kubernetes.io/component: grafana
  name: grafana-server
spec:
  ports:
    - name: http
      port: 3000
      protocol: TCP
      targetPort: 3000
  selector:
    app.kubernetes.io/name: lightbend-console
    app.kubernetes.io/component: grafana
  type: "ClusterIP"
---
# Source: cloudflow-enterprise-components/charts/enterprise-suite/templates/kube-state-metrics-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: prometheus-kube-state-metrics
  annotations:
    prometheus.io/scrape: "true"
  labels:
    app.kubernetes.io/name: lightbend-console
    helm.sh/chart: enterprise-suite-1.2.12
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    com.lightbend.cloudflow/instance-id: my-release
    app.kubernetes.io/component: kube-state-metrics
spec:
  clusterIP: None
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 8080
  selector:
    app.kubernetes.io/name: lightbend-console
    app.kubernetes.io/component: kube-state-metrics
  type: "ClusterIP"
---
# Source: cloudflow-enterprise-components/charts/enterprise-suite/templates/backend-deployment.yaml
kind: Deployment
apiVersion: apps/v1
metadata:
  labels:
    app.kubernetes.io/name: lightbend-console
    helm.sh/chart: enterprise-suite-1.2.12
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    com.lightbend.cloudflow/instance-id: my-release
    app.kubernetes.io/component: console-backend
  name: console-backend
spec:
  replicas: 1

  strategy:
    # Always Recreate to ensure the PVs get released. It's not possible to have two replicas sharing a PV during deployment.
    type: Recreate
    # Needed for helm upgrade to succeed.
    rollingUpdate: null

  selector:
    matchLabels:
      app: prometheus
      component: server

  template:
    metadata:
      labels:
        app.kubernetes.io/name: lightbend-console
        helm.sh/chart: enterprise-suite-1.2.12
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: my-release
        com.lightbend.cloudflow/instance-id: my-release
        app.kubernetes.io/component: console-backend
        # Deprecated - these should be removed eventually. Kept to support upgrades with the old labels.
        app: prometheus
        component: server
      annotations:
        prometheus.io/scrape: "true"
        checksum/console-api-config: 391022573824ca4db08269caefc574e6f335a8bc1191df07b3e79bb442cfa7f4
        checksum/bare-prometheus-config: d76b34562375eb7c958fe931d2951d717c3e58ceb78799e1e595d6038dc30246

    spec:
      serviceAccountName: prometheus-server

      securityContext:
      

      initContainers:
        - name: setup
          image: alpine:3.12
          resources:
            requests:
              cpu: 100m
              memory: 50Mi
          command:
            - /bin/sh
            - -c
          args:
            - mkdir -p /etc/config/rules;
              cp /etc/bare/prometheus.yml /etc/config/
          volumeMounts:
            - mountPath: /etc/config
              name: config-volume
            - mountPath: /etc/bare
              name: bare-prometheus

        # For vanilla K8s clusters, we need to match prometheus-data permissions to the prometheus user.
        # In prometheus 2.x, this is `65534`.
        # For Openshift the below will fail, but we can safely ignore it as Openshift remaps the user itself.
        - name: change-prometheus-data-volume-ownership
          image: busybox:1.32
          command:
            - sh
            - -c
            - "chown -Rc 65534:65534 /data 2>/dev/null || true"
          volumeMounts:
            - name: prometheus-data-volume
              mountPath: /data
              subPath: prometheus-data

        

      imagePullSecrets:
        - name: commercial-credentials

      containers:
        - name: console-api
          image: commercial-registry.lightbend.com/enterprise-suite-console-api:v1.2.6
          imagePullPolicy: IfNotPresent
          resources:
            requests:
              cpu: 100m
              memory: 50Mi

          args:
            - --configPath=/etc/config/
            - --storagePath=/monitor-data/
            - --staticConfig=/etc/console-api-static-rules/static-rules.yml
            - --defaultMonitorsFile=/etc/console-api-default-monitors/default-monitors.json
            - --prometheusTemplate=/etc/console-api-prometheus/prometheus.yml
            - --prometheusDomain=prometheus.io
            - --alertmanagers=
            - --defaultMonitorWarmup=1m
            - --namespace=$(CONSOLE_NAMESPACE)

          env:
          - name: CONSOLE_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace

          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
            - name: console-api-default-monitors-volume
              mountPath: /etc/console-api-default-monitors
            - name: console-api-static-rules-volume
              mountPath: /etc/console-api-static-rules
            - name: console-api-prometheus-volume
              mountPath: /etc/console-api-prometheus
            - name: prometheus-data-volume
              mountPath: /monitor-data
              subPath: monitor-data

          ports:
            - name: metrics
              containerPort: 8180

          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /status
              port: 8180
              scheme: HTTP

          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /status
              port: 8180
              scheme: HTTP
            initialDelaySeconds: 30

        - name: console-api-configmap-reload
          image: jimmidyson/configmap-reload:v0.4.0
          resources:
            requests:
              cpu: 100m
              memory: 50Mi
          args:
          - --volume-dir=/etc/console-api-default-monitors
          - --volume-dir=/etc/console-api-static-rules
          - --volume-dir=/etc/console-api-prometheus
          - --webhook-url=http://127.0.0.1:8180/reload
          volumeMounts:
          - name: console-api-default-monitors-volume
            mountPath: /etc/console-api-default-monitors
          - name: console-api-static-rules-volume
            mountPath: /etc/console-api-static-rules
          - name: console-api-prometheus-volume
            mountPath: /etc/console-api-prometheus

        - name: prometheus
          image: prom/prometheus:v2.21.0

          resources:
            requests:
              cpu: 100m
              memory: 250Mi

          args:
            - --config.file=/etc/config/prometheus.yml
            - --storage.tsdb.path=/data
            - --web.console.libraries=/etc/prometheus/console_libraries
            - --web.console.templates=/etc/prometheus/consoles
            - --web.enable-lifecycle
            
          ports:
            - name: metrics
              containerPort: 9090

          readinessProbe:
            httpGet:
              path: /-/ready
              port: 9090
            timeoutSeconds: 30

          livenessProbe:
            httpGet:
              path: /-/healthy
              port: 9090
            timeoutSeconds: 30
            initialDelaySeconds: 30

          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
              readOnly: true
            - name: prometheus-data-volume
              mountPath: /data
              subPath: prometheus-data

        

      terminationGracePeriodSeconds: 300

      volumes:
        # Used by console-api to write the final Prometheus configuration.
        - name: config-volume
          emptyDir: {}

        # Default monitors.
        - name: console-api-default-monitors-volume
          configMap:
            name: console-api-default-monitors

        # Static Prometheus rules.
        - name: console-api-static-rules-volume
          configMap:
            name: console-api-static-rules

        # Prometheus configuration template.
        - name: console-api-prometheus-volume
          configMap:
            name: console-api-prometheus

        

        # Prometheus data storage.
        - name: prometheus-data-volume
          persistentVolumeClaim:
            claimName: prometheus-storage
          
        # Bootstrap files for prometheus.
        - name: bare-prometheus
          configMap:
            name: bare-prometheus
---
# Source: cloudflow-enterprise-components/charts/enterprise-suite/templates/frontend-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: console-frontend
  labels:
    app.kubernetes.io/name: lightbend-console
    helm.sh/chart: enterprise-suite-1.2.12
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    com.lightbend.cloudflow/instance-id: my-release
    app.kubernetes.io/component: console-frontend

spec:
  selector:
    matchLabels:
      run: es-console

  template:
    metadata:
      annotations:
        checksum/es-console-config: 30a84fced8427eb0b9ef367c25d6a954ffab7b47c10b1acd991e4178776a83bf
      labels:
        app.kubernetes.io/name: lightbend-console
        helm.sh/chart: enterprise-suite-1.2.12
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: my-release
        com.lightbend.cloudflow/instance-id: my-release
        app.kubernetes.io/component: console-frontend
        # Deprecated - these should be removed eventually. Kept to support upgrades with the old labels.
        run: es-console
    spec:
      

      imagePullSecrets:
      - name: commercial-credentials

      containers:
      - name: es-console
        image: commercial-registry.lightbend.com/enterprise-suite-es-console:v1.4.10
        imagePullPolicy: IfNotPresent
        resources:
          requests:
            cpu: 100m
            memory: 50Mi
        ports:
        - containerPort: 8080
        volumeMounts:
        - name: ui-config
          mountPath: /helm-data/assets/js

      volumes:
      - name: ui-config
        configMap:
          name: console-frontend
          items:
          - key: config.js
            path: config.js
          - key: version.js
            path: version.js
---
# Source: cloudflow-enterprise-components/charts/enterprise-suite/templates/grafana-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  labels:
    app.kubernetes.io/name: lightbend-console
    helm.sh/chart: enterprise-suite-1.2.12
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    com.lightbend.cloudflow/instance-id: my-release
    app.kubernetes.io/component: grafana
spec:
  replicas: 1

  strategy:
    # Always Recreate to ensure the PVs get released. It's not possible to have two replicas sharing a PV during deployment.
    type: Recreate
    # Needed for helm upgrade to succeed.
    rollingUpdate: null

  selector:
    matchLabels:
      app: grafana
      component: server

  template:
    metadata:
      labels:
        app.kubernetes.io/name: lightbend-console
        helm.sh/chart: enterprise-suite-1.2.12
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: my-release
        com.lightbend.cloudflow/instance-id: my-release
        app.kubernetes.io/component: grafana
        # Deprecated - these should be removed eventually. Kept to support upgrades with the old labels.
        app: grafana
        component: server
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "3000"

    spec:
      

      imagePullSecrets:
      - name: commercial-credentials

      containers:
      - image: commercial-registry.lightbend.com/enterprise-suite-es-grafana:v0.6.0
        imagePullPolicy: IfNotPresent
        name: grafana-server
        resources:
          requests:
            cpu: 100m
            memory: 50Mi
        env:
          # The next two env variables set up anonymous access to grafana with editor access.
          - name: GF_AUTH_ANONYMOUS_ENABLED
            value: "true"
          - name: GF_AUTH_ANONYMOUS_ORG_ROLE
            value: "Editor"
          - name: GF_SERVER_ROOT_URL
            value: "/service/grafana/"
          - name: GF_SERVER_ROUTER_LOGGING
            value: "true"
          - name: GF_ANALYTICS_REPORTING_ENABLED
            value: "false"
          - name: GF_ANALYTICS_CHECK_FOR_UPDATES
            value: "false"
        ports:
          - containerPort: 3000
        readinessProbe:
          httpGet:
            path: /api/health
            port: 3000
        volumeMounts:
        - name: grafana-data
          mountPath: /var/lib/grafana

      volumes:
      - name: grafana-data
        
        persistentVolumeClaim:
          claimName: es-grafana-storage
---
# Source: cloudflow-enterprise-components/charts/enterprise-suite/templates/kube-state-metrics-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus-kube-state-metrics
  labels:
    app.kubernetes.io/name: lightbend-console
    helm.sh/chart: enterprise-suite-1.2.12
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    com.lightbend.cloudflow/instance-id: my-release
    app.kubernetes.io/component: kube-state-metrics
spec:
  replicas: 1

  strategy:
    # Always Recreate to ensure we don't get duplicate metrics.
    type: Recreate
    # Needed for helm upgrade to succeed.
    rollingUpdate: null

  selector:
    matchLabels:
      app: prometheus
      component: kube-state-metrics

  template:
    metadata:
      labels:
        app.kubernetes.io/name: lightbend-console
        helm.sh/chart: enterprise-suite-1.2.12
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: my-release
        com.lightbend.cloudflow/instance-id: my-release
        app.kubernetes.io/component: kube-state-metrics
        # Deprecated - these should be removed eventually. Kept to support upgrades with the old labels.
        app: prometheus
        component: kube-state-metrics

    spec:
      
      serviceAccountName: prometheus-kube-state-metrics
      containers:
        - name: prometheus-kube-state-metrics
          image: quay.io/coreos/kube-state-metrics:v1.9.7
          args:
            - --port=8080
            - --namespace=
            - --telemetry-port=8081
          resources:
            requests:
              cpu: 100m
              memory: 50Mi
          ports:
            - name: metrics
              containerPort: 8080
---
# Source: cloudflow-enterprise-components/templates/01-cloudflow-enterprise-operator-deployment.yaml
kind: Deployment
apiVersion: apps/v1
metadata:
  name: cloudflow-enterprise-operator
  labels: &CloudflowDeploymentLabels
    app.kubernetes.io/name: my-release
    app.kubernetes.io/component: cloudflow
spec:
  selector:
    matchLabels: *CloudflowDeploymentLabels
  template:
    metadata:
      labels: *CloudflowDeploymentLabels
      annotations: 
        prometheus.io/scrape: "true"
        prometheus.io/port: "2049"
    spec:
      serviceAccountName: cloudflow-enterprise-operator
      containers:
        - name: cloudflow-enterprise-operator
          image: "commercial-registry.lightbend.com/lightbend-cloudflow-enterprise-operator:2.3.4-RC5"
          ports:
          - containerPort: 5001
            name: http
            protocol: TCP
          - containerPort: 2049
            name: topo-metrics
            protocol: TCP
      imagePullSecrets:
        - name: "commercial-credentials"
---
# Source: cloudflow-enterprise-components/charts/enterprise-suite/templates/dump-values.yaml
# {"alpineImage":"alpine","alpineVersion":"3.12","apiGroupVersion":"rbac.authorization.k8s.io","busyboxImage":"busybox","busyboxVersion":"1.32","configMapReloadImage":"jimmidyson/configmap-reload","configMapReloadVersion":"v0.4.0","consoleAPI":{"defaultMonitorWarmup":"1m","defaultMonitorsConfigMap":"console-api-default-monitors","staticRulesConfigMap":"console-api-static-rules"},"consoleUIConfig":{"isMonitorEditEnabled":false,"logo":""},"daemonSetApiVersion":"apps/v1beta2","defaultCPURequest":"100m","defaultMemoryRequest":"50Mi","deploymentApiVersion":"apps/v1","elasticsearchImage":"elasticsearch","elasticsearchMemoryRequest":"510Mi","elasticsearchVersion":"7.3.2","enableElasticsearch":false,"esConsoleExposePort":30080,"esConsoleImage":"commercial-registry.lightbend.com/enterprise-suite-es-console","esConsoleVersion":"v1.4.10","esGrafanaImage":"commercial-registry.lightbend.com/enterprise-suite-es-grafana","esGrafanaVersion":"v0.6.0","esGrafanaVolumeSize":"32Gi","esMonitorImage":"commercial-registry.lightbend.com/enterprise-suite-console-api","esMonitorVersion":"v1.2.6","exposeServices":false,"global":{},"goDnsmasqImage":"lightbend-docker-registry.bintray.io/lightbend/go-dnsmasq","goDnsmasqVersion":"v0.1.7-1","imageCredentials":{"registry":"commercial-registry.lightbend.com"},"imagePullPolicy":"IfNotPresent","kibanaImage":"kibana","kubeStateMetricsImage":"quay.io/coreos/kube-state-metrics","kubeStateMetricsScrapeNamespaces":"","kubeStateMetricsVersion":"v1.9.7","logstashImage":"logstash","minikube":false,"prometheusDomain":"prometheus.io","prometheusImage":"prom/prometheus","prometheusMemoryRequest":"250Mi","prometheusVersion":"v2.21.0","prometheusVolumeSize":"256Gi","rbacApiVersion":"rbac.authorization.k8s.io/v1","usePersistentVolumes":true}
