---
# Source: stackstorm-ha/charts/mongodb/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-mongodb
  namespace: default
  labels:
    app.kubernetes.io/name: mongodb
    helm.sh/chart: mongodb-10.0.1
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
secrets:
  - name: my-release-mongodb
---
# Source: stackstorm-ha/charts/rabbitmq/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-rabbitmq
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-8.0.2
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
secrets:
  - name: my-release-rabbitmq
---
# Source: stackstorm-ha/templates/service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: stackstorm-ha
  labels:
    
    
    app.kubernetes.io/name: stackstorm-ha
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: backend
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
---
# Source: stackstorm-ha/charts/mongodb/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-release-mongodb
  namespace: default
  labels:
    app.kubernetes.io/name: mongodb
    helm.sh/chart: mongodb-10.0.1
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: mongodb
type: Opaque
data:
  mongodb-root-password:  "OGZBemRua3NkelBGVVdtNGE2OEVmWTduTWhCUGFh"
  mongodb-password:  "WGVMNVJ4d2o3RjBXdDQzdEZaVlRON0g4U2c1WERIbUs="
  mongodb-replica-set-key:  "ODJQSXREcHFyb3RpNVJuZ09BN1VxYkhIN2M2YkZVd3k="
---
# Source: stackstorm-ha/charts/rabbitmq/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-release-rabbitmq
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-8.0.2
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  rabbitmq-password: "OWpTK3cxdTA3TmJIdFprZTFtK2pXNENq"
  rabbitmq-erlang-cookie: "OE1ycVFkQ1E2QVE4VTNNYWNTdWJIRTVScWtTZnZOYVJIenZ4dUZjRw=="
---
# Source: stackstorm-ha/templates/secrets_datastore_crypto_key.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-release-st2-datastore-crypto-key
  annotations:
    description: StackStorm crypto key used to encrypt/decrypt KV records
  labels:
    
    
    app.kubernetes.io/name: st2
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: backend
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  # Datastore key used to encrypt/decrypt record for the KV store
  datastore_crypto_key: eyJhZXNLZXlTdHJpbmciOiJCbDZpWGptUEFpMG1BUFBOOGFTZGFTbzgxNGpCUTdoWTAzSFVjM09OMEUwIiwiaG1hY0tleSI6eyJobWFjS2V5U3RyaW5nIjoiSU5Tdzk3QmRNNkpUUVNwVmpqQ3JXU3lodXdLYTItMWJnc3JFZlU5OFhmUSIsInNpemUiOjI1Nn0sIm1vZGUiOiJDQkMiLCJzaXplIjoyNTZ9
---
# Source: stackstorm-ha/templates/secrets_rabbitmq.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-release-rabbitmq-definitions
  annotations:
    description: A rabbitmq definition which will be loaded by the rabbitmq subchart to enable mirroring for Rabbit HA
  labels:
    
    
    app.kubernetes.io/name: st2
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: backend
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  rabbitmq-definitions.json:  ewogICJ1c2VycyI6IFsKICAgIHsKICAgICAgIm5hbWUiOiAiYWRtaW4iLAogICAgICAicGFzc3dvcmQiOiAiOWpTK3cxdTA3TmJIdFprZTFtK2pXNENqIiwKICAgICAgInRhZ3MiOiAiYWRtaW5pc3RyYXRvciIKICAgIH0KICBdLAogICJwZXJtaXNzaW9ucyI6IFsKICAgIHsKICAgICAgInVzZXIiOiAiYWRtaW4iLAogICAgICAidmhvc3QiOiAiLyIsCiAgICAgICJjb25maWd1cmUiOiAiLioiLAogICAgICAid3JpdGUiOiAiLioiLAogICAgICAicmVhZCI6ICIuKiIKICAgIH0KICBdLAogICJ2aG9zdHMiOiBbCiAgICB7CiAgICAgICJuYW1lIjogIi8iCiAgICB9CiAgXSwKICAicG9saWNpZXMiOiBbCiAgICB7CiAgICAgICJ2aG9zdCI6Ii8iLAogICAgICAibmFtZSI6ImhhIiwKICAgICAgInBhdHRlcm4iOiIiLAogICAgICAiZGVmaW5pdGlvbiI6IHsKICAgICAgICAiaGEtbW9kZSI6ImFsbCIsCiAgICAgICAgImhhLXN5bmMtbW9kZSI6ICJhdXRvbWF0aWMiLAogICAgICAgICJoYS1zeW5jLWJhdGNoLXNpemUiOjEwCiAgICAgIH0KICAgIH0KICBdCn0K
---
# Source: stackstorm-ha/templates/secrets_ssh.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-release-st2-ssh
  annotations:
    description: StackStorm SSH secret key for 'stanley' user, used to run actions on remote machines
  labels:
    
    
    app.kubernetes.io/name: st2
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: backend
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  # SSH private key for the 'stanley' system user ('system_user.ssh_key_file' in st2.conf).
  private_key: "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"
---
# Source: stackstorm-ha/templates/secrets_st2apikeys.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-release-st2-apikeys
  annotations:
    description: A list of StackStorm API keys with metadata that will be imported into the system
  labels:
    
    
    app.kubernetes.io/name: st2
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: backend
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  apikeys.yaml: "bnVsbA=="
---
# Source: stackstorm-ha/templates/secrets_st2auth.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-release-st2-auth
  annotations:
    description: StackStorm username and password, used for basic .htaccess auth
  labels:
    
    
    app.kubernetes.io/name: st2
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: backend
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  # Username, used to login to StackStorm system (default: st2admin)
  ST2_AUTH_USERNAME: "c3QyYWRtaW4="
  # Password, used to login to StackStorm system (default: auto-generated)
  ST2_AUTH_PASSWORD: "MHcxQzE1Q29jVHVY"
---
# Source: stackstorm-ha/templates/secrets_st2kv.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-release-st2-kv
  annotations:
    description: Key/Value pairs to save in StackStorm's datastore
  labels:
    
    
    app.kubernetes.io/name: st2
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: backend
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  st2kv.yaml: "bnVsbA=="
---
# Source: stackstorm-ha/charts/mongodb/templates/replicaset/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-mongodb-scripts
  namespace: default
  labels:
    app.kubernetes.io/name: mongodb
    helm.sh/chart: mongodb-10.0.1
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: mongodb
data:
  setup.sh: |-
    #!/bin/bash

    echo "Advertised Hostname: $MONGODB_ADVERTISED_HOSTNAME"

    if [[ "$MY_POD_NAME" = "my-release-mongodb-0" ]]; then
        echo "Pod name matches initial primary pod name, configuring node as a primary"
        export MONGODB_REPLICA_SET_MODE="primary"
    else
        echo "Pod name doesn't match initial primary pod name, configuring node as a secondary"
        export MONGODB_REPLICA_SET_MODE="secondary"
        export MONGODB_INITIAL_PRIMARY_ROOT_PASSWORD="$MONGODB_ROOT_PASSWORD"
        export MONGODB_INITIAL_PRIMARY_PORT_NUMBER="$MONGODB_PORT_NUMBER"
        export MONGODB_ROOT_PASSWORD="" MONGODB_USERNAME="" MONGODB_DATABASE="" MONGODB_PASSWORD=""
    fi

    exec /opt/bitnami/scripts/mongodb/entrypoint.sh /opt/bitnami/scripts/mongodb/run.sh
---
# Source: stackstorm-ha/charts/rabbitmq/templates/configuration.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-rabbitmq-config
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-8.0.2
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
data:
  rabbitmq.conf: |-
    ## Username and password
    default_user = admin
    default_pass = CHANGEME
    ## Clustering
    cluster_formation.peer_discovery_backend  = rabbit_peer_discovery_k8s
    cluster_formation.k8s.host = kubernetes.default.svc.cluster.local
    cluster_formation.node_cleanup.interval = 10
    cluster_formation.node_cleanup.only_log_warning = true
    cluster_partition_handling = autoheal
    # queue master locator
    queue_master_locator = min-masters
    # enable guest user
    loopback_users.guest = false
    load_definitions = /app/rabbitmq-definitions.json
---
# Source: stackstorm-ha/charts/redis/templates/configmap-scripts.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-redis-scripts
  namespace: "default"
  labels:
    app: redis
    chart: redis-12.3.2
    heritage: Helm
    release: my-release
data:
  start-node.sh: |
    #!/bin/bash
    is_boolean_yes() {
        local -r bool="${1:-}"
        # comparison is performed without regard to the case of alphabetic characters
        shopt -s nocasematch
        if [[ "$bool" = 1 || "$bool" =~ ^(yes|true)$ ]]; then
            true
        else
            false
        fi
    }

    HEADLESS_SERVICE="my-release-redis-headless.default.svc.cluster.local"
    REDIS_SERVICE="my-release-redis.default.svc.cluster.local"

    export REDIS_REPLICATION_MODE="slave"
    if [[ -z "$(getent ahosts "$HEADLESS_SERVICE" | grep -v "^$(hostname -i) ")" ]]; then
      export REDIS_REPLICATION_MODE="master"
    fi

    if [[ -n $REDIS_PASSWORD_FILE ]]; then
      password_aux=`cat ${REDIS_PASSWORD_FILE}`
      export REDIS_PASSWORD=$password_aux
    fi

    if [[ -n $REDIS_MASTER_PASSWORD_FILE ]]; then
      password_aux=`cat ${REDIS_MASTER_PASSWORD_FILE}`
      export REDIS_MASTER_PASSWORD=$password_aux
    fi

    if [[ "$REDIS_REPLICATION_MODE" == "master" ]]; then
      echo "I am master"
      if [[ ! -f /opt/bitnami/redis/etc/master.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf
      fi
    else
      if [[ ! -f /opt/bitnami/redis/etc/replica.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/replica.conf /opt/bitnami/redis/etc/replica.conf
      fi

      if is_boolean_yes "$REDIS_TLS_ENABLED"; then
        sentinel_info_command="redis-cli -h $REDIS_SERVICE -p 26379 --tls --cert ${REDIS_TLS_CERT_FILE} --key ${REDIS_TLS_KEY_FILE} --cacert ${REDIS_TLS_CA_FILE} sentinel get-master-addr-by-name mymaster"
      else
        sentinel_info_command="redis-cli -h $REDIS_SERVICE -p 26379 sentinel get-master-addr-by-name mymaster"
      fi
      REDIS_SENTINEL_INFO=($($sentinel_info_command))
      REDIS_MASTER_HOST=${REDIS_SENTINEL_INFO[0]}
      REDIS_MASTER_PORT_NUMBER=${REDIS_SENTINEL_INFO[1]}


      # Immediately attempt to connect to the reported master. If it doesn't exist the connection attempt will either hang
      # or fail with "port unreachable" and give no data. The liveness check will then timeout waiting for the redis
      # container to be ready and restart the it. By then the new master will likely have been elected
      if is_boolean_yes "$REDIS_TLS_ENABLED"; then
        sentinel_info_command="redis-cli -h $REDIS_MASTER_HOST -p 26379 --tls --cert ${REDIS_TLS_CERT_FILE} --key ${REDIS_TLS_KEY_FILE} --cacert ${REDIS_TLS_CA_FILE} sentinel get-master-addr-by-name mymaster"
      else
        sentinel_info_command="redis-cli -h $REDIS_MASTER_HOST -p 26379 sentinel get-master-addr-by-name mymaster"
      fi

      if [[ ! ($($sentinel_info_command)) ]]; then
        # master doesn't actually exist, this probably means the remaining pods haven't elected a new one yet
        # and are reporting the old one still. Once this happens the container will get stuck and never see the new
        # master. We stop here to allow the container to not pass the liveness check and be restarted.
        exit 1
      fi
    fi

    if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then
      cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi
    ARGS=("--port" "${REDIS_PORT}")

    if [[ "$REDIS_REPLICATION_MODE" == "slave" ]]; then
      ARGS+=("--slaveof" "${REDIS_MASTER_HOST}" "${REDIS_MASTER_PORT_NUMBER}")
    fi
    ARGS+=("--protected-mode" "no")

    if [[ "$REDIS_REPLICATION_MODE" == "master" ]]; then
      ARGS+=("--include" "/opt/bitnami/redis/etc/master.conf")
    else
      ARGS+=("--include" "/opt/bitnami/redis/etc/replica.conf")
    fi

    ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
    exec /run.sh "${ARGS[@]}"

  start-sentinel.sh: |
    #!/bin/bash
    replace_in_file() {
        local filename="${1:?filename is required}"
        local match_regex="${2:?match regex is required}"
        local substitute_regex="${3:?substitute regex is required}"
        local posix_regex=${4:-true}

        local result

        # We should avoid using 'sed in-place' substitutions
        # 1) They are not compatible with files mounted from ConfigMap(s)
        # 2) We found incompatibility issues with Debian10 and "in-place" substitutions
        del=$'\001' # Use a non-printable character as a 'sed' delimiter to avoid issues
        if [[ $posix_regex = true ]]; then
            result="$(sed -E "s${del}${match_regex}${del}${substitute_regex}${del}g" "$filename")"
        else
            result="$(sed "s${del}${match_regex}${del}${substitute_regex}${del}g" "$filename")"
        fi
        echo "$result" > "$filename"
    }
    sentinel_conf_set() {
        local -r key="${1:?missing key}"
        local value="${2:-}"

        # Sanitize inputs
        value="${value//\\/\\\\}"
        value="${value//&/\\&}"
        value="${value//\?/\\?}"
        [[ "$value" = "" ]] && value="\"$value\""

        replace_in_file "/opt/bitnami/redis-sentinel/etc/sentinel.conf" "^#*\s*${key} .*" "${key} ${value}" false
    }
    sentinel_conf_add() {
        echo $'\n'"$@" >> "/opt/bitnami/redis-sentinel/etc/sentinel.conf"
    }
    is_boolean_yes() {
        local -r bool="${1:-}"
        # comparison is performed without regard to the case of alphabetic characters
        shopt -s nocasematch
        if [[ "$bool" = 1 || "$bool" =~ ^(yes|true)$ ]]; then
            true
        else
            false
        fi
    }
    host_id() {
      echo "$1" | openssl sha1 | awk '{print $2}'
    }

    HEADLESS_SERVICE="my-release-redis-headless.default.svc.cluster.local"
    REDIS_SERVICE="my-release-redis.default.svc.cluster.local"

    if [[ -n $REDIS_PASSWORD_FILE ]]; then
      password_aux=`cat ${REDIS_PASSWORD_FILE}`
      export REDIS_PASSWORD=$password_aux
    fi

    if [[ ! -f /opt/bitnami/redis-sentinel/etc/sentinel.conf ]]; then
      cp /opt/bitnami/redis-sentinel/mounted-etc/sentinel.conf /opt/bitnami/redis-sentinel/etc/sentinel.conf
      printf "\nsentinel myid %s" "$(host_id "$HOSTNAME")" >> /opt/bitnami/redis-sentinel/etc/sentinel.conf
    fi

    export REDIS_REPLICATION_MODE="slave"
    if [[ -z "$(getent ahosts "$HEADLESS_SERVICE" | grep -v "^$(hostname -i) ")" ]]; then
      export REDIS_REPLICATION_MODE="master"
    fi

    if [[ "$REDIS_REPLICATION_MODE" == "master" ]]; then
      REDIS_MASTER_HOST="$(hostname -i)"
      REDIS_MASTER_PORT_NUMBER="6379"
    else
      if is_boolean_yes "$REDIS_SENTINEL_TLS_ENABLED"; then
        sentinel_info_command="redis-cli -h $REDIS_SERVICE -p 26379 --tls --cert ${REDIS_SENTINEL_TLS_CERT_FILE} --key ${REDIS_SENTINEL_TLS_KEY_FILE} --cacert ${REDIS_SENTINEL_TLS_CA_FILE} sentinel get-master-addr-by-name mymaster"
      else
        sentinel_info_command="redis-cli -h $REDIS_SERVICE -p 26379 sentinel get-master-addr-by-name mymaster"
      fi
      REDIS_SENTINEL_INFO=($($sentinel_info_command))
      REDIS_MASTER_HOST=${REDIS_SENTINEL_INFO[0]}
      REDIS_MASTER_PORT_NUMBER=${REDIS_SENTINEL_INFO[1]}

      # Immediately attempt to connect to the reported master. If it doesn't exist the connection attempt will either hang
      # or fail with "port unreachable" and give no data. The liveness check will then timeout waiting for the sentinel
      # container to be ready and restart the it. By then the new master will likely have been elected
      if is_boolean_yes "$REDIS_SENTINEL_TLS_ENABLED"; then
        sentinel_info_command="redis-cli -h $REDIS_MASTER_HOST -p 26379 --tls --cert ${REDIS_SENTINEL_TLS_CERT_FILE} --key ${REDIS_SENTINEL_TLS_KEY_FILE} --cacert ${REDIS_SENTINEL_TLS_CA_FILE} sentinel get-master-addr-by-name mymaster"
      else
        sentinel_info_command="redis-cli -h $REDIS_MASTER_HOST -p 26379 sentinel get-master-addr-by-name mymaster"
      fi

      if [[ ! ($($sentinel_info_command)) ]]; then
        # master doesn't actually exist, this probably means the remaining pods haven't elected a new one yet
        # and are reporting the old one still. Once this happens the container will get stuck and never see the new
        # master. We stop here to allow the container to not pass the liveness check and be restarted.
        exit 1
      fi
    fi
    sentinel_conf_set "sentinel monitor" "mymaster "$REDIS_MASTER_HOST" "$REDIS_MASTER_PORT_NUMBER" 2"

    add_replica() {
      if [[ "$1" != "$REDIS_MASTER_HOST" ]]; then
        sentinel_conf_add "sentinel known-replica mymaster $1 6379"
      fi
    }
    # remove generated known sentinels and replicas
    tmp="$(sed -e '/^sentinel known-/d' -e '/^$/d' /opt/bitnami/redis-sentinel/etc/sentinel.conf)"
    echo "$tmp" > /opt/bitnami/redis-sentinel/etc/sentinel.conf

    for node in $(seq 0 3); do
      NAME="my-release-redis-node-$node"
      IP="$(getent hosts "$NAME.$HEADLESS_SERVICE" | awk ' {print $1 }')"
      if [[ "$NAME" != "$HOSTNAME" && -n "$IP" ]]; then
        sentinel_conf_add "sentinel known-sentinel mymaster $IP 26379 $(host_id "$NAME")"
        add_replica "$IP"
      fi
    done
    add_replica "$(hostname -i)"
    exec redis-server /opt/bitnami/redis-sentinel/etc/sentinel.conf --sentinel
---
# Source: stackstorm-ha/charts/redis/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-redis
  namespace: "default"
  labels:
    app: redis
    chart: redis-12.3.2
    heritage: Helm
    release: my-release
data:
  redis.conf: |-
    # User-supplied configuration:
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
  master.conf: |-
    dir /data
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
  replica.conf: |-
    dir /data
    slave-read-only yes
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
  sentinel.conf: |-
    dir "/tmp"
    bind 0.0.0.0
    port 26379
    sentinel monitor mymaster my-release-redis-node-0.my-release-redis-headless.default.svc.cluster.local 6379 2
    sentinel down-after-milliseconds mymaster 60000
    sentinel failover-timeout mymaster 18000
    sentinel parallel-syncs mymaster 1
---
# Source: stackstorm-ha/charts/redis/templates/health-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-redis-health
  namespace: "default"
  labels:
    app: redis
    chart: redis-12.3.2
    heritage: Helm
    release: my-release
data:
  ping_readiness_local.sh: |-
    #!/bin/bash
    export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_local.sh: |-
    #!/bin/bash
    export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$response" != "PONG" ] && [ "$response" != "LOADING Redis is loading the dataset in memory" ]; then
      echo "$response"
      exit 1
    fi
  ping_sentinel.sh: |-
    #!/bin/bash
    export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_SENTINEL_PORT \
        ping
    )
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  parse_sentinels.awk: |-
    /ip/ {FOUND_IP=1}
    /port/ {FOUND_PORT=1}
    /runid/ {FOUND_RUNID=1}
    !/ip|port|runid/ {
      if (FOUND_IP==1) {
        IP=$1; FOUND_IP=0;
      }
      else if (FOUND_PORT==1) {
        PORT=$1;
        FOUND_PORT=0;
      } else if (FOUND_RUNID==1) {
        printf "\nsentinel known-sentinel mymaster %s %s %s", IP, PORT, $0; FOUND_RUNID=0;
      }
    }
  ping_readiness_master.sh: |-
    #!/bin/bash
    export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_master.sh: |-
    #!/bin/bash
    export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$response" != "PONG" ] && [ "$response" != "LOADING Redis is loading the dataset in memory" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_readiness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_liveness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_liveness_master.sh" $1 || exit_status=$?
    exit $exit_status
---
# Source: stackstorm-ha/templates/configmaps_packs.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-st2-pack-configs
  annotations:
    description: StackStorm pack configs defined in helm values, shipped in (or copied to) '/opt/stackstorm/configs/'
  labels:
    
    
    app.kubernetes.io/name: st2
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: backend
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
data:
  core.yaml: |
    ---
    # example core pack config yaml
---
# Source: stackstorm-ha/templates/configmaps_post-start-script.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-st2actionrunner-post-start-script
  annotations:
    description: Custom postStart lifecycle event handler script for st2actionrunner
  labels:
    
    
    app.kubernetes.io/name: st2
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: backend
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
data:
  # k8s calls this script in parallel with starting st2actionrunner (ie the same time as ENTRYPOINT)
  # The pod will not be marked as "running" until this script completes successfully.
  # see: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/
  post-start.sh: |
    #!/bin/bash
    mkdir -p /home/stanley/.ssh
    cp -L /home/stanley/.ssh-key-vol/stanley_rsa /home/stanley/.ssh/stanley_rsa
    chown -R stanley:stanley /home/stanley/.ssh
    chmod 400 /home/stanley/.ssh/stanley_rsa
    chmod 500 /home/stanley/.ssh
---
# Source: stackstorm-ha/templates/configmaps_post-start-script.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-st2client-post-start-script
  annotations:
    description: Custom postStart lifecycle event handler script for st2client
  labels:
    
    
    app.kubernetes.io/name: st2
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: backend
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
data:
  # k8s calls this script in parallel with starting st2client (ie the same time as ENTRYPOINT)
  # The pod will not be marked as "running" until this script completes successfully.
  # see: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/
  post-start.sh: |
    #!/bin/bash
    mkdir -p /home/stanley/.ssh
    cp -L /home/stanley/.ssh-key-vol/stanley_rsa /home/stanley/.ssh/stanley_rsa
    chown -R stanley:stanley /home/stanley/.ssh
    chmod 400 /home/stanley/.ssh/stanley_rsa
    chmod 500 /home/stanley/.ssh
---
# Source: stackstorm-ha/templates/configmaps_st2-conf.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-st2-config
  annotations:
    description: Custom StackStorm config which will apply settings on top of default st2.conf
  labels:
    
    
    app.kubernetes.io/name: st2
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: backend
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
data:
  # Docker/K8s-based st2 config file used for templating service names and common overrides on top of original st2.conf.
  # The order of merging: st2.conf < st2.docker.conf < st2.user.conf
  st2.docker.conf: |
    [auth]
    api_url = http://my-release-st2api:9101/
    [system_user]
    user = stanley
    ssh_key_file = /home/stanley/.ssh/stanley_rsa
    [coordination]
    url = redis://my-release-redis-node-0.my-release-redis-headless.default.svc.cluster.local:26379?sentinel=mymaster&sentinel_fallback=my-release-redis-node-1.my-release-redis-headless.default.svc.cluster.local:26379&sentinel_fallback=my-release-redis-node-2.my-release-redis-headless.default.svc.cluster.local:26379
    [messaging]
    url = amqp://admin:9jS+w1u07NbHtZke1m+jW4Cj@my-release-rabbitmq:5672/
    [database]
    host = mongodb://my-release-mongodb-0.my-release-mongodb-headless.default.svc.cluster.local,my-release-mongodb-1.my-release-mongodb-headless.default.svc.cluster.local,my-release-mongodb-2.my-release-mongodb-headless.default.svc.cluster.local/st2?authSource=st2&replicaSet=rs0
    username = st2-admin
    password = XeL5Rxwj7F0Wt43tFZVTN7H8Sg5XDHmK
    db_name = st2
    port = 27017
    [keyvalue]
    encryption_key_path = /etc/st2/keys/datastore_key.json

  # User-defined st2 config with custom settings applied on top of everything else.
  # The order of merging: st2.conf < st2.docker.conf < st2.user.conf
  st2.user.conf: |
    [api]
    allow_origin = '*'
    # fixes no replicaset found bug;
    [database]
    # Connection and server selection timeout (in ms).
    connection_timeout = 5000
---
# Source: stackstorm-ha/templates/configmaps_st2-urls.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-st2-urls
  annotations:
    description: StackStorm service URLs, used across entire st2 cluster
  labels:
    
    
    app.kubernetes.io/name: st2
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: backend
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
data:
  ST2_AUTH_URL: http://my-release-st2auth:9100/
  ST2_API_URL: http://my-release-st2api:9101/
  ST2_STREAM_URL: http://my-release-st2stream:9102/
---
# Source: stackstorm-ha/templates/tests/st2tests-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-st2tests
  labels:
    
    
    app.kubernetes.io/name: st2tests
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: tests
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
data:
  st2tests.sh: |
    #!/usr/bin/env bats
    
    load "${BATS_HELPERS_DIR}/bats-support/load.bash"
    load "${BATS_HELPERS_DIR}/bats-assert/load.bash"
    load "${BATS_HELPERS_DIR}/bats-file/load.bash"
    
    @test 'st2 version deployed and python env are as expected' {
      run st2 --version
      assert_success
      # st2 3.7dev (4aac99ba8), on Python 3.8.10
      assert_line --partial "st2 ${ST2_VERSION}"
      assert_line --partial 'on Python 3.8.10'
    }
    
    @test 'ST2_AUTH_URL service endpoint is accessible and working' {
      run curl -v ${ST2_API_URL}
      assert_line --partial 'Content-Type: application/json'
      assert_line --partial 'St2-Api-Key'
    }
    
    @test 'ST2_API_URL service endpoint is accessible and working' {
      run curl -v ${ST2_API_URL}
      assert_line --partial 'Content-Type: application/json'
      assert_line --partial 'St2-Api-Key'
    }
    
    @test 'ST2_STREAM_URL service endpoint is accessible and working' {
      run curl -v ${ST2_API_URL}
      assert_line --partial 'Content-Type: application/json'
      assert_line --partial 'St2-Api-Key'
    }
    
    @test 'st2 user can log in with auth credentials' {
      run st2 login ${ST2_AUTH_USERNAME} --password ${ST2_AUTH_PASSWORD} -w
      assert_success
      assert_line "Logged in as ${ST2_AUTH_USERNAME}"
      assert_file_exist ~/.st2/config
    }
    
    @test 'st2 core pack is installed and loaded' {
      run st2 action list --pack=core
      assert_success
      assert_line --partial 'core.local'
    }
    
    @test "can execute simple st2 action 'core.local'" {
      run st2 run core.local cmd=id
      assert_success
      assert_line --partial 'return_code: 0'
      assert_line --partial "stderr: ''"
      assert_line --partial 'stdout: uid=1000(stanley) gid=1000(stanley) groups=1000(stanley)'
      assert_line --partial 'succeeded: true'
    }
    
    @test 'stanley_rsa file has correct permissions and ownership' {
      local ssh_dir="/home/stanley/.ssh"
      local private_key="${ssh_dir}/stanley_rsa"
      run st2 run core.local cmd="find ${ssh_dir} -printf '%p: %u %g %m\n'"
      assert_success
      assert_line --partial 'return_code: 0'
      assert_line --partial "stderr: ''"
      assert_line --partial "${ssh_dir}: stanley stanley 500"
      assert_line --partial "${private_key}: stanley stanley 400"
      assert_line --partial 'succeeded: true'
    }
    
    @test 'st2 chatops core rule is loaded' {
      run st2 rule list
      assert_success
      assert_line --partial 'chatops.notify'
    }
    
    @test 'st2 key/value operations are functional' {
      run st2 key set foo bar
      assert_success
    
      run st2 key get foo
      assert_success
      assert_line --partial 'bar'
    
      run st2 key delete foo
      assert_line --partial '"foo" has been successfully deleted'
      assert_success
    
      run st2 key get foo
      assert_line --partial '"foo" is not found'
      assert_failure
    }
    
    @test 'RBAC is loaded and enabled' {
      if [ $ST2_RBAC_ENABLED != "true" ]; then
        skip "disabled in Helm values"
      fi
    
      run st2 whoami
      assert_success
      assert_output --regexp 'RBAC:\s+ - Enabled: True'
      assert_line --partial 'Roles: system_admin'
    }
---
# Source: stackstorm-ha/charts/rabbitmq/templates/role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: my-release-rabbitmq-endpoint-reader
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-8.0.2
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create"]
---
# Source: stackstorm-ha/charts/rabbitmq/templates/rolebinding.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: my-release-rabbitmq-endpoint-reader
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-8.0.2
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
subjects:
  - kind: ServiceAccount
    name: my-release-rabbitmq
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: my-release-rabbitmq-endpoint-reader
---
# Source: stackstorm-ha/charts/mongodb/templates/replicaset/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-mongodb-headless
  namespace: default
  labels:
    app.kubernetes.io/name: mongodb
    helm.sh/chart: mongodb-10.0.1
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: mongodb
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: mongodb
      port: 27017
      targetPort: mongodb
  selector:
    app.kubernetes.io/name: mongodb
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: mongodb
---
# Source: stackstorm-ha/charts/rabbitmq/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-rabbitmq-headless
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-8.0.2
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
spec:
  clusterIP: None
  ports:
    - name: epmd
      port: 4369
      targetPort: epmd
    - name: amqp
      port: 5672
      targetPort: amqp
    - name: dist
      port: 25672
      targetPort: dist
    - name: http-stats
      port: 15672
      targetPort: stats
  selector: 
    app.kubernetes.io/name: rabbitmq
    app.kubernetes.io/instance: my-release
---
# Source: stackstorm-ha/charts/rabbitmq/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-rabbitmq
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-8.0.2
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: amqp
      port: 5672
      targetPort: amqp
      nodePort: null
    - name: epmd
      port: 4369
      targetPort: epmd
      nodePort: null
    - name: dist
      port: 25672
      targetPort: dist
      nodePort: null
    - name: http-stats
      port: 15672
      targetPort: stats
      nodePort: null
  selector: 
    app.kubernetes.io/name: rabbitmq
    app.kubernetes.io/instance: my-release
---
# Source: stackstorm-ha/charts/redis/templates/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-redis-headless
  namespace: "default"
  labels:
    app: redis
    chart: redis-12.3.2
    release: my-release
    heritage: Helm
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: redis
      port: 6379
      targetPort: redis
    - name: redis-sentinel
      port: 26379
      targetPort: redis-sentinel
  selector:
    app: redis
    release: my-release
---
# Source: stackstorm-ha/charts/redis/templates/redis-with-sentinel-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-redis
  namespace: "default"
  labels:
    app: redis
    chart: redis-12.3.2
    release: my-release
    heritage: Helm
spec:
  type: ClusterIP
  
  ports:
    - name: redis
      port: 6379
      targetPort: redis
    - name: redis-sentinel
      port: 26379
      targetPort: redis-sentinel
  selector:
    app: redis
    release: my-release
---
# Source: stackstorm-ha/templates/services.yaml
kind: Service
apiVersion: v1
metadata:
  name: my-release-st2auth
  annotations:
    description: StackStorm st2auth - all authentication is managed by this service.
  labels:
    
    
    app.kubernetes.io/name: st2auth
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: backend
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    
    app.kubernetes.io/name: st2auth
    app.kubernetes.io/instance: my-release
  type: ClusterIP
  ports:
  - protocol: TCP
    port: 9100
---
# Source: stackstorm-ha/templates/services.yaml
kind: Service
apiVersion: v1
metadata:
  name: my-release-st2api
  annotations:
    description: StackStorm st2api - service hosts the REST API endpoints that serve requests from WebUI, CLI, ChatOps and other st2 services.
  labels:
    
    
    app.kubernetes.io/name: st2api
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: backend
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    
    app.kubernetes.io/name: st2api
    app.kubernetes.io/instance: my-release
  type: ClusterIP
  ports:
  - protocol: TCP
    port: 9101
---
# Source: stackstorm-ha/templates/services.yaml
kind: Service
apiVersion: v1
metadata:
  name: my-release-st2stream
  annotations:
    description: StackStorm st2stream - exposes a server-sent event stream, used by the clients like WebUI and ChatOps to receive update from the st2stream server.
  labels:
    
    
    app.kubernetes.io/name: st2stream
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: backend
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    
    app.kubernetes.io/name: st2stream
    app.kubernetes.io/instance: my-release
  type: ClusterIP
  ports:
  - protocol: TCP
    port: 9102
---
# Source: stackstorm-ha/templates/services.yaml
kind: Service
apiVersion: v1
metadata:
  name: my-release-st2web
  annotations:
    description: StackStorm st2web, - an admin Web UI and main entry point for external API requests
  labels:
    
    
    app.kubernetes.io/name: st2web
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: frontend
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    
    app.kubernetes.io/name: st2web
    app.kubernetes.io/instance: my-release
  type: NodePort
  ports:
  - protocol: TCP
    port: 80
---
# Source: stackstorm-ha/templates/deployments.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-st2auth
  labels:
    
    
    app.kubernetes.io/name: st2auth
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: backend
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      
      app.kubernetes.io/name: st2auth
      app.kubernetes.io/instance: my-release
  # https://docs.stackstorm.com/reference/ha.html#st2auth
  # Multiple st2auth processes can be behind a load balancer in an active-active configuration.
  replicas: 2
  template:
    metadata:
      labels:
        
        
        app.kubernetes.io/name: st2auth
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/component: backend
        app.kubernetes.io/part-of: stackstorm
        app.kubernetes.io/version: "3.8"
        helm.sh/chart: stackstorm-ha-1.1.0
        app.kubernetes.io/managed-by: Helm
      annotations:
        checksum/config: a78938a4097d517234084d6ee782d36689697b04c83257f5b6d51d136898c929
        checksum/auth: e514ff549469999f89ac4cfd5a419bc20165925bbde8ab764fda36f51295a652
    spec:
      imagePullSecrets:
      initContainers:
      
      - name: wait-for-db
        image: docker.io/library/busybox:1.28
        imagePullPolicy: IfNotPresent
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 my-release-mongodb-headless 27017 && echo mongodb ok;
              do
                echo 'Waiting for MongoDB Connection...'
                sleep 2;
            done
      
      - name: wait-for-queue
        image: docker.io/library/busybox:1.28
        imagePullPolicy: IfNotPresent
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 my-release-rabbitmq 5672 && echo rabbitmq ok;
              do
                echo 'Waiting for RabbitMQ Connection...'
                sleep 2;
            done
      # Sidecar container for generating .htpasswd with st2 username & password pair and sharing produced file with the main st2auth container
      - name: generate-htpasswd
        image: 'stackstorm/st2auth:3.8'
        imagePullPolicy: IfNotPresent
        envFrom:
        - secretRef:
            name: my-release-st2-auth
        volumeMounts:
        - name: htpasswd-vol
          mountPath: /tmp/st2
        command:
          - 'sh'
          - '-ec'
          - printf "${ST2_AUTH_USERNAME}:$(openssl passwd -apr1 "${ST2_AUTH_PASSWORD}")\n" > /tmp/st2/htpasswd
      terminationGracePeriodSeconds: 30
      containers:
      - name: st2auth
        image: 'stackstorm/st2auth:3.8'
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 9100
        # TODO: Add liveness/readiness probes (#3)
        #livenessProbe:
        #readinessProbe:
        command:
          
          - /opt/stackstorm/st2/bin/st2auth
          - --config-file=/etc/st2/st2.conf
          - --config-file=/etc/st2/st2.docker.conf
          - --config-file=/etc/st2/st2.user.conf
        envFrom:
        - configMapRef:
            name: my-release-st2-urls
        volumeMounts:
        - name: st2-config-vol
          mountPath: /etc/st2/st2.docker.conf
          subPath: st2.docker.conf
        - name: st2-config-vol
          mountPath: /etc/st2/st2.user.conf
          subPath: st2.user.conf
        - name: htpasswd-vol
          mountPath: /etc/st2/htpasswd
          subPath: htpasswd
          readOnly: true
        resources:
          requests:
            cpu: 50m
            memory: 85Mi
      volumes:
        - name: st2-config-vol
          configMap:
            name: my-release-st2-config
        - name: htpasswd-vol
          emptyDir:
            medium: Memory
---
# Source: stackstorm-ha/templates/deployments.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-st2api
  labels:
    
    
    app.kubernetes.io/name: st2api
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: backend
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      
      app.kubernetes.io/name: st2api
      app.kubernetes.io/instance: my-release
  # https://docs.stackstorm.com/reference/ha.html#st2api
  # Multiple st2api process can be behind a load balancer in an active-active configuration.
  replicas: 2
  template:
    metadata:
      labels:
        
        
        app.kubernetes.io/name: st2api
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/component: backend
        app.kubernetes.io/part-of: stackstorm
        app.kubernetes.io/version: "3.8"
        helm.sh/chart: stackstorm-ha-1.1.0
        app.kubernetes.io/managed-by: Helm
      annotations:
        checksum/config: a78938a4097d517234084d6ee782d36689697b04c83257f5b6d51d136898c929
        checksum/datastore-key: 30b4b9ef85016aa8e1927fc063289a057bb4c1dedc0ea6ea8aa7707eb81aea5d
    spec:
      imagePullSecrets:
      initContainers:
      
      - name: wait-for-db
        image: docker.io/library/busybox:1.28
        imagePullPolicy: IfNotPresent
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 my-release-mongodb-headless 27017 && echo mongodb ok;
              do
                echo 'Waiting for MongoDB Connection...'
                sleep 2;
            done
      
      - name: wait-for-queue
        image: docker.io/library/busybox:1.28
        imagePullPolicy: IfNotPresent
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 my-release-rabbitmq 5672 && echo rabbitmq ok;
              do
                echo 'Waiting for RabbitMQ Connection...'
                sleep 2;
            done
      terminationGracePeriodSeconds: 30
      containers:
      - name: st2api
        image: 'stackstorm/st2api:3.8'
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 9101
        # TODO: Add liveness/readiness probes (#3)
        #livenessProbe:
        #readinessProbe:
        command:
          
          - /opt/stackstorm/st2/bin/st2api
          - --config-file=/etc/st2/st2.conf
          - --config-file=/etc/st2/st2.docker.conf
          - --config-file=/etc/st2/st2.user.conf
        envFrom:
        - configMapRef:
            name: my-release-st2-urls
        volumeMounts:
        - name: st2-config-vol
          mountPath: /etc/st2/st2.docker.conf
          subPath: st2.docker.conf
        - name: st2-config-vol
          mountPath: /etc/st2/st2.user.conf
          subPath: st2.user.conf
        - name: st2-encryption-key-vol
          mountPath: /etc/st2/keys
          readOnly: true
        
        resources:
          requests:
            cpu: 25m
            memory: 150Mi
      volumes:
        - name: st2-encryption-key-vol
          secret:
            secretName: my-release-st2-datastore-crypto-key
            items:
            - key: datastore_crypto_key
              path: datastore_key.json
        - name: st2-config-vol
          configMap:
            name: my-release-st2-config
---
# Source: stackstorm-ha/templates/deployments.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-st2stream
  labels:
    
    
    app.kubernetes.io/name: st2stream
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: backend
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      
      app.kubernetes.io/name: st2stream
      app.kubernetes.io/instance: my-release
  # https://docs.stackstorm.com/reference/ha.html#st2stream
  # Multiple st2stream process can be behind a load balancer in an active-active configuration.
  replicas: 2
  template:
    metadata:
      labels:
        
        
        app.kubernetes.io/name: st2stream
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/component: backend
        app.kubernetes.io/part-of: stackstorm
        app.kubernetes.io/version: "3.8"
        helm.sh/chart: stackstorm-ha-1.1.0
        app.kubernetes.io/managed-by: Helm
      annotations:
        checksum/config: a78938a4097d517234084d6ee782d36689697b04c83257f5b6d51d136898c929
    spec:
      imagePullSecrets:
      initContainers:
      
      - name: wait-for-db
        image: docker.io/library/busybox:1.28
        imagePullPolicy: IfNotPresent
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 my-release-mongodb-headless 27017 && echo mongodb ok;
              do
                echo 'Waiting for MongoDB Connection...'
                sleep 2;
            done
      
      - name: wait-for-queue
        image: docker.io/library/busybox:1.28
        imagePullPolicy: IfNotPresent
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 my-release-rabbitmq 5672 && echo rabbitmq ok;
              do
                echo 'Waiting for RabbitMQ Connection...'
                sleep 2;
            done
      terminationGracePeriodSeconds: 30
      containers:
      - name: st2stream
        image: 'stackstorm/st2stream:3.8'
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 9102
        # TODO: Add liveness/readiness probes (#3)
        #livenessProbe:
        #readinessProbe:
        command:
          
          - /opt/stackstorm/st2/bin/st2stream
          - --config-file=/etc/st2/st2.conf
          - --config-file=/etc/st2/st2.docker.conf
          - --config-file=/etc/st2/st2.user.conf
        envFrom:
        - configMapRef:
            name: my-release-st2-urls
        volumeMounts:
        - name: st2-config-vol
          mountPath: /etc/st2/st2.docker.conf
          subPath: st2.docker.conf
        - name: st2-config-vol
          mountPath: /etc/st2/st2.user.conf
          subPath: st2.user.conf
        resources:
          requests:
            cpu: 50m
            memory: 100Mi
      volumes:
        - name: st2-config-vol
          configMap:
            name: my-release-st2-config
---
# Source: stackstorm-ha/templates/deployments.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-st2web
  labels:
    
    
    app.kubernetes.io/name: st2web
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: frontend
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      
      app.kubernetes.io/name: st2web
      app.kubernetes.io/instance: my-release
  replicas: 2
  template:
    metadata:
      labels:
        
        
        app.kubernetes.io/name: st2web
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/component: frontend
        app.kubernetes.io/part-of: stackstorm
        app.kubernetes.io/version: "3.8"
        helm.sh/chart: stackstorm-ha-1.1.0
        app.kubernetes.io/managed-by: Helm
      annotations:
        checksum/config: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
    spec:
      imagePullSecrets:
      terminationGracePeriodSeconds: 30
      containers:
      - name: st2web
        image: 'stackstorm/st2web:3.8'
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
        # Probe to check if app is running. Failure will lead to a pod restart.
        livenessProbe:
          httpGet:
            scheme: HTTP
            path: /
            port: 80
          initialDelaySeconds: 1
        # Probe to check if app is ready to serve traffic. Failure will lead to temp stop serving traffic.
        # TODO: Failing to add readinessProbe, since st2 requires authorization (401) and we don't have `/healthz` endpoints yet (https://github.com/StackStorm/st2/issues/4020)
#        readinessProbe:
#          httpGet:
#            # Probes can't check several endpoints, - this should be implemented on app side (@see https://www.ianlewis.org/en/using-kubernetes-health-checks)
#            # Also multiple liveness checks are not available (@see https://github.com/kubernetes/kubernetes/issues/37218)
#            # So checking ST2_API only
#            scheme: HTTPS
#            path: /api/
#            port: 443
#          initialDelaySeconds: 3
        envFrom:
        - configMapRef:
            name: my-release-st2-urls
        volumeMounts: []
        resources:
          limits:
            memory: 100Mi
          requests:
            cpu: 50m
            memory: 25Mi
      volumes: []
---
# Source: stackstorm-ha/templates/deployments.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-st2rulesengine
  labels:
    
    
    app.kubernetes.io/name: st2rulesengine
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: backend
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      
      app.kubernetes.io/name: st2rulesengine
      app.kubernetes.io/instance: my-release
  # https://docs.stackstorm.com/reference/ha.html#st2rulesengine
  # Multiple st2rulesengine processes can run in active-active with only connections to MongoDB and RabbitMQ. All these will share the TriggerInstance load and naturally pick up more work if one or more of the processes becomes unavailable.
  replicas: 2
  template:
    metadata:
      labels:
        
        
        app.kubernetes.io/name: st2rulesengine
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/component: backend
        app.kubernetes.io/part-of: stackstorm
        app.kubernetes.io/version: "3.8"
        helm.sh/chart: stackstorm-ha-1.1.0
        app.kubernetes.io/managed-by: Helm
      annotations:
        checksum/config: a78938a4097d517234084d6ee782d36689697b04c83257f5b6d51d136898c929
        checksum/datastore-key: 944b22ce282edeaa4a56ad28f984a8ce27699d41e7e729e13e95686209b1f658
    spec:
      imagePullSecrets:
      initContainers:
      
      - name: wait-for-db
        image: docker.io/library/busybox:1.28
        imagePullPolicy: IfNotPresent
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 my-release-mongodb-headless 27017 && echo mongodb ok;
              do
                echo 'Waiting for MongoDB Connection...'
                sleep 2;
            done
      
      - name: wait-for-queue
        image: docker.io/library/busybox:1.28
        imagePullPolicy: IfNotPresent
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 my-release-rabbitmq 5672 && echo rabbitmq ok;
              do
                echo 'Waiting for RabbitMQ Connection...'
                sleep 2;
            done
      containers:
      - name: st2rulesengine
        image: 'stackstorm/st2rulesengine:3.8'
        imagePullPolicy: IfNotPresent
        # TODO: Add liveness/readiness probes (#3)
        #livenessProbe:
        #readinessProbe:
        command:
          
          - /opt/stackstorm/st2/bin/st2rulesengine
          - --config-file=/etc/st2/st2.conf
          - --config-file=/etc/st2/st2.docker.conf
          - --config-file=/etc/st2/st2.user.conf
        envFrom:
        - configMapRef:
            name: my-release-st2-urls
        volumeMounts:
        - name: st2-config-vol
          mountPath: /etc/st2/st2.docker.conf
          subPath: st2.docker.conf
        - name: st2-config-vol
          mountPath: /etc/st2/st2.user.conf
          subPath: st2.user.conf
        - name: st2-encryption-key-vol
          mountPath: /etc/st2/keys
          readOnly: true
        resources:
          requests:
            cpu: 25m
            memory: 75Mi
      volumes:
        - name: st2-config-vol
          configMap:
            name: my-release-st2-config
        - name: st2-encryption-key-vol
          secret:
            secretName: my-release-st2-datastore-crypto-key
            items:
            - key: datastore_crypto_key
              path: datastore_key.json
---
# Source: stackstorm-ha/templates/deployments.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-st2timersengine
  labels:
    
    
    app.kubernetes.io/name: st2timersengine
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: backend
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      
      app.kubernetes.io/name: st2timersengine
      app.kubernetes.io/instance: my-release
  # https://docs.stackstorm.com/reference/ha.html#st2timersengine
  # Only single replica is created as timersengine can't work in active-active mode at the moment and it relies on
  # K8s failover/reschedule capabilities to address cases when the process fails.
  replicas: 1
  template:
    metadata:
      labels:
        
        
        app.kubernetes.io/name: st2timersengine
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/component: backend
        app.kubernetes.io/part-of: stackstorm
        app.kubernetes.io/version: "3.8"
        helm.sh/chart: stackstorm-ha-1.1.0
        app.kubernetes.io/managed-by: Helm
      annotations:
        checksum/config: a78938a4097d517234084d6ee782d36689697b04c83257f5b6d51d136898c929
    spec:
      imagePullSecrets:
      initContainers:
      
      - name: wait-for-db
        image: docker.io/library/busybox:1.28
        imagePullPolicy: IfNotPresent
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 my-release-mongodb-headless 27017 && echo mongodb ok;
              do
                echo 'Waiting for MongoDB Connection...'
                sleep 2;
            done
      
      - name: wait-for-queue
        image: docker.io/library/busybox:1.28
        imagePullPolicy: IfNotPresent
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 my-release-rabbitmq 5672 && echo rabbitmq ok;
              do
                echo 'Waiting for RabbitMQ Connection...'
                sleep 2;
            done
      containers:
      - name: st2timersengine
        image: 'stackstorm/st2timersengine:3.8'
        imagePullPolicy: IfNotPresent
        # TODO: Add liveness/readiness probes (#3)
        #livenessProbe:
        #readinessProbe:
        command:
          
          - /opt/stackstorm/st2/bin/st2timersengine
          - --config-file=/etc/st2/st2.conf
          - --config-file=/etc/st2/st2.docker.conf
          - --config-file=/etc/st2/st2.user.conf
        envFrom:
        - configMapRef:
            name: my-release-st2-urls
        volumeMounts:
        - name: st2-config-vol
          mountPath: /etc/st2/st2.docker.conf
          subPath: st2.docker.conf
        - name: st2-config-vol
          mountPath: /etc/st2/st2.user.conf
          subPath: st2.user.conf
        resources:
          requests:
            cpu: 10m
            memory: 75Mi
      volumes:
        - name: st2-config-vol
          configMap:
            name: my-release-st2-config
---
# Source: stackstorm-ha/templates/deployments.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-st2workflowengine
  labels:
    
    
    app.kubernetes.io/name: st2workflowengine
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: backend
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      
      app.kubernetes.io/name: st2workflowengine
      app.kubernetes.io/instance: my-release
  # https://docs.stackstorm.com/reference/ha.html#st2workflowengine
  # Multiple st2workflowengine processes can run in active-active mode and will share the load and pick up more work if one or more of the processes become available.
  replicas: 2
  template:
    metadata:
      labels:
        
        
        app.kubernetes.io/name: st2workflowengine
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/component: backend
        app.kubernetes.io/part-of: stackstorm
        app.kubernetes.io/version: "3.8"
        helm.sh/chart: stackstorm-ha-1.1.0
        app.kubernetes.io/managed-by: Helm
      annotations:
        checksum/config: a78938a4097d517234084d6ee782d36689697b04c83257f5b6d51d136898c929
        checksum/datastore-key: 394fb59d3be8c042e1f2bf329467140beba395dae21fdfac72c18a9a2e63129a
    spec:
      terminationGracePeriodSeconds: 300
      imagePullSecrets:
      initContainers:
      
      - name: wait-for-db
        image: docker.io/library/busybox:1.28
        imagePullPolicy: IfNotPresent
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 my-release-mongodb-headless 27017 && echo mongodb ok;
              do
                echo 'Waiting for MongoDB Connection...'
                sleep 2;
            done
      
      - name: wait-for-queue
        image: docker.io/library/busybox:1.28
        imagePullPolicy: IfNotPresent
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 my-release-rabbitmq 5672 && echo rabbitmq ok;
              do
                echo 'Waiting for RabbitMQ Connection...'
                sleep 2;
            done
      containers:
      - name: st2workflowengine
        image: 'stackstorm/st2workflowengine:3.8'
        imagePullPolicy: IfNotPresent
        # TODO: Add liveness/readiness probes (#3)
        #livenessProbe:
        #readinessProbe:
        command:
          
          - /opt/stackstorm/st2/bin/st2workflowengine
          - --config-file=/etc/st2/st2.conf
          - --config-file=/etc/st2/st2.docker.conf
          - --config-file=/etc/st2/st2.user.conf
        envFrom:
        - configMapRef:
            name: my-release-st2-urls
        volumeMounts:
        - name: st2-config-vol
          mountPath: /etc/st2/st2.docker.conf
          subPath: st2.docker.conf
        - name: st2-config-vol
          mountPath: /etc/st2/st2.user.conf
          subPath: st2.user.conf
        - name: st2-encryption-key-vol
          mountPath: /etc/st2/keys
          readOnly: true
        resources:
          requests:
            cpu: 100m
            memory: 200Mi
      volumes:
        - name: st2-config-vol
          configMap:
            name: my-release-st2-config
        - name: st2-encryption-key-vol
          secret:
            secretName: my-release-st2-datastore-crypto-key
            items:
            - key: datastore_crypto_key
              path: datastore_key.json
---
# Source: stackstorm-ha/templates/deployments.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-st2scheduler
  labels:
    
    
    app.kubernetes.io/name: st2scheduler
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: backend
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      
      app.kubernetes.io/name: st2scheduler
      app.kubernetes.io/instance: my-release
  # https://docs.stackstorm.com/reference/ha.html#st2scheduler
  replicas: 2
  template:
    metadata:
      labels:
        
        
        app.kubernetes.io/name: st2scheduler
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/component: backend
        app.kubernetes.io/part-of: stackstorm
        app.kubernetes.io/version: "3.8"
        helm.sh/chart: stackstorm-ha-1.1.0
        app.kubernetes.io/managed-by: Helm
      annotations:
        checksum/config: a78938a4097d517234084d6ee782d36689697b04c83257f5b6d51d136898c929
        checksum/datastore-key: 0fc5eff9de6b2c6a109ea5ef0ffceb670f8e09720ce362be09565181dfa14c62
    spec:
      imagePullSecrets:
      initContainers:
      
      - name: wait-for-db
        image: docker.io/library/busybox:1.28
        imagePullPolicy: IfNotPresent
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 my-release-mongodb-headless 27017 && echo mongodb ok;
              do
                echo 'Waiting for MongoDB Connection...'
                sleep 2;
            done
      
      - name: wait-for-queue
        image: docker.io/library/busybox:1.28
        imagePullPolicy: IfNotPresent
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 my-release-rabbitmq 5672 && echo rabbitmq ok;
              do
                echo 'Waiting for RabbitMQ Connection...'
                sleep 2;
            done
      containers:
      - name: st2scheduler
        image: 'stackstorm/st2scheduler:3.8'
        imagePullPolicy: IfNotPresent
        # TODO: Add liveness/readiness probes (#3)
        #livenessProbe:
        #readinessProbe:
        command:
          
          - /opt/stackstorm/st2/bin/st2scheduler
          - --config-file=/etc/st2/st2.conf
          - --config-file=/etc/st2/st2.docker.conf
          - --config-file=/etc/st2/st2.user.conf
        envFrom:
        - configMapRef:
            name: my-release-st2-urls
        volumeMounts:
        - name: st2-config-vol
          mountPath: /etc/st2/st2.docker.conf
          subPath: st2.docker.conf
        - name: st2-config-vol
          mountPath: /etc/st2/st2.user.conf
          subPath: st2.user.conf
        - name: st2-encryption-key-vol
          mountPath: /etc/st2/keys
          readOnly: true
        resources:
          requests:
            cpu: 50m
            memory: 75Mi
      volumes:
        - name: st2-encryption-key-vol
          secret:
            secretName: my-release-st2-datastore-crypto-key
            items:
            - key: datastore_crypto_key
              path: datastore_key.json
        - name: st2-config-vol
          configMap:
            name: my-release-st2-config
---
# Source: stackstorm-ha/templates/deployments.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-st2notifier
  labels:
    
    
    app.kubernetes.io/name: st2notifier
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: backend
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      
      app.kubernetes.io/name: st2notifier
      app.kubernetes.io/instance: my-release
  # https://docs.stackstorm.com/reference/ha.html#st2notifier
  # st2notifier runs in active-active mode and requires for that coordination backend like Redis or Zookeeper
  replicas: 2
  template:
    metadata:
      labels:
        
        
        app.kubernetes.io/name: st2notifier
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/component: backend
        app.kubernetes.io/part-of: stackstorm
        app.kubernetes.io/version: "3.8"
        helm.sh/chart: stackstorm-ha-1.1.0
        app.kubernetes.io/managed-by: Helm
      annotations:
        checksum/config: a78938a4097d517234084d6ee782d36689697b04c83257f5b6d51d136898c929
    spec:
      imagePullSecrets:
      initContainers:
      
      - name: wait-for-db
        image: docker.io/library/busybox:1.28
        imagePullPolicy: IfNotPresent
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 my-release-mongodb-headless 27017 && echo mongodb ok;
              do
                echo 'Waiting for MongoDB Connection...'
                sleep 2;
            done
      
      - name: wait-for-queue
        image: docker.io/library/busybox:1.28
        imagePullPolicy: IfNotPresent
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 my-release-rabbitmq 5672 && echo rabbitmq ok;
              do
                echo 'Waiting for RabbitMQ Connection...'
                sleep 2;
            done
      containers:
      - name: st2notifier
        image: 'stackstorm/st2notifier:3.8'
        imagePullPolicy: IfNotPresent
        # TODO: Add liveness/readiness probes (#3)
        #livenessProbe:
        #readinessProbe:
        command:
          
          - /opt/stackstorm/st2/bin/st2notifier
          - --config-file=/etc/st2/st2.conf
          - --config-file=/etc/st2/st2.docker.conf
          - --config-file=/etc/st2/st2.user.conf
        envFrom:
        - configMapRef:
            name: my-release-st2-urls
        volumeMounts:
        - name: st2-config-vol
          mountPath: /etc/st2/st2.docker.conf
          subPath: st2.docker.conf
        - name: st2-config-vol
          mountPath: /etc/st2/st2.user.conf
          subPath: st2.user.conf
        resources:
          requests:
            cpu: 50m
            memory: 75Mi
      volumes:
        - name: st2-config-vol
          configMap:
            name: my-release-st2-config
---
# Source: stackstorm-ha/templates/deployments.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-st2sensorcontainer
  labels:
    
    
    app.kubernetes.io/name: st2sensorcontainer
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: backend
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      
      app.kubernetes.io/name: st2sensorcontainer
      app.kubernetes.io/instance: my-release
  # https://docs.stackstorm.com/reference/ha.html#st2sensorcontainer
  # It is possible to run st2sensorcontainer(s) in one of these modes:
  #   (1) run all sensors in one pod (1 deployment with 1 pod, the default); or
  #   (2) run multiple sensors per pod (2+ deployments with 1 pod each) using hash range partitions; or
  #   (3) run one sensor per pod using st2.packs.sensors.
  # Each sensor node needs to be provided with proper partition information to share work with other sensor nodes
  # so that the same sensor does not run on different nodes. See: https://docs.stackstorm.com/reference/sensor_partitioning.html
  replicas: 1
  template:
    metadata:
      labels:
        
        
        app.kubernetes.io/name: st2sensorcontainer
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/component: backend
        app.kubernetes.io/part-of: stackstorm
        app.kubernetes.io/version: "3.8"
        helm.sh/chart: stackstorm-ha-1.1.0
        app.kubernetes.io/managed-by: Helm
      annotations:
        checksum/config: a78938a4097d517234084d6ee782d36689697b04c83257f5b6d51d136898c929
        checksum/packs: 6eb74e88ae372be649084f6651b4d225693077d2e038f017ec44534223860ba9
        checksum/datastore-key: a5fe9a5f909d3b59dd631aa418c33abb75bd5001e1befa9c19375f5710f187f3
        stackstorm/sensor-mode: all-sensors-in-one-pod
    spec:
      imagePullSecrets:
      initContainers:
      
      - name: wait-for-db
        image: docker.io/library/busybox:1.28
        imagePullPolicy: IfNotPresent
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 my-release-mongodb-headless 27017 && echo mongodb ok;
              do
                echo 'Waiting for MongoDB Connection...'
                sleep 2;
            done
      
      - name: wait-for-queue
        image: docker.io/library/busybox:1.28
        imagePullPolicy: IfNotPresent
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 my-release-rabbitmq 5672 && echo rabbitmq ok;
              do
                echo 'Waiting for RabbitMQ Connection...'
                sleep 2;
            done
      containers:
      - name: st2sensorcontainer
        image: 'stackstorm/st2sensorcontainer:3.8'
        imagePullPolicy: IfNotPresent
        command:
          
          - /opt/stackstorm/st2/bin/st2sensorcontainer
          - --config-file=/etc/st2/st2.conf
          - --config-file=/etc/st2/st2.docker.conf
          - --config-file=/etc/st2/st2.user.conf
        envFrom:
        - configMapRef:
            name: my-release-st2-urls
        volumeMounts:
        - name: st2-config-vol
          mountPath: /etc/st2/st2.docker.conf
          subPath: st2.docker.conf
        - name: st2-config-vol
          mountPath: /etc/st2/st2.user.conf
          subPath: st2.user.conf
        
        - name: st2-encryption-key-vol
          mountPath: /etc/st2/keys
          readOnly: true
        resources:
          requests:
            cpu: 50m
            memory: 100Mi
      volumes:
        - name: st2-encryption-key-vol
          secret:
            secretName: my-release-st2-datastore-crypto-key
            items:
            - key: datastore_crypto_key
              path: datastore_key.json
        - name: st2-config-vol
          configMap:
            name: my-release-st2-config
---
# Source: stackstorm-ha/templates/deployments.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-st2actionrunner
  labels:
    
    
    app.kubernetes.io/name: st2actionrunner
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: backend
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      
      app.kubernetes.io/name: st2actionrunner
      app.kubernetes.io/instance: my-release
  # https://docs.stackstorm.com/reference/ha.html#st2actionrunner
  # Multiple st2actionrunner processes can run in active-active with only connections to MongoDB and RabbitMQ. Work gets naturally
  # distributed across runners via RabbitMQ. Adding more st2actionrunner processes increases the ability of StackStorm to execute actions.
  replicas: 5
  template:
    metadata:
      labels:
        
        
        app.kubernetes.io/name: st2actionrunner
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/component: backend
        app.kubernetes.io/part-of: stackstorm
        app.kubernetes.io/version: "3.8"
        helm.sh/chart: stackstorm-ha-1.1.0
        app.kubernetes.io/managed-by: Helm
      annotations:
        checksum/config: a78938a4097d517234084d6ee782d36689697b04c83257f5b6d51d136898c929
        checksum/ssh: d88c6517ccf69a61d20b7bc7823739a2924791d8429148ad98bf914bcccff432
        checksum/datastore-key: a2ca37f25ee36659b3dfc928727813c5dd7f9e8ee9017f0638253495715c2378
    spec:
      imagePullSecrets:
      initContainers:
      
      - name: wait-for-db
        image: docker.io/library/busybox:1.28
        imagePullPolicy: IfNotPresent
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 my-release-mongodb-headless 27017 && echo mongodb ok;
              do
                echo 'Waiting for MongoDB Connection...'
                sleep 2;
            done
      
      - name: wait-for-queue
        image: docker.io/library/busybox:1.28
        imagePullPolicy: IfNotPresent
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 my-release-rabbitmq 5672 && echo rabbitmq ok;
              do
                echo 'Waiting for RabbitMQ Connection...'
                sleep 2;
            done
      terminationGracePeriodSeconds: 300
      containers:
      - name: st2actionrunner
        image: 'stackstorm/st2actionrunner:3.8'
        imagePullPolicy: IfNotPresent
        # TODO: Add liveness/readiness probes (#3)
        #livenessProbe:
        #readinessProbe:
        command:
          
          - /opt/stackstorm/st2/bin/st2actionrunner
          - --config-file=/etc/st2/st2.conf
          - --config-file=/etc/st2/st2.docker.conf
          - --config-file=/etc/st2/st2.user.conf
        envFrom:
        - configMapRef:
            name: my-release-st2-urls
        volumeMounts:
        - name: st2-config-vol
          mountPath: /etc/st2/st2.docker.conf
          subPath: st2.docker.conf
        - name: st2-config-vol
          mountPath: /etc/st2/st2.user.conf
          subPath: st2.user.conf
        - name: st2-ssh-key-vol
          mountPath: /home/stanley/.ssh-key-vol/
        - name: st2-encryption-key-vol
          mountPath: /etc/st2/keys
          readOnly: true
        
        - name: st2-post-start-script-vol
          mountPath: /post-start.sh
          subPath: post-start.sh
        lifecycle:
          postStart:
            exec:
              command: ["/bin/bash", "/post-start.sh"]
        resources:
          requests:
            cpu: 75m
            memory: 200Mi
      volumes:
        - name: st2-encryption-key-vol
          secret:
            secretName: my-release-st2-datastore-crypto-key
            items:
            - key: datastore_crypto_key
              path: datastore_key.json
        - name: st2-config-vol
          configMap:
            name: my-release-st2-config
        - name: st2-ssh-key-vol
          secret:
            secretName: my-release-st2-ssh
            items:
            - key: private_key
              path: stanley_rsa
              # 0400 file permission
              mode: 256
        
        - name: st2-post-start-script-vol
          configMap:
            name: my-release-st2actionrunner-post-start-script
---
# Source: stackstorm-ha/templates/deployments.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-st2garbagecollector
  labels:
    
    
    app.kubernetes.io/name: st2garbagecollector
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: backend
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      
      app.kubernetes.io/name: st2garbagecollector
      app.kubernetes.io/instance: my-release
  # https://docs.stackstorm.com/reference/ha.html#st2garbagecollector
  # Having 1 st2garbagecollector unique replica is enough for periodic task like st2 history garbage collection
  replicas: 1
  template:
    metadata:
      labels:
        
        
        app.kubernetes.io/name: st2garbagecollector
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/component: backend
        app.kubernetes.io/part-of: stackstorm
        app.kubernetes.io/version: "3.8"
        helm.sh/chart: stackstorm-ha-1.1.0
        app.kubernetes.io/managed-by: Helm
      annotations:
        checksum/config: a78938a4097d517234084d6ee782d36689697b04c83257f5b6d51d136898c929
    spec:
      imagePullSecrets:
      initContainers:
      
      - name: wait-for-db
        image: docker.io/library/busybox:1.28
        imagePullPolicy: IfNotPresent
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 my-release-mongodb-headless 27017 && echo mongodb ok;
              do
                echo 'Waiting for MongoDB Connection...'
                sleep 2;
            done
      
      - name: wait-for-queue
        image: docker.io/library/busybox:1.28
        imagePullPolicy: IfNotPresent
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 my-release-rabbitmq 5672 && echo rabbitmq ok;
              do
                echo 'Waiting for RabbitMQ Connection...'
                sleep 2;
            done
      containers:
      - name: st2garbagecollector
        image: 'stackstorm/st2garbagecollector:3.8'
        imagePullPolicy: IfNotPresent
        # TODO: Add liveness/readiness probes (#3)
        #livenessProbe:
        #readinessProbe:
        command:
          
          - /opt/stackstorm/st2/bin/st2garbagecollector
          - --config-file=/etc/st2/st2.conf
          - --config-file=/etc/st2/st2.docker.conf
          - --config-file=/etc/st2/st2.user.conf
        envFrom:
        - configMapRef:
            name: my-release-st2-urls
        volumeMounts:
        - name: st2-config-vol
          mountPath: /etc/st2/st2.docker.conf
          subPath: st2.docker.conf
        - name: st2-config-vol
          mountPath: /etc/st2/st2.user.conf
          subPath: st2.user.conf
        resources:
          requests:
            cpu: 10m
            memory: 80Mi
      volumes:
        - name: st2-config-vol
          configMap:
            name: my-release-st2-config
---
# Source: stackstorm-ha/templates/deployments.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-st2client
  labels:
    
    
    app.kubernetes.io/name: st2client
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: backend
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      
      app.kubernetes.io/name: st2client
      app.kubernetes.io/instance: my-release
  replicas: 1
  template:
    metadata:
      labels:
        
        
        app.kubernetes.io/name: st2client
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/component: backend
        app.kubernetes.io/part-of: stackstorm
        app.kubernetes.io/version: "3.8"
        helm.sh/chart: stackstorm-ha-1.1.0
        app.kubernetes.io/managed-by: Helm
      annotations:
        checksum/config: a78938a4097d517234084d6ee782d36689697b04c83257f5b6d51d136898c929
        checksum/rbac: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum/packs: 6eb74e88ae372be649084f6651b4d225693077d2e038f017ec44534223860ba9
        checksum/auth: 596020b859d6e9542a58ae1293a70a3a8b74fe1d441b1e0e247ba1bea3921013
        checksum/ssh: f7c4320b022b55eb57dbcbe28c7caa0214d0848293b1f4e32b74c3ab233efb72
        checksum/datastore-key: b2b97da313e52b7a931d848700900943bfd2736e1e02fa4228037a76e54f588d
    spec:
      imagePullSecrets:
      initContainers:
      # Sidecar container for generating st2client config with st2 username & password pair and sharing produced file with the main container
      - name: generate-st2client-config
        image: 'stackstorm/st2actionrunner:3.8'
        imagePullPolicy: IfNotPresent
        envFrom:
        - configMapRef:
            name: my-release-st2-urls
        - secretRef:
            name: my-release-st2-auth
        volumeMounts:
        - name: st2client-config-vol
          mountPath: /root/.st2/
        # `st2 login` doesn't exit on failure correctly, use old methods instead. See bug: https://github.com/StackStorm/st2/issues/4338
        command:
          - 'sh'
          - '-ec'
          - |
            cat <<EOT > /root/.st2/config
            [credentials]
            username = ${ST2_AUTH_USERNAME}
            password = ${ST2_AUTH_PASSWORD}
            
            EOT
      containers:
      - name: st2client
        image: 'stackstorm/st2actionrunner:3.8'
        imagePullPolicy: IfNotPresent
        env:
        - name: ST2CLIENT
          value: "1"
        
        envFrom:
        - configMapRef:
            name: my-release-st2-urls
        volumeMounts:
        - name: st2-config-vol
          mountPath: /etc/st2/st2.docker.conf
          subPath: st2.docker.conf
        - name: st2-config-vol
          mountPath: /etc/st2/st2.user.conf
          subPath: st2.user.conf
        
        - name: st2client-config-vol
          mountPath: /root/.st2/
        - name: st2-ssh-key-vol
          mountPath: /home/stanley/.ssh-key-vol/
        - name: st2-encryption-key-vol
          mountPath: /etc/st2/keys
          readOnly: true
        
        - name: st2-pack-configs-vol
          mountPath: /opt/stackstorm/configs/
          readOnly: false
        - name: st2-post-start-script-vol
          mountPath: /post-start.sh
          subPath: post-start.sh
        command:
          
          - 'bash'
          - '-ec'
          - 'while true; do sleep 999; done'
        lifecycle:
          postStart:
            exec:
              command: ["/bin/bash", "/post-start.sh"]
        resources:
          requests:
            cpu: 5m
            memory: 5Mi
      volumes:
        - name: st2-encryption-key-vol
          secret:
            secretName: my-release-st2-datastore-crypto-key
            items:
            - key: datastore_crypto_key
              path: datastore_key.json
        - name: st2-config-vol
          configMap:
            name: my-release-st2-config
        
        - name: st2client-config-vol
          emptyDir:
            medium: Memory
        - name: st2-ssh-key-vol
          secret:
            secretName: my-release-st2-ssh
            items:
            - key: private_key
              path: stanley_rsa
              # 0400 file permission
              mode: 256
        
        
        - name: st2-pack-configs-vol
          configMap:
            name: my-release-st2-pack-configs
        - name: st2-post-start-script-vol
          configMap:
            name: my-release-st2client-post-start-script
---
# Source: stackstorm-ha/charts/mongodb/templates/replicaset/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-mongodb
  namespace: default
  labels:
    app.kubernetes.io/name: mongodb
    helm.sh/chart: mongodb-10.0.1
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: mongodb
spec:
  serviceName: my-release-mongodb-headless
  podManagementPolicy: OrderedReady
  replicas: 3
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: mongodb
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: mongodb
  template:
    metadata:
      labels:
        app.kubernetes.io/name: mongodb
        helm.sh/chart: mongodb-10.0.1
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: mongodb
    spec:
      
      serviceAccountName: my-release-mongodb
      securityContext:
        fsGroup: 1001
        sysctls: []
      containers:
        - name: mongodb
          image: docker.io/bitnami/mongodb:4.4
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          command:
            - /scripts/setup.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: MY_POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: K8S_SERVICE_NAME
              value: "my-release-mongodb-headless"
            - name: MONGODB_INITIAL_PRIMARY_HOST
              value: "my-release-mongodb-0.$(K8S_SERVICE_NAME).$(MY_POD_NAMESPACE).svc.cluster.local"
            - name: MONGODB_REPLICA_SET_NAME
              value: "rs0"
            - name: MONGODB_ADVERTISED_HOSTNAME
              value: "$(MY_POD_NAME).$(K8S_SERVICE_NAME).$(MY_POD_NAMESPACE).svc.cluster.local"
            - name: MONGODB_USERNAME
              value: "st2-admin"
            - name: MONGODB_DATABASE
              value: "st2"
            - name: MONGODB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-mongodb
                  key: mongodb-password
            - name: MONGODB_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-mongodb
                  key: mongodb-root-password
            - name: MONGODB_REPLICA_SET_KEY
              valueFrom:
                secretKeyRef:
                  name: my-release-mongodb
                  key: mongodb-replica-set-key
            - name: ALLOW_EMPTY_PASSWORD
              value: "no"
            - name: MONGODB_SYSTEM_LOG_VERBOSITY
              value: "0"
            - name: MONGODB_DISABLE_SYSTEM_LOG
              value: "no"
            - name: MONGODB_ENABLE_IPV6
              value: "no"
            - name: MONGODB_ENABLE_DIRECTORY_PER_DB
              value: "no"
          ports:
            - containerPort: 27017
              name: mongodb
          livenessProbe:
            exec:
              command:
                - mongo
                - --eval
                - "db.adminCommand('ping')"
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            exec:
              command:
                - mongo
                - --eval
                - "db.adminCommand('ping')"
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: datadir
              mountPath: /bitnami/mongodb
              subPath: 
            - name: scripts
              mountPath: /scripts/setup.sh
              subPath: setup.sh
      volumes:
        - name: scripts
          configMap:
            name: my-release-mongodb-scripts
            defaultMode: 0755
  volumeClaimTemplates:
    - metadata:
        name: datadir
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: stackstorm-ha/charts/rabbitmq/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-rabbitmq
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-8.0.2
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
spec:
  serviceName: my-release-rabbitmq-headless
  podManagementPolicy: OrderedReady
  replicas: 3
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: rabbitmq
      app.kubernetes.io/instance: my-release
  template:
    metadata:
      labels:
        app.kubernetes.io/name: rabbitmq
        helm.sh/chart: rabbitmq-8.0.2
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/managed-by: Helm
      annotations:
        checksum/secret: c7e347c27dd40108b0d160ae273c8e48521f81b65d2505309976b3abce101572
    spec:
      
      serviceAccountName: my-release-rabbitmq
      securityContext:
        fsGroup: 1001
        runAsUser: 1001
      terminationGracePeriodSeconds: 10
      containers:
        - name: rabbitmq
          image: docker.io/bitnami/rabbitmq:3.8.9-debian-10-r37
          imagePullPolicy: "IfNotPresent"
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: MY_POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: K8S_SERVICE_NAME
              value: "my-release-rabbitmq-headless"
            - name: K8S_ADDRESS_TYPE
              value: hostname
            - name: RABBITMQ_FORCE_BOOT
              value: "yes"
            - name: RABBITMQ_NODE_NAME
              value: "rabbit@$(MY_POD_NAME).$(K8S_SERVICE_NAME).$(MY_POD_NAMESPACE).svc.cluster.local"
            - name: K8S_HOSTNAME_SUFFIX
              value: ".$(K8S_SERVICE_NAME).$(MY_POD_NAMESPACE).svc.cluster.local"
            - name: RABBITMQ_MNESIA_DIR
              value: "/bitnami/rabbitmq/mnesia/$(RABBITMQ_NODE_NAME)"
            - name: RABBITMQ_LDAP_ENABLE
              value: "no"
            - name: RABBITMQ_LOGS
              value: "-"
            - name: RABBITMQ_ULIMIT_NOFILES
              value: "65536"
            - name: RABBITMQ_USE_LONGNAME
              value: "true"
            - name: RABBITMQ_ERL_COOKIE
              valueFrom:
                secretKeyRef:
                  name: my-release-rabbitmq
                  key: rabbitmq-erlang-cookie
            - name: RABBITMQ_USERNAME
              value: "admin"
            - name: RABBITMQ_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-rabbitmq
                  key: rabbitmq-password
            - name: RABBITMQ_PLUGINS
              value: "rabbitmq_management, rabbitmq_peer_discovery_k8s, rabbitmq_auth_backend_ldap"
          ports:
            - name: amqp
              containerPort: 5672
            - name: dist
              containerPort: 25672
            - name: stats
              containerPort: 15672
            - name: epmd
              containerPort: 4369
          livenessProbe:
            exec:
              command:
                - /bin/bash
                - -ec
                - rabbitmq-diagnostics -q check_running
            initialDelaySeconds: 120
            periodSeconds: 30
            timeoutSeconds: 20
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            exec:
              command:
                - /bin/bash
                - -ec
                - rabbitmq-diagnostics -q check_running
            initialDelaySeconds: 10
            periodSeconds: 30
            timeoutSeconds: 20
            successThreshold: 1
            failureThreshold: 3
          resources:
            limits: {}
            requests: {}
          lifecycle:
            preStop:
              exec:
                command:
                  - bash
                  - -ec
                  - rabbitmqctl stop_app
          volumeMounts:
            - name: configuration
              mountPath: /bitnami/rabbitmq/conf
            - name: data
              mountPath: /bitnami/rabbitmq/mnesia
            - name: load-definition-volume
              mountPath: /app
              readOnly: true
      volumes:
        - name: configuration
          configMap:
            name: my-release-rabbitmq-config
            items:
              - key: rabbitmq.conf
                path: rabbitmq.conf
        - name: load-definition-volume
          secret:
            secretName: "my-release-rabbitmq-definitions"
  volumeClaimTemplates:
    - metadata:
        name: data
        labels:
          app.kubernetes.io/name: rabbitmq
          app.kubernetes.io/instance: my-release
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: stackstorm-ha/charts/redis/templates/redis-node-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-redis-node
  namespace: "default"
  labels:
    app: redis
    chart: redis-12.3.2
    release: my-release
    heritage: Helm
spec:
  replicas: 3
  serviceName: my-release-redis-headless
  selector:
    matchLabels:
      app: redis
      release: my-release
      role: node
  template:
    metadata:
      labels:
        app: redis
        release: my-release
        chart: redis-12.3.2
        role: node
      annotations:
        checksum/health: 5ce4fac9bd960e09f06dff79dfc9291f23dfb2237e507ff9f946c4f9ec96c98c
        checksum/configmap: df610c12d8427fce4e6d83d7d84ce472933834d0fdcea55b0e666f9cfcffaf6c
        checksum/secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
    spec:
      
      securityContext:
        fsGroup: 1001
      serviceAccountName: default
      containers:
        - name: redis
          image: docker.io/bitnami/redis:6.0.9-debian-10-r66
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          command:
            - /bin/bash
            - -c
            - /opt/bitnami/scripts/start-scripts/start-node.sh
          env:
            - name: REDIS_MASTER_PORT_NUMBER
              value: "6379"
            - name: ALLOW_EMPTY_PASSWORD
              value: "yes"
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DATA_DIR
              value: /data
          ports:
            - name: redis
              containerPort: 6379
          livenessProbe:
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local.sh 5
          readinessProbe:
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 10
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local.sh 5
          resources:
            null
          volumeMounts:
            - name: start-scripts
              mountPath: /opt/bitnami/scripts/start-scripts
            - name: health
              mountPath: /health
            - name: redis-data
              mountPath: /data
            - name: config
              mountPath: /opt/bitnami/redis/mounted-etc
            - name: redis-tmp-conf
              mountPath: /opt/bitnami/redis/etc
        - name: sentinel
          image: docker.io/bitnami/redis-sentinel:6.0.9-debian-10-r66
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          command:
            - /bin/bash
            - -c
            - /opt/bitnami/scripts/start-scripts/start-sentinel.sh
          env:
            - name: ALLOW_EMPTY_PASSWORD
              value: "yes"
            - name: REDIS_SENTINEL_TLS_ENABLED
              value: "no"
            - name: REDIS_SENTINEL_PORT
              value: "26379"
          ports:
            - name: redis-sentinel
              containerPort: 26379
          livenessProbe:
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_sentinel.sh 5
          readinessProbe:
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_sentinel.sh 5
          resources:
            null
          volumeMounts:
            - name: start-scripts
              mountPath: /opt/bitnami/scripts/start-scripts
            - name: health
              mountPath: /health
            - name: redis-data
              mountPath: /data
              subPath: 
            - name: config
              mountPath: /opt/bitnami/redis-sentinel/mounted-etc
            - name: sentinel-tmp-conf
              mountPath: /opt/bitnami/redis-sentinel/etc
      volumes:
        - name: start-scripts
          configMap:
            name: my-release-redis-scripts
            defaultMode: 0755
        - name: health
          configMap:
            name: my-release-redis-health
            defaultMode: 0755
        - name: config
          configMap:
            name: my-release-redis
        - name: sentinel-tmp-conf
          emptyDir: {}
        - name: redis-tmp-conf
          emptyDir: {}
  volumeClaimTemplates:
    - metadata:
        name: redis-data
        labels:
          app: redis
          release: my-release
          heritage: Helm
          component: slave
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
        
        selector:
  updateStrategy:
    type: RollingUpdate
---
# Source: stackstorm-ha/templates/deployments.yaml
# Notify users about breaking change regarding packs, to not destroy current installations
---
# Source: stackstorm-ha/templates/secrets_datastore_crypto_key.yaml
# Notify users about breaking change regarding secrets, to not destroy current installations
---
# Source: stackstorm-ha/templates/secrets_rabbitmq.yaml
# This configuration is a workaround to https://github.com/bitnami/charts/issues/4635
# This code block should be dropped once the above issue is resolved and definitions can be defined as shown in
# https://github.com/bitnami/charts/tree/master/bitnami/rabbitmq#load-definitions
---
# Source: stackstorm-ha/templates/secrets_ssh.yaml
# Notify users about breaking change regarding secrets, to not destroy current installations
---
# Source: stackstorm-ha/templates/secrets_st2auth.yaml
# Notify users about breaking change regarding secrets, to not destroy current installations
---
# Source: stackstorm-ha/templates/tests/st2tests-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "my-release-st2tests"
  labels:
    
    
    app.kubernetes.io/name: st2tests
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: tests
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test-success
    "helm.sh/hook-delete-policy": hook-succeeded
spec:
  initContainers:
  # Sidecar container to copy BATS framework to the main container
  - name: bats-core
    image: bats/bats:1.8.2
    command:
      - bash
      - -ec
      - |
        cp -R /opt/bats /tools/
    volumeMounts:
      - name: tools
        mountPath: /tools
  - name: bats-addons
    image: alpine/git:2.36.3
    command:
      - ash
      - -ec
      - |
        git clone --config advice.detachedHead=false --depth 1 --branch v0.3.0 \
          https://github.com/ztombol/bats-assert /tools/bats-assert
        git clone --config advice.detachedHead=false --depth 1 --branch v0.2.0 \
          https://github.com/ztombol/bats-file /tools/bats-file
        git clone --config advice.detachedHead=false --depth 1 --branch v0.3.0 \
          https://github.com/ztombol/bats-support /tools/bats-support
    volumeMounts:
      - name: tools
        mountPath: /tools
  # Run the actual BATS tests
  containers:
  - name: st2tests
    image: 'stackstorm/st2actionrunner:3.8'
    imagePullPolicy: IfNotPresent
    envFrom:
    - configMapRef:
        name: my-release-st2-urls
    - secretRef:
        name: my-release-st2-auth
    env:
    - name: BATS_HELPERS_DIR
      value: /tools
    - name: ST2_VERSION
      value: "3.8"
    - name: ST2_RBAC_ENABLED
      value: "false"
    volumeMounts:
    - name: tools
      mountPath: /tools
    - name: tests
      mountPath: /tests
    command:
      - /tools/bats/bin/bats
      - /tests/st2tests.sh
  volumes:
    - name: tools
      emptyDir: {}
    - name: tests
      configMap:
        name: my-release-st2tests
  restartPolicy: Never
---
# Source: stackstorm-ha/templates/jobs.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: my-release-job-st2-apikey-load
  labels:
    
    
    app.kubernetes.io/name: st2
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: backend
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
  annotations:
    helm.sh/hook: post-install, post-upgrade, post-rollback
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "6"
spec:
  template:
    metadata:
      name: job-st2-apikey-load
      labels:
        
        
        app.kubernetes.io/name: st2
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/component: backend
        app.kubernetes.io/part-of: stackstorm
        app.kubernetes.io/version: "3.8"
        helm.sh/chart: stackstorm-ha-1.1.0
        app.kubernetes.io/managed-by: Helm
      annotations:
        # TODO: Investigate/propose running Helm hook only on condition when ConfigMap or Secret has changed
        checksum/urls: 82c1b3c718d444c60b5b194e4daa511d80a7a07e88b6e2cd6af4fcf7d32f0b14
        checksum/apikeys: f2e11d50e9883afdb739198ae1dfbae3405a874a8177cbe653849e74ceba8866
    spec:
      imagePullSecrets:
      initContainers:
      
      - name: wait-for-db
        image: docker.io/library/busybox:1.28
        imagePullPolicy: IfNotPresent
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 my-release-mongodb-headless 27017 && echo mongodb ok;
              do
                echo 'Waiting for MongoDB Connection...'
                sleep 2;
            done
      - name: wait-for-api
        image: docker.io/library/busybox:1.28
        imagePullPolicy: IfNotPresent
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 my-release-st2api 9101 && echo st2api ready;
              do sleep 2;
            done
      # Sidecar container for generating st2client config with st2 username & password pair and sharing produced file with the main container
      - name: generate-st2client-config
        image: 'stackstorm/st2actionrunner:3.8'
        imagePullPolicy: IfNotPresent
        envFrom:
        - configMapRef:
            name: my-release-st2-urls
        - secretRef:
            name: my-release-st2-auth
        volumeMounts:
        - name: st2client-config-vol
          mountPath: /root/.st2/
        # `st2 login` doesn't exit on failure correctly, use old methods instead. See bug: https://github.com/StackStorm/st2/issues/4338
        command:
          - 'sh'
          - '-ec'
          - |
            cat <<EOT > /root/.st2/config
            [credentials]
            username = ${ST2_AUTH_USERNAME}
            password = ${ST2_AUTH_PASSWORD}
            
            EOT
      containers:
      - name: st2-apikey-load
        image: 'stackstorm/st2actionrunner:3.8'
        imagePullPolicy: IfNotPresent
        command:
          - st2
          - apikey
          - load
          - /etc/st2/apikeys.yaml
        envFrom:
        - configMapRef:
            name: my-release-st2-urls
        volumeMounts:
        - name: st2client-config-vol
          mountPath: /root/.st2/
        - name: st2-apikeys-vol
          mountPath: /etc/st2/apikeys.yaml
          subPath: apikeys.yaml
        # TODO: Find out default resource limits for this specific service (#5)
        #resources:
      volumes:
        - name: st2client-config-vol
          emptyDir:
            medium: Memory
        - name: st2-apikeys-vol
          secret:
            secretName: my-release-st2-apikeys
      restartPolicy: OnFailure
---
# Source: stackstorm-ha/templates/jobs.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: my-release-job-st2-key-load
  labels:
    
    
    app.kubernetes.io/name: st2
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: backend
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
  annotations:
    helm.sh/hook: post-install, post-upgrade, post-rollback
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "6"
spec:
  template:
    metadata:
      name: job-st2-key-load
      labels:
        
        
        app.kubernetes.io/name: st2
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/component: backend
        app.kubernetes.io/part-of: stackstorm
        app.kubernetes.io/version: "3.8"
        helm.sh/chart: stackstorm-ha-1.1.0
        app.kubernetes.io/managed-by: Helm
      annotations:
        # TODO: Investigate/propose running Helm hook only on condition when ConfigMap or Secret has changed
        checksum/config: a78938a4097d517234084d6ee782d36689697b04c83257f5b6d51d136898c929
        checksum/urls: 82c1b3c718d444c60b5b194e4daa511d80a7a07e88b6e2cd6af4fcf7d32f0b14
    spec:
      imagePullSecrets:
      initContainers:
      
      - name: wait-for-db
        image: docker.io/library/busybox:1.28
        imagePullPolicy: IfNotPresent
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 my-release-mongodb-headless 27017 && echo mongodb ok;
              do
                echo 'Waiting for MongoDB Connection...'
                sleep 2;
            done
      # Sidecar container for generating st2client config with st2 username & password pair and sharing produced file with the main container
      - name: generate-st2client-config
        image: 'stackstorm/st2actionrunner:3.8'
        imagePullPolicy: IfNotPresent
        envFrom:
        - configMapRef:
            name: my-release-st2-urls
        - secretRef:
            name: my-release-st2-auth
        volumeMounts:
        - name: st2client-config-vol
          mountPath: /root/.st2/
        # `st2 login` doesn't exit on failure correctly, use old methods instead. See bug: https://github.com/StackStorm/st2/issues/4338
        command:
          - 'sh'
          - '-ec'
          - |
            cat <<EOT > /root/.st2/config
            [credentials]
            username = ${ST2_AUTH_USERNAME}
            password = ${ST2_AUTH_PASSWORD}
            
            EOT
      containers:
      - name: st2-key-load
        image: 'stackstorm/st2actionrunner:3.8'
        imagePullPolicy: IfNotPresent
        command:
          - st2
          - key
          - load
          - "--convert"  # Convert non-string types (hash, array, boolean, int, float) to a JSON string before loading
          - /etc/st2/st2kv.yaml
        envFrom:
        - configMapRef:
            name: my-release-st2-urls
        volumeMounts:
        - name: st2-config-vol
          mountPath: /etc/st2/st2.docker.conf
          subPath: st2.docker.conf
        - name: st2-config-vol
          mountPath: /etc/st2/st2.user.conf
          subPath: st2.user.conf
        - name: st2client-config-vol
          mountPath: /root/.st2/
        - name: st2-kv-vol
          mountPath: /etc/st2/st2kv.yaml
          subPath: st2kv.yaml
        # TODO: Find out default resource limits for this specific service (#5)
        #resources:
      volumes:
        - name: st2-config-vol
          configMap:
            name: my-release-st2-config
        - name: st2client-config-vol
          emptyDir:
            medium: Memory
        - name: st2-kv-vol
          secret:
            secretName: my-release-st2-kv
      restartPolicy: OnFailure
---
# Source: stackstorm-ha/templates/jobs.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: my-release-job-st2-register-content
  labels:
    
    
    app.kubernetes.io/name: st2-register-content
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: backend
    app.kubernetes.io/part-of: stackstorm
    app.kubernetes.io/version: "3.8"
    helm.sh/chart: stackstorm-ha-1.1.0
    app.kubernetes.io/managed-by: Helm
  annotations:
    helm.sh/hook: post-install, post-upgrade, post-rollback
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "7"
spec:
  template:
    metadata:
      name: job-st2-register-content
      labels:
        
        
        app.kubernetes.io/name: st2-register-content
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/component: backend
        app.kubernetes.io/part-of: stackstorm
        app.kubernetes.io/version: "3.8"
        helm.sh/chart: stackstorm-ha-1.1.0
        app.kubernetes.io/managed-by: Helm
      annotations:
        # TODO: Investigate/propose running Helm hook only on condition when ConfigMap or Secret has changed
        checksum/config: a78938a4097d517234084d6ee782d36689697b04c83257f5b6d51d136898c929
        checksum/packs: 6eb74e88ae372be649084f6651b4d225693077d2e038f017ec44534223860ba9
    spec:
      imagePullSecrets:
      initContainers:
      
      - name: wait-for-db
        image: docker.io/library/busybox:1.28
        imagePullPolicy: IfNotPresent
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 my-release-mongodb-headless 27017 && echo mongodb ok;
              do
                echo 'Waiting for MongoDB Connection...'
                sleep 2;
            done
      
      containers:
      - name: st2-register-content
        image: 'stackstorm/st2actionrunner:3.8'
        imagePullPolicy: IfNotPresent
        command:
          - st2-register-content
          - --config-file=/etc/st2/st2.conf
          - --config-file=/etc/st2/st2.docker.conf
          - --config-file=/etc/st2/st2.user.conf
          - --register-all
          - --register-fail-on-failure
        volumeMounts:
        
        - name: st2-config-vol
          mountPath: /etc/st2/st2.docker.conf
          subPath: st2.docker.conf
        - name: st2-config-vol
          mountPath: /etc/st2/st2.user.conf
          subPath: st2.user.conf
        
        - name: st2-pack-configs-vol
          mountPath: /opt/stackstorm/configs/
          readOnly: false
        # TODO: Find out default resource limits for this specific service (#5)
        #resources:
      volumes:
        
        - name: st2-config-vol
          configMap:
            name: my-release-st2-config
        
        
        - name: st2-pack-configs-vol
          configMap:
            name: my-release-st2-pack-configs
      restartPolicy: OnFailure
