---
# Source: crowd/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-crowd
  labels:
    helm.sh/chart: crowd-1.20.0
    app.kubernetes.io/name: crowd
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "5.3.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: crowd/templates/config-jvm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-crowd-jvm-config
  labels:
    helm.sh/chart: crowd-1.20.0
    app.kubernetes.io/name: crowd
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "5.3.0"
    app.kubernetes.io/managed-by: Helm
    
data:
  additional_jvm_args: >-
    -Dcluster.node.name=${KUBE_POD_NAME}
    -Datlassian.logging.cloud.enabled=false
    -XX:ActiveProcessorCount=2
    
  max_heap: 768m
  min_heap: 384m
---
# Source: crowd/templates/configmap-values-analytics.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-crowd-helm-values
  labels:
    helm.sh/chart: crowd-1.20.0
    app.kubernetes.io/name: crowd
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "5.3.0"
    app.kubernetes.io/managed-by: Helm
    
data:
  values.yaml: |
    additionalConfigMaps: []
    additionalContainers: []
    additionalFiles: []
    additionalHosts: []
    additionalInitContainers: []
    additionalLabels: {}
    affinity: {}
    atlassianAnalyticsAndSupport:
      analytics:
        enabled: true
      helmValues:
        enabled: true
    common:
      global: {}
    crowd:
      accessLog:
        enabled: true
        localHomeSubPath: logs
        mountPath: /opt/atlassian/crowd/apache-tomcat/logs
      additionalBundledPlugins: []
      additionalCertificates:
        customCmd: null
        secretName: null
      additionalEnvironmentVariables: []
      additionalJvmArgs: []
      additionalLibraries: []
      additionalPorts: []
      additionalVolumeClaimTemplates: []
      additionalVolumeMounts: []
      containerSecurityContext: {}
      livenessProbe:
        customProbe: {}
        enabled: false
        failureThreshold: 12
        initialDelaySeconds: 60
        periodSeconds: 5
        timeoutSeconds: 1
      ports:
        http: 8095
      postStart:
        command: null
      readinessProbe:
        customProbe: {}
        enabled: true
        failureThreshold: 10
        initialDelaySeconds: 10
        periodSeconds: 5
        timeoutSeconds: 1
      resources:
        container:
          requests:
            cpu: "2"
            memory: 1G
        jvm:
          maxHeap: 768m
          minHeap: 384m
      securityContext:
        fsGroup: 2004
      securityContextEnabled: true
      service:
        annotations: {}
        loadBalancerIP: null
        port: 80
        sessionAffinity: None
        sessionAffinityConfig:
          clientIP:
            timeoutSeconds: null
        type: ClusterIP
      setPermissions: true
      shutdown:
        command: /shutdown-wait.sh
        terminationGracePeriodSeconds: 30
      startupProbe:
        enabled: false
        failureThreshold: 120
        initialDelaySeconds: 60
        periodSeconds: 5
      tomcatConfig:
        acceptCount: "100"
        accessLogsMaxDays: null
        connectionTimeout: "20000"
        enableLookups: "false"
        generateByHelm: false
        maxHttpHeaderSize: "8192"
        maxThreads: "150"
        mgmtPort: "8020"
        minSpareThreads: "25"
        port: "8095"
        protocol: HTTP/1.1
        proxyInternalIps: null
        proxyName: null
        proxyPort: null
        redirectPort: "8443"
        scheme: null
        secure: null
      topologySpreadConstraints: []
      umask: "0022"
      useHelmReleaseNameAsContainerName: false
    fluentd:
      command: null
      customConfigFile: false
      elasticsearch:
        enabled: true
        extraVolumes: []
        hostname: elasticsearch
        indexNamePrefix: crowd
      enabled: false
      fluentdCustomConfig: {}
      httpPort: 9880
      imageRepo: fluent/fluentd-kubernetes-daemonset
      imageTag: v1.11.5-debian-elasticsearch7-1.2
      resources: {}
    image:
      pullPolicy: IfNotPresent
      repository: atlassian/crowd
      tag: ""
    ingress:
      annotations: {}
      className: nginx
      create: false
      host: null
      https: true
      maxBodySize: 250m
      nginx: true
      openShiftRoute: false
      path: /
      proxyConnectTimeout: 60
      proxyReadTimeout: 60
      proxySendTimeout: 60
      routeHttpHeaders: {}
      tlsSecretName: null
    monitoring:
      exposeJmxMetrics: false
      fetchJmxExporterJar: true
      grafana:
        createDashboards: false
        dashboardAnnotations: {}
        dashboardLabels: {}
      jmxExporterCustomConfig: {}
      jmxExporterCustomJarLocation: null
      jmxExporterImageRepo: bitnami/jmx-exporter
      jmxExporterImageTag: 0.18.0
      jmxExporterInitContainer:
        customSecurityContext: {}
        resources: {}
        runAsRoot: true
      jmxExporterPort: 9999
      jmxExporterPortType: ClusterIP
      jmxServiceAnnotations: {}
      serviceMonitor:
        create: false
        prometheusLabelSelector: {}
        scrapeIntervalSeconds: 30
    nodeSelector: {}
    openshift:
      runWithRestrictedSCC: false
    ordinals:
      enabled: false
      start: 0
    podAnnotations: {}
    podDisruptionBudget:
      annotations: {}
      enabled: false
      labels: {}
      maxUnavailable: null
      minAvailable: null
    podLabels: {}
    replicaCount: 1
    serviceAccount:
      annotations: {}
      create: true
      imagePullSecrets: []
      name: null
    terminationGracePeriodSeconds: 30
    testPods:
      affinity: {}
      annotations: {}
      image:
        permissionsTestContainer: debian:stable-slim
        statusTestContainer: alpine:latest
      labels: {}
      nodeSelector: {}
      resources: {}
      schedulerName: null
      tolerations: []
    tolerations: []
    volumes:
      additional: []
      localHome:
        customVolume: {}
        mountPath: /var/atlassian/application-data/crowd
        persistentVolumeClaim:
          create: false
          resources:
            requests:
              storage: 1Gi
          storageClassName: null
        persistentVolumeClaimRetentionPolicy:
          whenDeleted: null
          whenScaled: null
      sharedHome:
        customVolume: {}
        mountPath: /var/atlassian/application-data/crowd/shared
        nfsPermissionFixer:
          command: null
          enabled: true
          imageRepo: alpine
          imageTag: latest
          mountPath: /shared-home
          resources: {}
        persistentVolumeClaim:
          create: false
          resources:
            requests:
              storage: 1Gi
          storageClassName: null
        subPath: null
  analytics.json: |
    
    {
      "imageTag": "5.3.0",
      "replicas": 1,
      "isJmxEnabled": false,
      "ingressType": "NONE",
      "k8sVersion": "1.30",
      "serviceType": "CLUSTER_IP",
      "dbType": "UNKNOWN",
      "isSharedHomePVCCreated": false,
      "isServiceMonitorCreated": false,
      "isGrafanaDashboardsCreated": false,
      "isRunOnOpenshift": false,
      "isRunWithRestrictedSCC": false,
      "isOpenshiftRouteCreated": false
    }
---
# Source: crowd/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-crowd
  labels:
    helm.sh/chart: crowd-1.20.0
    app.kubernetes.io/name: crowd
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "5.3.0"
    app.kubernetes.io/managed-by: Helm
    
  annotations:
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: crowd
    app.kubernetes.io/instance: my-release
---
# Source: crowd/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-crowd
  labels:
    helm.sh/chart: crowd-1.20.0
    app.kubernetes.io/name: crowd
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "5.3.0"
    app.kubernetes.io/managed-by: Helm
    
spec:
  
  replicas: 1
  serviceName: my-release-crowd
  selector:
    matchLabels:
      app.kubernetes.io/name: crowd
      app.kubernetes.io/instance: my-release
  template:
    metadata:
      annotations:
        checksum/config-jvm: 71db4838a406dbf7a9063795dfe3f5c1e22c34d99635fd9c81dad01021e9e341
        
      labels:
        app.kubernetes.io/name: crowd
        app.kubernetes.io/instance: my-release
        
    spec:
      serviceAccountName: my-release-crowd
      terminationGracePeriodSeconds: 30
      hostAliases:
        
      securityContext:
        
        
        fsGroup: 2004
      initContainers:
        
        - name: nfs-permission-fixer
          image: alpine:latest
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 0 # make sure we run as root so we get the ability to change the volume permissions
          volumeMounts:
            - name: shared-home
              mountPath: "/shared-home"
          command: ["sh", "-c", "(chgrp 2004 /shared-home; chmod g+w /shared-home)"]
        
      containers:
        - name: crowd
          image: "atlassian/crowd:5.3.0"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8095
              protocol: TCP
            
            
          readinessProbe:
            httpGet:
              port: 8095
              path: /crowd/status
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 1
            failureThreshold: 10
          resources:
            requests:
              cpu: "2"
              memory: 1G
          volumeMounts:
            
            - name: local-home
              mountPath: "/var/atlassian/application-data/crowd"
            - name: local-home
              mountPath: "/opt/atlassian/crowd/apache-tomcat/logs"
              subPath: "logs"
            - name: shared-home
              mountPath: "/var/atlassian/application-data/crowd/shared"
            - name: helm-values
              mountPath: /opt/atlassian/helm
            
            
            
            
          env:
            - name: KUBE_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            
            - name: ATL_TOMCAT_SCHEME
              value: "https"
            - name: ATL_TOMCAT_SECURE
              value: "true"
            
            - name: ATL_TOMCAT_PORT
              value: "8095"
            
            - name: ATL_TOMCAT_ACCESS_LOG
              value: "true"
            - name: UMASK
              value: "0022"
            - name: SET_PERMISSIONS
              value: "true"
            - name: ATL_PRODUCT_HOME_SHARED
              value: "/var/atlassian/application-data/crowd/shared"
            - name: JVM_SUPPORT_RECOMMENDED_ARGS
              valueFrom:
                configMapKeyRef:
                  key: additional_jvm_args
                  name: my-release-crowd-jvm-config
            - name: JVM_MINIMUM_MEMORY
              valueFrom:
                configMapKeyRef:
                  key: min_heap
                  name: my-release-crowd-jvm-config
            - name: JVM_MAXIMUM_MEMORY
              valueFrom:
                configMapKeyRef:
                  key: max_heap
                  name: my-release-crowd-jvm-config
            
          lifecycle:
            preStop:
              exec:
                command: ["sh", "-c", "/shutdown-wait.sh"]
        
        
        
      volumes:
        
        
        
        - name: local-home
        
          emptyDir: {}
        - name: shared-home
        
          emptyDir: {}
        - name: helm-values
          configMap:
            name: my-release-crowd-helm-values
---
# Source: crowd/templates/tests/test-application-status.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "my-release-crowd-application-status-test"
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"
    
  labels:
    helm.sh/chart: crowd-1.20.0
    app.kubernetes.io/name: crowd
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "5.3.0"
    app.kubernetes.io/managed-by: Helm
    
spec:
  containers:
    - name: test
      image: alpine:latest
      env:
        - name: STATUS_URL
          value: "http://my-release-crowd:80/crowd/status"
      command:
        - /bin/sh
        - -ec
        - |
          apk add -q jq curl
          STATUS=$(curl -s "$STATUS_URL")
          echo "Verifying application state is RUNNING or FIRST_RUN: $STATUS"
          echo $STATUS | jq -e '.state|test("RUNNING|FIRST_RUN")'
  restartPolicy: Never
---
# Source: crowd/templates/tests/test-shared-home-permissions.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "my-release-crowd-shared-home-permissions-test"
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"
    
  labels:
    helm.sh/chart: crowd-1.20.0
    app.kubernetes.io/name: crowd
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "5.3.0"
    app.kubernetes.io/managed-by: Helm
    
spec:
  containers:
    - name: test
      image: debian:stable-slim
      imagePullPolicy: IfNotPresent
      securityContext:
        # We assume that the UID and GID used by the product images are the same, which in practice they are
        runAsUser: 2004
        runAsGroup: 2004
      volumeMounts:
        - name: shared-home
          mountPath: /shared-home
      command:
        - /bin/sh
        - -ec
        - |
          ls -ld /shared-home
          echo "Creating temporary file in shared home as user $(id -u):$(id -g)"
          touch /shared-home/permissions-test
          ls -l /shared-home/permissions-test
          rm /shared-home/permissions-test
  volumes:
    
    - name: shared-home
    
      emptyDir: {}
  restartPolicy: Never
