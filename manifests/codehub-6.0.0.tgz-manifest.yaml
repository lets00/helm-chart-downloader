---
# Source: codehub/charts/postgresql/templates/primary/networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: my-release-postgresql
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 16.2.0
    helm.sh/chart: postgresql-14.3.3
    app.kubernetes.io/component: primary
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/component: primary
  policyTypes:
    - Ingress
    - Egress
  egress:
    - {}
  ingress:
    - ports:
        - port: 5432
---
# Source: codehub/templates/hub/networkpolicy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: codehub
    app.kubernetes.io/version: 4.22.0-r0
    helm.sh/chart: codehub-6.0.0
    app.kubernetes.io/component: hub
  name: my-release-codehub-hub
  namespace: "default"
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: codehub
      app.kubernetes.io/component: hub
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
      - namespaceSelector: {}
        podSelector:
          matchLabels:
            app.kubernetes.io/name: prometheus
    # Prometheus pods with label "app.kubernetes.io/component: prometheus" --> Hub
    - from:
      - namespaceSelector: {}
        podSelector:
          matchLabels:
            app.kubernetes.io/component: prometheus
    # Pods with label "hub.jupyter.org/network-access-hub" --> Hub
    - ports:
        - port: 8081
      from:
        - podSelector:
            matchLabels:
              hub.jupyter.org/network-access-hub: "true"
          namespaceSelector:
            matchLabels: {}
        - namespaceSelector:
            matchLabels:
              hub.jupyter.org/network-access-hub: "true"
  egress:
    # Hub --> Proxy API
    - ports:
        - port: 8001
      to:
        - podSelector:
            matchLabels:
              app.kubernetes.io/instance: my-release
              app.kubernetes.io/name: codehub
              app.kubernetes.io/component: proxy
    # Hub --> Single User
    - ports:
        - port: 
      to:
        - podSelector:
            matchLabels:
              app.kubernetes.io/instance: my-release
              app.kubernetes.io/name: codehub
              app.kubernetes.io/component: singleuser
    ## DNS access
    - ports:
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53
    ## PostgreSQL access
    - ports:
        - protocol: UDP
          port: 5432
        - protocol: TCP
          port: 5432
    ## Hub --> Any IP:PORT
    ##
    - to:
---
# Source: codehub/templates/proxy/networkpolicy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: codehub
    app.kubernetes.io/version: 4.6.1
    helm.sh/chart: codehub-6.0.0
    app.kubernetes.io/component: proxy
  name: my-release-codehub-proxy
  namespace: "default"
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: codehub
      app.kubernetes.io/component: proxy
  policyTypes:
    - Ingress
    - Egress
  ingress:
    ## Any IP --> Proxy
    ##
    - ports:
        - port: 8000
    - from:
      - namespaceSelector: {}
        podSelector:
          matchLabels:
            app.kubernetes.io/name: prometheus
    
    # Pods with label "hub.jupyter.org/network-access-proxy-api" --> Proxy API
    - ports:
        - port: 8001
      from:
        - podSelector:
            matchLabels:
              hub.jupyter.org/network-access-proxy-api: "true"
          namespaceSelector:
            matchLabels: {}
        - namespaceSelector:
            matchLabels:
              hub.jupyter.org/network-access-proxy-api: "true"
  egress:
    # Proxy --> Hub
    - ports:
        - port: 8081
      to:
        - podSelector:
            matchLabels:
              app.kubernetes.io/instance: my-release
              app.kubernetes.io/name: codehub
              app.kubernetes.io/component: hub
    # Proxy --> Single User
    - ports:
        - port: 
      to:
        - podSelector:
            matchLabels:
              app.kubernetes.io/instance: my-release
              app.kubernetes.io/name: codehub
              app.kubernetes.io/component: singleuser
    ## DNS access
    - ports:
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53
---
# Source: codehub/templates/singleuser/networkpolicy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  labels:
    app.kubernetes.io/name: codehub
    helm.sh/chart: codehub-6.0.0
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "4.22.0-r0"
    app.kubernetes.io/component: singleuser
  name: my-release-codehub-singleuser
  namespace: "default"
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: codehub
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: singleuser
  policyTypes:
    - Ingress
    - Egress
  ingress:
    # Pods with label "hub.jupyter.org/network-access-singleuser" --> Single User
    - ports:
        - port: 
      from:
        - podSelector:
            matchLabels:
              hub.jupyter.org/network-access-singleuser: "true"
          namespaceSelector:
            matchLabels: {}
        - namespaceSelector:
            matchLabels:
              hub.jupyter.org/network-access-singleuser: "true"
  egress:
    # Single User --> Hub
    - ports:
        - port: 8081
      to:
        - podSelector:
            matchLabels:
              app.kubernetes.io/name: codehub
              app.kubernetes.io/instance: my-release
              app.kubernetes.io/component: hub
    ## Limit access to the Cloud Metadata API
    - to:
        - ipBlock:
            cidr: 0.0.0.0/0
            except:
              - 169.254.169.254/32
    ## DNS access
    - ports:
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53
    ## PostgreSQL access
    - ports:
        - protocol: UDP
          port: 5432
        - protocol: TCP
          port: 5432
---
# Source: codehub/charts/postgresql/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-postgresql
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 16.2.0
    helm.sh/chart: postgresql-14.3.3
automountServiceAccountToken: false
---
# Source: codehub/templates/hub/service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-codehub-hub
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: codehub
    app.kubernetes.io/version: 4.22.0-r0
    helm.sh/chart: codehub-6.0.0
    app.kubernetes.io/component: hub
automountServiceAccountToken: true
---
# Source: codehub/templates/image-puller/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-codehub-image-puller
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: codehub
    app.kubernetes.io/version: 4.0.2
    helm.sh/chart: codehub-6.0.0
    app.kubernetes.io/component: image-puller
automountServiceAccountToken: false
---
# Source: codehub/templates/proxy/service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-codehub-proxy
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: codehub
    app.kubernetes.io/version: 4.6.1
    helm.sh/chart: codehub-6.0.0
    app.kubernetes.io/component: proxy
automountServiceAccountToken: false
---
# Source: codehub/templates/singleuser/service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-codehub-singleuser
  namespace: "default"
  labels:
    app.kubernetes.io/name: codehub
    helm.sh/chart: codehub-6.0.0
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "4.22.0-r0"
    app.kubernetes.io/component: singleuser
  annotations:
automountServiceAccountToken: false
---
# Source: codehub/charts/postgresql/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-release-postgresql
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 16.2.0
    helm.sh/chart: postgresql-14.3.3
type: Opaque
data:
  postgres-password: "YWtSZG1mWWg0Rw=="
  password: "czV2cGVrb3F1Vw=="
  # We don't auto-generate LDAP password when it's not provided as we do for other passwords
---
# Source: codehub/templates/hub/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-release-codehub-hub
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: codehub
    app.kubernetes.io/version: 4.22.0-r0
    helm.sh/chart: codehub-6.0.0
    app.kubernetes.io/component: hub
type: Opaque
data:
  apiToken: "ZWVhYzU4MjVkNDkwYmQxYzc0ZjliMzFmMTE3ZGU0MmQ0MzJhYjE2YmUyYzk3ZjE1ODhiZjBiNzg4NWYyMDkyYQ=="
  hub.config.JupyterHub.cookie_secret: "ZjdjNzY3MWUxOTk1ODAwNzA3ZDM3NzllMDEyNjdiZjBkZDRhY2NhYmI5NmE3Mjk5NmM4YThmMzkwZDY2MjBjMg=="
  hub.config.CryptKeeper.keys: "ZWY5MGRiY2Q4YmQyNTk0NjQxMWRiMjIzZWVmZDFkOWYyYTc5ODVmOTc3ZTY2ODViMGZlNGI3YjE0MzYxZGI0YQ=="
---
# Source: codehub/templates/hub/configmap-values.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: codehub
    app.kubernetes.io/version: 4.22.0-r0
    helm.sh/chart: codehub-6.0.0
    app.kubernetes.io/component: hub
  name: my-release-codehub-hub-values
  namespace: "default"
data:
  values.yaml: |
    Chart:
      Name: codehub
      Version: 6.0.0
    Release:
      Name: my-release
      Namespace: default
      Service: Helm
    hub:
      config:
        JupyterHub:
          admin_access: true
          authenticator_class: gitlab
          GitLabOAuthenticator:
            gitlab_url: https://gitlab.com
            allowed_gitlab_groups: []
            allowed_project_ids: []
            client_id: 
            client_secret: 
          Authenticator:
            admin_users:
              - user
      cookieSecret:
      concurrentSpawnLimit: 64
      consecutiveFailureLimit: 5
      activeServerLimit:
      db:
        type: postgres
        url: postgresql://bn_jupyterhub@my-release-postgresql:5432/bitnami_jupyterhub
      services: {}
      allowNamedServers: false
      namedServerLimitPerUser:
      authenticatePrometheus: false
      redirectToServer:
      shutdownOnLogout:
    singleuser:
      podNameTemplate: my-release-codehub-codehub-{username}
      networkTools:
        image:
          name: docker.io/bitnami/os-shell
          tag: 12-debian-12-r16
          digest: 
          pullPolicy: IfNotPresent
          pullSecrets:
          
      cloudMetadata:
        blockWithIptables: false
      events: true
      profileList:
        - description: "https://lab.frogg.it/doca/codehub-container-images/-/tree/main/code-server"
          default: true
          display_name: "Code-Server"
          kubespawner_override:
            image: docker.io/captnbp/code-server:4.23.0-r0
            image_pull_policy: IfNotPresent
            extra_containers:
              - image: docker.io/captnbp/nginx:4.21.2-r0
                name: nginx
                resources:
                  limits:
                    cpu: 100m
                    memory: 100Mi
                  requests:
                    cpu: 100m
                    memory: 50Mi
                securityContext:
                  allowPrivilegeEscalation: false
                  capabilities:
                    drop:
                    - ALL
                  privileged: false
                  runAsGroup: 101
                  runAsNonRoot: true
                  runAsUser: 101
                  seLinuxOptions: null
                  seccompProfile:
                    type: RuntimeDefault
              - image: docker.io/captnbp/oauth:4.21.2-r0
                name: oauth
                ports:
                - containerPort: 9095
                  name: oauth
                  protocol: TCP
                resources:
                  limits:
                    cpu: 100m
                    memory: 100Mi
                  requests:
                    cpu: 100m
                    memory: 50Mi
                securityContext:
                  allowPrivilegeEscalation: false
                  capabilities:
                    drop:
                    - ALL
                  privileged: false
                  readOnlyFileSystem: true
                  runAsGroup: 33
                  runAsNonRoot: true
                  runAsUser: 33
                  seLinuxOptions: null
                  seccompProfile:
                    type: RuntimeDefault
            container_security_context:
              allowPrivilegeEscalation: false
              capabilities:
                drop:
                - ALL
              privileged: false
              readOnlyRootFilesystem: false
              runAsNonRoot: true
              runAsUser: 1000
              seLinuxOptions: null
              seccompProfile:
                type: RuntimeDefault
            fsGid: 1000
            mem_guarantee: 
            mem_limit: 
            cpu_guarantee: 
            cpu_limit: 
            common_labels:
              app.kubernetes.io/component: singleuser
              app.kubernetes.io/name: codehub
              helm.sh/chart: codehub-6.0.0
              app.kubernetes.io/instance: my-release
              app.kubernetes.io/managed-by: Helm
              app.kubernetes.io/version: "4.22.0-r0"
            extra_labels:
              hub.jupyter.org/network-access-hub: "true"
            notebook_dir: /home/coder
            port: 8888
            k8s_api_request_timeout: 10
      serviceAccountName: my-release-codehub-singleuser
      storage:
        type: dynamic
        extraLabels:
          app.kubernetes.io/component: singleuser
          app.kubernetes.io/name: codehub
          helm.sh/chart: codehub-6.0.0
          app.kubernetes.io/instance: my-release
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/version: "4.22.0-r0"
        capacity: 20Gi
        homeMountPath: /home/coder
        dynamic:
          
          pvcNameTemplate: my-release-codehub-claim-{username}{servername}
          volumeNameTemplate: my-release-codehub-volume-{username}{servername}
          storageAccessModes:
            - ReadWriteOnce
      startTimeout: 300
      defaultUrl:
    cull:
      enabled: true
      users: false
      removeNamedServers: false
      timeout: 3600
      every: 600
      concurrency: 10
      maxAge: 0
---
# Source: codehub/templates/hub/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: codehub
    app.kubernetes.io/version: 4.22.0-r0
    helm.sh/chart: codehub-6.0.0
    app.kubernetes.io/component: hub
  name: my-release-codehub-hub
  namespace: "default"
data:
  ## Taken from upstream JupyterHub chart
  ## Source: https://jupyterhub.github.io/helm-chart/jupyterhub-2.0.0.tgz
  jupyterhub_config.py: |
    import glob
    import os
    import re
    import sys
    from binascii import a2b_hex

    from jupyterhub.utils import url_path_join
    from kubernetes_asyncio import client
    from tornado.httpclient import AsyncHTTPClient

    # Make sure that modules placed in the same directory as the jupyterhub config are added to the pythonpath
    configuration_directory = os.path.dirname(os.path.realpath(__file__))
    sys.path.insert(0, configuration_directory)

    from z2jh import (
        get_config,
        get_name,
        get_name_env,
        get_secret_value,
        set_config_if_not_none,
    )


    def camelCaseify(s):
        """convert snake_case to camelCase

        For the common case where some_value is set from someValue
        so we don't have to specify the name twice.
        """
        return re.sub(r"_([a-z])", lambda m: m.group(1).upper(), s)


    # Configure JupyterHub to use the curl backend for making HTTP requests,
    # rather than the pure-python implementations. The default one starts
    # being too slow to make a large number of requests to the proxy API
    # at the rate required.
    AsyncHTTPClient.configure("tornado.curl_httpclient.CurlAsyncHTTPClient")

    c.JupyterHub.spawner_class = "kubespawner.KubeSpawner"

    # Connect to a proxy running in a different pod. Note that *_SERVICE_*
    # environment variables are set by Kubernetes for Services
    # Adapted by Bitnami to allow other service names
    c.ConfigurableHTTPProxy.api_url = (
        f"http://my-release-codehub-proxy-api:{os.environ['PROXY_API_SERVICE_PORT']}"
    )
    c.ConfigurableHTTPProxy.should_start = False

    # Do not shut down user pods when hub is restarted
    c.JupyterHub.cleanup_servers = False

    # Check that the proxy has routes appropriately setup
    c.JupyterHub.last_activity_interval = 60

    # Don't wait at all before redirecting a spawning user to the progress page
    c.JupyterHub.tornado_settings = {
        "slow_spawn_timeout": 0,
    }

    # configure the hub db connection
    db_type = get_config("hub.db.type")
    if db_type == "sqlite-pvc":
        c.JupyterHub.db_url = "sqlite:///jupyterhub.sqlite"
    elif db_type == "sqlite-memory":
        c.JupyterHub.db_url = "sqlite://"
    else:
        set_config_if_not_none(c.JupyterHub, "db_url", "hub.db.url")
    db_password = get_secret_value("hub.db.password", None)
    if db_password is not None:
        if db_type == "mysql":
            os.environ["MYSQL_PWD"] = db_password
        elif db_type == "postgres":
            os.environ["PGPASSWORD"] = db_password
        else:
            print(f"Warning: hub.db.password is ignored for hub.db.type={db_type}")


    # c.JupyterHub configuration from Helm chart's configmap
    for trait, cfg_key in (
        ("concurrent_spawn_limit", None),
        ("active_server_limit", None),
        ("base_url", None),
        ("allow_named_servers", None),
        ("named_server_limit_per_user", None),
        ("authenticate_prometheus", None),
        ("redirect_to_server", None),
        ("shutdown_on_logout", None),
        ("template_paths", None),
        ("template_vars", None),
    ):
        if cfg_key is None:
            cfg_key = camelCaseify(trait)
        set_config_if_not_none(c.JupyterHub, trait, "hub." + cfg_key)

    # hub_bind_url configures what the JupyterHub process within the hub pod's
    # container should listen to.
    hub_container_port = 8081
    c.JupyterHub.hub_bind_url = f"http://:{hub_container_port}"

    # hub_connect_url is the URL for connecting to the hub for use by external
    # JupyterHub services such as the proxy. Note that *_SERVICE_* environment
    # variables are set by Kubernetes for Services.
    c.JupyterHub.hub_connect_url = (
        f"http://my-release-codehub-hub:{os.environ['HUB_SERVICE_PORT']}"
    )

    # implement common labels
    # this duplicates the codehub.commonLabels helper
    common_labels = c.KubeSpawner.common_labels = {}
    common_labels["app"] = get_config(
        "nameOverride",
        default=get_config("Chart.Name", "jupyterhub"),
    )
    common_labels["heritage"] = "jupyterhub"
    chart_name = get_config("Chart.Name")
    chart_version = get_config("Chart.Version")
    if chart_name and chart_version:
        common_labels["chart"] = "{}-{}".format(
            chart_name,
            chart_version.replace("+", "_"),
        )
    release = get_config("Release.Name")
    if release:
        common_labels["release"] = release

    c.KubeSpawner.namespace = os.environ.get("POD_NAMESPACE", "default")

    # Max number of consecutive failures before the Hub restarts itself
    # requires jupyterhub 0.9.2
    set_config_if_not_none(
        c.Spawner,
        "consecutive_failure_limit",
        "hub.consecutiveFailureLimit",
    )

    for trait, cfg_key in (
        ("pod_name_template", None),
        ("start_timeout", None),
        ("events_enabled", "events"),
        ("container_security_context", "container_security_context"),
        ("service_account", "serviceAccountName"),
        ("storage_extra_labels", "storage.extraLabels"),
        ("node_affinity_required", "extraNodeAffinity.required"),
        ("node_affinity_preferred", "extraNodeAffinity.preferred"),
        ("pod_affinity_required", "extraPodAffinity.required"),
        ("pod_affinity_preferred", "extraPodAffinity.preferred"),
        ("pod_anti_affinity_required", "extraPodAntiAffinity.required"),
        ("pod_anti_affinity_preferred", "extraPodAntiAffinity.preferred"),
        ("profile_list", None),
        ("extra_pod_config", None),
    ):
        if cfg_key is None:
            cfg_key = camelCaseify(trait)
        set_config_if_not_none(c.KubeSpawner, trait, "singleuser." + cfg_key)

    # scheduling:
    if get_config("scheduling.userScheduler.enabled"):
        c.KubeSpawner.scheduler_name = get_name("user-scheduler")
    if get_config("scheduling.podPriority.enabled"):
        c.KubeSpawner.priority_class_name = get_name("priority")

    # add node-purpose affinity
    match_node_purpose = get_config("scheduling.userPods.nodeAffinity.matchNodePurpose")
    if match_node_purpose:
        node_selector = dict(
            matchExpressions=[
                dict(
                    key="hub.jupyter.org/node-purpose",
                    operator="In",
                    values=["user"],
                )
            ],
        )
        if match_node_purpose == "prefer":
            c.KubeSpawner.node_affinity_preferred.append(
                dict(
                    weight=100,
                    preference=node_selector,
                ),
            )
        elif match_node_purpose == "require":
            c.KubeSpawner.node_affinity_required.append(node_selector)
        elif match_node_purpose == "ignore":
            pass
        else:
            raise ValueError(
                f"Unrecognized value for matchNodePurpose: {match_node_purpose}"
            )

    # Configure dynamically provisioning pvc
    storage_type = get_config("singleuser.storage.type")
    if storage_type == "dynamic":
        pvc_name_template = get_config("singleuser.storage.dynamic.pvcNameTemplate")
        c.KubeSpawner.pvc_name_template = pvc_name_template
        volume_name_template = get_config("singleuser.storage.dynamic.volumeNameTemplate")
        c.KubeSpawner.storage_pvc_ensure = True
        set_config_if_not_none(
            c.KubeSpawner, "storage_class", "singleuser.storage.dynamic.storageClass"
        )
        set_config_if_not_none(
            c.KubeSpawner,
            "storage_access_modes",
            "singleuser.storage.dynamic.storageAccessModes",
        )
        set_config_if_not_none(
            c.KubeSpawner, "storage_capacity", "singleuser.storage.capacity"
        )

        # Add volumes to singleuser pods
        c.KubeSpawner.volumes = [
            {
                "name": volume_name_template,
                "persistentVolumeClaim": {"claimName": pvc_name_template},
            }
        ]
        c.KubeSpawner.volume_mounts = [
            {
                "mountPath": get_config("singleuser.storage.homeMountPath"),
                "name": volume_name_template,
            }
        ]
    elif storage_type == "static":
        pvc_claim_name = get_config("singleuser.storage.static.pvcName")
        c.KubeSpawner.volumes = [
            {"name": "home", "persistentVolumeClaim": {"claimName": pvc_claim_name}}
        ]

        c.KubeSpawner.volume_mounts = [
            {
                "mountPath": get_config("singleuser.storage.homeMountPath"),
                "name": "home",
                "subPath": get_config("singleuser.storage.static.subPath"),
            }
        ]

    # Inject singleuser.extraFiles as volumes and volumeMounts with data loaded from
    # the dedicated k8s Secret prepared to hold the extraFiles actual content.
    extra_files = get_config("singleuser.extraFiles", {})
    if extra_files:
        volume = {
            "name": "files",
        }
        items = []
        for file_key, file_details in extra_files.items():
            # Each item is a mapping of a key in the k8s Secret to a path in this
            # abstract volume, the goal is to enable us to set the mode /
            # permissions only though so we don't change the mapping.
            item = {
                "key": file_key,
                "path": file_key,
            }
            if "mode" in file_details:
                item["mode"] = file_details["mode"]
            items.append(item)
        volume["secret"] = {
            "secretName": get_name("singleuser"),
            "items": items,
        }
        c.KubeSpawner.volumes.append(volume)

        volume_mounts = []
        for file_key, file_details in extra_files.items():
            volume_mounts.append(
                {
                    "mountPath": file_details["mountPath"],
                    "subPath": file_key,
                    "name": "files",
                }
            )
        c.KubeSpawner.volume_mounts.extend(volume_mounts)

    # Inject extraVolumes / extraVolumeMounts
    c.KubeSpawner.volumes.extend(get_config("singleuser.storage.extraVolumes", []))
    c.KubeSpawner.volume_mounts.extend(
        get_config("singleuser.storage.extraVolumeMounts", [])
    )

    c.JupyterHub.services = []
    c.JupyterHub.load_roles = []

    # jupyterhub-idle-culler's permissions are scoped to what it needs only, see
    # https://github.com/jupyterhub/jupyterhub-idle-culler#permissions.
    #
    if get_config("cull.enabled", False):
        jupyterhub_idle_culler_role = {
            "name": "jupyterhub-idle-culler",
            "scopes": [
                "list:users",
                "read:users:activity",
                "read:servers",
                "delete:servers",
                # "admin:users", # dynamically added if --cull-users is passed
            ],
            # assign the role to a jupyterhub service, so it gains these permissions
            "services": ["jupyterhub-idle-culler"],
        }

        cull_cmd = ["python3", "-m", "jupyterhub_idle_culler"]
        base_url = c.JupyterHub.get("base_url", "/")
        cull_cmd.append("--url=http://localhost:8081" + url_path_join(base_url, "hub/api"))

        cull_timeout = get_config("cull.timeout")
        if cull_timeout:
            cull_cmd.append(f"--timeout={cull_timeout}")

        cull_every = get_config("cull.every")
        if cull_every:
            cull_cmd.append(f"--cull-every={cull_every}")

        cull_concurrency = get_config("cull.concurrency")
        if cull_concurrency:
            cull_cmd.append(f"--concurrency={cull_concurrency}")

        if get_config("cull.users"):
            cull_cmd.append("--cull-users")
            jupyterhub_idle_culler_role["scopes"].append("admin:users")

        if not get_config("cull.adminUsers"):
            cull_cmd.append("--cull-admin-users=false")

        if get_config("cull.removeNamedServers"):
            cull_cmd.append("--remove-named-servers")

        cull_max_age = get_config("cull.maxAge")
        if cull_max_age:
            cull_cmd.append(f"--max-age={cull_max_age}")

        c.JupyterHub.services.append(
            {
                "name": "jupyterhub-idle-culler",
                "command": cull_cmd,
            }
        )
        c.JupyterHub.load_roles.append(jupyterhub_idle_culler_role)

    for key, service in get_config("hub.services", {}).items():
        # c.JupyterHub.services is a list of dicts, but
        # hub.services is a dict of dicts to make the config mergable
        service.setdefault("name", key)

        # As the api_token could be exposed in hub.existingSecret, we need to read
        # it it from there or fall back to the chart managed k8s Secret's value.
        service.pop("apiToken", None)
        service["api_token"] = get_secret_value(f"hub.services.{key}.apiToken")

        c.JupyterHub.services.append(service)

    for key, role in get_config("hub.loadRoles", {}).items():
        # c.JupyterHub.load_roles is a list of dicts, but
        # hub.loadRoles is a dict of dicts to make the config mergable
        role.setdefault("name", key)

        c.JupyterHub.load_roles.append(role)

    # respect explicit null command (distinct from unspecified)
    # this avoids relying on KubeSpawner.cmd's default being None
    _unspecified = object()
    specified_cmd = get_config("singleuser.cmd", _unspecified)
    if specified_cmd is not _unspecified:
        c.Spawner.cmd = specified_cmd

    set_config_if_not_none(c.Spawner, "default_url", "singleuser.defaultUrl")

    cloud_metadata = get_config("singleuser.cloudMetadata", {})

    if cloud_metadata.get("blockWithIptables") == True:
        # Use iptables to block access to cloud metadata by default
        network_tools_image_name = get_config("singleuser.networkTools.image.name")
        network_tools_image_tag = get_config("singleuser.networkTools.image.tag")
        network_tools_resources = get_config("singleuser.networkTools.resources")
        ip_block_container = client.V1Container(
            name="block-cloud-metadata",
            image=f"{network_tools_image_name}:{network_tools_image_tag}",
            command=[
                "/bin/bash"
            ],
            args=[
                "-ec",
                "install_packages iptables && iptables -A OUTPUT -d " + cloud_metadata.get("ip", "169.254.169.254") + " -j DROP"
            ],
            security_context=client.V1SecurityContext(
                privileged=True,
                run_as_user=0,
                capabilities=client.V1Capabilities(add=["NET_ADMIN"]),
            ),
            resources=network_tools_resources,
        )

        c.KubeSpawner.init_containers.append(ip_block_container)


    if get_config("debug.enabled", False):
        c.JupyterHub.log_level = "DEBUG"
        c.Spawner.debug = True

    # load potentially seeded secrets
    #
    # NOTE: ConfigurableHTTPProxy.auth_token is set through an environment variable
    #       that is set using the chart managed secret.
    c.JupyterHub.cookie_secret = get_secret_value("hub.config.JupyterHub.cookie_secret")
    # NOTE: CryptKeeper.keys should be a list of strings, but we have encoded as a
    #       single string joined with ; in the k8s Secret.
    #
    c.CryptKeeper.keys = get_secret_value("hub.config.CryptKeeper.keys").split(";")

    # load hub.config values, except potentially seeded secrets already loaded
    for app, cfg in get_config("hub.config", {}).items():
        if app == "JupyterHub":
            cfg.pop("proxy_auth_token", None)
            cfg.pop("cookie_secret", None)
            cfg.pop("services", None)
        elif app == "ConfigurableHTTPProxy":
            cfg.pop("auth_token", None)
        elif app == "CryptKeeper":
            cfg.pop("keys", None)
        c[app].update(cfg)

    # load /usr/local/etc/jupyterhub/jupyterhub_config.d config files
    config_dir = "/usr/local/etc/jupyterhub/jupyterhub_config.d"
    if os.path.isdir(config_dir):
        for file_path in sorted(glob.glob(f"{config_dir}/*.py")):
            file_name = os.path.basename(file_path)
            print(f"Loading {config_dir} config: {file_name}")
            with open(file_path) as f:
                file_content = f.read()
            # compiling makes debugging easier: https://stackoverflow.com/a/437857
            exec(compile(source=file_content, filename=file_name, mode="exec"))

    # execute hub.extraConfig entries
    for key, config_py in sorted(get_config("hub.extraConfig", {}).items()):
        print(f"Loading extra config: {key}")
        exec(config_py)

    from kubespawner.spawner import KubeSpawner as KSO
    from tornado import gen
    class KubeSpawner(KSO):
        def get_pod_manifest(self):
            if self.extra_containers:
                for container in self.extra_containers:
                    if "name" in container and container["name"] == "oauth":
                        container["env"] = [ {'name': k, 'value': v} for k, v in (self.get_env() or {}).items()]
                    if "name" in container and container["name"] == "nginx":
                        container["env"] = [ {'name': k, 'value': v} for k, v in (self.get_env() or {}).items()]
            return super(KubeSpawner, self).get_pod_manifest()
    c.JupyterHub.spawner_class = KubeSpawner

  ## Taken from upstream JupyterHub chart
  ## Source: https://jupyterhub.github.io/helm-chart/jupyterhub-2.0.0.tgz
  z2jh.py: |
    """
    Utility methods for use in jupyterhub_config.py and dynamic subconfigs.

    Methods here can be imported by extraConfig in values.yaml
    """
    import os
    from collections.abc import Mapping
    from functools import lru_cache

    import yaml


    # memoize so we only load config once
    @lru_cache
    def _load_config():
        """Load the Helm chart configuration used to render the Helm templates of
        the chart from a mounted k8s Secret, and merge in values from an optionally
        mounted secret (hub.existingSecret)."""

        cfg = {}
        for source in ("config/values.yaml", "secret/values.yaml", "existing-secret/values.yaml"):
            path = f"/usr/local/etc/jupyterhub/{source}"
            if os.path.exists(path):
                print(f"Loading {path}")
                with open(path) as f:
                    values = yaml.safe_load(f)
                cfg = _merge_dictionaries(cfg, values)
            else:
                print(f"No config at {path}")
        return cfg


    @lru_cache
    def _get_config_value(key):
        """Load value from the k8s ConfigMap given a key."""

        path = f"/usr/local/etc/jupyterhub/config/{key}"
        if os.path.exists(path):
            with open(path) as f:
                return f.read()
        else:
            raise Exception(f"{path} not found!")


    @lru_cache
    def get_secret_value(key, default="never-explicitly-set"):
        """Load value from the user managed k8s Secret or the default k8s Secret
        given a key."""

        for source in ("existing-secret", "secret"):
            path = f"/usr/local/etc/jupyterhub/{source}/{key}"
            if os.path.exists(path):
                with open(path) as f:
                    return f.read()
        if default != "never-explicitly-set":
            return default
        raise Exception(f"{key} not found in either k8s Secret!")


    def get_name(name):
        """Returns the fullname of a resource given its short name"""
        return _get_config_value(name)


    def get_name_env(name, suffix=""):
        """Returns the fullname of a resource given its short name along with a
        suffix, converted to uppercase with dashes replaced with underscores. This
        is useful to reference named services associated environment variables, such
        as PROXY_PUBLIC_SERVICE_PORT."""
        env_key = _get_config_value(name) + suffix
        env_key = env_key.upper().replace("-", "_")
        return os.environ[env_key]


    def _merge_dictionaries(a, b):
        """Merge two dictionaries recursively.

        Simplified From https://stackoverflow.com/a/7205107
        """
        merged = a.copy()
        for key in b:
            if key in a:
                if isinstance(a[key], Mapping) and isinstance(b[key], Mapping):
                    merged[key] = _merge_dictionaries(a[key], b[key])
                else:
                    merged[key] = b[key]
            else:
                merged[key] = b[key]
        return merged


    def get_config(key, default=None):
        """
        Find a config item of a given name & return it

        Parses everything as YAML, so lists and dicts are available too

        get_config("a.b.c") returns config['a']['b']['c']
        """
        value = _load_config()
        # resolve path in yaml
        for level in key.split("."):
            if not isinstance(value, dict):
                # a parent is a scalar or null,
                # can't resolve full path
                return default
            if level not in value:
                return default
            else:
                value = value[level]
        return value


    def set_config_if_not_none(cparent, name, key):
        """
        Find a config item of a given name, set the corresponding Jupyter
        configuration item if not None
        """
        data = get_config(key)
        if data is not None:
            setattr(cparent, name, data)
---
# Source: codehub/templates/hub/role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: my-release-codehub-hub
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: codehub
    app.kubernetes.io/version: 4.22.0-r0
    helm.sh/chart: codehub-6.0.0
    app.kubernetes.io/component: hub
rules:
  - apiGroups: [""]
    resources: ["pods", "persistentvolumeclaims"]
    verbs: ["get", "watch", "list", "create", "delete"]
  - apiGroups: [""]       # "" indicates the core API group
    resources: ["events"]
    verbs: ["get", "watch", "list"]
---
# Source: codehub/templates/hub/rolebinding.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: my-release-codehub-hub
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: codehub
    app.kubernetes.io/version: 4.22.0-r0
    helm.sh/chart: codehub-6.0.0
    app.kubernetes.io/component: hub
subjects:
  - kind: ServiceAccount
    name: my-release-codehub-hub
    namespace: default
roleRef:
  kind: Role
  name: my-release-codehub-hub
  apiGroup: rbac.authorization.k8s.io
---
# Source: codehub/charts/postgresql/templates/primary/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-postgresql-hl
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 16.2.0
    helm.sh/chart: postgresql-14.3.3
    app.kubernetes.io/component: primary
  annotations:
    # Use this annotation in addition to the actual publishNotReadyAddresses
    # field below because the annotation will stop being respected soon but the
    # field is broken in some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  # We want all pods in the StatefulSet to have their addresses published for
  # the sake of the other Postgresql pods even before they're ready, since they
  # have to be able to talk to each other in order to become ready.
  publishNotReadyAddresses: true
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/component: primary
---
# Source: codehub/charts/postgresql/templates/primary/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-postgresql
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 16.2.0
    helm.sh/chart: postgresql-14.3.3
    app.kubernetes.io/component: primary
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
      nodePort: null
  selector:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/component: primary
---
# Source: codehub/templates/hub/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-codehub-hub
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: codehub
    app.kubernetes.io/version: 4.22.0-r0
    helm.sh/chart: codehub-6.0.0
    app.kubernetes.io/component: hub
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: http
      port: 8081
      targetPort: http
      protocol: TCP
      nodePort: null
  selector:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/name: codehub
    app.kubernetes.io/component: hub
---
# Source: codehub/templates/proxy/service-api.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-codehub-proxy-api
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: codehub
    app.kubernetes.io/version: 4.6.1
    helm.sh/chart: codehub-6.0.0
    app.kubernetes.io/component: proxy
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: http
      port: 8001
      targetPort: api
      protocol: TCP
      nodePort: null
  selector:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/name: codehub
    app.kubernetes.io/component: proxy
---
# Source: codehub/templates/proxy/service-metrics.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-codehub-proxy-metrics
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: codehub
    app.kubernetes.io/version: 4.6.1
    helm.sh/chart: codehub-6.0.0
    app.kubernetes.io/component: proxy
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: http
      port: 8002
      targetPort: metrics
      protocol: TCP
      nodePort: null
  selector:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/name: codehub
    app.kubernetes.io/component: proxy
---
# Source: codehub/templates/proxy/service-public.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-codehub-proxy-public
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: codehub
    app.kubernetes.io/version: 4.6.1
    helm.sh/chart: codehub-6.0.0
    app.kubernetes.io/component: proxy
  annotations:
    {}
spec:
  type: LoadBalancer
  externalTrafficPolicy: "Cluster"
  sessionAffinity: None
  ports:
    - name: http
      port: 80
      targetPort: http
      protocol: TCP
  selector:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/name: codehub
    app.kubernetes.io/component: proxy
---
# Source: codehub/templates/image-puller/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: my-release-codehub-image-puller
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: codehub
    app.kubernetes.io/version: 4.22.0-r0
    helm.sh/chart: codehub-6.0.0
    app.kubernetes.io/component: image-puller
spec:
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: codehub
      app.kubernetes.io/component: image-puller
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: codehub
        app.kubernetes.io/version: 4.22.0-r0
        helm.sh/chart: codehub-6.0.0
        app.kubernetes.io/component: image-puller
    spec:
      
      serviceAccountName: my-release-codehub-image-puller
      automountServiceAccountToken: 
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: my-release
                    app.kubernetes.io/name: codehub
                    app.kubernetes.io/component: image-puller
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      initContainers:
        - name: pull-0
          image: docker.io/captnbp/code-server:4.23.0-r0
          imagePullPolicy: IfNotPresent
          command:
            - /bin/sh
            - -c
            - echo "Pulling complete"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: false
            runAsGroup: 0
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: null
            seccompProfile:
              type: RuntimeDefault
      containers:
        - name: pause
          image: docker.io/bitnami/os-shell:12-debian-12-r16
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: false
            runAsGroup: 0
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: null
            seccompProfile:
              type: RuntimeDefault
          command:
            - /bin/sh
            - -c
            - sleep infinity
          envFrom:
---
# Source: codehub/templates/hub/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-codehub-hub
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: codehub
    app.kubernetes.io/version: 4.22.0-r0
    helm.sh/chart: codehub-6.0.0
    app.kubernetes.io/component: hub
spec:
  strategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: codehub
      app.kubernetes.io/component: hub
  template:
    metadata:
      annotations:
        checksum/hub-config: d30d31cc595072dc75fa0c748281c27153b398982333328105605c1618fb1f54
        checksum/hub-secret: 6002408ab52930f5ff39441178ba0edf8c536af9a3d032d3cc8bca079d4c4773
      labels:
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: codehub
        app.kubernetes.io/version: 4.22.0-r0
        helm.sh/chart: codehub-6.0.0
        app.kubernetes.io/component: hub
        hub.jupyter.org/network-access-proxy-api: "true"
        hub.jupyter.org/network-access-proxy-http: "true"
        hub.jupyter.org/network-access-singleuser: "true"
    spec:
      serviceAccountName: my-release-codehub-hub
      
      automountServiceAccountToken: true
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: my-release
                    app.kubernetes.io/name: codehub
                    app.kubernetes.io/component: hub
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      initContainers:
        # NOTE: The value postgresql.image is not available unless postgresql.enabled is not set. We could change this to use os-shell if
        # it had the binary wait-for-port.
        # This init container is for avoiding CrashLoopback errors in the Hub container because the PostgreSQL container is not ready
        - name: wait-for-db
          image: docker.io/bitnami/postgresql:16.2.0-debian-12-r8
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
          args:
            - -ec
            - |
              #!/bin/bash

              set -o errexit
              set -o nounset
              set -o pipefail

              . /opt/bitnami/scripts/libos.sh
              . /opt/bitnami/scripts/liblog.sh
              . /opt/bitnami/scripts/libpostgresql.sh

              check_postgresql_connection() {
                  echo "SELECT 1" | postgresql_remote_execute "$POSTGRESQL_CLIENT_DATABASE_HOST" "$POSTGRESQL_CLIENT_DATABASE_PORT_NUMBER" "$POSTGRESQL_CLIENT_DATABASE_NAME" "$POSTGRESQL_CLIENT_POSTGRES_USER" "$POSTGRESQL_CLIENT_CREATE_DATABASE_PASSWORD"
              }

              info "Connecting to the PostgreSQL instance $POSTGRESQL_CLIENT_DATABASE_HOST:$POSTGRESQL_CLIENT_DATABASE_PORT_NUMBER"
              if ! retry_while "check_postgresql_connection"; then
                  error "Could not connect to the database server"
                  return 1
              else
                  info "Connected to the PostgreSQL instance"
              fi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: false
            runAsGroup: 0
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: null
            seccompProfile:
              type: RuntimeDefault
          env:
            - name: POSTGRESQL_CLIENT_DATABASE_HOST
              value: my-release-postgresql
            - name: POSTGRESQL_CLIENT_DATABASE_NAME
              value: bitnami_jupyterhub
            - name: POSTGRESQL_CLIENT_DATABASE_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_CLIENT_CREATE_DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-postgresql
                  key: password
            - name: POSTGRESQL_CLIENT_POSTGRES_USER
              value: "bn_jupyterhub"
      containers:
        - name: hub
          image: docker.io/bitnami/jupyterhub:4.0.2-debian-11-r63
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: false
            runAsGroup: 0
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: null
            seccompProfile:
              type: RuntimeDefault
          command:
            - jupyterhub
          args:
            - --config
            - /etc/jupyterhub/jupyterhub_config.py
            - --upgrade-db
          ports:
            - name: http
              containerPort: 8081
          env:
            - name: PYTHONUNBUFFERED
              value: "1"
            - name: HELM_RELEASE_NAME
              value: "my-release"
            - name: PROXY_API_SERVICE_PORT
              value: "8001"
            - name: HUB_SERVICE_PORT
              value: "8081"
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: CONFIGPROXY_AUTH_TOKEN
              valueFrom:
                secretKeyRef:
                  name: my-release-codehub-hub
                  key: apiToken
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-postgresql
                  key: password
          envFrom:
          startupProbe:
            failureThreshold: 30
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 3
            httpGet:
              path: /hub/health
              port: http
          livenessProbe:
            failureThreshold: 30
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 3
            httpGet:
              path: /hub/health
              port: http
          readinessProbe:
            failureThreshold: 30
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 3
            httpGet:
              path: /hub/health
              port: http
          volumeMounts:
            - mountPath: /etc/jupyterhub/jupyterhub_config.py
              subPath: jupyterhub_config.py
              name: config
            - mountPath: /etc/jupyterhub/z2jh.py
              subPath: z2jh.py
              name: config
            - mountPath: /usr/local/etc/jupyterhub/config/
              name: values
            - mountPath: /usr/local/etc/jupyterhub/secret/
              name: secret
            - mountPath: /cert/
              name: server-cert
      volumes:
        - name: values
          configMap:
            name: my-release-codehub-hub-values
        - name: config
          configMap:
            name: my-release-codehub-hub
        - name: secret
          secret:
            secretName: my-release-codehub-hub
        - name: server-cert
          secret:
            secretName: my-release-codehub-hub-crt
---
# Source: codehub/templates/proxy/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-codehub-proxy
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: codehub
    app.kubernetes.io/version: 4.6.1
    helm.sh/chart: codehub-6.0.0
    app.kubernetes.io/component: proxy
spec:
  strategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: codehub
      app.kubernetes.io/component: proxy
  template:
    metadata:
      annotations:
        checksum/hub-secret: 0de501757619b3a80193a0054cc23fac414b17bf53cb170dba5775e851135902
      labels:
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: codehub
        app.kubernetes.io/version: 4.6.1
        helm.sh/chart: codehub-6.0.0
        app.kubernetes.io/component: proxy
        hub.jupyter.org/network-access-hub: "true"
        hub.jupyter.org/network-access-singleuser: "true"
    spec:
      serviceAccountName: my-release-codehub-proxy
      automountServiceAccountToken: false
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: my-release
                    app.kubernetes.io/name: codehub
                    app.kubernetes.io/component: proxy
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      containers:
        - name: proxy
          image: docker.io/bitnami/configurable-http-proxy:4.6.1-debian-12-r11
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: false
            runAsGroup: 0
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: null
            seccompProfile:
              type: RuntimeDefault
          args:
            - configurable-http-proxy
            - "--ip=::"
            - "--api-ip=::"
            - --api-port=8001
            - --default-target=http://my-release-codehub-hub:8081
            - --error-target=http://my-release-codehub-hub:8081/hub/error
            - --port=8000
            - "--metrics-ip=::"
            - --metrics-port=8002
            - --ssl-key=/server-cert/tls.key
            - --ssl-cert=/server-cert/tls.crt
            - --ssl-protocol=TLSv1_2
          ports:
            - name: http
              containerPort: 8000
              protocol: TCP
            - name: api
              containerPort: 8001
              protocol: TCP
            - name: metrics
              containerPort: 8002
              protocol: TCP
          env:
            - name: CONFIGPROXY_AUTH_TOKEN
              valueFrom:
                secretKeyRef:
                  name: my-release-codehub-hub
                  key: apiToken
          envFrom:
          startupProbe:
            failureThreshold: 30
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 3
            httpGet:
              path: /_chp_healthz
              port: http
              scheme: HTTPS
          livenessProbe:
            failureThreshold: 30
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 3
            httpGet:
              path: /_chp_healthz
              port: http
              scheme: HTTPS
          readinessProbe:
            failureThreshold: 30
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 3
            httpGet:
              path: /_chp_healthz
              port: http
              scheme: HTTPS
          volumeMounts:
            - mountPath: /server-cert/
              name: server-cert
      volumes:
        - name: server-cert
          secret:
            secretName: my-release-codehub-proxy-crt
---
# Source: codehub/charts/postgresql/templates/primary/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-postgresql
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 16.2.0
    helm.sh/chart: postgresql-14.3.3
    app.kubernetes.io/component: primary
spec:
  replicas: 1
  serviceName: my-release-postgresql-hl
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/component: primary
  template:
    metadata:
      name: my-release-postgresql
      labels:
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: postgresql
        app.kubernetes.io/version: 16.2.0
        helm.sh/chart: postgresql-14.3.3
        app.kubernetes.io/component: primary
    spec:
      serviceAccountName: my-release-postgresql
      
      automountServiceAccountToken: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: my-release
                    app.kubernetes.io/name: postgresql
                    app.kubernetes.io/component: primary
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      hostNetwork: false
      hostIPC: false
      containers:
        - name: postgresql
          image: docker.io/bitnami/postgresql:16.2.0-debian-12-r8
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: false
            runAsGroup: 0
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: "/bitnami/postgresql"
            - name: PGDATA
              value: "/bitnami/postgresql/data"
            # Authentication
            - name: POSTGRES_USER
              value: "bn_jupyterhub"
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-postgresql
                  key: password
            - name: POSTGRES_POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-postgresql
                  key: postgres-password
            - name: POSTGRES_DATABASE
              value: "bitnami_jupyterhub"
            # Replication
            # Initdb
            # Standby
            # LDAP
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
            # TLS
            - name: POSTGRESQL_ENABLE_TLS
              value: "no"
            # Audit
            - name: POSTGRESQL_LOG_HOSTNAME
              value: "false"
            - name: POSTGRESQL_LOG_CONNECTIONS
              value: "false"
            - name: POSTGRESQL_LOG_DISCONNECTIONS
              value: "false"
            - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
              value: "off"
            # Others
            - name: POSTGRESQL_CLIENT_MIN_MESSAGES
              value: "error"
            - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
              value: "pgaudit"
          ports:
            - name: tcp-postgresql
              containerPort: 5432
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "bn_jupyterhub" -d "dbname=bitnami_jupyterhub" -h 127.0.0.1 -p 5432
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/sh
                - -c
                - -e
                - |
                  exec pg_isready -U "bn_jupyterhub" -d "dbname=bitnami_jupyterhub" -h 127.0.0.1 -p 5432
                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
          volumeMounts:
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - name: empty-dir
              mountPath: /opt/bitnami/postgresql/conf
              subPath: app-conf-dir
            - name: empty-dir
              mountPath: /opt/bitnami/postgresql/tmp
              subPath: app-tmp-dir
            - name: empty-dir
              mountPath: /opt/bitnami/postgresql/logs
              subPath: app-logs-dir
            - name: dshm
              mountPath: /dev/shm
            - name: data
              mountPath: /bitnami/postgresql
      volumes:
        - name: empty-dir
          emptyDir: {}
        - name: dshm
          emptyDir:
            medium: Memory
  volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: codehub/templates/ca-issuer.yaml
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: my-release-codehub-ca
  namespace: "default"
  labels:
    app.kubernetes.io/name: codehub
    helm.sh/chart: codehub-6.0.0
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "4.22.0-r0"
    app.kubernetes.io/component: http-ca
spec:
  secretTemplate:
    labels:
      app.kubernetes.io/name: codehub
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: http-ca
  subject:
    organizationalUnits:
      - 
    organizations:
      - codehub
    countries:
      - fr
  isCA: true
  commonName: my-release-codehub-ca
  secretName: my-release-codehub-ca-crt
  privateKey:
    algorithm: RSA
    size: 4096
  duration: 87600h # 10y
  issuerRef:
    name: my-release-codehub-self-signed
    kind: Issuer
    group: cert-manager.io
---
# Source: codehub/templates/proxy/certificates.yaml
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: my-release-codehub-proxy-crt
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: codehub
    app.kubernetes.io/version: 4.6.1
    helm.sh/chart: codehub-6.0.0
    app.kubernetes.io/component: proxy
spec:
  secretTemplate:
    labels:
      app.kubernetes.io/name: codehub
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: proxy
  secretName: my-release-codehub-proxy-crt
  duration: 4320h # 180d
  renewBefore: 2160h # 90d
  commonName: "my-release-codehub"
  subject:
    organizationalUnits:
      - 
    organizations:
      - codehub
    countries:
      - fr
  dnsNames:
    - "my-release-codehub-proxy-api"
    - "my-release-codehub-proxy-public"
    - "my-release-codehub-proxy-api.default.svc.cluster.local"
    - "my-release-codehub-proxy-public.default.svc.cluster.local"
    - "localhost"
  ipAddresses:
    - "127.0.0.1"
    - "::1"
  usages:
    - server auth
  privateKey:
    algorithm: RSA
    encoding: PKCS1
    size: 4096
  issuerRef:
    name: my-release-codehub-http
    kind: Issuer
    group: cert-manager.io
---
# Source: codehub/templates/ca-issuer.yaml
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: my-release-codehub-self-signed
  namespace: "default"
  labels:
    app.kubernetes.io/name: codehub
    helm.sh/chart: codehub-6.0.0
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "4.22.0-r0"
    app.kubernetes.io/component: self-signed-issuer
spec:
  selfSigned: {}
---
# Source: codehub/templates/ca-issuer.yaml
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: my-release-codehub-http
  namespace: "default"
  labels:
    app.kubernetes.io/name: codehub
    helm.sh/chart: codehub-6.0.0
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "4.22.0-r0"
    app.kubernetes.io/component: issuer
spec:
  ca:
    secretName: my-release-codehub-ca-crt
---
# Source: codehub/templates/hub/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: my-release-codehub-hub
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: codehub
    app.kubernetes.io/version: 4.22.0-r0
    helm.sh/chart: codehub-6.0.0
    app.kubernetes.io/component: hub
spec:
  endpoints:
    - port: http
      path: /hub/metrics
      interval: 30s
  namespaceSelector:
    matchNames:
      - default
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: codehub
      app.kubernetes.io/component: hub
---
# Source: codehub/templates/proxy/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: my-release-codehub-proxy-api
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: codehub
    app.kubernetes.io/version: 4.6.1
    helm.sh/chart: codehub-6.0.0
    app.kubernetes.io/component: proxy
spec:
  endpoints:
    - port: metrics
      path: /metrics
      interval: 30s
  namespaceSelector:
    matchNames:
      - default
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: codehub
      app.kubernetes.io/component: proxy
