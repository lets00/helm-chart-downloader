---
# Source: dagster/charts/dagster-user-deployments/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-dagster-user-deployments-user-deployments
  labels: 
    helm.sh/chart: dagster-user-deployments-1.7.10
    app.kubernetes.io/name: dagster-user-deployments
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.7.10"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
automountServiceAccountToken: false
---
# Source: dagster/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-dagster
  labels: 
    helm.sh/chart: dagster-1.7.10
    app.kubernetes.io/name: dagster
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.7.10"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
automountServiceAccountToken: false
---
# Source: dagster/charts/postgresql/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-release-postgresql
  labels:
    app: postgresql
    chart: postgresql-8.1.0
    release: "my-release"
    heritage: "Helm"
type: Opaque
data:
  postgresql-password: "dGVzdA=="
---
# Source: dagster/templates/secret-postgres.yaml
apiVersion: v1
kind: Secret
metadata:
  name: dagster-postgresql-secret
  labels:
    app: "dagster"
    chart: "dagster-1.7.10"
    release: "my-release"
    heritage: "Helm"
type: Opaque
data:
  postgresql-password: "dGVzdA=="
---
# Source: dagster/charts/dagster-user-deployments/templates/configmap-env-shared.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-dagster-user-deployments-user-deployments-shared-env
  labels:
    app: dagster-user-deployments
    chart: dagster-user-deployments-1.7.10
    release: my-release
    heritage: Helm
data:
  
  
  
  DAGSTER_HOME: "/opt/dagster/dagster_home"
  DAGSTER_K8S_PIPELINE_RUN_NAMESPACE: "default"
  DAGSTER_K8S_PIPELINE_RUN_ENV_CONFIGMAP: "my-release-dagster-user-deployments-pipeline-env"
---
# Source: dagster/charts/dagster-user-deployments/templates/configmap-env-user.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-dagster-user-deployments-k8s-example-user-code-1-user-env
  labels:
    app: dagster-user-deployments
    chart: dagster-user-deployments-1.7.10
    release: my-release
    heritage: Helm
data:
  # If this is a map, we write it to this configmap. If it's a list, we include it
  # directly on the container (can use more k8s spec like valueFrom).
---
# Source: dagster/templates/configmap-env-daemon.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-dagster-daemon-env
  labels:
    app: dagster
    chart: dagster-1.7.10
    release: my-release
    heritage: Helm
data:
  
  DAGSTER_HOME: "/opt/dagster/dagster_home"
  DAGSTER_K8S_PIPELINE_RUN_NAMESPACE: "default"
  DAGSTER_K8S_PIPELINE_RUN_ENV_CONFIGMAP: "my-release-dagster-pipeline-env"
  DAGSTER_K8S_PIPELINE_RUN_IMAGE: "docker.io/dagster/user-code-example:1.7.10"
  DAGSTER_K8S_PIPELINE_RUN_IMAGE_PULL_POLICY: "Always"
  # This is a list by default, but for backcompat it can be a map. If it's a list, the env is applied directly
  # to the container.
---
# Source: dagster/templates/configmap-env-flower.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-dagster-flower-env
  labels:
    app: dagster
    chart: dagster-1.7.10
    release: my-release
    heritage: Helm
data:
  
  DAGSTER_HOME: "/opt/dagster/dagster_home"
  DAGSTER_K8S_PIPELINE_RUN_NAMESPACE: "default"
  DAGSTER_K8S_PIPELINE_RUN_ENV_CONFIGMAP: "my-release-dagster-pipeline-env"
  DAGSTER_K8S_PIPELINE_RUN_IMAGE: "docker.io/dagster/user-code-example:1.7.10"
  DAGSTER_K8S_PIPELINE_RUN_IMAGE_PULL_POLICY: "Always"
---
# Source: dagster/templates/configmap-env-pipeline-run.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-dagster-pipeline-env
  labels:
    app: dagster
    chart: dagster-1.7.10
    release: my-release
    heritage: Helm
data:
  
  DAGSTER_HOME: "/opt/dagster/dagster_home"
  DAGSTER_K8S_PIPELINE_RUN_NAMESPACE: "default"
  DAGSTER_K8S_PIPELINE_RUN_ENV_CONFIGMAP: "my-release-dagster-pipeline-env"
  DAGSTER_K8S_PIPELINE_RUN_IMAGE: "docker.io/dagster/user-code-example:1.7.10"
  DAGSTER_K8S_PIPELINE_RUN_IMAGE_PULL_POLICY: "Always"
---
# Source: dagster/templates/configmap-env-webserver.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-dagster-webserver-env
  labels:
    app: dagster
    chart: dagster-1.7.10
    release: my-release
    heritage: Helm
data:
  
  DAGSTER_HOME: "/opt/dagster/dagster_home"
  DAGSTER_K8S_PIPELINE_RUN_NAMESPACE: "default"
  DAGSTER_K8S_PIPELINE_RUN_ENV_CONFIGMAP: "my-release-dagster-pipeline-env"
  DAGSTER_K8S_PIPELINE_RUN_IMAGE: "docker.io/dagster/user-code-example:1.7.10"
  DAGSTER_K8S_PIPELINE_RUN_IMAGE_PULL_POLICY: "Always"
  # This is a list by default, but for backcompat it can be a map. If it's a list, the env is applied directly
  # to the container.
---
# Source: dagster/templates/configmap-instance.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-dagster-instance
  labels:
    app: dagster
    chart: dagster-1.7.10
    release: my-release
    heritage: Helm
data:
  dagster.yaml: |
    scheduler:      
      module: dagster.core.scheduler
      class: DagsterDaemonScheduler

    schedule_storage:
      module: dagster_postgres.schedule_storage
      class: PostgresScheduleStorage
      config:        
        postgres_db:
          username: test
          password:
            env: DAGSTER_PG_PASSWORD
          hostname: my-release-postgresql
          db_name: test
          port: 5432
          params:
            {}

    run_launcher:      
      module: dagster_k8s
      class: K8sRunLauncher
      config:
        load_incluster_config: true
        job_namespace: default
        image_pull_policy: Always
        service_account_name: my-release-dagster
        dagster_home: "/opt/dagster/dagster_home"
        instance_config_map: "my-release-dagster-instance"
        postgres_password_secret: "dagster-postgresql-secret"

    run_storage:
      module: dagster_postgres.run_storage
      class: PostgresRunStorage
      config:        
        postgres_db:
          username: test
          password:
            env: DAGSTER_PG_PASSWORD
          hostname: my-release-postgresql
          db_name: test
          port: 5432
          params:
            {}

    event_log_storage:
      module: dagster_postgres.event_log
      class: PostgresEventLogStorage
      config:        
        postgres_db:
          username: test
          password:
            env: DAGSTER_PG_PASSWORD
          hostname: my-release-postgresql
          db_name: test
          port: 5432
          params:
            {}
    run_coordinator:      
      module: dagster.core.run_coordinator
      class: QueuedRunCoordinator
      config:
        
        max_concurrent_runs: -1
        dequeue_use_threads: true
        dequeue_num_workers: 4
    compute_logs:      
      module: dagster.core.storage.noop_compute_log_manager
      class: NoOpComputeLogManager
    run_monitoring:
      enabled: true
      start_timeout_seconds:  300
      max_resume_run_attempts: 0
      poll_interval_seconds: 120
      free_slots_after_run_end_seconds: 0
    run_retries:
      enabled: true
    sensors:
      use_threads: true
      num_workers: 4
    schedules:
      use_threads: true
      num_workers: 4

    telemetry:
      enabled: true
---
# Source: dagster/templates/configmap-workspace.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-dagster-workspace-yaml
  labels:
    app: dagster
    chart: dagster-1.7.10
    release: my-release
    heritage: Helm
data:
  workspace.yaml: |
    load_from:
      - grpc_server:
          host: k8s-example-user-code-1
          port: 3030
          location_name: k8s-example-user-code-1
---
# Source: dagster/charts/dagster-user-deployments/templates/role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: my-release-dagster-user-deployments-role
  labels: 
    helm.sh/chart: dagster-user-deployments-1.7.10
    app.kubernetes.io/name: dagster-user-deployments
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.7.10"
    app.kubernetes.io/managed-by: Helm

# Allow the Dagster service account to read and write Kubernetes jobs, deployments, pods, and events.
rules:
  - apiGroups: ["batch"]
    resources: ["jobs"]
    verbs: ["create", "delete", "deletecollection", "get", "list", "patch", "update", "watch"]
  - apiGroups: ["batch"]
    resources: ["jobs/status"]
    verbs: ["get", "watch", "list"]
  # The empty arg "" corresponds to the core API group
  - apiGroups: [""]
    resources: ["pods", "events"]
    verbs: ["create", "delete", "deletecollection", "get", "list", "patch", "update", "watch"]
  - apiGroups: [""]
    resources: ["pods/log", "pods/status"]
    verbs: ["get", "watch", "list"]
---
# Source: dagster/templates/role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: my-release-dagster-role
  labels:
    app: dagster
    chart: dagster-1.7.10
    release: "my-release"
    heritage: "Helm"

# Allow the Dagster service account to read and write Kubernetes jobs, deployments, pods, and events.
rules:
  - apiGroups: ["batch"]
    resources: ["jobs"]
    verbs: ["create", "delete", "deletecollection", "get", "list", "patch", "update", "watch"]
  - apiGroups: ["batch"]
    resources: ["jobs/status"]
    verbs: ["get", "watch", "list"]
  # The empty arg "" corresponds to the core API group
  - apiGroups: [""]
    resources: ["pods", "events"]
    verbs: ["create", "delete", "deletecollection", "get", "list", "patch", "update", "watch"]
  - apiGroups: [""]
    resources: ["pods/log", "pods/status"]
    verbs: ["get", "watch", "list"]
---
# Source: dagster/charts/dagster-user-deployments/templates/rolebinding.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: my-release-dagster-user-deployments-rolebinding
  labels: 
    helm.sh/chart: dagster-user-deployments-1.7.10
    app.kubernetes.io/name: dagster-user-deployments
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.7.10"
    app.kubernetes.io/managed-by: Helm

subjects:
- kind: ServiceAccount
  name: my-release-dagster-user-deployments-user-deployments
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: my-release-dagster-user-deployments-role
---
# Source: dagster/templates/rolebinding.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: my-release-dagster-rolebinding
  labels:
    app: dagster
    chart: dagster-1.7.10
    release: "my-release"
    heritage: "Helm"
subjects:
- kind: ServiceAccount
  name: my-release-dagster
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: my-release-dagster-role
---
# Source: dagster/charts/dagster-user-deployments/templates/service-user.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8s-example-user-code-1
  labels:
    helm.sh/chart: dagster-user-deployments-1.7.10
    app.kubernetes.io/name: dagster-user-deployments
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.7.10"
    app.kubernetes.io/managed-by: Helm
    component: user-deployments
    deployment: k8s-example-user-code-1
  annotations:
spec:
  type: "ClusterIP"
  ports:
    - port: 3030
      protocol: TCP
      name: grpc
  selector:
    app.kubernetes.io/name: dagster-user-deployments
    app.kubernetes.io/instance: my-release
    component: user-deployments
    deployment: k8s-example-user-code-1
---
# Source: dagster/charts/postgresql/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-postgresql-headless
  labels:
    app: postgresql
    chart: postgresql-8.1.0
    release: "my-release"
    heritage: "Helm"
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app: postgresql
    release: "my-release"
---
# Source: dagster/charts/postgresql/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-postgresql
  labels:
    app: postgresql
    chart: postgresql-8.1.0
    release: "my-release"
    heritage: "Helm"
spec:
  type: ClusterIP
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app: postgresql
    release: "my-release"
    role: master
---
# Source: dagster/templates/service-webserver.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-dagster-webserver
  labels:
    helm.sh/chart: dagster-1.7.10
    app.kubernetes.io/name: dagster
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.7.10"
    app.kubernetes.io/managed-by: Helm
    component: dagster-webserver
  annotations:
spec:
  type: ClusterIP
  ports:
    - port: 80
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: dagster
    app.kubernetes.io/instance: my-release
    component: dagster-webserver
---
# Source: dagster/charts/dagster-user-deployments/templates/deployment-user.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-dagster-user-deployments-k8s-example-user-code-1
  labels:
    helm.sh/chart: dagster-user-deployments-1.7.10
    app.kubernetes.io/name: dagster-user-deployments
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.7.10"
    app.kubernetes.io/managed-by: Helm
    component: user-deployments
    deployment: k8s-example-user-code-1
  annotations: 
    {}
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: dagster-user-deployments
      app.kubernetes.io/instance: my-release
      component: user-deployments
      deployment: k8s-example-user-code-1
  template:
    metadata:
      labels:
        app.kubernetes.io/name: dagster-user-deployments
        app.kubernetes.io/instance: my-release
        component: user-deployments
        deployment: k8s-example-user-code-1
      annotations:
        checksum/dagster-user-deployment: 6d4f5f06f08087454f62e749630b3ceaf6c40fa1ca8849af47c52e45fb479042
    spec:
      imagePullSecrets: 
        []
      serviceAccountName: my-release-dagster-user-deployments-user-deployments
      automountServiceAccountToken: true
      securityContext: 
        {}
      containers:
        - name: dagster-user-deployments
          securityContext: 
            {}
          imagePullPolicy: Always
          image: "docker.io/dagster/user-code-example:1.7.10"
          args: ["dagster", "api", "grpc", "-h", "0.0.0.0", "-p", "3030", "--python-file","/example_project/example_repo/repo.py"]
          env:
            - name: DAGSTER_CURRENT_IMAGE
              value: "docker.io/dagster/user-code-example:1.7.10"
            - name: DAGSTER_PG_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: dagster-postgresql-secret
                  key: postgresql-password
            # uses the auto_envvar_prefix of the dagster cli to set the --container-context arg
            # on 'dagster api grpc'
            - name: DAGSTER_CLI_API_GRPC_CONTAINER_CONTEXT
              value: "{\"k8s\":{\"env_config_maps\":[\"my-release-dagster-user-deployments-k8s-example-user-code-1-user-env\"],\"image_pull_policy\":\"Always\",\"namespace\":\"default\",\"run_k8s_config\":{\"pod_spec_config\":{\"automount_service_account_token\":true}},\"service_account_name\":\"my-release-dagster-user-deployments-user-deployments\"}}"
            # If this is a map, we write it to a configmap. If it's a list, we include it here (can use more k8s spec like valueFrom).
          envFrom:
            - configMapRef:
                name: my-release-dagster-user-deployments-user-deployments-shared-env
            - configMapRef:
                name: my-release-dagster-user-deployments-k8s-example-user-code-1-user-env
            - secretRef:
                name: dagster-celery-config-secret
                optional: true
          resources: 
            {}
        # Only disable readiness if explicitly set to false
          readinessProbe:
            exec:
              command: ["dagster", "api", "grpc-health-check", "-p", "3030"]
            periodSeconds:
              20
            timeoutSeconds:
              10
            successThreshold:
              1
            failureThreshold:
              1
      nodeSelector: 
        {}
      affinity: 
        {}
      tolerations:
        []
---
# Source: dagster/templates/deployment-daemon.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-dagster-daemon
  labels:
    helm.sh/chart: dagster-1.7.10
    app.kubernetes.io/name: dagster
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.7.10"
    app.kubernetes.io/managed-by: Helm
    component: dagster-daemon
    deployment: daemon
  annotations:
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app.kubernetes.io/name: dagster
      app.kubernetes.io/instance: my-release
      component: dagster-daemon
      deployment: daemon
  template:
    metadata:
      labels:
        app.kubernetes.io/name: dagster
        app.kubernetes.io/instance: my-release
        component: dagster-daemon
        deployment: daemon
      annotations:
        checksum/dagster-workspace: bd9a5ea9640765be0828344170e7e013b1971c62110b2ccf145ba9c8144e6c4a
        checksum/dagster-instance: 7c28b9d2f6149b633988233cd3d3c76968987e3955d4041ddc3324d70cd32b91
    spec:
      imagePullSecrets:
        []
      serviceAccountName: my-release-dagster
      automountServiceAccountToken: true
      securityContext:
        {}
      initContainers:
        - name: check-db-ready
          image: "library/postgres:14.6"
          imagePullPolicy: "IfNotPresent"
          command: ['sh', '-c', 'until pg_isready -h my-release-postgresql -p 5432 -U test; do echo waiting for database; sleep 2; done;']
          securityContext:
            {}
          resources:
            {}
        - name: "init-user-deployment-k8s-example-user-code-1"
          image: "docker.io/busybox:1.28"
          command: ['sh', '-c', "until nslookup k8s-example-user-code-1; do echo waiting for user service; sleep 2; done"]
          securityContext:
            {}
      containers:
        - name: dagster
          securityContext:
            {}
          imagePullPolicy: Always
          image: "docker.io/dagster/dagster-celery-k8s:1.7.10"
          command: [
            "/bin/bash",
            "-c",
            "dagster-daemon run -w /dagster-workspace/workspace.yaml"
          ]
          env:
            - name: DAGSTER_PG_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: "dagster-postgresql-secret"
                  key: postgresql-password
            - name: DAGSTER_DAEMON_HEARTBEAT_TOLERANCE
              value: "1800"
            # This is a list by default, but for backcompat it can be a map. As a map it's written to the daemon-env
            # configmap.
          envFrom:
            - configMapRef:
                name: my-release-dagster-daemon-env
          volumeMounts:
            - name: dagster-instance
              mountPath: "/opt/dagster/dagster_home/dagster.yaml"
              subPath: dagster.yaml
            # Do not use `subPath` to allow the configmap to update if modified
            - name: dagster-workspace-yaml
              mountPath: "/dagster-workspace/"
          resources:
            {}
      nodeSelector:
        {}
      volumes:
        - name: dagster-instance
          configMap:
            name: my-release-dagster-instance
        - name: dagster-workspace-yaml
          configMap:
            name: my-release-dagster-workspace-yaml
      affinity:
        {}
      tolerations:
        []
---
# Source: dagster/templates/deployment-webserver.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-dagster-webserver
  labels:
    helm.sh/chart: dagster-1.7.10
    app.kubernetes.io/name: dagster
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.7.10"
    app.kubernetes.io/managed-by: Helm
    component: dagster-webserver
  annotations:
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: dagster
      app.kubernetes.io/instance: my-release
      component: dagster-webserver
  template:
    metadata:
      labels:
        app.kubernetes.io/name: dagster
        app.kubernetes.io/instance: my-release
        component: dagster-webserver
      annotations:
        checksum/dagster-workspace: bd9a5ea9640765be0828344170e7e013b1971c62110b2ccf145ba9c8144e6c4a
        checksum/dagster-instance: 7c28b9d2f6149b633988233cd3d3c76968987e3955d4041ddc3324d70cd32b91
    spec:
      serviceAccountName: my-release-dagster
      automountServiceAccountToken: true
      securityContext:
        {}
      initContainers:
        - name: check-db-ready
          image: "library/postgres:14.6"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until pg_isready -h my-release-postgresql -p 5432 -U test; do echo waiting for database; sleep 2; done;']
          securityContext:
            {}
        - name: "init-user-deployment-k8s-example-user-code-1"
          image: "docker.io/busybox:1.28"
          command: ['sh', '-c', "until nslookup k8s-example-user-code-1; do echo waiting for user service; sleep 2; done"]
          securityContext:
            {}
      containers:
        - name: dagster
          securityContext:
            {}
          imagePullPolicy: Always
          image: "docker.io/dagster/dagster-celery-k8s:1.7.10"
          command: [
            "/bin/bash",
            "-c",
            "dagster-webserver -h 0.0.0.0 -p 80 -w /dagster-workspace/workspace.yaml"
          ]
          env:
            - name: DAGSTER_PG_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: "dagster-postgresql-secret"
                  key: postgresql-password
            # This is a list by default, but for backcompat it can be a map. As
            # a map it's written to the webserver-env configmap.
          envFrom:
            - configMapRef:
                name: my-release-dagster-webserver-env
          volumeMounts:
            - name: dagster-instance
              mountPath: "/opt/dagster/dagster_home/dagster.yaml"
              subPath: dagster.yaml
            # Do not use `subPath` to allow the configmap to update if modified
            - name: dagster-workspace-yaml
              mountPath: "/dagster-workspace/"
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          resources:
            {}
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /server_info
              port: 80
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 10

      volumes:
        - name: dagster-instance
          configMap:
            name: my-release-dagster-instance
        - name: dagster-workspace-yaml
          configMap:
            name: my-release-dagster-workspace-yaml
---
# Source: dagster/charts/postgresql/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-postgresql
  labels:
    app: postgresql
    chart: postgresql-8.1.0
    release: "my-release"
    heritage: "Helm"
spec:
  serviceName: my-release-postgresql-headless
  replicas: 1
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app: postgresql
      release: "my-release"
      role: master
  template:
    metadata:
      name: my-release-postgresql
      labels:
        app: postgresql
        chart: postgresql-8.1.0
        release: "my-release"
        heritage: "Helm"
        role: master
    spec:      
      securityContext:
        fsGroup: 1001
      initContainers:
        - name: init-chmod-data
          image: docker.io/bitnami/minideb:stretch
          imagePullPolicy: "Always"
          resources:
            requests:
              cpu: 250m
              memory: 256Mi
          command:
            - /bin/sh
            - -c
            - |
              mkdir -p /bitnami/postgresql/data
              chmod 700 /bitnami/postgresql/data
              find /bitnami/postgresql -mindepth 0 -maxdepth 1 -not -name ".snapshot" -not -name "lost+found" | \
                xargs chown -R 1001:1001
              chmod -R 777 /dev/shm
          securityContext:
            runAsUser: 0
          volumeMounts:
            - name: data
              mountPath: /bitnami/postgresql
              subPath: 
            - name: dshm
              mountPath: /dev/shm
      containers:
        - name: my-release-postgresql
          image: docker.io/library/postgres:14.6
          imagePullPolicy: "IfNotPresent"
          resources:
            requests:
              cpu: 250m
              memory: 256Mi
          securityContext:
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: "/bitnami/postgresql"
            - name: PGDATA
              value: "/bitnami/postgresql/data"
            - name: POSTGRES_USER
              value: "test"
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-postgresql
                  key: postgresql-password
            - name: POSTGRES_DB
              value: "test"
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
          ports:
            - name: tcp-postgresql
              containerPort: 5432
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "test" -d "test" -h 127.0.0.1 -p 5432
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - -e
                - |
                  pg_isready -U "test" -d "test" -h 127.0.0.1 -p 5432
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          volumeMounts:
            - name: dshm
              mountPath: /dev/shm
            - name: data
              mountPath: /bitnami/postgresql
              subPath: 
      volumes:
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 1Gi
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
