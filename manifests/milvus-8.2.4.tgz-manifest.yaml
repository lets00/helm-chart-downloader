---
# Source: milvus/charts/etcd/templates/networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: my-release-etcd
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: etcd
    app.kubernetes.io/version: 3.5.14
    helm.sh/chart: etcd-10.2.2
    app.kubernetes.io/component: etcd
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: etcd
      app.kubernetes.io/component: etcd
  policyTypes:
    - Ingress
    - Egress
  egress:
    - {}
  ingress:
    # Allow inbound connections
    - ports:
        - port: 2379
        - port: 2380
---
# Source: milvus/charts/kafka/templates/networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: my-release-kafka
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: 3.7.0
    helm.sh/chart: kafka-29.3.1
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: kafka
  policyTypes:
    - Ingress
    - Egress
  egress:
    - {}
  ingress:
    # Allow client connections
    - ports:
        - port: 9092
        - port: 9094
        - port: 9093
---
# Source: milvus/charts/minio/templates/networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: my-release-minio
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: minio
    app.kubernetes.io/version: 2024.6.6
    helm.sh/chart: minio-14.6.7
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: minio
  policyTypes:
    - Ingress
    - Egress
  egress:
    - {}
  ingress:
    # Allow inbound connections
    - ports:
        - port: 9001
        - port: 9000
---
# Source: milvus/charts/minio/templates/provisioning-networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: my-release-minio-provisioning
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: minio
    app.kubernetes.io/version: 2024.6.6
    helm.sh/chart: minio-14.6.7
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/component: minio-provisioning
  policyTypes:
    - Ingress
    - Egress
  egress:
    - {}
  ingress:
---
# Source: milvus/templates/attu/networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: my-release-milvus-attu
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.1
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: attu
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: attu
  policyTypes:
    - Ingress
    - Egress
  egress:
    - {}
  ingress:
    - ports:
        - port: 3000
---
# Source: milvus/templates/data-coordinator/networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: my-release-milvus-data-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: data-coordinator
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: data-coordinator
  policyTypes:
    - Ingress
    - Egress
  egress:
    - {}
  ingress:
    - ports:
        - port: 19530
---
# Source: milvus/templates/data-node/networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: my-release-milvus-data-node
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: data-node
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: data-node
  policyTypes:
    - Ingress
    - Egress
  egress:
    # Allow dns resolution
    - ports:
        - port: 53
          protocol: UDP
        - port: 53
          protocol: TCP
    # Allow outbound connections to other cluster pods
    - ports:
        - port: 19530
        - port: 19530
        - port: 19530
        - port: 19530
        - port: 19530
        - port: 19530
        - port: 19530
        - port: 19530
        - port: 19529
      to:
        - podSelector:
            matchLabels:
              app.kubernetes.io/instance: my-release
              app.kubernetes.io/name: milvus
    # Allow outbound connections to S3
    - ports:
        - port: 80
        - port: 9000
      to: 
        - podSelector:
            matchLabels:
              app.kubernetes.io/name: minio
              app.kubernetes.io/instance: my-release
    # Allow outbound connections to etcd
    - ports:
        - port: 2379
      to:
        - podSelector:
            matchLabels:
              app.kubernetes.io/name: etcd
              app.kubernetes.io/instance: my-release
    # Allow outbound connections to kafka
    - ports:
        - port: 9092
      to:
        - podSelector:
            matchLabels:
              app.kubernetes.io/name: kafka
              app.kubernetes.io/instance: my-release
  ingress:
    - ports:
        - port: 19530
---
# Source: milvus/templates/index-coordinator/networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: my-release-milvus-index-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: index-coordinator
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: index-coordinator
  policyTypes:
    - Ingress
    - Egress
  egress:
    - {}
  ingress:
    - ports:
        - port: 19530
---
# Source: milvus/templates/index-node/networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: my-release-milvus-index-node
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: index-node
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: index-node
  policyTypes:
    - Ingress
    - Egress
  egress:
    - {}
  ingress:
    - ports:
        - port: 19530
---
# Source: milvus/templates/proxy/networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: my-release-milvus-proxy
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: proxy
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: proxy
  policyTypes:
    - Ingress
    - Egress
  egress:
    - {}
  ingress:
    - ports:
        - port: 19530
        - port: 19529
---
# Source: milvus/templates/query-coordinator/networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: my-release-milvus-query-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: query-coordinator
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: query-coordinator
  policyTypes:
    - Ingress
    - Egress
  egress:
    - {}
  ingress:
    - ports:
        - port: 19530
---
# Source: milvus/templates/query-node/networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: my-release-milvus-query-node
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: query-node
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: query-node
  policyTypes:
    - Ingress
    - Egress
  egress:
    - {}
  ingress:
    - ports:
        - port: 19530
---
# Source: milvus/templates/root-coordinator/networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: my-release-milvus-root-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: root-coordinator
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: root-coordinator
  policyTypes:
    - Ingress
    - Egress
  egress:
    - {}
  ingress:
    - ports:
        - port: 19530
---
# Source: milvus/charts/etcd/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-release-etcd
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: etcd
    app.kubernetes.io/version: 3.5.14
    helm.sh/chart: etcd-10.2.2
    app.kubernetes.io/component: etcd
spec:
  minAvailable: 51%
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: etcd
      app.kubernetes.io/component: etcd
---
# Source: milvus/charts/kafka/templates/broker/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-release-kafka-broker
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: 3.7.0
    helm.sh/chart: kafka-29.3.1
    app.kubernetes.io/component: broker
    app.kubernetes.io/part-of: kafka
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: kafka
      app.kubernetes.io/component: broker
      app.kubernetes.io/part-of: kafka
---
# Source: milvus/charts/kafka/templates/controller-eligible/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-release-kafka-controller
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: 3.7.0
    helm.sh/chart: kafka-29.3.1
    app.kubernetes.io/component: controller-eligible
    app.kubernetes.io/part-of: kafka
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: kafka
      app.kubernetes.io/component: controller-eligible
      app.kubernetes.io/part-of: kafka
---
# Source: milvus/charts/minio/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-release-minio
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: minio
    app.kubernetes.io/version: 2024.6.6
    helm.sh/chart: minio-14.6.7
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: minio
---
# Source: milvus/templates/attu/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-release-milvus-attu
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.1
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: attu
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: attu
---
# Source: milvus/templates/data-coordinator/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-release-milvus-data-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: data-coordinator
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: data-coordinator
---
# Source: milvus/templates/data-node/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-release-milvus-data-node
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: data-node
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: data-node
---
# Source: milvus/templates/index-coordinator/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-release-milvus-index-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: index-coordinator
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: index-coordinator
---
# Source: milvus/templates/index-node/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-release-milvus-index-node
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: index-node
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: index-node
---
# Source: milvus/templates/proxy/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-release-milvus-proxy
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: proxy
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: proxy
---
# Source: milvus/templates/query-coordinator/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-release-milvus-query-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: query-coordinator
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: query-coordinator
---
# Source: milvus/templates/query-node/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-release-milvus-query-node
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: query-node
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: query-node
---
# Source: milvus/templates/root-coordinator/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-release-milvus-root-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: root-coordinator
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: root-coordinator
---
# Source: milvus/charts/etcd/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: false
metadata:
  name: my-release-etcd
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: etcd
    app.kubernetes.io/version: 3.5.14
    helm.sh/chart: etcd-10.2.2
---
# Source: milvus/charts/kafka/templates/provisioning/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-kafka-provisioning
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: 3.7.0
    helm.sh/chart: kafka-29.3.1
automountServiceAccountToken: false
---
# Source: milvus/charts/kafka/templates/rbac/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-kafka
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: 3.7.0
    helm.sh/chart: kafka-29.3.1
    app.kubernetes.io/component: kafka
automountServiceAccountToken: false
---
# Source: milvus/charts/minio/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-minio
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: minio
    app.kubernetes.io/version: 2024.6.6
    helm.sh/chart: minio-14.6.7
automountServiceAccountToken: false
secrets:
  - name: my-release-minio
---
# Source: milvus/templates/attu/service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-milvus-attu
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.1
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: attu
automountServiceAccountToken: false
---
# Source: milvus/templates/data-coordinator/service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-milvus-data-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: data-coordinator
automountServiceAccountToken: false
---
# Source: milvus/templates/data-node/service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-milvus-data-node
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: data-node
automountServiceAccountToken: false
---
# Source: milvus/templates/index-coordinator/service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-milvus-index-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: index-coordinator
automountServiceAccountToken: false
---
# Source: milvus/templates/index-node/service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-milvus-index-node
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: index-node
automountServiceAccountToken: false
---
# Source: milvus/templates/proxy/service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-milvus-proxy
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: proxy
automountServiceAccountToken: false
---
# Source: milvus/templates/query-coordinator/service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-milvus-query-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: query-coordinator
automountServiceAccountToken: false
---
# Source: milvus/templates/query-node/service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-milvus-query-node
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: query-node
automountServiceAccountToken: false
---
# Source: milvus/templates/root-coordinator/service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-milvus-root-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: root-coordinator
automountServiceAccountToken: false
---
# Source: milvus/charts/etcd/templates/token-secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-release-etcd-jwt-token
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: etcd
    app.kubernetes.io/version: 3.5.14
    helm.sh/chart: etcd-10.2.2
type: Opaque
data:
  jwt-token.pem: "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"
---
# Source: milvus/charts/kafka/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-release-kafka-user-passwords
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: 3.7.0
    helm.sh/chart: kafka-29.3.1
type: Opaque
data:
  client-passwords: "UVpDUFZYZ0dGag=="
  system-user-password: "UVpDUFZYZ0dGag=="
  inter-broker-password: "RVAzQ1U2dkZ6aA=="
  controller-password: "QmZTVzcxWmNLVQ=="
---
# Source: milvus/charts/kafka/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-release-kafka-kraft-cluster-id
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: 3.7.0
    helm.sh/chart: kafka-29.3.1
type: Opaque
data:
  kraft-cluster-id: "VHM5QUlhVUhjNjFBMTQ1Y0M5OFJXYQ=="
---
# Source: milvus/charts/minio/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-release-minio
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: minio
    app.kubernetes.io/version: 2024.6.6
    helm.sh/chart: minio-14.6.7
type: Opaque
data:
  root-user: "YWRtaW4="
  root-password: "enZ3a0tQcXQ3bA=="
---
# Source: milvus/charts/kafka/templates/controller-eligible/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-kafka-controller-configuration
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: 3.7.0
    helm.sh/chart: kafka-29.3.1
    app.kubernetes.io/component: controller-eligible
    app.kubernetes.io/part-of: kafka
data:
  server.properties: |-
    # Listeners configuration
    listeners=CLIENT://:9092,INTERNAL://:9094,CONTROLLER://:9093
    advertised.listeners=CLIENT://advertised-address-placeholder:9092,INTERNAL://advertised-address-placeholder:9094
    listener.security.protocol.map=CLIENT:SASL_PLAINTEXT,INTERNAL:SASL_PLAINTEXT,CONTROLLER:SASL_PLAINTEXT
    # KRaft process roles
    process.roles=controller,broker
    #node.id=
    controller.listener.names=CONTROLLER
    controller.quorum.voters=0@my-release-kafka-controller-0.my-release-kafka-controller-headless.default.svc.cluster.local:9093
    # Kraft Controller listener SASL settings
    sasl.mechanism.controller.protocol=PLAIN
    listener.name.controller.sasl.enabled.mechanisms=PLAIN
    listener.name.controller.plain.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="controller_user" password="controller-password-placeholder" user_controller_user="controller-password-placeholder";
    # Kafka data logs directory
    log.dir=/bitnami/kafka/data
    # Kafka application logs directory
    logs.dir=/opt/bitnami/kafka/logs

    # Common Kafka Configuration
    
    sasl.enabled.mechanisms=PLAIN
    # Interbroker configuration
    inter.broker.listener.name=INTERNAL
    sasl.mechanism.inter.broker.protocol=PLAIN
    # Listeners SASL JAAS configuration
    listener.name.client.plain.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required user_user="password-placeholder-0";
    listener.name.internal.plain.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="inter_broker_user" password="interbroker-password-placeholder" user_inter_broker_user="interbroker-password-placeholder" user_user="password-placeholder-0";
    # End of SASL JAAS configuration

    # Custom Kafka Configuration
    offsets.topic.replication.factor=1
---
# Source: milvus/charts/kafka/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-kafka-scripts
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: 3.7.0
    helm.sh/chart: kafka-29.3.1
data:
  kafka-init.sh: |-
    #!/bin/bash

    set -o errexit
    set -o nounset
    set -o pipefail

    error(){
      local message="${1:?missing message}"
      echo "ERROR: ${message}"
      exit 1
    }

    retry_while() {
        local -r cmd="${1:?cmd is missing}"
        local -r retries="${2:-12}"
        local -r sleep_time="${3:-5}"
        local return_value=1

        read -r -a command <<< "$cmd"
        for ((i = 1 ; i <= retries ; i+=1 )); do
            "${command[@]}" && return_value=0 && break
            sleep "$sleep_time"
        done
        return $return_value
    }

    replace_in_file() {
        local filename="${1:?filename is required}"
        local match_regex="${2:?match regex is required}"
        local substitute_regex="${3:?substitute regex is required}"
        local posix_regex=${4:-true}

        local result

        # We should avoid using 'sed in-place' substitutions
        # 1) They are not compatible with files mounted from ConfigMap(s)
        # 2) We found incompatibility issues with Debian10 and "in-place" substitutions
        local -r del=$'\001' # Use a non-printable character as a 'sed' delimiter to avoid issues
        if [[ $posix_regex = true ]]; then
            result="$(sed -E "s${del}${match_regex}${del}${substitute_regex}${del}g" "$filename")"
        else
            result="$(sed "s${del}${match_regex}${del}${substitute_regex}${del}g" "$filename")"
        fi
        echo "$result" > "$filename"
    }

    kafka_conf_set() {
        local file="${1:?missing file}"
        local key="${2:?missing key}"
        local value="${3:?missing value}"

        # Check if the value was set before
        if grep -q "^[#\\s]*$key\s*=.*" "$file"; then
            # Update the existing key
            replace_in_file "$file" "^[#\\s]*${key}\s*=.*" "${key}=${value}" false
        else
            # Add a new key
            printf '\n%s=%s' "$key" "$value" >>"$file"
        fi
    }

    replace_placeholder() {
      local placeholder="${1:?missing placeholder value}"
      local password="${2:?missing password value}"
      local -r del=$'\001' # Use a non-printable character as a 'sed' delimiter to avoid issues with delimiter symbols in sed string
      sed -i "s${del}$placeholder${del}$password${del}g" "$KAFKA_CONFIG_FILE"
    }

    append_file_to_kafka_conf() {
        local file="${1:?missing source file}"
        local conf="${2:?missing kafka conf file}"

        cat "$1" >> "$2"
    }

    configure_external_access() {
      # Configure external hostname
      if [[ -f "/shared/external-host.txt" ]]; then
        host=$(cat "/shared/external-host.txt")
      elif [[ -n "${EXTERNAL_ACCESS_HOST:-}" ]]; then
        host="$EXTERNAL_ACCESS_HOST"
      elif [[ -n "${EXTERNAL_ACCESS_HOSTS_LIST:-}" ]]; then
        read -r -a hosts <<<"$(tr ',' ' ' <<<"${EXTERNAL_ACCESS_HOSTS_LIST}")"
        host="${hosts[$POD_ID]}"
      elif [[ "$EXTERNAL_ACCESS_HOST_USE_PUBLIC_IP" =~ ^(yes|true)$ ]]; then
        host=$(curl -s https://ipinfo.io/ip)
      else
        error "External access hostname not provided"
      fi

      # Configure external port
      if [[ -f "/shared/external-port.txt" ]]; then
        port=$(cat "/shared/external-port.txt")
      elif [[ -n "${EXTERNAL_ACCESS_PORT:-}" ]]; then
        if [[ "${EXTERNAL_ACCESS_PORT_AUTOINCREMENT:-}" =~ ^(yes|true)$ ]]; then
          port="$((EXTERNAL_ACCESS_PORT + POD_ID))"
        else
          port="$EXTERNAL_ACCESS_PORT"
        fi
      elif [[ -n "${EXTERNAL_ACCESS_PORTS_LIST:-}" ]]; then
        read -r -a ports <<<"$(tr ',' ' ' <<<"${EXTERNAL_ACCESS_PORTS_LIST}")"
        port="${ports[$POD_ID]}"
      else
        error "External access port not provided"
      fi
      # Configure Kafka advertised listeners
      sed -i -E "s|^(advertised\.listeners=\S+)$|\1,EXTERNAL://${host}:${port}|" "$KAFKA_CONFIG_FILE"
    }
    configure_kafka_sasl() {

      # Replace placeholders with passwords
      replace_placeholder "interbroker-password-placeholder" "$KAFKA_INTER_BROKER_PASSWORD"
      replace_placeholder "controller-password-placeholder" "$KAFKA_CONTROLLER_PASSWORD"
      read -r -a passwords <<<"$(tr ',;' ' ' <<<"${KAFKA_CLIENT_PASSWORDS:-}")"
      for ((i = 0; i < ${#passwords[@]}; i++)); do
          replace_placeholder "password-placeholder-${i}\"" "${passwords[i]}\""
      done
    }

    export KAFKA_CONFIG_FILE=/config/server.properties
    cp /configmaps/server.properties $KAFKA_CONFIG_FILE

    # Get pod ID and role, last and second last fields in the pod name respectively
    POD_ID=$(echo "$MY_POD_NAME" | rev | cut -d'-' -f 1 | rev)
    POD_ROLE=$(echo "$MY_POD_NAME" | rev | cut -d'-' -f 2 | rev)

    # Configure node.id and/or broker.id
    if [[ -f "/bitnami/kafka/data/meta.properties" ]]; then
        if grep -q "broker.id" /bitnami/kafka/data/meta.properties; then
          ID="$(grep "broker.id" /bitnami/kafka/data/meta.properties | awk -F '=' '{print $2}')"
          kafka_conf_set "$KAFKA_CONFIG_FILE" "node.id" "$ID"
        else
          ID="$(grep "node.id" /bitnami/kafka/data/meta.properties | awk -F '=' '{print $2}')"
          kafka_conf_set "$KAFKA_CONFIG_FILE" "node.id" "$ID"
        fi
    else
        ID=$((POD_ID + KAFKA_MIN_ID))
        kafka_conf_set "$KAFKA_CONFIG_FILE" "node.id" "$ID"
    fi
    replace_placeholder "advertised-address-placeholder" "${MY_POD_NAME}.my-release-kafka-${POD_ROLE}-headless.default.svc.cluster.local"
    if [[ "${EXTERNAL_ACCESS_ENABLED:-false}" =~ ^(yes|true)$ ]]; then
      configure_external_access
    fi
    configure_kafka_sasl
    if [ -f /secret-config/server-secret.properties ]; then
      append_file_to_kafka_conf /secret-config/server-secret.properties $KAFKA_CONFIG_FILE
    fi
---
# Source: milvus/charts/minio/templates/provisioning-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-minio-provisioning
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: minio
    app.kubernetes.io/version: 2024.6.6
    helm.sh/chart: minio-14.6.7
    app.kubernetes.io/component: minio-provisioning
data:
---
# Source: milvus/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-milvus
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
data:
  00_milvus_default.yaml: |
    # etcd configuration
    etcd:
      endpoints:
        - http://my-release-etcd-0.my-release-etcd-headless:2379
        - http://my-release-etcd-1.my-release-etcd-headless:2379
        - http://my-release-etcd-2.my-release-etcd-headless:2379
    metastore:
      type: etcd
    
    # S3 configuration
    minio:
      address: my-release-minio
      port: 80
      accessKeyID: "{{ MILVUS_S3_ACCESS_ID }}"
      secretAccessKey: "{{ MILVUS_S3_SECRET_ACCESS_KEY }}"
      useSSL: false
      bucketName: milvus
      rootPath: file
      useIAM: false
    
    # Kafka configuration
    kafka:
      brokerList:
        - my-release-kafka-controller-0.my-release-kafka-controller-headless:9092
      securityProtocol: SASL_PLAINTEXT
      saslMechanisms: PLAIN
      saslUsername: user
      saslPassword: "{{ MILVUS_KAFKA_PASSWORD }}"
    
    # Data coordinator
    dataCoord:
      address: my-release-milvus-data-coordinator
      port: 19530
    
    # Root coordinator
    rootCoord:
      address: my-release-milvus-root-coordinator
      port: 19530
    
    # Index coordinator
    indexCoord:
      address: my-release-milvus-index-coordinator
      port: 19530
    
    # Query coordinator
    queryCoord:
      address: my-release-milvus-query-coordinator
      port: 19530
    
    # Data node
    dataNode:
      port: 19530
    
    # Index node
    indexNode:
      port: 19530
    
    # Query node
    queryNode:
      port: 19530
    
    proxy:
      port: 19530
      accessLog:
        localPath: /dev
        filename: stdout
      http:
        enabled: true
    
    # Log configuration
    log:
      level: info
      stdout: true
    
    # Common configuration
    common:
      storageType: minio
      security:
        authorizationEnabled: false
---
# Source: milvus/templates/data-coordinator/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-milvus-data-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
data:
  03_data_coordinator_default.yaml: |
    # Override the port for internal binding (the external components will use the service port defined in milvus.defaultConfig)
    dataCoord:
      port: 19530
      enableActiveStandby: true
---
# Source: milvus/templates/data-node/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-milvus-data-node
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
data:
  03_data_node_default.yaml: |
    # Override the port for internal binding (the external components will use the service port defined in milvus.defaultConfig)
    dataNode:
      port: 19530
      enableDisk: true
---
# Source: milvus/templates/index-coordinator/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-milvus-index-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
data:
  03_index_coordinator_default.yaml: |
    # Override the port for internal binding (the external components will use the service port defined in milvus.defaultConfig)
    indexCoord:
      port: 19530
      enableActiveStandby: true
---
# Source: milvus/templates/index-node/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-milvus-index-node
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
data:
  03_index_node_default.yaml: |
    # Override the port for internal binding (the external components will use the service port defined in milvus.defaultConfig)
    indexNode:
      port: 19530
      enableDisk: true
---
# Source: milvus/templates/proxy/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-milvus-proxy
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
data:
  03_index_node_default.yaml: |
    # Override the port for internal binding (the external components will use the service port defined in milvus.defaultConfig)
    proxy:
      port: 19530
      internalPort: 19529
---
# Source: milvus/templates/query-coordinator/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-milvus-query-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
data:
  03_query_coordinator_default.yaml: |
    # Override the port for internal binding (the external components will use the service port defined in milvus.defaultConfig)
    queryCoord:
      port: 19530
      enableActiveStandby: true
---
# Source: milvus/templates/query-node/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-milvus-query-node
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
data:
  03_query_node_default.yaml: |
    # Override the port for internal binding (the external components will use the service port defined in milvus.defaultConfig)
    queryNode:
      port: 19530
      enableDisk: true
---
# Source: milvus/templates/root-coordinator/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-milvus-root-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
data:
  03_root_coordinator_default.yaml: |
    # Override the port for internal binding (the external components will use the service port defined in milvus.defaultConfig)
    rootCoord:
      port: 19530
      enableActiveStandby: true
---
# Source: milvus/charts/minio/templates/pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: my-release-minio
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: minio
    app.kubernetes.io/version: 2024.6.6
    helm.sh/chart: minio-14.6.7
spec:
  accessModes:
    - "ReadWriteOnce"
  resources:
    requests:
      storage: "8Gi"
---
# Source: milvus/charts/etcd/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-etcd-headless
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: etcd
    app.kubernetes.io/version: 3.5.14
    helm.sh/chart: etcd-10.2.2
    app.kubernetes.io/component: etcd
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: client
      port: 2379
      targetPort: client
    - name: peer
      port: 2380
      targetPort: peer
  selector:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/name: etcd
    app.kubernetes.io/component: etcd
---
# Source: milvus/charts/etcd/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-etcd
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: etcd
    app.kubernetes.io/version: 3.5.14
    helm.sh/chart: etcd-10.2.2
    app.kubernetes.io/component: etcd
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: "client"
      port: 2379
      targetPort: client
      nodePort: null
    - name: "peer"
      port: 2380
      targetPort: peer
      nodePort: null
  selector:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/name: etcd
    app.kubernetes.io/component: etcd
---
# Source: milvus/charts/kafka/templates/controller-eligible/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-kafka-controller-headless
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: 3.7.0
    helm.sh/chart: kafka-29.3.1
    app.kubernetes.io/component: controller-eligible
    app.kubernetes.io/part-of: kafka
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: tcp-interbroker
      port: 9094
      protocol: TCP
      targetPort: interbroker
    - name: tcp-client
      port: 9092
      protocol: TCP
      targetPort: client
    - name: tcp-controller
      protocol: TCP
      port: 9093
      targetPort: controller
  selector:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/name: kafka
    app.kubernetes.io/component: controller-eligible
    app.kubernetes.io/part-of: kafka
---
# Source: milvus/charts/kafka/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-kafka
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: 3.7.0
    helm.sh/chart: kafka-29.3.1
    app.kubernetes.io/component: kafka
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-client
      port: 9092
      protocol: TCP
      targetPort: client
      nodePort: null
  selector:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/name: kafka
    app.kubernetes.io/part-of: kafka
---
# Source: milvus/charts/minio/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-minio
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: minio
    app.kubernetes.io/version: 2024.6.6
    helm.sh/chart: minio-14.6.7
spec:
  type: ClusterIP
  ports:
    - name: minio-api
      port: 80
      targetPort: minio-api
      nodePort: null
    - name: minio-console
      port: 9001
      targetPort: minio-console
      nodePort: null
  selector:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/name: minio
---
# Source: milvus/templates/attu/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-milvus-attu
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.1
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: attu
spec:
  type: LoadBalancer
  sessionAffinity: None
  externalTrafficPolicy: "Cluster"
  
  loadBalancerSourceRanges: []
  
  ports:
    - name: http
      port: 80
      targetPort: http
      protocol: TCP
  selector:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/name: milvus
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: attu
---
# Source: milvus/templates/data-coordinator/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-milvus-data-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: data-coordinator
spec:
  type: ClusterIP
  sessionAffinity: None
  
  ports:
    - name: grpc
      port: 19530
      targetPort: grpc
      protocol: TCP
      nodePort: null
  selector:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/name: milvus
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: data-coordinator
---
# Source: milvus/templates/data-node/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-milvus-data-node
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: data-node
spec:
  type: ClusterIP
  sessionAffinity: None
  
  ports:
    - name: grpc
      port: 19530
      targetPort: grpc
      protocol: TCP
      nodePort: null
  selector:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/name: milvus
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: data-node
---
# Source: milvus/templates/index-coordinator/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-milvus-index-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: index-coordinator
spec:
  type: ClusterIP
  sessionAffinity: None
  
  ports:
    - name: grpc
      port: 19530
      targetPort: grpc
      protocol: TCP
      nodePort: null
  selector:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/name: milvus
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: index-coordinator
---
# Source: milvus/templates/index-node/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-milvus-index-node
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: index-node
spec:
  type: ClusterIP
  sessionAffinity: None
  
  ports:
    - name: grpc
      port: 19530
      targetPort: grpc
      protocol: TCP
      nodePort: null
  selector:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/name: milvus
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: index-node
---
# Source: milvus/templates/proxy/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-milvus-proxy
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: proxy
spec:
  type: LoadBalancer
  sessionAffinity: None
  externalTrafficPolicy: "Cluster"
  
  loadBalancerSourceRanges: []
  
  ports:
    - name: grpc
      port: 19530
      targetPort: grpc
      protocol: TCP
  selector:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/name: milvus
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: proxy
---
# Source: milvus/templates/query-coordinator/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-milvus-query-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: query-coordinator
spec:
  type: ClusterIP
  sessionAffinity: None
  
  ports:
    - name: grpc
      port: 19530
      targetPort: grpc
      protocol: TCP
      nodePort: null
  selector:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/name: milvus
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: query-coordinator
---
# Source: milvus/templates/query-node/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-milvus-query-node
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: query-node
spec:
  type: ClusterIP
  sessionAffinity: None
  
  ports:
    - name: grpc
      port: 19530
      targetPort: grpc
      protocol: TCP
      nodePort: null
  selector:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/name: milvus
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: query-node
---
# Source: milvus/templates/root-coordinator/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-milvus-root-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: root-coordinator
spec:
  type: ClusterIP
  sessionAffinity: None
  
  ports:
    - name: grpc
      port: 19530
      targetPort: grpc
      protocol: TCP
      nodePort: null
  selector:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/name: milvus
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: root-coordinator
---
# Source: milvus/charts/minio/templates/standalone/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-minio
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: minio
    app.kubernetes.io/version: 2024.6.6
    helm.sh/chart: minio-14.6.7
spec:
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: minio
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: minio
        app.kubernetes.io/version: 2024.6.6
        helm.sh/chart: minio-14.6.7
      annotations:
        checksum/credentials-secret: e1560b23be78c9c9437eba11e6a5a312a72239593752d7626a15210a92f55f95
    spec:
      
      serviceAccountName: my-release-minio
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: my-release
                    app.kubernetes.io/name: minio
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      automountServiceAccountToken: false
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: OnRootMismatch
        supplementalGroups: []
        sysctls: []
      containers:
        - name: minio
          image: docker.io/bitnami/minio:2024.6.6-debian-12-r0
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MINIO_SCHEME
              value: "http"
            - name: MINIO_FORCE_NEW_KEYS
              value: "no"
            - name: MINIO_API_PORT_NUMBER
              value: "9000"
            - name: MINIO_ROOT_USER
              valueFrom:
                secretKeyRef:
                  name: my-release-minio
                  key: root-user
            - name: MINIO_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-minio
                  key: root-password
            - name: MINIO_DEFAULT_BUCKETS
              value: milvus
            - name: MINIO_BROWSER
              value: "on"
            - name: MINIO_PROMETHEUS_AUTH_TYPE
              value: "public"
            - name: MINIO_CONSOLE_PORT_NUMBER
              value: "9001"
            - name: MINIO_DATA_DIR
              value: "/bitnami/minio/data"
          envFrom:
          ports:
            - name: minio-api
              containerPort: 9000
              protocol: TCP
            - name: minio-console
              containerPort: 9001
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /minio/health/live
              port: minio-api
              scheme: "HTTP"
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
          readinessProbe:
            tcpSocket:
              port: minio-api
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 5
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          volumeMounts:
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - name: empty-dir
              mountPath: /opt/bitnami/minio/tmp
              subPath: app-tmp-dir
            - name: empty-dir
              mountPath: /.mc
              subPath: app-mc-dir
            - name: data
              mountPath: /bitnami/minio/data
      volumes:
        - name: empty-dir
          emptyDir: {}
        - name: data
          persistentVolumeClaim:
            claimName: my-release-minio
---
# Source: milvus/templates/attu/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-milvus-attu
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.1
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: attu
spec:
  replicas: 1
  strategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: attu
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: milvus
        app.kubernetes.io/version: 2.4.1
        helm.sh/chart: milvus-8.2.4
        app.kubernetes.io/part-of: milvus
        app.kubernetes.io/component: attu
    spec:
      enableServiceLinks: false
      serviceAccountName: my-release-milvus-attu
      
      automountServiceAccountToken: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: my-release
                    app.kubernetes.io/name: milvus
                    app.kubernetes.io/component: attu
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      initContainers:
        - name: wait-for-proxy
          image: docker.io/bitnami/milvus:2.4.4-debian-12-r2 
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              check_proxy() {
                  local -r proxy_host="${1:-?missing proxy}"
                  if wait-for-port --timeout=5 --host=${proxy_host} --state=inuse 19530; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              host="my-release-milvus-proxy"
        
              echo "Checking connection to $host"
              if retry_while "check_proxy $host"; then
                  echo "Connected to $host"
              else
                  echo "Error connecting to $host"
                  exit 1
              fi
        
              echo "Connection success"
              exit 0
      containers:
        - name: attu
          image: docker.io/bitnami/attu:2.4.1-debian-12-r0
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          env:
            - name: MILVUS_URL
              value: http://my-release-milvus-proxy:19530
          envFrom:
          ports:
            - containerPort: 3000
              name: http
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          livenessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: http
          readinessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /
              port: http
          volumeMounts:
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - name: empty-dir
              mountPath: /.npm
              subPath: npm-tmp-dir
            - name: empty-dir
              mountPath: /.yarn
              subPath: yarn-dir
            - name: empty-dir
              mountPath: /.cache/yarn
              subPath: yarn-cache-dir
      volumes:
        - name: empty-dir
          emptyDir: {}
---
# Source: milvus/templates/data-coordinator/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-milvus-data-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: data-coordinator
spec:
  replicas: 1
  strategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: data-coordinator
  template:
    metadata:
      annotations:
        checksum/common-config: f05d4186385409564933c3aea16c2ac1c21ef2432368c51034e2b6e9352544f1
        checksum/common-config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum/config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
      labels:
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: milvus
        app.kubernetes.io/version: 2.4.4
        helm.sh/chart: milvus-8.2.4
        app.kubernetes.io/part-of: milvus
        app.kubernetes.io/component: data-coordinator
    spec:
      enableServiceLinks: false
      serviceAccountName: my-release-milvus-data-coordinator
      
      automountServiceAccountToken: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: my-release
                    app.kubernetes.io/name: milvus
                    app.kubernetes.io/component: data-coordinator
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      initContainers:
        - name: wait-for-etcd
          image: docker.io/bitnami/os-shell:12-debian-12-r22
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              etcd_hosts=(
                "http://my-release-etcd:2379"
              )
        
              check_etcd() {
                  local -r etcd_host="${1:-?missing etcd}"
                  local params_cert=""
        
                  if echo $etcd_host | grep https; then
                     params_cert="--cacert /bitnami/milvus/conf/cert/etcd/client/ca.crt --cert /bitnami/milvus/conf/cert/etcd/client/tls.crt --key /bitnami/milvus/conf/cert/etcd/client/tls.key"
                  fi
                  if [ ! -z  ]; then
                    params_cert=$params_cert" --pass "
                  fi
                  if curl --max-time 5 "${etcd_host}/version" $params_cert | grep etcdcluster; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              for host in "${etcd_hosts[@]}"; do
                  echo "Checking connection to $host"
                  if retry_while "check_etcd $host"; then
                      echo "Connected to $host"
                  else
                      echo "Error connecting to $host"
                      exit 1
                  fi
              done
        
              echo "Connection success"
              exit 0
        - name: wait-for-kafka
          image: docker.io/bitnami/milvus:2.4.4-debian-12-r2 
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              kafka_hosts=(
                "my-release-kafka"
              )
        
              check_kafka() {
                  local -r kafka_host="${1:-?missing kafka}"
                  if wait-for-port --timeout=5 --host=${kafka_host} --state=inuse 9092; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              for host in "${kafka_hosts[@]}"; do
                  echo "Checking connection to $host"
                  if retry_while "check_kafka $host"; then
                      echo "Connected to $host"
                  else
                      echo "Error connecting to $host"
                      exit 1
                  fi
              done
        
              echo "Connection success"
              exit 0
        - name: wait-for-s3
          image: docker.io/bitnami/os-shell:12-debian-12-r22
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              check_s3() {
                  local -r s3_host="${1:-?missing s3}"
                  if curl --max-time 5 "${s3_host}" | grep "RequestId"; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              host=my-release-minio:80
        
              echo "Checking connection to $host"
              if retry_while "check_s3 $host"; then
                echo "Connected to $host"
              else
                echo "Error connecting to $host"
                exit 1
              fi
        
              echo "Connection success"
              exit 0
        # This init container renders and merges the Milvus configuration files.
        # We need to use a volume because we're working with ReadOnlyRootFilesystem
        - name: prepare-milvus
          image: docker.io/bitnami/milvus:2.4.4-debian-12-r2
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              # Remove previously existing files and copy the default configuration files to ensure they are present in mounted configs directory
              rm -rf /bitnami/milvus/rendered-conf/*
              cp -r /opt/bitnami/milvus/configs/. /bitnami/milvus/rendered-conf
              # Build final milvus.yaml with the sections of the different files
              find /bitnami/milvus/conf -type f -name *.yaml -print0 | sort -z | xargs -0 yq eval-all '. as $item ireduce ({}; . * $item )' /bitnami/milvus/rendered-conf/milvus.yaml > /bitnami/milvus/rendered-conf/pre-render-config_00.yaml
        
              # Kafka settings
              # HACK: In order to enable Kafka we need to remove all Pulsar settings from the configuration file
              # https://github.com/milvus-io/milvus/blob/master/configs/milvus.yaml#L110
              yq 'del(.pulsar)' /bitnami/milvus/rendered-conf/pre-render-config_00.yaml > /bitnami/milvus/rendered-conf/pre-render-config_01.yaml
              # Kafka TLS settings
        
              # Milvus server TLS settings
              yq e '.common.security.tlsMode = 0' /bitnami/milvus/rendered-conf/pre-render-config_01.yaml > /bitnami/milvus/rendered-conf/pre-render-config_02.yaml
        
              render-template /bitnami/milvus/rendered-conf/pre-render-config_02.yaml > /bitnami/milvus/rendered-conf/milvus.yaml
              rm /bitnami/milvus/rendered-conf/pre-render-config*
              chmod 644 /bitnami/milvus/rendered-conf/milvus.yaml
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MILVUS_KAFKA_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-kafka-user-passwords
                  key: system-user-password
            - name: MILVUS_S3_ACCESS_ID
              valueFrom:
                secretKeyRef:
                  name: my-release-minio
                  key: root-user
            - name: MILVUS_S3_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: my-release-minio
                  key: root-password
          envFrom:
          volumeMounts:
            - name: config-common
              mountPath: /bitnami/milvus/conf/00_default
            - name: component-config-default
              mountPath: /bitnami/milvus/conf/02_component_default
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - name: empty-dir
              mountPath: /bitnami/milvus/rendered-conf/
              subPath: app-rendered-conf-dir
      containers:
        - name: milvus
          image: docker.io/bitnami/milvus:2.4.4-debian-12-r2
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          args:
            - run
            - datacoord
          env:
            - name: METRICS_PORT
              value: "9091"
          envFrom:
          ports:
            - containerPort: 19530
              name: grpc
            - containerPort: 9091
              name: http-metrics
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          livenessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: http-metrics
          readinessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /healthz
              port: http-metrics
          volumeMounts:
            - name: empty-dir
              mountPath: /opt/bitnami/milvus/configs
              subPath: app-rendered-conf-dir
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - name: empty-dir
              mountPath: /opt/bitnami/milvus/tmp
              subPath: app-tmp-dir
              # We are using a s3 backend, so this data dir is temporary
            - name: empty-dir
              mountPath: /bitnami/milvus/data
              subPath: app-data-dir
      volumes:
        - name: empty-dir
          emptyDir: {}
        - name: config-common
          configMap:
            name: my-release-milvus
        - name: component-config-default
          configMap:
            name: my-release-milvus-data-coordinator
---
# Source: milvus/templates/data-node/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-milvus-data-node
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: data-node
spec:
  replicas: 1
  strategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: data-node
  template:
    metadata:
      annotations:
        checksum/common-config: f05d4186385409564933c3aea16c2ac1c21ef2432368c51034e2b6e9352544f1
        checksum/common-config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum/config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
      labels:
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: milvus
        app.kubernetes.io/version: 2.4.4
        helm.sh/chart: milvus-8.2.4
        app.kubernetes.io/part-of: milvus
        app.kubernetes.io/component: data-node
    spec:
      enableServiceLinks: false
      serviceAccountName: my-release-milvus-data-node
      
      automountServiceAccountToken: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: my-release
                    app.kubernetes.io/name: milvus
                    app.kubernetes.io/component: data-node
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      initContainers:
        - name: wait-for-etcd
          image: docker.io/bitnami/os-shell:12-debian-12-r22
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              etcd_hosts=(
                "http://my-release-etcd:2379"
              )
        
              check_etcd() {
                  local -r etcd_host="${1:-?missing etcd}"
                  local params_cert=""
        
                  if echo $etcd_host | grep https; then
                     params_cert="--cacert /bitnami/milvus/conf/cert/etcd/client/ca.crt --cert /bitnami/milvus/conf/cert/etcd/client/tls.crt --key /bitnami/milvus/conf/cert/etcd/client/tls.key"
                  fi
                  if [ ! -z  ]; then
                    params_cert=$params_cert" --pass "
                  fi
                  if curl --max-time 5 "${etcd_host}/version" $params_cert | grep etcdcluster; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              for host in "${etcd_hosts[@]}"; do
                  echo "Checking connection to $host"
                  if retry_while "check_etcd $host"; then
                      echo "Connected to $host"
                  else
                      echo "Error connecting to $host"
                      exit 1
                  fi
              done
        
              echo "Connection success"
              exit 0
        - name: wait-for-kafka
          image: docker.io/bitnami/milvus:2.4.4-debian-12-r2 
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              kafka_hosts=(
                "my-release-kafka"
              )
        
              check_kafka() {
                  local -r kafka_host="${1:-?missing kafka}"
                  if wait-for-port --timeout=5 --host=${kafka_host} --state=inuse 9092; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              for host in "${kafka_hosts[@]}"; do
                  echo "Checking connection to $host"
                  if retry_while "check_kafka $host"; then
                      echo "Connected to $host"
                  else
                      echo "Error connecting to $host"
                      exit 1
                  fi
              done
        
              echo "Connection success"
              exit 0
        - name: wait-for-s3
          image: docker.io/bitnami/os-shell:12-debian-12-r22
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              check_s3() {
                  local -r s3_host="${1:-?missing s3}"
                  if curl --max-time 5 "${s3_host}" | grep "RequestId"; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              host=my-release-minio:80
        
              echo "Checking connection to $host"
              if retry_while "check_s3 $host"; then
                echo "Connected to $host"
              else
                echo "Error connecting to $host"
                exit 1
              fi
        
              echo "Connection success"
              exit 0
        # This init container renders and merges the Milvus configuration files.
        # We need to use a volume because we're working with ReadOnlyRootFilesystem
        - name: prepare-milvus
          image: docker.io/bitnami/milvus:2.4.4-debian-12-r2
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              # Remove previously existing files and copy the default configuration files to ensure they are present in mounted configs directory
              rm -rf /bitnami/milvus/rendered-conf/*
              cp -r /opt/bitnami/milvus/configs/. /bitnami/milvus/rendered-conf
              # Build final milvus.yaml with the sections of the different files
              find /bitnami/milvus/conf -type f -name *.yaml -print0 | sort -z | xargs -0 yq eval-all '. as $item ireduce ({}; . * $item )' /bitnami/milvus/rendered-conf/milvus.yaml > /bitnami/milvus/rendered-conf/pre-render-config_00.yaml
        
              # Kafka settings
              # HACK: In order to enable Kafka we need to remove all Pulsar settings from the configuration file
              # https://github.com/milvus-io/milvus/blob/master/configs/milvus.yaml#L110
              yq 'del(.pulsar)' /bitnami/milvus/rendered-conf/pre-render-config_00.yaml > /bitnami/milvus/rendered-conf/pre-render-config_01.yaml
              # Kafka TLS settings
        
              # Milvus server TLS settings
              yq e '.common.security.tlsMode = 0' /bitnami/milvus/rendered-conf/pre-render-config_01.yaml > /bitnami/milvus/rendered-conf/pre-render-config_02.yaml
        
              render-template /bitnami/milvus/rendered-conf/pre-render-config_02.yaml > /bitnami/milvus/rendered-conf/milvus.yaml
              rm /bitnami/milvus/rendered-conf/pre-render-config*
              chmod 644 /bitnami/milvus/rendered-conf/milvus.yaml
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MILVUS_KAFKA_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-kafka-user-passwords
                  key: system-user-password
            - name: MILVUS_S3_ACCESS_ID
              valueFrom:
                secretKeyRef:
                  name: my-release-minio
                  key: root-user
            - name: MILVUS_S3_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: my-release-minio
                  key: root-password
          envFrom:
          volumeMounts:
            - name: config-common
              mountPath: /bitnami/milvus/conf/00_default
            - name: component-config-default
              mountPath: /bitnami/milvus/conf/02_component_default
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - name: empty-dir
              mountPath: /bitnami/milvus/rendered-conf/
              subPath: app-rendered-conf-dir
      containers:
        - name: milvus
          image: docker.io/bitnami/milvus:2.4.4-debian-12-r2
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          args:
            - run
            - datanode
          env:
            - name: METRICS_PORT
              value: "9091"
          envFrom:
          ports:
            - containerPort: 19530
              name: grpc
            - containerPort: 9091
              name: http-metrics
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          livenessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: http-metrics
          readinessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /healthz
              port: http-metrics
          volumeMounts:
            - name: empty-dir
              mountPath: /opt/bitnami/milvus/configs
              subPath: app-rendered-conf-dir
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - name: empty-dir
              mountPath: /opt/bitnami/milvus/tmp
              subPath: app-tmp-dir
              # We are using a s3 backend, so this data dir is temporary
            - name: empty-dir
              mountPath: /bitnami/milvus/data
              subPath: app-data-dir
      volumes:
        - name: empty-dir
          emptyDir: {}
        - name: config-common
          configMap:
            name: my-release-milvus
        - name: component-config-default
          configMap:
            name: my-release-milvus-data-node
---
# Source: milvus/templates/index-coordinator/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-milvus-index-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: index-coordinator
spec:
  replicas: 1
  strategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: index-coordinator
  template:
    metadata:
      annotations:
        checksum/common-config: f05d4186385409564933c3aea16c2ac1c21ef2432368c51034e2b6e9352544f1
        checksum/common-config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum/config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
      labels:
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: milvus
        app.kubernetes.io/version: 2.4.4
        helm.sh/chart: milvus-8.2.4
        app.kubernetes.io/part-of: milvus
        app.kubernetes.io/component: index-coordinator
    spec:
      enableServiceLinks: false
      serviceAccountName: my-release-milvus-index-coordinator
      
      automountServiceAccountToken: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: my-release
                    app.kubernetes.io/name: milvus
                    app.kubernetes.io/component: index-coordinator
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      initContainers:
        - name: wait-for-etcd
          image: docker.io/bitnami/os-shell:12-debian-12-r22
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              etcd_hosts=(
                "http://my-release-etcd:2379"
              )
        
              check_etcd() {
                  local -r etcd_host="${1:-?missing etcd}"
                  local params_cert=""
        
                  if echo $etcd_host | grep https; then
                     params_cert="--cacert /bitnami/milvus/conf/cert/etcd/client/ca.crt --cert /bitnami/milvus/conf/cert/etcd/client/tls.crt --key /bitnami/milvus/conf/cert/etcd/client/tls.key"
                  fi
                  if [ ! -z  ]; then
                    params_cert=$params_cert" --pass "
                  fi
                  if curl --max-time 5 "${etcd_host}/version" $params_cert | grep etcdcluster; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              for host in "${etcd_hosts[@]}"; do
                  echo "Checking connection to $host"
                  if retry_while "check_etcd $host"; then
                      echo "Connected to $host"
                  else
                      echo "Error connecting to $host"
                      exit 1
                  fi
              done
        
              echo "Connection success"
              exit 0
        - name: wait-for-kafka
          image: docker.io/bitnami/milvus:2.4.4-debian-12-r2 
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              kafka_hosts=(
                "my-release-kafka"
              )
        
              check_kafka() {
                  local -r kafka_host="${1:-?missing kafka}"
                  if wait-for-port --timeout=5 --host=${kafka_host} --state=inuse 9092; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              for host in "${kafka_hosts[@]}"; do
                  echo "Checking connection to $host"
                  if retry_while "check_kafka $host"; then
                      echo "Connected to $host"
                  else
                      echo "Error connecting to $host"
                      exit 1
                  fi
              done
        
              echo "Connection success"
              exit 0
        - name: wait-for-s3
          image: docker.io/bitnami/os-shell:12-debian-12-r22
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              check_s3() {
                  local -r s3_host="${1:-?missing s3}"
                  if curl --max-time 5 "${s3_host}" | grep "RequestId"; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              host=my-release-minio:80
        
              echo "Checking connection to $host"
              if retry_while "check_s3 $host"; then
                echo "Connected to $host"
              else
                echo "Error connecting to $host"
                exit 1
              fi
        
              echo "Connection success"
              exit 0
        # This init container renders and merges the Milvus configuration files.
        # We need to use a volume because we're working with ReadOnlyRootFilesystem
        - name: prepare-milvus
          image: docker.io/bitnami/milvus:2.4.4-debian-12-r2
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              # Remove previously existing files and copy the default configuration files to ensure they are present in mounted configs directory
              rm -rf /bitnami/milvus/rendered-conf/*
              cp -r /opt/bitnami/milvus/configs/. /bitnami/milvus/rendered-conf
              # Build final milvus.yaml with the sections of the different files
              find /bitnami/milvus/conf -type f -name *.yaml -print0 | sort -z | xargs -0 yq eval-all '. as $item ireduce ({}; . * $item )' /bitnami/milvus/rendered-conf/milvus.yaml > /bitnami/milvus/rendered-conf/pre-render-config_00.yaml
        
              # Kafka settings
              # HACK: In order to enable Kafka we need to remove all Pulsar settings from the configuration file
              # https://github.com/milvus-io/milvus/blob/master/configs/milvus.yaml#L110
              yq 'del(.pulsar)' /bitnami/milvus/rendered-conf/pre-render-config_00.yaml > /bitnami/milvus/rendered-conf/pre-render-config_01.yaml
              # Kafka TLS settings
        
              # Milvus server TLS settings
              yq e '.common.security.tlsMode = 0' /bitnami/milvus/rendered-conf/pre-render-config_01.yaml > /bitnami/milvus/rendered-conf/pre-render-config_02.yaml
        
              render-template /bitnami/milvus/rendered-conf/pre-render-config_02.yaml > /bitnami/milvus/rendered-conf/milvus.yaml
              rm /bitnami/milvus/rendered-conf/pre-render-config*
              chmod 644 /bitnami/milvus/rendered-conf/milvus.yaml
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MILVUS_KAFKA_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-kafka-user-passwords
                  key: system-user-password
            - name: MILVUS_S3_ACCESS_ID
              valueFrom:
                secretKeyRef:
                  name: my-release-minio
                  key: root-user
            - name: MILVUS_S3_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: my-release-minio
                  key: root-password
          envFrom:
          volumeMounts:
            - name: config-common
              mountPath: /bitnami/milvus/conf/00_default
            - name: component-config-default
              mountPath: /bitnami/milvus/conf/02_component_default
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - name: empty-dir
              mountPath: /bitnami/milvus/rendered-conf/
              subPath: app-rendered-conf-dir
      containers:
        - name: milvus
          image: docker.io/bitnami/milvus:2.4.4-debian-12-r2
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          args:
            - run
            - indexcoord
          env:
            - name: METRICS_PORT
              value: "9091"
          envFrom:
          ports:
            - containerPort: 19530
              name: grpc
            - containerPort: 9091
              name: http-metrics
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          livenessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: http-metrics
          readinessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /healthz
              port: http-metrics
          volumeMounts:
            - name: empty-dir
              mountPath: /opt/bitnami/milvus/configs
              subPath: app-rendered-conf-dir
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - name: empty-dir
              mountPath: /opt/bitnami/milvus/tmp
              subPath: app-tmp-dir
              # We are using a s3 backend, so this data dir is temporary
            - name: empty-dir
              mountPath: /bitnami/milvus/data
              subPath: app-data-dir
      volumes:
        - name: empty-dir
          emptyDir: {}
        - name: config-common
          configMap:
            name: my-release-milvus
        - name: component-config-default
          configMap:
            name: my-release-milvus-index-coordinator
---
# Source: milvus/templates/index-node/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-milvus-index-node
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: index-node
spec:
  replicas: 1
  strategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: index-node
  template:
    metadata:
      annotations:
        checksum/common-config: f05d4186385409564933c3aea16c2ac1c21ef2432368c51034e2b6e9352544f1
        checksum/common-config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum/config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
      labels:
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: milvus
        app.kubernetes.io/version: 2.4.4
        helm.sh/chart: milvus-8.2.4
        app.kubernetes.io/part-of: milvus
        app.kubernetes.io/component: index-node
    spec:
      enableServiceLinks: false
      serviceAccountName: my-release-milvus-index-node
      
      automountServiceAccountToken: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: my-release
                    app.kubernetes.io/name: milvus
                    app.kubernetes.io/component: index-node
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      initContainers:
        - name: wait-for-etcd
          image: docker.io/bitnami/os-shell:12-debian-12-r22
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              etcd_hosts=(
                "http://my-release-etcd:2379"
              )
        
              check_etcd() {
                  local -r etcd_host="${1:-?missing etcd}"
                  local params_cert=""
        
                  if echo $etcd_host | grep https; then
                     params_cert="--cacert /bitnami/milvus/conf/cert/etcd/client/ca.crt --cert /bitnami/milvus/conf/cert/etcd/client/tls.crt --key /bitnami/milvus/conf/cert/etcd/client/tls.key"
                  fi
                  if [ ! -z  ]; then
                    params_cert=$params_cert" --pass "
                  fi
                  if curl --max-time 5 "${etcd_host}/version" $params_cert | grep etcdcluster; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              for host in "${etcd_hosts[@]}"; do
                  echo "Checking connection to $host"
                  if retry_while "check_etcd $host"; then
                      echo "Connected to $host"
                  else
                      echo "Error connecting to $host"
                      exit 1
                  fi
              done
        
              echo "Connection success"
              exit 0
        - name: wait-for-kafka
          image: docker.io/bitnami/milvus:2.4.4-debian-12-r2 
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              kafka_hosts=(
                "my-release-kafka"
              )
        
              check_kafka() {
                  local -r kafka_host="${1:-?missing kafka}"
                  if wait-for-port --timeout=5 --host=${kafka_host} --state=inuse 9092; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              for host in "${kafka_hosts[@]}"; do
                  echo "Checking connection to $host"
                  if retry_while "check_kafka $host"; then
                      echo "Connected to $host"
                  else
                      echo "Error connecting to $host"
                      exit 1
                  fi
              done
        
              echo "Connection success"
              exit 0
        - name: wait-for-s3
          image: docker.io/bitnami/os-shell:12-debian-12-r22
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              check_s3() {
                  local -r s3_host="${1:-?missing s3}"
                  if curl --max-time 5 "${s3_host}" | grep "RequestId"; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              host=my-release-minio:80
        
              echo "Checking connection to $host"
              if retry_while "check_s3 $host"; then
                echo "Connected to $host"
              else
                echo "Error connecting to $host"
                exit 1
              fi
        
              echo "Connection success"
              exit 0
        # This init container renders and merges the Milvus configuration files.
        # We need to use a volume because we're working with ReadOnlyRootFilesystem
        - name: prepare-milvus
          image: docker.io/bitnami/milvus:2.4.4-debian-12-r2
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              # Remove previously existing files and copy the default configuration files to ensure they are present in mounted configs directory
              rm -rf /bitnami/milvus/rendered-conf/*
              cp -r /opt/bitnami/milvus/configs/. /bitnami/milvus/rendered-conf
              # Build final milvus.yaml with the sections of the different files
              find /bitnami/milvus/conf -type f -name *.yaml -print0 | sort -z | xargs -0 yq eval-all '. as $item ireduce ({}; . * $item )' /bitnami/milvus/rendered-conf/milvus.yaml > /bitnami/milvus/rendered-conf/pre-render-config_00.yaml
        
              # Kafka settings
              # HACK: In order to enable Kafka we need to remove all Pulsar settings from the configuration file
              # https://github.com/milvus-io/milvus/blob/master/configs/milvus.yaml#L110
              yq 'del(.pulsar)' /bitnami/milvus/rendered-conf/pre-render-config_00.yaml > /bitnami/milvus/rendered-conf/pre-render-config_01.yaml
              # Kafka TLS settings
        
              # Milvus server TLS settings
              yq e '.common.security.tlsMode = 0' /bitnami/milvus/rendered-conf/pre-render-config_01.yaml > /bitnami/milvus/rendered-conf/pre-render-config_02.yaml
        
              render-template /bitnami/milvus/rendered-conf/pre-render-config_02.yaml > /bitnami/milvus/rendered-conf/milvus.yaml
              rm /bitnami/milvus/rendered-conf/pre-render-config*
              chmod 644 /bitnami/milvus/rendered-conf/milvus.yaml
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MILVUS_KAFKA_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-kafka-user-passwords
                  key: system-user-password
            - name: MILVUS_S3_ACCESS_ID
              valueFrom:
                secretKeyRef:
                  name: my-release-minio
                  key: root-user
            - name: MILVUS_S3_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: my-release-minio
                  key: root-password
          envFrom:
          volumeMounts:
            - name: config-common
              mountPath: /bitnami/milvus/conf/00_default
            - name: component-config-default
              mountPath: /bitnami/milvus/conf/02_component_default
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - name: empty-dir
              mountPath: /bitnami/milvus/rendered-conf/
              subPath: app-rendered-conf-dir
      containers:
        - name: milvus
          image: docker.io/bitnami/milvus:2.4.4-debian-12-r2
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          args:
            - run
            - indexnode
          env:
            - name: METRICS_PORT
              value: "9091"
          envFrom:
          ports:
            - containerPort: 19530
              name: grpc
            - containerPort: 9091
              name: http-metrics
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          livenessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: http-metrics
          readinessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /healthz
              port: http-metrics
          volumeMounts:
            - name: empty-dir
              mountPath: /opt/bitnami/milvus/configs
              subPath: app-rendered-conf-dir
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - name: empty-dir
              mountPath: /opt/bitnami/milvus/tmp
              subPath: app-tmp-dir
              # We are using a s3 backend, so this data dir is temporary
            - name: empty-dir
              mountPath: /bitnami/milvus/data
              subPath: app-data-dir
      volumes:
        - name: empty-dir
          emptyDir: {}
        - name: config-common
          configMap:
            name: my-release-milvus
        - name: component-config-default
          configMap:
            name: my-release-milvus-index-node
---
# Source: milvus/templates/proxy/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-milvus-proxy
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: proxy
spec:
  replicas: 1
  strategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: proxy
  template:
    metadata:
      annotations:
        checksum/common-config: f05d4186385409564933c3aea16c2ac1c21ef2432368c51034e2b6e9352544f1
        checksum/common-config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum/config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
      labels:
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: milvus
        app.kubernetes.io/version: 2.4.4
        helm.sh/chart: milvus-8.2.4
        app.kubernetes.io/part-of: milvus
        app.kubernetes.io/component: proxy
    spec:
      enableServiceLinks: false
      serviceAccountName: my-release-milvus-proxy
      
      automountServiceAccountToken: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: my-release
                    app.kubernetes.io/name: milvus
                    app.kubernetes.io/component: proxy
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      initContainers:
        - name: wait-for-etcd
          image: docker.io/bitnami/os-shell:12-debian-12-r22
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              etcd_hosts=(
                "http://my-release-etcd:2379"
              )
        
              check_etcd() {
                  local -r etcd_host="${1:-?missing etcd}"
                  local params_cert=""
        
                  if echo $etcd_host | grep https; then
                     params_cert="--cacert /bitnami/milvus/conf/cert/etcd/client/ca.crt --cert /bitnami/milvus/conf/cert/etcd/client/tls.crt --key /bitnami/milvus/conf/cert/etcd/client/tls.key"
                  fi
                  if [ ! -z  ]; then
                    params_cert=$params_cert" --pass "
                  fi
                  if curl --max-time 5 "${etcd_host}/version" $params_cert | grep etcdcluster; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              for host in "${etcd_hosts[@]}"; do
                  echo "Checking connection to $host"
                  if retry_while "check_etcd $host"; then
                      echo "Connected to $host"
                  else
                      echo "Error connecting to $host"
                      exit 1
                  fi
              done
        
              echo "Connection success"
              exit 0
        - name: wait-for-kafka
          image: docker.io/bitnami/milvus:2.4.4-debian-12-r2 
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              kafka_hosts=(
                "my-release-kafka"
              )
        
              check_kafka() {
                  local -r kafka_host="${1:-?missing kafka}"
                  if wait-for-port --timeout=5 --host=${kafka_host} --state=inuse 9092; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              for host in "${kafka_hosts[@]}"; do
                  echo "Checking connection to $host"
                  if retry_while "check_kafka $host"; then
                      echo "Connected to $host"
                  else
                      echo "Error connecting to $host"
                      exit 1
                  fi
              done
        
              echo "Connection success"
              exit 0
        - name: wait-for-s3
          image: docker.io/bitnami/os-shell:12-debian-12-r22
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              check_s3() {
                  local -r s3_host="${1:-?missing s3}"
                  if curl --max-time 5 "${s3_host}" | grep "RequestId"; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              host=my-release-minio:80
        
              echo "Checking connection to $host"
              if retry_while "check_s3 $host"; then
                echo "Connected to $host"
              else
                echo "Error connecting to $host"
                exit 1
              fi
        
              echo "Connection success"
              exit 0
        # This init container renders and merges the Milvus configuration files.
        # We need to use a volume because we're working with ReadOnlyRootFilesystem
        - name: prepare-milvus
          image: docker.io/bitnami/milvus:2.4.4-debian-12-r2
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              # Remove previously existing files and copy the default configuration files to ensure they are present in mounted configs directory
              rm -rf /bitnami/milvus/rendered-conf/*
              cp -r /opt/bitnami/milvus/configs/. /bitnami/milvus/rendered-conf
              # Build final milvus.yaml with the sections of the different files
              find /bitnami/milvus/conf -type f -name *.yaml -print0 | sort -z | xargs -0 yq eval-all '. as $item ireduce ({}; . * $item )' /bitnami/milvus/rendered-conf/milvus.yaml > /bitnami/milvus/rendered-conf/pre-render-config_00.yaml
        
              # Kafka settings
              # HACK: In order to enable Kafka we need to remove all Pulsar settings from the configuration file
              # https://github.com/milvus-io/milvus/blob/master/configs/milvus.yaml#L110
              yq 'del(.pulsar)' /bitnami/milvus/rendered-conf/pre-render-config_00.yaml > /bitnami/milvus/rendered-conf/pre-render-config_01.yaml
              # Kafka TLS settings
        
              # Milvus server TLS settings
              yq e '.common.security.tlsMode = 0' /bitnami/milvus/rendered-conf/pre-render-config_01.yaml > /bitnami/milvus/rendered-conf/pre-render-config_02.yaml
        
              render-template /bitnami/milvus/rendered-conf/pre-render-config_02.yaml > /bitnami/milvus/rendered-conf/milvus.yaml
              rm /bitnami/milvus/rendered-conf/pre-render-config*
              chmod 644 /bitnami/milvus/rendered-conf/milvus.yaml
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MILVUS_KAFKA_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-kafka-user-passwords
                  key: system-user-password
            - name: MILVUS_S3_ACCESS_ID
              valueFrom:
                secretKeyRef:
                  name: my-release-minio
                  key: root-user
            - name: MILVUS_S3_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: my-release-minio
                  key: root-password
          envFrom:
          volumeMounts:
            - name: config-common
              mountPath: /bitnami/milvus/conf/00_default
            - name: component-config-default
              mountPath: /bitnami/milvus/conf/02_component_default
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - name: empty-dir
              mountPath: /bitnami/milvus/rendered-conf/
              subPath: app-rendered-conf-dir
      containers:
        - name: milvus
          image: docker.io/bitnami/milvus:2.4.4-debian-12-r2
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          args:
            - run
            - proxy
          env:
            - name: METRICS_PORT
              value: "9091"
          envFrom:
          ports:
            - containerPort: 19530
              name: grpc
            - containerPort: 19529
              name: grpc-internal
            - containerPort: 9091
              name: http-metrics
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          livenessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: http-metrics
          readinessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /healthz
              port: http-metrics
          volumeMounts:
            - name: empty-dir
              mountPath: /opt/bitnami/milvus/configs
              subPath: app-rendered-conf-dir
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - name: empty-dir
              mountPath: /opt/bitnami/milvus/tmp
              subPath: app-tmp-dir
              # We are using a s3 backend, so this data dir is temporary
            - name: empty-dir
              mountPath: /bitnami/milvus/data
              subPath: app-data-dir
      volumes:
        - name: empty-dir
          emptyDir: {}
        - name: config-common
          configMap:
            name: my-release-milvus
        - name: component-config-default
          configMap:
            name: my-release-milvus-proxy
---
# Source: milvus/templates/query-coordinator/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-milvus-query-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: query-coordinator
spec:
  replicas: 1
  strategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: query-coordinator
  template:
    metadata:
      annotations:
        checksum/common-config: f05d4186385409564933c3aea16c2ac1c21ef2432368c51034e2b6e9352544f1
        checksum/common-config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum/config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
      labels:
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: milvus
        app.kubernetes.io/version: 2.4.4
        helm.sh/chart: milvus-8.2.4
        app.kubernetes.io/part-of: milvus
        app.kubernetes.io/component: query-coordinator
    spec:
      enableServiceLinks: false
      serviceAccountName: my-release-milvus-query-coordinator
      
      automountServiceAccountToken: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: my-release
                    app.kubernetes.io/name: milvus
                    app.kubernetes.io/component: query-coordinator
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      initContainers:
        - name: wait-for-etcd
          image: docker.io/bitnami/os-shell:12-debian-12-r22
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              etcd_hosts=(
                "http://my-release-etcd:2379"
              )
        
              check_etcd() {
                  local -r etcd_host="${1:-?missing etcd}"
                  local params_cert=""
        
                  if echo $etcd_host | grep https; then
                     params_cert="--cacert /bitnami/milvus/conf/cert/etcd/client/ca.crt --cert /bitnami/milvus/conf/cert/etcd/client/tls.crt --key /bitnami/milvus/conf/cert/etcd/client/tls.key"
                  fi
                  if [ ! -z  ]; then
                    params_cert=$params_cert" --pass "
                  fi
                  if curl --max-time 5 "${etcd_host}/version" $params_cert | grep etcdcluster; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              for host in "${etcd_hosts[@]}"; do
                  echo "Checking connection to $host"
                  if retry_while "check_etcd $host"; then
                      echo "Connected to $host"
                  else
                      echo "Error connecting to $host"
                      exit 1
                  fi
              done
        
              echo "Connection success"
              exit 0
        - name: wait-for-kafka
          image: docker.io/bitnami/milvus:2.4.4-debian-12-r2 
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              kafka_hosts=(
                "my-release-kafka"
              )
        
              check_kafka() {
                  local -r kafka_host="${1:-?missing kafka}"
                  if wait-for-port --timeout=5 --host=${kafka_host} --state=inuse 9092; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              for host in "${kafka_hosts[@]}"; do
                  echo "Checking connection to $host"
                  if retry_while "check_kafka $host"; then
                      echo "Connected to $host"
                  else
                      echo "Error connecting to $host"
                      exit 1
                  fi
              done
        
              echo "Connection success"
              exit 0
        - name: wait-for-s3
          image: docker.io/bitnami/os-shell:12-debian-12-r22
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              check_s3() {
                  local -r s3_host="${1:-?missing s3}"
                  if curl --max-time 5 "${s3_host}" | grep "RequestId"; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              host=my-release-minio:80
        
              echo "Checking connection to $host"
              if retry_while "check_s3 $host"; then
                echo "Connected to $host"
              else
                echo "Error connecting to $host"
                exit 1
              fi
        
              echo "Connection success"
              exit 0
        # This init container renders and merges the Milvus configuration files.
        # We need to use a volume because we're working with ReadOnlyRootFilesystem
        - name: prepare-milvus
          image: docker.io/bitnami/milvus:2.4.4-debian-12-r2
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              # Remove previously existing files and copy the default configuration files to ensure they are present in mounted configs directory
              rm -rf /bitnami/milvus/rendered-conf/*
              cp -r /opt/bitnami/milvus/configs/. /bitnami/milvus/rendered-conf
              # Build final milvus.yaml with the sections of the different files
              find /bitnami/milvus/conf -type f -name *.yaml -print0 | sort -z | xargs -0 yq eval-all '. as $item ireduce ({}; . * $item )' /bitnami/milvus/rendered-conf/milvus.yaml > /bitnami/milvus/rendered-conf/pre-render-config_00.yaml
        
              # Kafka settings
              # HACK: In order to enable Kafka we need to remove all Pulsar settings from the configuration file
              # https://github.com/milvus-io/milvus/blob/master/configs/milvus.yaml#L110
              yq 'del(.pulsar)' /bitnami/milvus/rendered-conf/pre-render-config_00.yaml > /bitnami/milvus/rendered-conf/pre-render-config_01.yaml
              # Kafka TLS settings
        
              # Milvus server TLS settings
              yq e '.common.security.tlsMode = 0' /bitnami/milvus/rendered-conf/pre-render-config_01.yaml > /bitnami/milvus/rendered-conf/pre-render-config_02.yaml
        
              render-template /bitnami/milvus/rendered-conf/pre-render-config_02.yaml > /bitnami/milvus/rendered-conf/milvus.yaml
              rm /bitnami/milvus/rendered-conf/pre-render-config*
              chmod 644 /bitnami/milvus/rendered-conf/milvus.yaml
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MILVUS_KAFKA_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-kafka-user-passwords
                  key: system-user-password
            - name: MILVUS_S3_ACCESS_ID
              valueFrom:
                secretKeyRef:
                  name: my-release-minio
                  key: root-user
            - name: MILVUS_S3_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: my-release-minio
                  key: root-password
          envFrom:
          volumeMounts:
            - name: config-common
              mountPath: /bitnami/milvus/conf/00_default
            - name: component-config-default
              mountPath: /bitnami/milvus/conf/02_component_default
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - name: empty-dir
              mountPath: /bitnami/milvus/rendered-conf/
              subPath: app-rendered-conf-dir
      containers:
        - name: milvus
          image: docker.io/bitnami/milvus:2.4.4-debian-12-r2
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          args:
            - run
            - querycoord
          env:
            - name: METRICS_PORT
              value: "9091"
          envFrom:
          ports:
            - containerPort: 19530
              name: grpc
            - containerPort: 9091
              name: http-metrics
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          livenessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: http-metrics
          readinessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /healthz
              port: http-metrics
          volumeMounts:
            - name: empty-dir
              mountPath: /opt/bitnami/milvus/configs
              subPath: app-rendered-conf-dir
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - name: empty-dir
              mountPath: /opt/bitnami/milvus/tmp
              subPath: app-tmp-dir
              # We are using a s3 backend, so this data dir is temporary
            - name: empty-dir
              mountPath: /bitnami/milvus/data
              subPath: app-data-dir
      volumes:
        - name: empty-dir
          emptyDir: {}
        - name: config-common
          configMap:
            name: my-release-milvus
        - name: component-config-default
          configMap:
            name: my-release-milvus-query-coordinator
---
# Source: milvus/templates/query-node/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-milvus-query-node
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: query-node
spec:
  replicas: 1
  strategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: query-node
  template:
    metadata:
      annotations:
        checksum/common-config: f05d4186385409564933c3aea16c2ac1c21ef2432368c51034e2b6e9352544f1
        checksum/common-config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum/config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
      labels:
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: milvus
        app.kubernetes.io/version: 2.4.4
        helm.sh/chart: milvus-8.2.4
        app.kubernetes.io/part-of: milvus
        app.kubernetes.io/component: query-node
    spec:
      enableServiceLinks: false
      serviceAccountName: my-release-milvus-query-node
      
      automountServiceAccountToken: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: my-release
                    app.kubernetes.io/name: milvus
                    app.kubernetes.io/component: query-node
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      initContainers:
        - name: wait-for-etcd
          image: docker.io/bitnami/os-shell:12-debian-12-r22
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              etcd_hosts=(
                "http://my-release-etcd:2379"
              )
        
              check_etcd() {
                  local -r etcd_host="${1:-?missing etcd}"
                  local params_cert=""
        
                  if echo $etcd_host | grep https; then
                     params_cert="--cacert /bitnami/milvus/conf/cert/etcd/client/ca.crt --cert /bitnami/milvus/conf/cert/etcd/client/tls.crt --key /bitnami/milvus/conf/cert/etcd/client/tls.key"
                  fi
                  if [ ! -z  ]; then
                    params_cert=$params_cert" --pass "
                  fi
                  if curl --max-time 5 "${etcd_host}/version" $params_cert | grep etcdcluster; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              for host in "${etcd_hosts[@]}"; do
                  echo "Checking connection to $host"
                  if retry_while "check_etcd $host"; then
                      echo "Connected to $host"
                  else
                      echo "Error connecting to $host"
                      exit 1
                  fi
              done
        
              echo "Connection success"
              exit 0
        - name: wait-for-kafka
          image: docker.io/bitnami/milvus:2.4.4-debian-12-r2 
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              kafka_hosts=(
                "my-release-kafka"
              )
        
              check_kafka() {
                  local -r kafka_host="${1:-?missing kafka}"
                  if wait-for-port --timeout=5 --host=${kafka_host} --state=inuse 9092; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              for host in "${kafka_hosts[@]}"; do
                  echo "Checking connection to $host"
                  if retry_while "check_kafka $host"; then
                      echo "Connected to $host"
                  else
                      echo "Error connecting to $host"
                      exit 1
                  fi
              done
        
              echo "Connection success"
              exit 0
        - name: wait-for-s3
          image: docker.io/bitnami/os-shell:12-debian-12-r22
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              check_s3() {
                  local -r s3_host="${1:-?missing s3}"
                  if curl --max-time 5 "${s3_host}" | grep "RequestId"; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              host=my-release-minio:80
        
              echo "Checking connection to $host"
              if retry_while "check_s3 $host"; then
                echo "Connected to $host"
              else
                echo "Error connecting to $host"
                exit 1
              fi
        
              echo "Connection success"
              exit 0
        # This init container renders and merges the Milvus configuration files.
        # We need to use a volume because we're working with ReadOnlyRootFilesystem
        - name: prepare-milvus
          image: docker.io/bitnami/milvus:2.4.4-debian-12-r2
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              # Remove previously existing files and copy the default configuration files to ensure they are present in mounted configs directory
              rm -rf /bitnami/milvus/rendered-conf/*
              cp -r /opt/bitnami/milvus/configs/. /bitnami/milvus/rendered-conf
              # Build final milvus.yaml with the sections of the different files
              find /bitnami/milvus/conf -type f -name *.yaml -print0 | sort -z | xargs -0 yq eval-all '. as $item ireduce ({}; . * $item )' /bitnami/milvus/rendered-conf/milvus.yaml > /bitnami/milvus/rendered-conf/pre-render-config_00.yaml
        
              # Kafka settings
              # HACK: In order to enable Kafka we need to remove all Pulsar settings from the configuration file
              # https://github.com/milvus-io/milvus/blob/master/configs/milvus.yaml#L110
              yq 'del(.pulsar)' /bitnami/milvus/rendered-conf/pre-render-config_00.yaml > /bitnami/milvus/rendered-conf/pre-render-config_01.yaml
              # Kafka TLS settings
        
              # Milvus server TLS settings
              yq e '.common.security.tlsMode = 0' /bitnami/milvus/rendered-conf/pre-render-config_01.yaml > /bitnami/milvus/rendered-conf/pre-render-config_02.yaml
        
              render-template /bitnami/milvus/rendered-conf/pre-render-config_02.yaml > /bitnami/milvus/rendered-conf/milvus.yaml
              rm /bitnami/milvus/rendered-conf/pre-render-config*
              chmod 644 /bitnami/milvus/rendered-conf/milvus.yaml
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MILVUS_KAFKA_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-kafka-user-passwords
                  key: system-user-password
            - name: MILVUS_S3_ACCESS_ID
              valueFrom:
                secretKeyRef:
                  name: my-release-minio
                  key: root-user
            - name: MILVUS_S3_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: my-release-minio
                  key: root-password
          envFrom:
          volumeMounts:
            - name: config-common
              mountPath: /bitnami/milvus/conf/00_default
            - name: component-config-default
              mountPath: /bitnami/milvus/conf/02_component_default
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - name: empty-dir
              mountPath: /bitnami/milvus/rendered-conf/
              subPath: app-rendered-conf-dir
      containers:
        - name: milvus
          image: docker.io/bitnami/milvus:2.4.4-debian-12-r2
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          args:
            - run
            - querynode
          env:
            - name: METRICS_PORT
              value: "9091"
          envFrom:
          ports:
            - containerPort: 19530
              name: grpc
            - containerPort: 9091
              name: http-metrics
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          livenessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: http-metrics
          readinessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /healthz
              port: http-metrics
          volumeMounts:
            - name: empty-dir
              mountPath: /opt/bitnami/milvus/configs
              subPath: app-rendered-conf-dir
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - name: empty-dir
              mountPath: /opt/bitnami/milvus/tmp
              subPath: app-tmp-dir
              # We are using a s3 backend, so this data dir is temporary
            - name: empty-dir
              mountPath: /bitnami/milvus/data
              subPath: app-data-dir
      volumes:
        - name: empty-dir
          emptyDir: {}
        - name: config-common
          configMap:
            name: my-release-milvus
        - name: component-config-default
          configMap:
            name: my-release-milvus-query-node
---
# Source: milvus/templates/root-coordinator/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-milvus-root-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.4.4
    helm.sh/chart: milvus-8.2.4
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: root-coordinator
spec:
  replicas: 1
  strategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: root-coordinator
  template:
    metadata:
      annotations:
        checksum/common-config: f05d4186385409564933c3aea16c2ac1c21ef2432368c51034e2b6e9352544f1
        checksum/common-config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum/config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
      labels:
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: milvus
        app.kubernetes.io/version: 2.4.4
        helm.sh/chart: milvus-8.2.4
        app.kubernetes.io/part-of: milvus
        app.kubernetes.io/component: root-coordinator
    spec:
      enableServiceLinks: false
      serviceAccountName: my-release-milvus-root-coordinator
      
      automountServiceAccountToken: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: my-release
                    app.kubernetes.io/name: milvus
                    app.kubernetes.io/component: root-coordinator
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      initContainers:
        - name: wait-for-etcd
          image: docker.io/bitnami/os-shell:12-debian-12-r22
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              etcd_hosts=(
                "http://my-release-etcd:2379"
              )
        
              check_etcd() {
                  local -r etcd_host="${1:-?missing etcd}"
                  local params_cert=""
        
                  if echo $etcd_host | grep https; then
                     params_cert="--cacert /bitnami/milvus/conf/cert/etcd/client/ca.crt --cert /bitnami/milvus/conf/cert/etcd/client/tls.crt --key /bitnami/milvus/conf/cert/etcd/client/tls.key"
                  fi
                  if [ ! -z  ]; then
                    params_cert=$params_cert" --pass "
                  fi
                  if curl --max-time 5 "${etcd_host}/version" $params_cert | grep etcdcluster; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              for host in "${etcd_hosts[@]}"; do
                  echo "Checking connection to $host"
                  if retry_while "check_etcd $host"; then
                      echo "Connected to $host"
                  else
                      echo "Error connecting to $host"
                      exit 1
                  fi
              done
        
              echo "Connection success"
              exit 0
        - name: wait-for-kafka
          image: docker.io/bitnami/milvus:2.4.4-debian-12-r2 
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              kafka_hosts=(
                "my-release-kafka"
              )
        
              check_kafka() {
                  local -r kafka_host="${1:-?missing kafka}"
                  if wait-for-port --timeout=5 --host=${kafka_host} --state=inuse 9092; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              for host in "${kafka_hosts[@]}"; do
                  echo "Checking connection to $host"
                  if retry_while "check_kafka $host"; then
                      echo "Connected to $host"
                  else
                      echo "Error connecting to $host"
                      exit 1
                  fi
              done
        
              echo "Connection success"
              exit 0
        - name: wait-for-s3
          image: docker.io/bitnami/os-shell:12-debian-12-r22
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              check_s3() {
                  local -r s3_host="${1:-?missing s3}"
                  if curl --max-time 5 "${s3_host}" | grep "RequestId"; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              host=my-release-minio:80
        
              echo "Checking connection to $host"
              if retry_while "check_s3 $host"; then
                echo "Connected to $host"
              else
                echo "Error connecting to $host"
                exit 1
              fi
        
              echo "Connection success"
              exit 0
        # This init container renders and merges the Milvus configuration files.
        # We need to use a volume because we're working with ReadOnlyRootFilesystem
        - name: prepare-milvus
          image: docker.io/bitnami/milvus:2.4.4-debian-12-r2
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              # Remove previously existing files and copy the default configuration files to ensure they are present in mounted configs directory
              rm -rf /bitnami/milvus/rendered-conf/*
              cp -r /opt/bitnami/milvus/configs/. /bitnami/milvus/rendered-conf
              # Build final milvus.yaml with the sections of the different files
              find /bitnami/milvus/conf -type f -name *.yaml -print0 | sort -z | xargs -0 yq eval-all '. as $item ireduce ({}; . * $item )' /bitnami/milvus/rendered-conf/milvus.yaml > /bitnami/milvus/rendered-conf/pre-render-config_00.yaml
        
              # Kafka settings
              # HACK: In order to enable Kafka we need to remove all Pulsar settings from the configuration file
              # https://github.com/milvus-io/milvus/blob/master/configs/milvus.yaml#L110
              yq 'del(.pulsar)' /bitnami/milvus/rendered-conf/pre-render-config_00.yaml > /bitnami/milvus/rendered-conf/pre-render-config_01.yaml
              # Kafka TLS settings
        
              # Milvus server TLS settings
              yq e '.common.security.tlsMode = 0' /bitnami/milvus/rendered-conf/pre-render-config_01.yaml > /bitnami/milvus/rendered-conf/pre-render-config_02.yaml
        
              render-template /bitnami/milvus/rendered-conf/pre-render-config_02.yaml > /bitnami/milvus/rendered-conf/milvus.yaml
              rm /bitnami/milvus/rendered-conf/pre-render-config*
              chmod 644 /bitnami/milvus/rendered-conf/milvus.yaml
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MILVUS_KAFKA_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-kafka-user-passwords
                  key: system-user-password
            - name: MILVUS_S3_ACCESS_ID
              valueFrom:
                secretKeyRef:
                  name: my-release-minio
                  key: root-user
            - name: MILVUS_S3_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: my-release-minio
                  key: root-password
          envFrom:
          volumeMounts:
            - name: config-common
              mountPath: /bitnami/milvus/conf/00_default
            - name: component-config-default
              mountPath: /bitnami/milvus/conf/02_component_default
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - name: empty-dir
              mountPath: /bitnami/milvus/rendered-conf/
              subPath: app-rendered-conf-dir
      containers:
        - name: milvus
          image: docker.io/bitnami/milvus:2.4.4-debian-12-r2
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          args:
            - run
            - rootcoord
          env:
            - name: METRICS_PORT
              value: "9091"
          envFrom:
          ports:
            - containerPort: 19530
              name: grpc
            - containerPort: 9091
              name: http-metrics
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          livenessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: http-metrics
          readinessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /healthz
              port: http-metrics
          volumeMounts:
            - name: empty-dir
              mountPath: /opt/bitnami/milvus/configs
              subPath: app-rendered-conf-dir
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - name: empty-dir
              mountPath: /opt/bitnami/milvus/tmp
              subPath: app-tmp-dir
              # We are using a s3 backend, so this data dir is temporary
            - name: empty-dir
              mountPath: /bitnami/milvus/data
              subPath: app-data-dir
      volumes:
        - name: empty-dir
          emptyDir: {}
        - name: config-common
          configMap:
            name: my-release-milvus
        - name: component-config-default
          configMap:
            name: my-release-milvus-root-coordinator
---
# Source: milvus/charts/etcd/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-etcd
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: etcd
    app.kubernetes.io/version: 3.5.14
    helm.sh/chart: etcd-10.2.2
    app.kubernetes.io/component: etcd
spec:
  replicas: 3
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: etcd
      app.kubernetes.io/component: etcd
  serviceName: my-release-etcd-headless
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: etcd
        app.kubernetes.io/version: 3.5.14
        helm.sh/chart: etcd-10.2.2
        app.kubernetes.io/component: etcd
      annotations:
        checksum/token-secret: a739c630f7d5f4242df2d9bf0e666016d4cc5955bef73bc5b046aa74fc74fe25
    spec:
      
      automountServiceAccountToken: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: my-release
                    app.kubernetes.io/name: etcd
                    app.kubernetes.io/component: etcd
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: "my-release-etcd"
      containers:
        - name: etcd
          image: docker.io/bitnami/etcd:3.5.14-debian-12-r1
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: MY_STS_NAME
              value: "my-release-etcd"
            - name: ETCDCTL_API
              value: "3"
            - name: ETCD_ON_K8S
              value: "yes"
            - name: ETCD_START_FROM_SNAPSHOT
              value: "no"
            - name: ETCD_DISASTER_RECOVERY
              value: "no"
            - name: ETCD_NAME
              value: "$(MY_POD_NAME)"
            - name: ETCD_DATA_DIR
              value: "/bitnami/etcd/data"
            - name: ETCD_LOG_LEVEL
              value: "info"
            - name: ALLOW_NONE_AUTHENTICATION
              value: "yes"
            - name: ETCD_AUTH_TOKEN
              value: "jwt,priv-key=/opt/bitnami/etcd/certs/token/jwt-token.pem,sign-method=RS256,ttl=10m"
            - name: ETCD_ADVERTISE_CLIENT_URLS
              value: "http://$(MY_POD_NAME).my-release-etcd-headless.default.svc.cluster.local:2379,http://my-release-etcd.default.svc.cluster.local:2379"
            - name: ETCD_LISTEN_CLIENT_URLS
              value: "http://0.0.0.0:2379"
            - name: ETCD_INITIAL_ADVERTISE_PEER_URLS
              value: "http://$(MY_POD_NAME).my-release-etcd-headless.default.svc.cluster.local:2380"
            - name: ETCD_LISTEN_PEER_URLS
              value: "http://0.0.0.0:2380"
            - name: ETCD_INITIAL_CLUSTER_TOKEN
              value: "etcd-cluster-k8s"
            - name: ETCD_INITIAL_CLUSTER_STATE
              value: "new"
            - name: ETCD_INITIAL_CLUSTER
              value: "my-release-etcd-0=http://my-release-etcd-0.my-release-etcd-headless.default.svc.cluster.local:2380,my-release-etcd-1=http://my-release-etcd-1.my-release-etcd-headless.default.svc.cluster.local:2380,my-release-etcd-2=http://my-release-etcd-2.my-release-etcd-headless.default.svc.cluster.local:2380"
            - name: ETCD_CLUSTER_DOMAIN
              value: "my-release-etcd-headless.default.svc.cluster.local"
          envFrom:
          ports:
            - name: client
              containerPort: 2379
              protocol: TCP
            - name: peer
              containerPort: 2380
              protocol: TCP
          livenessProbe:
            httpGet:
              port: 2379 
              path: /livez
              scheme: "HTTP"
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
          readinessProbe:
            exec:
              command:
                - /opt/bitnami/scripts/etcd/healthcheck.sh
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
          lifecycle:
            preStop:
              exec:
                command:
                  - /opt/bitnami/scripts/etcd/prestop.sh
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          volumeMounts:
            - name: empty-dir
              mountPath: /opt/bitnami/etcd/conf/
              subPath: app-conf-dir
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - name: data
              mountPath: /bitnami/etcd
            - name: etcd-jwt-token
              mountPath: /opt/bitnami/etcd/certs/token/
              readOnly: true
      volumes:
        - name: empty-dir
          emptyDir: {}
        - name: etcd-jwt-token
          secret:
            secretName: my-release-etcd-jwt-token
            defaultMode: 256
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: milvus/charts/kafka/templates/controller-eligible/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-kafka-controller
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: 3.7.0
    helm.sh/chart: kafka-29.3.1
    app.kubernetes.io/component: controller-eligible
    app.kubernetes.io/part-of: kafka
spec:
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/name: kafka
      app.kubernetes.io/component: controller-eligible
      app.kubernetes.io/part-of: kafka
  serviceName: my-release-kafka-controller-headless
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: kafka
        app.kubernetes.io/version: 3.7.0
        helm.sh/chart: kafka-29.3.1
        app.kubernetes.io/component: controller-eligible
        app.kubernetes.io/part-of: kafka
      annotations:
        checksum/configuration: 8eb661e4ef7eb32010425b0c765fc3f0d37399e63b631c045c28c4b067e74b82
        checksum/passwords-secret: 4e2216e6014f438c0a62f012d56ad12ddff6ce1f8a8cab2d5dba1a864de1866e
    spec:
      
      automountServiceAccountToken: false
      hostNetwork: false
      hostIPC: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: my-release
                    app.kubernetes.io/name: kafka
                    app.kubernetes.io/component: controller-eligible
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        seccompProfile:
          type: RuntimeDefault
        supplementalGroups: []
        sysctls: []
      serviceAccountName: my-release-kafka
      enableServiceLinks: true
      initContainers:
        - name: kafka-init
          image: docker.io/bitnami/kafka:3.7.0-debian-12-r7
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
          resources:
            limits: {}
            requests: {} 
          command:
            - /bin/bash
          args:
            - -ec
            - |
              /scripts/kafka-init.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                    fieldPath: metadata.name
            - name: KAFKA_VOLUME_DIR
              value: "/bitnami/kafka"
            - name: KAFKA_MIN_ID
              value: "0"
            - name: KAFKA_CLIENT_USERS
              value: "user"
            - name: KAFKA_CLIENT_PASSWORDS
              valueFrom:
                secretKeyRef:
                  name: my-release-kafka-user-passwords
                  key: client-passwords
            - name: KAFKA_INTER_BROKER_USER
              value: "inter_broker_user"
            - name: KAFKA_INTER_BROKER_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-kafka-user-passwords
                  key: inter-broker-password
            - name: KAFKA_CONTROLLER_USER
              value: "controller_user"
            - name: KAFKA_CONTROLLER_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-kafka-user-passwords
                  key: controller-password
          volumeMounts:
            - name: data
              mountPath: /bitnami/kafka
            - name: kafka-config
              mountPath: /config
            - name: kafka-configmaps
              mountPath: /configmaps
            - name: kafka-secret-config
              mountPath: /secret-config
            - name: scripts
              mountPath: /scripts
            - name: tmp
              mountPath: /tmp
      containers:
        - name: kafka
          image: docker.io/bitnami/kafka:3.7.0-debian-12-r7
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: KAFKA_HEAP_OPTS
              value: "-Xmx1024m -Xms1024m"
            - name: KAFKA_KRAFT_CLUSTER_ID
              valueFrom:
                secretKeyRef:
                  name: my-release-kafka-kraft-cluster-id
                  key: kraft-cluster-id
          ports:
            - name: controller
              containerPort: 9093
            - name: client
              containerPort: 9092
            - name: interbroker
              containerPort: 9094
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - pgrep
                - -f
                - kafka
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: "controller"
          resources:
            limits:
              cpu: 750m
              ephemeral-storage: 1024Mi
              memory: 768Mi
            requests:
              cpu: 500m
              ephemeral-storage: 50Mi
              memory: 512Mi
          volumeMounts:
            - name: data
              mountPath: /bitnami/kafka
            - name: logs
              mountPath: /opt/bitnami/kafka/logs
            - name: kafka-config
              mountPath: /opt/bitnami/kafka/config/server.properties
              subPath: server.properties
            - name: tmp
              mountPath: /tmp
      volumes:
        - name: kafka-configmaps
          configMap:
            name: my-release-kafka-controller-configuration
        - name: kafka-secret-config
          emptyDir: {}
        - name: kafka-config
          emptyDir: {}
        - name: tmp
          emptyDir: {}
        - name: scripts
          configMap:
            name: my-release-kafka-scripts
            defaultMode: 493
        - name: logs
          emptyDir: {}
  volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: milvus/charts/minio/templates/provisioning-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: my-release-minio-provisioning
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: minio
    app.kubernetes.io/version: 2024.6.6
    helm.sh/chart: minio-14.6.7
    app.kubernetes.io/component: minio-provisioning
  annotations:
    helm.sh/hook: post-install,post-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
spec: 
  parallelism: 1
  template:
    metadata:
      labels:
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/version: 2024.6.6
        helm.sh/chart: minio-14.6.7
        app.kubernetes.io/component: minio-provisioning
    spec:
      
      restartPolicy: OnFailure
      terminationGracePeriodSeconds: 0
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: my-release-minio
      initContainers:
        - name: wait-for-available-minio
          image: docker.io/bitnami/minio:2024.6.6-debian-12-r0
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          command:
            - /bin/bash
            - -c
            - |-
              set -e;
              echo "Waiting for Minio";
              wait-for-port \
                --host=my-release-minio \
                --state=inuse \
                --timeout=120 \
                80;
              echo "Minio is available";
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 1024Mi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
      containers:
        - name: minio
          image: docker.io/bitnami/minio:2024.6.6-debian-12-r0
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          command:
            - /bin/bash
            - -c
            - >-
              set -e;
              echo "Start Minio provisioning";

              function attachPolicy() {
                local tmp=$(mc admin $1 info provisioning $2 | sed -n -e 's/^Policy.*: \(.*\)$/\1/p');
                IFS=',' read -r -a CURRENT_POLICIES <<< "$tmp";
                if [[ ! "${CURRENT_POLICIES[*]}" =~ "$3" ]]; then
                  mc admin policy attach provisioning $3 --$1=$2;
                fi;
              };

              function detachDanglingPolicies() {
                local tmp=$(mc admin $1 info provisioning $2 | sed -n -e 's/^Policy.*: \(.*\)$/\1/p');
                IFS=',' read -r -a CURRENT_POLICIES <<< "$tmp";
                IFS=',' read -r -a DESIRED_POLICIES <<< "$3";
                for current in "${CURRENT_POLICIES[@]}"; do
                  if [[ ! "${DESIRED_POLICIES[*]}" =~ "${current}" ]]; then
                    mc admin policy detach provisioning $current --$1=$2;
                  fi;
                done;
              }

              function addUsersFromFile() {
                local username=$(grep -oP '^username=\K.+' $1);
                local password=$(grep -oP '^password=\K.+' $1);
                local disabled=$(grep -oP '^disabled=\K.+' $1);
                local policies_list=$(grep -oP '^policies=\K.+' $1);
                local set_policies=$(grep -oP '^setPolicies=\K.+' $1);

                mc admin user add provisioning "${username}" "${password}";

                IFS=',' read -r -a POLICIES <<< "${policies_list}";
                for policy in "${POLICIES[@]}"; do
                  attachPolicy user "${username}" "${policy}";
                done;
                if [ "${set_policies}" == "true" ]; then
                  detachDanglingPolicies user "${username}" "${policies_list}";
                fi;

                local user_status="enable";
                if [[ "${disabled}" != "" && "${disabled,,}" == "true" ]]; then
                  user_status="disable";
                fi;

                mc admin user "${user_status}" provisioning "${username}";
              };
              mc alias set provisioning $MINIO_SCHEME://my-release-minio:80 $MINIO_ROOT_USER $MINIO_ROOT_PASSWORD;

              mc admin service restart provisioning;
              
              mc anonymous set download provisioning/milvus;

              echo "End Minio provisioning";
          env:
            - name: MINIO_SCHEME
              value: "http"
            - name: MINIO_ROOT_USER
              valueFrom:
                secretKeyRef:
                  name: my-release-minio
                  key: root-user
            - name: MINIO_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-minio
                  key: root-password
          envFrom:
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 1024Mi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          volumeMounts:
            - name: empty-dir
              mountPath: /.mc
              subPath: app-mc-dir
            - name: empty-dir
              mountPath: /opt/bitnami/minio/tmp
              subPath: app-tmp-dir
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - name: minio-provisioning
              mountPath: /etc/ilm
      volumes:
        - name: empty-dir
          emptyDir: {}
        - name: minio-provisioning
          configMap:
            name: my-release-minio-provisioning
