---
# Source: splice-helm/charts/hadoop/templates/dn-poddisruptionbudget.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: my-release-hdfs-dn
  labels:
    app: hadoop
    chart: hadoop-0.0.1
    release: my-release
    component: hdfs-dn
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  selector:
    matchLabels:
      app: hadoop
      release: my-release
      component: hdfs-dn
      dnsprefix: splicedb
  minAvailable: 2
---
# Source: splice-helm/charts/hadoop/templates/jn-poddisruptionbudget.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: my-release-hdfs-jn
  labels:
    app: hadoop
    chart: hadoop-0.0.1
    release: my-release
    component: hdfs-jn
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  selector:
    matchLabels:
      app: hadoop
      release: my-release
      component: hdfs-jn
      dnsprefix: splicedb
  minAvailable: 1
---
# Source: splice-helm/charts/hadoop/templates/nn-poddisruptionbudget.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: my-release-hdfs-nn
  labels:
    app: hadoop
    chart: hadoop-0.0.1
    release: my-release
    component: hdfs-nn
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  selector:
    matchLabels:
      app: hadoop
      release: my-release
      component: hdfs-nn
      dnsprefix: splicedb
  minAvailable: 1
---
# Source: splice-helm/charts/hbase/templates/master-poddisruptionbudget.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: my-release-hmaster
  labels:
    app: hbase
    chart: hbase-0.0.1
    release: my-release
    component: hmaster
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  selector:
    matchLabels:
      app: hbase
      release: my-release
      component: hmaster
      dnsprefix: splicedb
  minAvailable: 1
---
# Source: splice-helm/charts/hbase/templates/olap-poddisruptionbudget.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: my-release-olap
  labels:
    app: splice-olap
    chart: hbase-0.0.1
    release: my-release
    component: olap
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  selector:
    matchLabels:
      app: splice-olap
      release: my-release
      component: olap
      dnsprefix: splicedb
  minAvailable: 1
---
# Source: splice-helm/charts/hbase/templates/region-poddisruptionbudget.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: my-release-hregion
  labels:
    app: hbase
    chart: hbase-0.0.1
    release: my-release
    component: hregion
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  selector:
    matchLabels:
      app: hbase
      release: my-release
      component: hregion
      dnsprefix: splicedb
  minAvailable: 1
---
# Source: splice-helm/charts/jupyterhub/templates/hub/pdb.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: hub
  labels:
    component: hub
    app: jupyterhub
    release: my-release
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
    chart: jupyterhub-0.8.2
    heritage: Helm
spec:
  minAvailable: 1
  selector:
    matchLabels:
      component: hub
      app: jupyterhub
      release: my-release
      frameworkId: splicedb-dev1
      dnsprefix: splicedb
---
# Source: splice-helm/charts/jupyterhub/templates/proxy/pdb.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: proxy
  labels:
    component: proxy
    app: jupyterhub
    release: my-release
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
    chart: jupyterhub-0.8.2
    heritage: Helm
spec:
  minAvailable: 1
  selector:
    matchLabels:
      component: proxy
      app: jupyterhub
      release: my-release
      frameworkId: splicedb-dev1
      dnsprefix: splicedb
---
# Source: splice-helm/charts/jupyterhub/templates/scheduling/user-placeholder/pdb.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: user-placeholder
  labels:
    component: user-placeholder
    app: jupyterhub
    release: my-release
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
    chart: jupyterhub-0.8.2
    heritage: Helm
spec:
  minAvailable: 0
  selector:
    matchLabels:
      component: user-placeholder
      app: jupyterhub
      release: my-release
      frameworkId: splicedb-dev1
      dnsprefix: splicedb
---
# Source: splice-helm/charts/jupyterhub/templates/scheduling/user-scheduler/pdb.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: user-scheduler
  labels:
    component: user-scheduler
    app: jupyterhub
    release: my-release
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
    chart: jupyterhub-0.8.2
    heritage: Helm
spec:
  minAvailable: 1
  selector:
    matchLabels:
      component: user-scheduler
      app: jupyterhub
      release: my-release
      frameworkId: splicedb-dev1
      dnsprefix: splicedb
---
# Source: splice-helm/charts/kafka/templates/poddisruptionbudget.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: my-release-kafka
  labels:
    app: splice-kafka
    chart: kafka-0.0.1
    release: my-release
    component: kafka-broker
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  selector:
    matchLabels:
    app: splice-kafka
    release: my-release
    component: kafka-broker
    dnsprefix: splicedb
  minAvailable: 1
---
# Source: splice-helm/charts/mlmanager/templates/bobby-poddisruptionbudget.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: my-release-bobby
  labels:
    app: splice-bobby
    chart: mlmanager-0.0.1
    release: my-release
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  selector:
    matchLabels:
      app: splice-bobby
      release: my-release
      dnsprefix: splicedb
  maxUnavailable: 1
---
# Source: splice-helm/charts/mlmanager/templates/mlflow-poddisruptionbudget.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: my-release-mlflow
  labels:
    app: splice-mlflow
    chart: mlmanager-0.0.1
    release: my-release
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  selector:
    matchLabels:
      app: splice-mlflow
      release: my-release
      dnsprefix: splicedb
  maxUnavailable: 1
---
# Source: splice-helm/charts/zookeeper/templates/poddisruptionbudget.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: my-release-zookeeper
  labels:
    app: splice-zookeeper
    chart: zookeeper-0.0.1
    release: my-release
    frameworkId: splicedb-dev1
spec:
  selector:
    matchLabels:
      app: splice-zookeeper
      release: my-release
  minAvailable: 1
---
# Source: splice-helm/charts/haproxy-controller/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: haproxy-ingress
    component: controller
    release: my-release
  name: my-release-haproxy-controller
---
# Source: splice-helm/charts/hbase/templates/olap-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-olap
---
# Source: splice-helm/charts/jupyterhub/templates/hub/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: hub
  labels:
    component: hub
    app: jupyterhub
    release: my-release
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
    chart: jupyterhub-0.8.2
    heritage: Helm
---
# Source: splice-helm/charts/nginx-ingress/templates/controller-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: nginx-ingress
    chart: nginx-ingress-1.23.0-splice.9
    heritage: Helm
    release: my-release
  name: my-release-nginx-ingress
---
# Source: splice-helm/charts/nginx-ingress/templates/default-backend-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: nginx-ingress
    chart: nginx-ingress-1.23.0-splice.9
    heritage: Helm
    release: my-release
  name: my-release-nginx-ingress-backend
---
# Source: splice-helm/charts/rbac-operator/templates/service_account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: splicedb-operator
---
# Source: splice-helm/charts/rbac/templates/service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: spark
  namespace: default
---
# Source: splice-helm/charts/haproxy-controller/templates/defaultcertsecret.yaml
apiVersion: v1
kind: Secret
type: kubernetes.io/tls
metadata:
  labels:
    app: haproxy-ingress
    component: controller
    release: my-release
  name: my-release-haproxy-controller-default-cert
data:
  tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURLRENDQWhDZ0F3SUJBZ0lSQUptNEVrcDlHdmNzTFNYV204UGVySTR3RFFZSktvWklodmNOQVFFTEJRQXcKSURFZU1Cd0dBMVVFQXhNVmEzVmlaWEp1WlhSbGN5MXBibWR5WlhOekxXTmhNQjRYRFRJME1EWXhOakF3TXpZegpNMW9YRFRJMU1EWXhOakF3TXpZek0xb3dIVEViTUJrR0ExVUVBeE1TYlhrdGNtVnNaV0Z6WlM1a1pXWmhkV3gwCk1JSUJJakFOQmdrcWhraUc5dzBCQVFFRkFBT0NBUThBTUlJQkNnS0NBUUVBeWtjQTNjc096NW1zOFN5clFKRXcKZWN1WmgvajNQNzVBVGpBc0QyTWFtTzFlYVdMcTlHaXRDZkJ4RXRTR05sOXZlVWJRWTFuVHFkR1NUdXpyZmM3WgprRmpCOE1nQ2lwVSsxWmhCMnh3SHNqc3BrWitnWGcvMnVBVnlBTk44SkZ2dFh2aHlCZjFzQkhHTVBoVElUcVZPClhCL0x3WUVicG9HdUE1Y1ZOcjZPRUduRmt2MlB5aHZLS1BXUjN0TkFUYkRlczVpeG9zamd0c1hGZXFTQWJYUEUKZEFqN3NFcnVsMkt4QjZpOWpyaHRJcWphc0tTR0swVGxDVFdYRWI0RjZqQ3ByangwelAxUnNqMlpPYzd2bnNFdgpkWUdYNTRoVk45N0w1a0h3OFU4NVVNMjQ2VFpMSkw3dVBDZ0Y4M0ZTT21pSnc5bzhrL1NlU0xtai9JMllGVFRnCnh3SURBUUFCbzJBd1hqQU9CZ05WSFE4QkFmOEVCQU1DQmFBd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSEF3RUcKQ0NzR0FRVUZCd01DTUF3R0ExVWRFd0VCL3dRQ01BQXdId1lEVlIwakJCZ3dGb0FVbGtsRHpqa013ZHZxQVdSWQphNFBISlFBY3NvZ3dEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBR3VmbDJnWHUyUHd0b1lJNllRUHorWXArN2RiClh6aDVuaDZoUE51cit5ejlGYWdYYVQ2TDhmTmlIZlVPQzFOV3NybFZJWTZiUGt4YmxzWVBHMUJ0RmM0MWZFVFgKZ0FKQjYvOXRYekNGN3AzYnRvU1Zjbk56a2R1VEtOYVBOR3pDYXRUSFhLOXB5ZGh2RmZtSlhtMDdzbHRXZGFMOQpycWxiT0NaNXAvMGVuaVlWVHRhMFYyWjBCTGVmQXV2NmVjSi9CVVBWTTRUWkxwTXZId1ZZUElXemNvZHQwRFlxClN4QTdIQmcvK3hvZDE5bGNZR1NSRldWVHFXcGtKakkyaFk4MjN4WjhJYVlpcHEzbXFUZzJsdHp2T1dhNnl4eHcKYTdxTWROMk4xbzJmbmhKTG8wZHQ3SFlPZWd4QzVaYktMZjR4Nm9XcW94cmdIUVVKQ2FzNURiQlpqT2s9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
  tls.key: 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
---
# Source: splice-helm/charts/jupyterhub/templates/hub/secret.yaml
kind: Secret
apiVersion: v1
metadata:
  name: hub-secret
  labels:
    component: hub
    app: jupyterhub
    release: my-release
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
    chart: jupyterhub-0.8.2
    heritage: Helm
type: Opaque
data:
  proxy.token: "MDdjNGNmODdjZDE0YzZiMTczZGNmNTEwNzcwMjBjNDM3N2ViNTc0OWViMGE3MTkwYzg4NDQwOWI3OGEwYWU2YQ=="
  auth.state.crypto-key: "ZWQ0M2VmZGRiMjk1NGNmZWFhMzEzNzk1MzE0NTc0ZTNhNTg4Mjg0ZjYwY2U2OTAxNDRkYTc3NmNjMDg1NjdlOA=="
  values.yaml: "YXV0aDoge30KaHViOgogIHNlcnZpY2VzOiB7fQ=="
---
# Source: splice-helm/charts/hadoop/templates/hadoop-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-hadoop-config
  labels:
    app: hadoop
    chart: hadoop-0.0.1
    release: my-release
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
data:
  core-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
      <property>
          <name>fs.adl.impl</name>
          <value>org.apache.hadoop.fs.adl.AdlFileSystem</value>
      </property>
      <property>
          <name>fs.AbstractFileSystem.adl.impl</name>
          <value>org.apache.hadoop.fs.adl.Adl</value>
          <final>true</final>
      </property>
      <property>
          <name>fs.adl.impl</name>
          <value>org.apache.hadoop.fs.adl.AdlFileSystem</value>
      </property>
      <property>
          <name>fs.AbstractFileSystem.adl.impl</name>
          <value>org.apache.hadoop.fs.adl.Adl</value>
      </property>
      <property>
          <name>fs.adl.oauth2.access.token.provider.type</name>
          <value>ClientCredential</value>
      </property>
      <property>
          <name>fs.adl.oauth2.access.token.provider</name>
          <value>org.apache.hadoop.fs.adls.oauth2.ConfCredentialBasedAccessTokenProvider</value>
      </property>
      <property>
          <name>fs.defaultFS</name>
          <value>hdfs://hdfs</value>
      </property>
      <property>
          <name>fs.trash.interval</name>
          <value>1</value>
      </property>
      <property>
          <name>io.compression.codecs</name>
          <value>org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.DeflateCodec,org.apache.hadoop.io.compress.SnappyCodec,org.apache.hadoop.io.compress.Lz4Codec</value>
      </property>
      <property>
          <name>hadoop.security.authentication</name>
          <value>simple</value>
      </property>
      <property>
          <name>hadoop.security.authorization</name>
          <value>false</value>
      </property>
      <property>
          <name>hadoop.rpc.protection</name>
          <value>authentication</value>
      </property>
      <property>
          <name>hadoop.ssl.require.client.cert</name>
          <value>false</value>
          <final>true</final>
      </property>
      <property>
          <name>hadoop.ssl.keystores.factory.class</name>
          <value>org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory</value>
          <final>true</final>
      </property>
      <property>
          <name>hadoop.ssl.server.conf</name>
          <value>ssl-server.xml</value>
          <final>true</final>
      </property>
      <property>
          <name>hadoop.ssl.client.conf</name>
          <value>ssl-client.xml</value>
          <final>true</final>
      </property>
      <property>
          <name>hadoop.security.auth_to_local</name>
          <value>DEFAULT</value>
      </property>
      <property>
          <name>hadoop.proxyuser.hue.hosts</name>
          <value>*</value>
      </property>
      <property>
          <name>hadoop.proxyuser.hue.groups</name>
          <value>*</value>
      </property>
      <property>
          <name>hadoop.proxyuser.root.hosts</name>
          <value>*</value>
      </property>
      <property>
          <name>hadoop.proxyuser.root.groups</name>
          <value>*</value>
      </property>
      <property>
          <name>hadoop.proxyuser.HTTP.hosts</name>
          <value>*</value>
      </property>
      <property>
          <name>hadoop.proxyuser.HTTP.groups</name>
          <value>*</value>
      </property>
      <property>
          <name>hadoop.security.group.mapping</name>
          <value>org.apache.hadoop.security.ShellBasedUnixGroupsMapping</value>
      </property>
      <property>
          <name>hadoop.security.instrumentation.requires.admin</name>
          <value>false</value>
      </property>
      <property>
          <name>hadoop.proxyuser.httpfs.groups</name>
          <value>*</value>
      </property>
      <property>
          <name>hadoop.proxyuser.httpfs.hosts</name>
          <value>*</value>
      </property>
      <property>
          <name>ha.zookeeper.parent-znode</name>
          <value>/hadoop-0.0.1-hdfs/hadoop-ha</value>
      </property>
      <property>
          <name>fs.s3a.aws.credentials.provider</name>
          <value>org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider,org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider,com.amazonaws.auth.InstanceProfileCredentialsProvider</value>
      </property>
      <property>
          <name>hive.exec.orc.split.strategy</name>
          <value>BI</value>
      </property>
      <property>
          <name>fs.s3a.access.key</name>
          <value></value>
      </property>
      <property>
          <name>fs.s3a.secret.key</name>
          <value></value>
      </property>
      <property>
          <name>dfs.adls.oauth2.access.token.provider.type</name>
          <value>ClientCredential</value>
      </property>
      <property>
          <name>dfs.adls.oauth2.client.id</name>
          <value></value>
      </property>
      <property>
          <name>dfs.adls.oauth2.credential</name>
          <value></value>
      </property>
      <property>
          <name>dfs.adls.oauth2.refresh.url</name>
          <value>https://login.microsoftonline.com//oauth2/token</value>
      </property>
      <property>
          <name>fs.gs.impl</name>
          <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem</value>
      </property>
      <property>
          <name>fs.gs.working.dir</name>
          <value>/</value>
      </property>
      <property>
          <name>fs.gs.path.encoding</name>
          <value>uri-path</value>
      </property>
      <property>
          <name>fs.gs.auth.service.account.enable</name>
          <value>true</value>
      </property>
      <property>
          <name>fs.gs.auth.service.account.email</name>
          <value></value>
      </property>
      <property>
          <name>fs.gs.auth.service.account.private.key.id</name>
          <value></value>
      </property>
      <property>
          <name>fs.gs.auth.service.account.private.key</name>
          <value></value>
      </property>
    </configuration>

  hdfs-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
      <property>
        <name>dfs.replication</name>
        <value>3</value>
      </property>
      <property>
        <name>dfs.nameservice.id</name>
        <value>hdfs</value>
      </property>
      <property>
        <name>dfs.nameservices</name>
        <value>hdfs</value>
      </property>
      <property>
        <name>dfs.ha.namenodes.hdfs</name>
        <value>nn0,nn1</value>
      </property>
      <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://my-release-hdfs-jn-1.my-release-hdfs-jn.default.svc.cluster.local:8485;my-release-hdfs-jn-2.my-release-hdfs-jn.default.svc.cluster.local:8485;my-release-hdfs-jn-0.my-release-hdfs-jn.default.svc.cluster.local:8485/hdfs</value>
      </property>
      <property>
        <name>dfs.namenode.safemode.threshold-pct</name>
        <value>0.9</value>
      </property>
      <property>
        <name>dfs.namenode.heartbeat.recheck-interval</name>
        <value>60000</value>
      </property>
      <property>
        <name>dfs.namenode.handler.count</name>
        <value>256</value>
      </property>
      <property>
        <name>dfs.namenode.service.handler.count</name>
        <value>60</value>
      </property>
      <property>
        <name>dfs.namenode.invalidate.work.pct.per.iteration</name>
        <value>0.95</value>
      </property>
      <property>
        <name>dfs.namenode.replication.work.multiplier.per.iteration</name>
        <value>4</value>
      </property>
      <property>
        <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
        <value>false</value>
      </property>
      <property>
        <name>dfs.webhdfs.enabled</name>
        <value>true</value>
      </property>
      <property>
        <name>dfs.namenode.rpc-address.hdfs.nn0</name>
        <value>my-release-hdfs-nn-0.my-release-hdfs-nn.default.svc.cluster.local:8020</value>
      </property>
      <property>
        <name>dfs.namenode.http-address.hdfs.nn0</name>
        <value>my-release-hdfs-nn-0.my-release-hdfs-nn.default.svc.cluster.local:50070</value>
      </property>
      <property>
        <name>dfs.namenode.rpc-bind-host.hdfs.nn0</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>dfs.namenode.http-bind-host.hdfs.nn0</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>dfs.namenode.rpc-address.hdfs.nn1</name>
        <value>my-release-hdfs-nn-1.my-release-hdfs-nn.default.svc.cluster.local:8020</value>
      </property>
      <property>
        <name>dfs.namenode.http-address.hdfs.nn1</name>
        <value>my-release-hdfs-nn-1.my-release-hdfs-nn.default.svc.cluster.local:50070</value>
      </property>
      <property>
        <name>dfs.namenode.rpc-bind-host.hdfs.nn1</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>dfs.namenode.http-bind-host.hdfs.nn1</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>dfs.journalnode.rpc-address</name>
        <value>0.0.0.0:8485</value>
      </property>
      <property>
        <name>dfs.journalnode.http-address</name>
        <value>0.0.0.0:8480</value>
      </property>
      <property>
        <name>dfs.datanode.address</name>
        <value>0.0.0.0:9003</value>
      </property>
      <property>
        <name>dfs.datanode.http.address</name>
        <value>0.0.0.0:9004</value>
      </property>
      <property>
        <name>dfs.datanode.ipc.address</name>
        <value>0.0.0.0:9005</value>
      </property>
      <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///name-data</value>
      </property>
      <property>
        <name>dfs.journalnode.edits.dir</name>
        <value>/journal-data</value>
      </property>
      <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///data-data-0</value>
      </property>
      <property>
        <name>dfs.datanode.balance.bandwidthPerSec</name>
        <value>41943040</value>
      </property>
      <property>
        <name>dfs.datanode.handler.count</name>
        <value>20</value>
      </property>
      <property>
        <name>dfs.datanode.max.transfer.threads</name>
        <value>8192</value>
      </property>
      <property>
        <name>dfs.datanode.max.transfer.threads</name>
        <value>8192</value>
      </property>
      <property>
        <name>ha.zookeeper.quorum</name>
        <value>my-release-zookeeper-1.my-release-zookeeper-headless.default.svc.cluster.local:2181,my-release-zookeeper-2.my-release-zookeeper-headless.default.svc.cluster.local:2181,my-release-zookeeper-0.my-release-zookeeper-headless.default.svc.cluster.local:2181</value>
      </property>
      <property>
        <name>dfs.ha.fencing.methods</name>
        <value>shell(/bin/true)</value>
      </property>
      <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
      </property>
      <property>
        <name>dfs.image.compress</name>
        <value>false</value>
      </property>
      <property>
        <name>dfs.image.compression.codec</name>
        <value>org.apache.hadoop.io.compress.SnappyCodec</value>
      </property>
      <property>
        <name>dfs.client.read.shortcircuit</name>
        <value>false</value>
      </property>
      <property>
        <name>dfs.client.read.shortcircuit.streams.cache.size</name>
        <value>1000</value>
      </property>
      <property>
        <name>dfs.client.read.shortcircuit.streams.cache.size.expiry.ms</name>
        <value>1000</value>
      </property>
      <property>
        <name>dfs.client.failover.proxy.provider.hdfs</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
      </property>
      <property>
        <name>dfs.client.write.retries</name>
        <value>6</value>
      </property>
      <property>
        <name>dfs.client.write.locateFollowingBlock.retries</name>
        <value>10</value>
      </property>
      <property>
        <name>dfs.client.block.write.replace-datanode-on-failure.policy</name>
        <value>ALWAYS</value>
      </property>
      <property>
        <name>dfs.namenode.replication.min</name>
        <value>2</value>
      </property>
      <property>
        <name>dfs.domain.socket.path</name>
        <value>/var/lib/hadoop-hdfs/dn_socket</value>
      </property>
      <property>
        <name>dfs.permissions.enabled</name>
        <value>true</value>
      </property>
      <property>
        <name>dfs.permissions.superusergroup</name>
        <value>hdfs</value>
      </property>
      <property>
        <name>presto.s3.access-key</name>
        <value></value>
      </property>
      <property>
        <name>presto.s3.secret-key</name>
        <value></value>
      </property>
      <property>
        <name>dfs.adls.oauth2.access.token.provider.type</name>
        <value>ClientCredential</value>
      </property>
      <property>
        <name>dfs.adls.oauth2.client.id</name>
        <value></value>
      </property>
      <property>
        <name>dfs.adls.oauth2.credential</name>
        <value></value>
      </property>
      <property>
        <name>dfs.adls.oauth2.refresh.url</name>
        <value>https://login.microsoftonline.com//oauth2/token</value>
      </property>
    </configuration>
---
# Source: splice-helm/charts/haproxy-controller/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app: haproxy-ingress
    component: controller
    release: my-release
  name: my-release-haproxy-controller
data:
  syslog-server: 'address:stdout, format: raw, facility:daemon, level: info'
  timeout-tunnel: 23h
---
# Source: splice-helm/charts/haproxy-controller/templates/configmaptcp.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app: haproxy-ingress
    component: controller
    release: my-release
  name: my-release-haproxy-controller-tcp
data:
  1527:
    default/my-release-hregion:1527
  9092:
    default/my-release-kafka:9092
---
# Source: splice-helm/charts/hbase/templates/hbase-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-hbase-config
  labels:
    app: my-release-hbase
    chart: hbase-0.0.1
    release: my-release
    component: hmaster
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
data:
  hbase-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <!--
    /*
    *
    * Licensed to the Apache Software Foundation (ASF) under one
    * or more contributor license agreements.  See the NOTICE file
    * distributed with this work for additional information
    * regarding copyright ownership.  The ASF licenses this file
    * to you under the Apache License, Version 2.0 (the
    * "License"); you may not use this file except in compliance
    * with the License.  You may obtain a copy of the License at
    *
    *     http://www.apache.org/licenses/LICENSE-2.0
    *
    * Unless required by applicable law or agreed to in writing, software
    * distributed under the License is distributed on an "AS IS" BASIS,
    * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    * See the License for the specific language governing permissions and
    * limitations under the License.
    */
    -->
    <configuration>
      <property>
        <name>hbase.balancer.period</name>
        <value>60000</value>
      </property>
      <property>
        <name>hbase.bulkload.staging.dir</name>
        <value>/tmp/splicedb-staging</value>
      </property>
      <property>
        <name>hbase.mapreduce.bulkload.max.hfiles.perRegion.perFamily</name>
        <value>1024</value>
      </property>
      <property>
        <name>hbase.client.ipc.pool.size</name>
        <value>10</value>
      </property>
      <property>
        <name>hbase.client.keyvalue.maxsize</name>
        <value>10485760</value>
      </property>
      <property>
        <name>hbase.client.max.perregion.tasks</name>
        <value>100</value>
      </property>
      <property>
        <name>hbase.client.pause</name>
        <value>100</value>
      </property>
      <property>
        <name>hbase.client.primaryCallTimeout.get</name>
        <value>10</value>
      </property>
      <property>
        <name>hbase.client.primaryCallTimeout.multiget</name>
        <value>10</value>
      </property>
      <property>
        <name>hbase.client.retries.number</name>
        <value>100</value>
      </property>
      <property>
        <name>hbase.client.scanner.caching</name>
        <value>1000</value>
      </property>
      <property>
        <name>hbase.client.scanner.timeout.period</name>
        <value>1200000</value>
      </property>
      <property>
        <name>hbase.client.write.buffer</name>
        <value>2097152</value>
      </property>
      <property>
        <name>hbase.cluster.distributed</name>
        <value>true</value>
      </property>
      <property>
        <name>hbase.coprocessor.abortonerror</name>
        <value>true</value>
      </property>
      <property>
        <name>splicemachine.enterprise.key</name>
        <value></value>
      </property>
      <property>
        <name>hbase.coprocessor.master.classes</name>
        <value>com.splicemachine.hbase.SpliceMasterObserver</value>
      </property>
      <property>
        <name>hbase.coprocessor.region.classes</name>
        <value>org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint,com.splicemachine.hbase.MemstoreAwareObserver,com.splicemachine.derby.hbase.SpliceIndexObserver,com.splicemachine.derby.hbase.SpliceIndexEndpoint,com.splicemachine.hbase.RegionSizeEndpoint,com.splicemachine.si.data.hbase.coprocessor.TxnLifecycleEndpoint,com.splicemachine.si.data.hbase.coprocessor.SIObserver,com.splicemachine.hbase.BackupEndpointObserver</value>
      </property>
      <property>
        <name>hbase.coprocessor.regionserver.classes</name>
        <value>com.splicemachine.hbase.RegionServerLifecycleObserver,com.splicemachine.si.data.hbase.coprocessor.SpliceRSRpcServices</value>
      </property>
      <property>
        <name>dfs.client.read.shortcircuit.buffer.size</name>
        <value>131072</value>
      </property>
      <property>
        <name>hbase.hash.type</name>
        <value>murmur</value>
      </property>
      <property>
        <name>hfile.block.bloom.cacheonwrite</name>
        <value>true</value>
      </property>
      <property>
        <name>hfile.block.cache.size</name>
        <value>0.25</value>
      </property>
      <property>
        <name>hbase.hregion.majorcompaction</name>
        <value>604800000</value>
      </property>
      <property>
        <name>hbase.hregion.majorcompaction.jitter</name>
        <value>0.5</value>
      </property>
      <property>
        <name>hbase.hregion.max.filesize</name>
        <value>1073741824</value>
      </property>
      <property>
        <name>hbase.hregion.memstore.block.multiplier</name>
        <value>4</value>
      </property>
      <property>
        <name>hbase.hregion.memstore.flush.size</name>
        <value>134217728</value>
      </property>
      <property>
        <name>hbase.hregion.memstore.mslab.chunksize</name>
        <value>2097152</value>
      </property>
      <property>
        <name>hbase.hregion.memstore.mslab.enabled</name>
        <value>true</value>
      </property>
      <property>
        <name>hbase.hregion.memstore.mslab.max.allocation</name>
        <value>262144</value>
      </property>
      <property>
        <name>hbase.hregion.preclose.flush.size</name>
        <value>5242880</value>
      </property>
      <property>
        <name>hbase.hstore.blockingStoreFiles</name>
        <value>20</value>
      </property>
      <property>
        <name>hbase.hstore.blockingWaitTime</name>
        <value>90000</value>
      </property>
      <property>
        <name>hbase.hstore.compactionThreshold</name>
        <value>5</value>
      </property>
      <property>
        <name>hbase.hstore.compaction.max</name>
        <value>7</value>
      </property>
      <property>
        <name>hbase.hstore.compaction.max.size</name>
        <value>260046848</value>
      </property>
      <property>
        <name>hbase.hstore.compaction.min</name>
        <value>3</value>
      </property>
      <property>
        <name>hbase.hstore.compaction.min.size</name>
        <value>136314880</value>
      </property>
      <property>
        <name>hbase.hstore.defaultengine.compactionpolicy.class</name>
        <value>com.splicemachine.compactions.SpliceDefaultCompactionPolicy</value>
      </property>
      <property>
        <name>hbase.hstore.defaultengine.compactor.class</name>
        <value>com.splicemachine.compactions.SpliceDefaultCompactor</value>
      </property>
      <property>
        <name>hbase.htable.threads.max</name>
        <value>96</value>
      </property>
      <property>
        <name>io.storefile.bloom.error.rate</name>
        <value>0.005</value>
      </property>
      <property>
        <name>hbase.ipc.client.allowsInterrupt</name>
        <value>true</value>
      </property>
      <property>
        <name>hbase.ipc.server.read.threadpool.size</name>
        <value>10</value>
      </property>
      <property>
        <name>hbase.ipc.warn.response.size</name>
        <value>-1</value>
      </property>
      <property>
        <name>hbase.ipc.warn.response.time</name>
        <value>-1</value>
      </property>
      <property>
        <name>hbase.master.executor.closeregion.threads</name>
        <value>5</value>
      </property>
      <property>
        <name>hbase.master.executor.openregion.threads</name>
        <value>5</value>
      </property>
      <property>
        <name>hbase.master.executor.serverops.threads</name>
        <value>12</value>
      </property>
      <property>
        <name>hbase.master.handler.count</name>
        <value>25</value>
      </property>
      <property>
        <name>hbase.master.info.port</name>
        <value>16010</value>
      </property>
      <property>
        <name>hbase.master.loadbalance.bytable</name>
        <value>false</value>
      </property>
      <property>
        <name>hbase.master.logcleaner.ttl</name>
        <value>60000</value>
      </property>
      <property>
        <name>hbase.master.namespace.init.timeout</name>
        <value>2400000</value>
      </property>
      <property>
        <name>hbase.master.port</name>
        <value>16000</value>
      </property>
      <property>
        <name>hbase.mvcc.impl</name>
        <value>org.apache.hadoop.hbase.regionserver.SIMultiVersionConsistencyControl</value>
      </property>
      <property>
        <name>hbase.region.replica.replication.enabled</name>
        <value>false</value>
      </property>
      <property>
        <name>hbase.regions.slop</name>
        <value>0</value>
      </property>
      <property>
        <name>hbase.regionserver.executor.openregion.threads</name>
        <value>50</value>
      </property>
      <property>
        <name>hbase.regionserver.global.memstore.size</name>
        <value>0.25</value>
      </property>
      <property>
        <name>hbase.regionserver.global.memstore.size.lower.limit</name>
        <value>0.9</value>
      </property>
      <property>
        <name>hbase.regionserver.handler.count</name>
        <value>400</value>
      </property>
      <property>
        <name>hbase.regionserver.hlog.blocksize</name>
        <value>134217728</value>
      </property>
      <property>
        <name>hbase.regionserver.info.port</name>
        <value>16030</value>
      </property>
      <property>
        <name>hbase.regionserver.logroll.period</name>
        <value>3600000</value>
      </property>
      <property>
        <name>hbase.regionserver.maxlogs</name>
        <value>48</value>
      </property>
      <property>
        <name>hbase.regionserver.metahandler.count</name>
        <value>200</value>
      </property>
      <property>
        <name>hbase.regionserver.msginterval</name>
        <value>3000</value>
      </property>
      <property>
        <name>hbase.regionserver.nbreservationblocks</name>
        <value>4</value>
      </property>
      <property>
        <name>hbase.regionserver.optionallogflushinterval</name>
        <value>1000</value>
      </property>
      <property>
        <name>hbase.regionserver.port</name>
        <value>16020</value>
      </property>
      <property>
        <name>hbase.regionserver.regionSplitLimit</name>
        <value>2147483647</value>
      </property>
      <property>
        <name>hbase.regionserver.thread.compaction.large</name>
        <value>4</value>
      </property>
      <property>
        <name>hbase.regionserver.thread.compaction.small</name>
        <value>4</value>
      </property>
      <property>
        <name>hbase.regionserver.wal.enablecompression</name>
        <value>true</value>
      </property>
      <property>
        <name>hbase.rootdir</name>
        <value>/splicedb/hbase</value>
      </property>
      <property>
        <name>hbase.fs.tmp.dir</name>
        <value>/tmp/splicedb-staging</value>
      </property>
      <property>
        <name>hbase.row.level.authorization</name>
        <value>false</value>
      </property>
      <property>
        <name>hbase.rpc.protection</name>
        <value>authentication</value>
      </property>
      <property>
        <name>hbase.rpc.timeout</name>
        <value>1200000</value>
      </property>
      <property>
        <name>hbase.security.authentication</name>
        <value>simple</value>
      </property>
      <property>
        <name>hbase.security.authorization</name>
        <value>false</value>
      </property>
      <property>
        <name>hbase.server.thread.wakefrequency</name>
        <value>10000</value>
      </property>
      <property>
        <name>hbase.snapshot.enabled</name>
        <value>true</value>
      </property>
      <property>
        <name>hbase.snapshot.master.timeout.millis</name>
        <value>60000</value>
      </property>
      <property>
        <name>hbase.snapshot.region.timeout</name>
        <value>60000</value>
      </property>
      <property>
        <name>hbase.splitlog.manager.timeout</name>
        <value>300000</value>
      </property>
      <property>
        <name>hbase.status.multicast.port</name>
        <value>16100</value>
      </property>
      <property>
        <name>hbase.superuser</name>
        <value>root</value>
      </property>
      <property>
        <name>hbase.wal.disruptor.batch</name>
        <value>true</value>
      </property>
      <property>
        <name>hbase.wal.provider</name>
        <value>multiwal</value>
      </property>
      <property>
        <name>hbase.wal.regiongrouping.numgroups</name>
        <value>16</value>
      </property>
      <property>
        <name>hbase.wal.storage.policy</name>
        <value>NONE</value>
      </property>
      <property>
        <name>hbase.zookeeper.property.tickTime</name>
        <value>6000</value>
      </property>
      <property>
        <name>hbase.zookeeper.session.timeout</name>
        <value>1200000</value>
      </property>
      <property>
        <name>hbase.zookeeper.znode.parent</name>
        <value>/hbase</value>
      </property>
      <property>
        <name>hbase.zookeeper.znode.rootserver</name>
        <value>root-region-server</value>
      </property>
      <property>
        <name>hbase.hstore.defaultengine.compactor.class</name>
        <value>com.splicemachine.compactions.SpliceDefaultCompactor</value>
      </property>
      <property>
        <name>hbase.hstore.defaultengine.compactionpolicy.class</name>
        <value>com.splicemachine.compactions.SpliceDefaultCompactionPolicy</value>
      </property>
      <property>
        <name>splice.debug.logStatementContext</name>
        <value>true</value>
      </property>
      <property>
        <name>splice.authentication</name>
        <value>NATIVE</value>
      </property>
      <property>
        <name>splice.independent.write.threads</name>
        <value>120</value>
      </property>
      <property>
        <name>splice.dependent.write.threads</name>
        <value>80</value>
      </property>
      <property>
        <name>splice.authentication.ldap.server</name>
        <value></value>
      </property>
      <property>
        <name>splice.authentication.ldap.searchAuthDN</name>
        <value></value>
      </property>
      <property>
        <name>splice.authentication.ldap.searchAuth.password</name>
        <value></value>
      </property>
      <property>
        <name>splice.authentication.ldap.searchBase</name>
        <value></value>
      </property>
      <property>
        <name>splice.authentication.ldap.searchFilter</name>
        <value></value>
      </property>
      <property>
        <name>splice.authentication.ldap.mapGroupAttr</name>
        <value>CLOUDADMIN=splice</value>
      </property>
      <property>
        <name>presto.s3.staging-directory</name>
        <value>/spark/tmp0,/spark/tmp1</value>
      </property>
      <property>
        <name>splice.olap_server.external</name>
        <value>true</value>
      </property>
      <property>
        <name>splice.olap_server.deployment.mode</name>
        <value>KUBERNETES</value>
      </property>
      <property>
        <name>presto.s3.socket-timeout</name>
        <value>120s</value>
      </property>
      <property>
        <name>hbase.master.balancer.stochastic.regionCountCost</name>
        <value>1500</value>
      </property>
      <property>
        <name>hbase.rowlock.wait.duration</name>
        <value>10</value>
      </property>
      <property>
        <name>hbase.zookeeper.quorum</name>
        <value>my-release-zookeeper-1.my-release-zookeeper-headless.default.svc.cluster.local:2181,my-release-zookeeper-2.my-release-zookeeper-headless.default.svc.cluster.local:2181,my-release-zookeeper-0.my-release-zookeeper-headless.default.svc.cluster.local:2181</value>
      </property>
      <property>
        <name>splice.authentication.native.algorithm</name>
        <value>SHA-512</value>
      </property>
      <property>
        <name>splice.client.numConnections</name>
        <value>1</value>
      </property>
      <property>
        <name>splice.client.write.maxDependentWrites</name>
        <value>60000</value>
      </property>
      <property>
        <name>splice.client.write.maxIndependentWrites</name>
        <value>60000</value>
      </property>
      <property>
        <name>splice.compression</name>
        <value>snappy</value>
      </property>
      <property>
        <name>splice.ignore.missing.transactions</name>
        <value>true</value>
      </property>
      <property>
        <name>splice.marshal.kryoPoolSize</name>
        <value>1100</value>
      </property>
      <property>
        <name>splice.olap.shuffle.partitions</name>
        <value>200</value>
      </property>
      <property>
        <name>splice.olap_server.clientWaitTime</name>
        <value>900000</value>
      </property>
      <property>
        <name>splice.optimizer.broadcastRegionRowThreshold</name>
        <value>1000000</value>
      </property>
      <property>
        <name>splice.ring.bufferSize</name>
        <value>131072</value>
      </property>
      <property>
        <name>splice.splitBlockSize</name>
        <value>67108864</value>
      </property>
      <property>
        <name>splice.timestamp_server.clientWaitTime</name>
        <value>120000</value>
      </property>
      <property>
        <name>splice.txn.activeTxns.cacheSize</name>
        <value>10240</value>
      </property>
      <property>
        <name>splice.txn.completedTxns.concurrency</name>
        <value>128</value>
      </property>
      <property>
        <name>splice.txn.concurrencyLevel</name>
        <value>4096</value>
      </property>
      <property>
        <name>splice.timestamp_server.port</name>
        <value>16012</value>
      </property>
      <property>
        <name>splice.olap_server.port</name>
        <value>16040</value>
      </property>
      <property>
        <name>splice.writer.maxThreads</name>
        <value>20</value>
      </property>
      <property>
        <name>hbase.master.hfilecleaner.plugins</name>
        <value>org.apache.hadoop.hbase.master.cleaner.TimeToLiveHFileCleaner</value>
      </property>
      <property>
        <name>hbase.unsafe.stream.capability.enforce</name>
        <value>false</value>
      </property>
      <property>
        <name>hbase.master.info.bindAddress</name>
        <value>my-release-hmaster-0.my-release-hmaster.default.svc.cluster.local</value>
      </property>
      <property>
        <name>splice.olap.log4j.configuration</name>
        <value>file:/etc/hbase/conf/log4j.properties</value>
      </property>
    </configuration>

  hbase-env.sh: |
    #!/usr/bin/env bash
    #
    # *
    # * Licensed to the Apache Software Foundation (ASF) under one
    # * or more contributor license agreements.  See the NOTICE file
    # * distributed with this work for additional information
    # * regarding copyright ownership.  The ASF licenses this file
    # * to you under the Apache License, Version 2.0 (the
    # * "License"); you may not use this file except in compliance
    # * with the License.  You may obtain a copy of the License at
    # *
    # *     http://www.apache.org/licenses/LICENSE-2.0
    # *
    # * Unless required by applicable law or agreed to in writing, software
    # * distributed under the License is distributed on an "AS IS" BASIS,
    # * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # * See the License for the specific language governing permissions and
    # * limitations under the License.
    # *
    # Set environment variables here.
    # This script sets variables multiple times over the course of starting an hbase process,
    # so try to keep things idempotent unless you want to take an even deeper look
    # into the startup scripts (bin/hbase, etc.)
    # The java implementation to use.  Java 1.8+ required.
    # export JAVA_HOME=/usr/java/jdk1.8.0/
    # Extra Java CLASSPATH elements.  Optional.
    export HBASE_CLASSPATH_PREFIX=/etc/hbase/conf:$(echo /usr/lib/splicemachine/lib/*.jar | tr ' ' ':'):$(echo /usr/lib/spark/jars/* | tr ' ' ':')
    export HBASE_CLASSPATH=$(echo /usr/lib/hadoop/lib/*.jar | tr ' ' ':'):$(echo /usr/lib/mlmanager/lib/* | tr ' ' ':'):/usr/lib/hadoop/hadoop-azure-datalake.jar:$(echo /usr/lib/hive/lib/*.jar | tr ' ' ':')
    export OLAP_CLASSPATH="/etc/hadoop/conf:/etc/hbase/conf:/etc/spark/conf:/usr/lib/splicemachine/lib/*:/usr/lib/spark/jars/*:/usr/lib/hbase/lib/*:/usr/lib/hadoop-yarn/lib/*:/usr/lib/hadoop/lib/*"
    # The maximum amount of heap to use. Default is left to JVM default.
    # export HBASE_HEAPSIZE=1G
    # Uncomment below if you intend to use off heap cache. For example, to allocate 8G of
    # offheap, set the value to "8G".
    # export HBASE_OFFHEAPSIZE=1G
    # Extra Java runtime options.
    # Below are what we set by default.  May only work with SUN JVM.
    # For more on why as well as other possible settings,
    # see http://hbase.apache.org/book.html#performance
    # DBAAS-3180 - We are going to use +UseG1GC for region, and +UseConcMarkSweepGC for master
    # Cannot specify on a global scale and then override later.
    # export HBASE_OPTS="$HBASE_OPTS -XX:+UseConcMarkSweepGC"
    # Uncomment one of the below three options to enable java garbage collection logging for the server-side processes.
    # This enables basic gc logging to the .out file.
    # export SERVER_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps"
    # This enables basic gc logging to its own file.
    # If FILE-PATH is not replaced, the log file(.gc) would still be generated in the HBASE_LOG_DIR .
    # export SERVER_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:<FILE-PATH>"
    # This enables basic GC logging to its own file with automatic log rolling. Only applies to jdk 1.6.0_34+ and 1.7.0_2+.
    # If FILE-PATH is not replaced, the log file(.gc) would still be generated in the HBASE_LOG_DIR .
    # export SERVER_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:<FILE-PATH> -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=1 -XX:GCLogFileSize=512M"
    # Uncomment one of the below three options to enable java garbage collection logging for the client processes.
    # This enables basic gc logging to the .out file.
    # export CLIENT_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps"
    # This enables basic gc logging to its own file.
    # If FILE-PATH is not replaced, the log file(.gc) would still be generated in the HBASE_LOG_DIR .
    # export CLIENT_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:<FILE-PATH>"
    # This enables basic GC logging to its own file with automatic log rolling. Only applies to jdk 1.6.0_34+ and 1.7.0_2+.
    # If FILE-PATH is not replaced, the log file(.gc) would still be generated in the HBASE_LOG_DIR .
    # export CLIENT_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:<FILE-PATH> -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=1 -XX:GCLogFileSize=512M"
    # See the package documentation for org.apache.hadoop.hbase.io.hfile for other configurations
    # needed setting up off-heap block caching.
    # FOR Splice Machine
    # build these out in a clear manner
    SPLICE_HBASE_REGIONSERVER_OPTS=""
    SPLICE_HBASE_REGIONSERVER_OPTS="$SPLICE_HBASE_REGIONSERVER_OPTS -Xms20g -Xmx20g -XX:+HeapDumpOnOutOfMemoryError -XX:MaxDirectMemorySize=2g -XX:+AlwaysPreTouch -XX:MaxNewSize=2g -XX:InitiatingHeapOccupancyPercent=60 -XX:+UseG1GC -XX:ParallelGCThreads=24 -XX:+ParallelRefProcEnabled -XX:MaxGCPauseMillis=5000"
    SPLICE_HBASE_REGIONSERVER_OPTS="$SPLICE_HBASE_REGIONSERVER_OPTS -Dderby.drda.logConnections=true"
    SPLICE_HBASE_REGIONSERVER_OPTS="$SPLICE_HBASE_REGIONSERVER_OPTS -Dderby.drda.timeSlice=0"
    SPLICE_HBASE_REGIONSERVER_OPTS="$SPLICE_HBASE_REGIONSERVER_OPTS -Dlog4j.debug=true"
    SPLICE_HBASE_MASTER_OPTS=""
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Xms4g -Xmx4g -XX:+HeapDumpOnOutOfMemoryError -XX:MaxDirectMemorySize=2g -XX:+AlwaysPreTouch -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dlog4j.debug=true"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.olap_server.external=true"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.olap_server.deployment.mode=KUBERNETES"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.enabled=true"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.submit.deployMode=client"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.app.name=my-release-hmaster-spark"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.master=k8s://https://kubernetes.default.svc.cluster.local:443"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.ui.port=4040"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.acls.enable=true"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.admin.acls=*"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.admin.acls.groups=*"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.logConf=true"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.ui.filters=org.apache.shiro.web.servlet.IniShiroFilter"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.driver.maxResultSize=2g"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.driver.cores=2"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.executor.instances=4"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.dynamicAllocation.enabled=false"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.dynamicAllocation.executorIdleTimeout=120"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.dynamicAllocation.cachedExecutorIdleTimeout=120"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.dynamicAllocation.minExecutors=1"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.dynamicAllocation.maxExecutors=4"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.io.compression.lz4.blockSize=32k"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kryo.referenceTracking=false"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kryo.registrator=com.splicemachine.derby.impl.SpliceSparkKryoRegistrator"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kryoserializer.buffer.max=512m"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kryoserializer.buffer=4m"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.locality.wait=0"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.memory.fraction=0.5"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.scheduler.mode=FAIR"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.serializer=org.apache.spark.serializer.KryoSerializer"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.shuffle.service.enabled=false"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.shuffle.io.maxRetries=30"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.shuffle.io.retryWait=10"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.sql.shuffle.partitions=1200"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.driver.extraLibraryPath=/usr/lib/hadoop/lib/native:/usr/lib/hbase/lib/native/Linux-amd64-64"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.driver.extraClassPath=/etc/hbase/conf:/usr/lib/mlmanager/lib/*"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.driver.extraJavaOptions=\"-Dlog4j.debug=true -Dlog4j.configuration=log4j.properties\""
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.executor.extraLibraryPath=/usr/lib/hadoop/lib/native:/usr/lib/hbase/lib/native/Linux-amd64-64"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.executor.extraClassPath=/etc/spark/conf:/usr/lib/splicemachine/lib/*:/usr/lib/spark/jars/*:/usr/lib/hbase/lib/*:/usr/lib/hadoop/lib/*:/usr/lib/hadoop/hadoop-azure-datalake.jar:/usr/lib/hive/lib/*:/usr/lib/mlmanager/lib/*"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.ui.retainedJobs=150"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.ui.retainedStages=250"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.worker.ui.retainedExecutors=100"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.worker.ui.retainedDrivers=100"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.streaming.ui.retainedBatches=100"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.executor.cores=6"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.executor.memory=20480m"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.executor.memoryOverhead=7168m"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.network.timeout=120s"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.scheduler.minRegisteredResourcesRatio=0"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.scheduler.maxRegisteredResourcesWaitingTime=30s"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.executor.hearbeatInterval=10s"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dspark.compaction.reserved.slots=4"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.local.dir=/spark/tmp0,/spark/tmp1"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.files=/tmp/spark-config/hbase-site.xml,/tmp/hdfs-config/hdfs-site.xml,/tmp/hdfs-config/core-site.xml"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dlog4j.configuration=log4j.properties"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.namespace=default"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.container.image=docker.io/splicemachine/sm_k8_spark-3.0.0:0.0.64"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.container.image.pullPolicy=IfNotPresent"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.authenticate.driver.serviceAccountName=spark"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.driverEnv.SPARK_CONF_DIR=/etc/spark/conf"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.driverEnv.HADOOP_CONF_DIR=/etc/hadoop/conf"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.driverEnv.HBASE_CONF_DIR=/etc/hbase/conf"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.executor.label.app=hbase"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.executor.label.release=my-release"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.executorEnv.RELEASE_NAME=my-release"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.executorEnv.FRAMEWORKID=splicedb-dev1"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.executorEnv.JAVASCOPE=true"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.executorEnv.ZOOKEEPER_QUORUM=my-release-zookeeper-1.my-release-zookeeper-headless.default.svc.cluster.local:2181,my-release-zookeeper-2.my-release-zookeeper-headless.default.svc.cluster.local:2181,my-release-zookeeper-0.my-release-zookeeper-headless.default.svc.cluster.local:2181"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.executorEnv.RELEASE_NAME=my-release"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.executorEnv.FRAMEWORKID=splicedb-dev1"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.executorEnv.JAVASCOPE=true"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.executorEnv.ZOOKEEPER_QUORUM=my-release-zookeeper-1.my-release-zookeeper-headless.default.svc.cluster.local:2181,my-release-zookeeper-2.my-release-zookeeper-headless.default.svc.cluster.local:2181,my-release-zookeeper-0.my-release-zookeeper-headless.default.svc.cluster.local:2181"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.executorEnv.TZ=UTC"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.executorEnv.TZ=UTC"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.executor.label.component=sparkexec"
    
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.namespace=default"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.local.dir.tmpfs=false"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.maxRemoteBlockSizeFetchToMem=134217728"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.executor.request.cores=6"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.executor.request.memory=27648m"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.io.compression.codec=snappy"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.kafka.bootstrapServers=my-release-kafka-headless.default.svc.cluster.local:9092"
    # DBAAS-1018: escape these quotes exactly once
    export EXTRA_QUOTE_OPTS="-Dsplice.spark.executor.extraJavaOptions=\"-Dlog4j.debug=true -Dlog4j.configuration=log4j.properties -XX:MaxDirectMemorySize=8g\""
    # Uncomment and adjust to enable JMX exporting
    # See jmxremote.password and jmxremote.access in $JRE_HOME/lib/management to configure remote password access.
    # More details at: http://java.sun.com/javase/6/docs/technotes/guides/management/agent.html
    # NOTE: HBase provides an alternative JMX implementation to fix the random ports issue, please see JMX
    # section in HBase Reference Guide for instructions.
    export HBASE_JMX_BASE="-Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false"
    export HBASE_MASTER_OPTS="$HBASE_MASTER_OPTS $HBASE_JMX_BASE $SPLICE_HBASE_MASTER_OPTS -Dcom.sun.management.jmxremote.port=10101"
    export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS $HBASE_JMX_BASE $SPLICE_HBASE_REGIONSERVER_OPTS -Dcom.sun.management.jmxremote.port=10102"
    # export HBASE_THRIFT_OPTS="$HBASE_THRIFT_OPTS $HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10103"
    # export HBASE_ZOOKEEPER_OPTS="$HBASE_ZOOKEEPER_OPTS $HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10104"
    # export HBASE_REST_OPTS="$HBASE_REST_OPTS $HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10105"
    # File naming hosts on which HRegionServers will run.  $HBASE_HOME/conf/regionservers by default.
    # export HBASE_REGIONSERVERS=${HBASE_HOME}/conf/regionservers
    # Uncomment and adjust to keep all the Region Server pages mapped to be memory resident
    #HBASE_REGIONSERVER_MLOCK=true
    #HBASE_REGIONSERVER_UID="hbase"
    # File naming hosts on which backup HMaster will run.  $HBASE_HOME/conf/backup-masters by default.
    # export HBASE_BACKUP_MASTERS=${HBASE_HOME}/conf/backup-masters
    # Extra ssh options.  Empty by default.
    # export HBASE_SSH_OPTS="-o ConnectTimeout=1 -o SendEnv=HBASE_CONF_DIR"
    # Where log files are stored.  $HBASE_HOME/logs by default.
    export HBASE_LOG_DIR=/var/log/hbase
    # Enable remote JDWP debugging of major HBase processes. Meant for Core Developers
    # export HBASE_MASTER_OPTS="$HBASE_MASTER_OPTS -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8070"
    # export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8071"
    # export HBASE_THRIFT_OPTS="$HBASE_THRIFT_OPTS -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8072"
    # export HBASE_ZOOKEEPER_OPTS="$HBASE_ZOOKEEPER_OPTS -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8073"
    # A string representing this instance of hbase. $USER by default.
    export HBASE_IDENT_STRING=hbase
    # The scheduling priority for daemon processes.  See 'man nice'.
    # export HBASE_NICENESS=10
    # The directory where pid files are stored. /tmp by default.
    # export HBASE_PID_DIR=/var/hbase/pids
    # Seconds to sleep between slave commands.  Unset by default.  This
    # can be useful in large clusters, where, e.g., slave rsyncs can
    # otherwise arrive faster than the master can service them.
    # export HBASE_SLAVE_SLEEP=0.1
    # Tell HBase whether it should manage it's own instance of ZooKeeper or not.
    # export HBASE_MANAGES_ZK=true
    # The default log rolling policy is RFA, where the log file is rolled as per the size defined for the
    # RFA appender. Please refer to the log4j.properties file to see more details on this appender.
    # In case one needs to do log rolling on a date change, one should set the environment property
    # HBASE_ROOT_LOGGER to "<DESIRED_LOG LEVEL>,DRFA".
    # For example:
    # HBASE_ROOT_LOGGER=DEBUG,DRFA
    # The reason for changing default to RFA is to avoid the boundary case of filling out disk space as
    # DRFA doesn't put any cap on the log size. Please refer to HBase-5655 for more context.
    

  fairscheduler.xml: |
    <?xml version="1.0"?>
    <allocations>
        <pool name="import">
            <schedulingMode>FAIR</schedulingMode>
            <weight>10</weight>
            <minShare>0</minShare>
        </pool>
        <pool name="query">
            <schedulingMode>FAIR</schedulingMode>
            <weight>20</weight>
            <minShare>0</minShare>
        </pool>
        <pool name="admin">
            <schedulingMode>FAIR</schedulingMode>
            <weight>1</weight>
            <minShare>0</minShare>
        </pool>
        <pool name="compaction">
            <schedulingMode>FAIR</schedulingMode>
            <weight>15</weight>
            <minShare>2</minShare>
        </pool>
        <pool name="urgent">
            <schedulingMode>FAIR</schedulingMode>
            <weight>1000</weight>
            <minShare>0</minShare>
        </pool>
    </allocations>
    

  log4j.properties: |
    status = error
    name = PropertiesConfig
    property.filename = /var/log/hbase/hbase.log
    filters = threshold
    filter.threshold.type = ThresholdFilter
    filter.threshold.level = info
    log4j.rootLogger=INFO,RFA,RFAConsole
    log4j.logger.splice-derby=INFO, spliceDerby,spliceDerbyConsole
    log4j.additivity.splice-derby=false
    log4j.logger.org.mortbay.jetty=INFO, jettyLog,jettyLogConsole
    # Logging Threshold
    log4j.threshold=ALL
    #
    # Rolling File Appender
    #
    hbase.log.maxfilesize=500MB
    hbase.log.maxbackupindex=8
    log4j.appender.RFA=org.apache.log4j.RollingFileAppender
    log4j.appender.RFA.File=/var/log/hbase/hbase.log
    log4j.appender.RFA.MaxFileSize=${hbase.log.maxfilesize}
    log4j.appender.RFA.MaxBackupIndex=${hbase.log.maxbackupindex}
    log4j.appender.RFA.layout=org.apache.log4j.PatternLayout
    log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [{{TASK_NAME}}] [{{FRAMEWORK_NAME}}] [hbase] [%t] [%c{2}]: %m%n
    log4j.appender.RFAConsole=org.apache.log4j.ConsoleAppender
    log4j.appender.RFAConsole.target=System.err
    log4j.appender.RFAConsole.layout=org.apache.log4j.PatternLayout
    log4j.appender.RFAConsole.layout.ConversionPattern=%d{ISO8601} %-5p [{{TASK_NAME}}] [{{FRAMEWORK_NAME}}] [hbase] [%t] [%c{2}]: %m%n
    #
    # Security audit appender
    #
    hbase.security.log.file=SecurityAuth.audit
    hbase.security.log.maxfilesize=256MB
    hbase.security.log.maxbackupindex=5
    log4j.appender.RFAS=org.apache.log4j.RollingFileAppender
    log4j.appender.RFAS.File=/var/log/hbase/${hbase.security.log.file}
    log4j.appender.RFAS.MaxFileSize=${hbase.security.log.maxfilesize}
    log4j.appender.RFAS.MaxBackupIndex=${hbase.security.log.maxbackupindex}
    log4j.appender.RFAS.layout=org.apache.log4j.PatternLayout
    log4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %-5p [{{TASK_NAME}}] [{{FRAMEWORK_NAME}}] [hbase-security] [%t] [%c{2}]: %m%n
    log4j.appender.RFASConsole=org.apache.log4j.ConsoleAppender
    log4j.appender.RFASConsole.target=System.err
    log4j.appender.RFASConsole.layout=org.apache.log4j.PatternLayout
    log4j.appender.RFASConsole.layout.ConversionPattern=%d{ISO8601} %-5p [{{TASK_NAME}}] [{{FRAMEWORK_NAME}}] [hbase-security] [%t] [%c{2}]: %m%n
    log4j.category.SecurityLogger=${hbase.security.logger}
    log4j.additivity.SecurityLogger=false
    #
    # spliceDerby
    # Add spliceDerby to the rootlogger above if you want to use this
    #
    log4j.appender.spliceDerby=org.apache.log4j.RollingFileAppender
    log4j.appender.spliceDerby.File=/var/log/hbase/splice-derby.log
    log4j.appender.spliceDerby.MaxFileSize=${hbase.log.maxfilesize}
    log4j.appender.spliceDerby.MaxBackupIndex=${hbase.log.maxbackupindex}
    log4j.appender.spliceDerby.layout=org.apache.log4j.PatternLayout
    log4j.appender.spliceDerby.layout.ConversionPattern=%d{ISO8601} %-5p [{{TASK_NAME}}] [{{FRAMEWORK_NAME}}] [splicederby] [%t] [%c{2}]: %m%n
    log4j.appender.spliceDerbyConsole=org.apache.log4j.ConsoleAppender
    log4j.appender.spliceDerbyConsole.target=System.err
    log4j.appender.spliceDerbyConsole.layout=org.apache.log4j.PatternLayout
    log4j.appender.spliceDerbyConsole.layout.ConversionPattern=%d{ISO8601} %-5p [{{TASK_NAME}}] [{{FRAMEWORK_NAME}}] [splicederby] [%t] [%c{2}]: %m%n
    #
    # spliceDerby
    # Add spliceDerby to the rootlogger above if you want to use this
    #
    log4j.appender.jettyLog=org.apache.log4j.RollingFileAppender
    log4j.appender.jettyLog.File=/var/log/hbase/mortbay-jetty.log
    log4j.appender.jettyLog.MaxFileSize=${hbase.log.maxfilesize}
    log4j.appender.jettyLog.MaxBackupIndex=${hbase.log.maxbackupindex}
    log4j.appender.jettyLog.layout=org.apache.log4j.PatternLayout
    log4j.appender.jettyLog.layout.ConversionPattern=%d{ISO8601} %-5p [{{TASK_NAME}}] [{{FRAMEWORK_NAME}}] [jetty] [%t] [%c{2}]: %m%n
    log4j.appender.jettyLogConsole=org.apache.log4j.ConsoleAppender
    log4j.appender.jettyLogConsole.target=System.err
    log4j.appender.jettyLogConsole.layout=org.apache.log4j.PatternLayout
    log4j.appender.jettyLogConsole.layout.ConversionPattern=%d{ISO8601} %-5p [{{TASK_NAME}}] [{{FRAMEWORK_NAME}}] [jetty] [%t] [%c{2}]: %m%n
    # Custom Logging levels
    log4j.logger.org.apache.zookeeper=${hbase.log.level}
    log4j.logger.org.apache.hadoop.hbase=${hbase.log.level}
    log4j.logger.org.apache.hadoop.hbase.zookeeper.ZKUtil=${hbase.log.level}
    log4j.logger.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher=${hbase.log.level}
    # Splice Machine
    log4j.logger.org.apache.hadoop.hbase.wal=ERROR
    log4j.logger.org.apache.hadoop.hbase.master=INFO
    #log4j.logger.org.apache.hadoop.hbase.regionserver=TRACE
    log4j.logger.org.apache.hadoop.hbase.zookeeper.ZKTableStateManager=WARN
    log4j.logger.org.apache.hadoop.hbase.io.hfile.CacheConfig=WARN
    log4j.logger.org.apache.hadoop.hbase.MetaTableAccessor=WARN
    log4j.logger.org.apache.hadoop.hbase.coprocessor.CoprocessorHost=WARN
    log4j.logger.org.apache.spark.ContextCleaner=WARN
    log4j.logger.org.apache.spark.scheduler=WARN
    log4j.logger.org.apache.spark.storage=WARN
    log4j.logger.org.apache.spark.SparkContext=WARN
    log4j.logger.org.apache.spark.executor=WARN
    log4j.logger.org.apache.spark.rdd.NewHadoopRDD=WARN
    log4j.logger.com.splicemachine.stream=WARN
    log4j.logger.org.apache.hadoop.metrics2.impl.MetricsConfig=WARN
    log4j.logger.org.apache.hadoop.metrics2.impl.MetricsSinkAdapter=WARN
    log4j.logger.org.apache.hadoop.metrics2.impl.MetricsSystemImpl=WARN
    

  shiro.ini: |

    [main]
    # Custom Splice Machine Realm
    spliceRealm = com.splicemachine.shiro.SpliceDatabaseRealm
    securityManager.realm = $spliceRealm
    sessionManager = org.apache.shiro.web.session.mgt.DefaultWebSessionManager
    spliceRealm.databaseName = splicedb
    spliceRealm.serverName = my-release-hregion.default.svc.cluster.local
    spliceRealm.serverPort = 1527
    securityManager.sessionManager = $sessionManager
    # 86,400,000 milliseconds = 24 hour
    securityManager.sessionManager.globalSessionTimeout = 86400000
    [urls]
    /** = authcBasic
---
# Source: splice-helm/charts/hbase/templates/olap-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-olap-config
  labels:
    app: my-release-hbase
    chart: hbase-0.0.1
    release: my-release
    component: olap
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
data:
  hbase-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <!--
    /*
    *
    * Licensed to the Apache Software Foundation (ASF) under one
    * or more contributor license agreements.  See the NOTICE file
    * distributed with this work for additional information
    * regarding copyright ownership.  The ASF licenses this file
    * to you under the Apache License, Version 2.0 (the
    * "License"); you may not use this file except in compliance
    * with the License.  You may obtain a copy of the License at
    *
    *     http://www.apache.org/licenses/LICENSE-2.0
    *
    * Unless required by applicable law or agreed to in writing, software
    * distributed under the License is distributed on an "AS IS" BASIS,
    * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    * See the License for the specific language governing permissions and
    * limitations under the License.
    */
    -->
    <configuration>
      <property>
        <name>hbase.balancer.period</name>
        <value>60000</value>
      </property>
      <property>
        <name>hbase.bulkload.staging.dir</name>
        <value>/tmp/splicedb-staging</value>
      </property>
      <property>
        <name>hbase.mapreduce.bulkload.max.hfiles.perRegion.perFamily</name>
        <value>1024</value>
      </property>
      <property>
        <name>hbase.client.ipc.pool.size</name>
        <value>10</value>
      </property>
      <property>
        <name>hbase.client.keyvalue.maxsize</name>
        <value>10485760</value>
      </property>
      <property>
        <name>hbase.client.max.perregion.tasks</name>
        <value>100</value>
      </property>
      <property>
        <name>hbase.client.pause</name>
        <value>100</value>
      </property>
      <property>
        <name>hbase.client.primaryCallTimeout.get</name>
        <value>10</value>
      </property>
      <property>
        <name>hbase.client.primaryCallTimeout.multiget</name>
        <value>10</value>
      </property>
      <property>
        <name>hbase.client.retries.number</name>
        <value>100</value>
      </property>
      <property>
        <name>hbase.client.scanner.caching</name>
        <value>1000</value>
      </property>
      <property>
        <name>hbase.client.scanner.timeout.period</name>
        <value>1200000</value>
      </property>
      <property>
        <name>hbase.client.write.buffer</name>
        <value>2097152</value>
      </property>
      <property>
        <name>hbase.cluster.distributed</name>
        <value>true</value>
      </property>
      <property>
        <name>hbase.coprocessor.abortonerror</name>
        <value>true</value>
      </property>
      <property>
        <name>splicemachine.enterprise.key</name>
        <value></value>
      </property>
      <property>
        <name>hbase.coprocessor.master.classes</name>
        <value>com.splicemachine.hbase.SpliceMasterObserver</value>
      </property>
      <property>
        <name>hbase.coprocessor.region.classes</name>
        <value>org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint,com.splicemachine.hbase.MemstoreAwareObserver,com.splicemachine.derby.hbase.SpliceIndexObserver,com.splicemachine.derby.hbase.SpliceIndexEndpoint,com.splicemachine.hbase.RegionSizeEndpoint,com.splicemachine.si.data.hbase.coprocessor.TxnLifecycleEndpoint,com.splicemachine.si.data.hbase.coprocessor.SIObserver,com.splicemachine.hbase.BackupEndpointObserver</value>
      </property>
      <property>
        <name>hbase.coprocessor.regionserver.classes</name>
        <value>com.splicemachine.hbase.RegionServerLifecycleObserver,com.splicemachine.si.data.hbase.coprocessor.SpliceRSRpcServices</value>
      </property>
      <property>
        <name>dfs.client.read.shortcircuit.buffer.size</name>
        <value>131072</value>
      </property>
      <property>
        <name>hbase.hash.type</name>
        <value>murmur</value>
      </property>
      <property>
        <name>hfile.block.bloom.cacheonwrite</name>
        <value>true</value>
      </property>
      <property>
        <name>hfile.block.cache.size</name>
        <value>0.25</value>
      </property>
      <property>
        <name>hbase.hregion.majorcompaction</name>
        <value>604800000</value>
      </property>
      <property>
        <name>hbase.hregion.majorcompaction.jitter</name>
        <value>0.5</value>
      </property>
      <property>
        <name>hbase.hregion.max.filesize</name>
        <value>1073741824</value>
      </property>
      <property>
        <name>hbase.hregion.memstore.block.multiplier</name>
        <value>4</value>
      </property>
      <property>
        <name>hbase.hregion.memstore.flush.size</name>
        <value>134217728</value>
      </property>
      <property>
        <name>hbase.hregion.memstore.mslab.chunksize</name>
        <value>2097152</value>
      </property>
      <property>
        <name>hbase.hregion.memstore.mslab.enabled</name>
        <value>true</value>
      </property>
      <property>
        <name>hbase.hregion.memstore.mslab.max.allocation</name>
        <value>262144</value>
      </property>
      <property>
        <name>hbase.hregion.preclose.flush.size</name>
        <value>5242880</value>
      </property>
      <property>
        <name>hbase.hstore.blockingStoreFiles</name>
        <value>20</value>
      </property>
      <property>
        <name>hbase.hstore.blockingWaitTime</name>
        <value>90000</value>
      </property>
      <property>
        <name>hbase.hstore.compactionThreshold</name>
        <value>5</value>
      </property>
      <property>
        <name>hbase.hstore.compaction.max</name>
        <value>7</value>
      </property>
      <property>
        <name>hbase.hstore.compaction.max.size</name>
        <value>260046848</value>
      </property>
      <property>
        <name>hbase.hstore.compaction.min</name>
        <value>3</value>
      </property>
      <property>
        <name>hbase.hstore.compaction.min.size</name>
        <value>136314880</value>
      </property>
      <property>
        <name>hbase.hstore.defaultengine.compactionpolicy.class</name>
        <value>com.splicemachine.compactions.SpliceDefaultCompactionPolicy</value>
      </property>
      <property>
        <name>hbase.hstore.defaultengine.compactor.class</name>
        <value>com.splicemachine.compactions.SpliceDefaultCompactor</value>
      </property>
      <property>
        <name>hbase.htable.threads.max</name>
        <value>96</value>
      </property>
      <property>
        <name>io.storefile.bloom.error.rate</name>
        <value>0.005</value>
      </property>
      <property>
        <name>hbase.ipc.client.allowsInterrupt</name>
        <value>true</value>
      </property>
      <property>
        <name>hbase.ipc.server.read.threadpool.size</name>
        <value>10</value>
      </property>
      <property>
        <name>hbase.ipc.warn.response.size</name>
        <value>-1</value>
      </property>
      <property>
        <name>hbase.ipc.warn.response.time</name>
        <value>-1</value>
      </property>
      <property>
        <name>hbase.master.executor.closeregion.threads</name>
        <value>5</value>
      </property>
      <property>
        <name>hbase.master.executor.openregion.threads</name>
        <value>5</value>
      </property>
      <property>
        <name>hbase.master.executor.serverops.threads</name>
        <value>12</value>
      </property>
      <property>
        <name>hbase.master.handler.count</name>
        <value>25</value>
      </property>
      <property>
        <name>hbase.master.info.port</name>
        <value>16010</value>
      </property>
      <property>
        <name>hbase.master.loadbalance.bytable</name>
        <value>false</value>
      </property>
      <property>
        <name>hbase.master.logcleaner.ttl</name>
        <value>60000</value>
      </property>
      <property>
        <name>hbase.master.namespace.init.timeout</name>
        <value>2400000</value>
      </property>
      <property>
        <name>hbase.master.port</name>
        <value>16000</value>
      </property>
      <property>
        <name>hbase.mvcc.impl</name>
        <value>org.apache.hadoop.hbase.regionserver.SIMultiVersionConsistencyControl</value>
      </property>
      <property>
        <name>hbase.region.replica.replication.enabled</name>
        <value>false</value>
      </property>
      <property>
        <name>hbase.regions.slop</name>
        <value>0</value>
      </property>
      <property>
        <name>hbase.regionserver.executor.openregion.threads</name>
        <value>50</value>
      </property>
      <property>
        <name>hbase.regionserver.global.memstore.size</name>
        <value>0.25</value>
      </property>
      <property>
        <name>hbase.regionserver.global.memstore.size.lower.limit</name>
        <value>0.9</value>
      </property>
      <property>
        <name>hbase.regionserver.handler.count</name>
        <value>400</value>
      </property>
      <property>
        <name>hbase.regionserver.hlog.blocksize</name>
        <value>134217728</value>
      </property>
      <property>
        <name>hbase.regionserver.info.port</name>
        <value>16030</value>
      </property>
      <property>
        <name>hbase.regionserver.logroll.period</name>
        <value>3600000</value>
      </property>
      <property>
        <name>hbase.regionserver.maxlogs</name>
        <value>48</value>
      </property>
      <property>
        <name>hbase.regionserver.metahandler.count</name>
        <value>200</value>
      </property>
      <property>
        <name>hbase.regionserver.msginterval</name>
        <value>3000</value>
      </property>
      <property>
        <name>hbase.regionserver.nbreservationblocks</name>
        <value>4</value>
      </property>
      <property>
        <name>hbase.regionserver.optionallogflushinterval</name>
        <value>1000</value>
      </property>
      <property>
        <name>hbase.regionserver.port</name>
        <value>16020</value>
      </property>
      <property>
        <name>hbase.regionserver.regionSplitLimit</name>
        <value>2147483647</value>
      </property>
      <property>
        <name>hbase.regionserver.thread.compaction.large</name>
        <value>4</value>
      </property>
      <property>
        <name>hbase.regionserver.thread.compaction.small</name>
        <value>4</value>
      </property>
      <property>
        <name>hbase.regionserver.wal.enablecompression</name>
        <value>true</value>
      </property>
      <property>
        <name>hbase.rootdir</name>
        <value>/splicedb/hbase</value>
      </property>
      <property>
        <name>hbase.fs.tmp.dir</name>
        <value>/tmp/splicedb-staging</value>
      </property>
      <property>
        <name>hbase.row.level.authorization</name>
        <value>false</value>
      </property>
      <property>
        <name>hbase.rpc.protection</name>
        <value>authentication</value>
      </property>
      <property>
        <name>hbase.rpc.timeout</name>
        <value>1200000</value>
      </property>
      <property>
        <name>hbase.security.authentication</name>
        <value>simple</value>
      </property>
      <property>
        <name>hbase.security.authorization</name>
        <value>false</value>
      </property>
      <property>
        <name>hbase.server.thread.wakefrequency</name>
        <value>10000</value>
      </property>
      <property>
        <name>hbase.snapshot.enabled</name>
        <value>true</value>
      </property>
      <property>
        <name>hbase.snapshot.master.timeout.millis</name>
        <value>60000</value>
      </property>
      <property>
        <name>hbase.snapshot.region.timeout</name>
        <value>60000</value>
      </property>
      <property>
        <name>hbase.splitlog.manager.timeout</name>
        <value>300000</value>
      </property>
      <property>
        <name>hbase.status.multicast.port</name>
        <value>16100</value>
      </property>
      <property>
        <name>hbase.superuser</name>
        <value>root</value>
      </property>
      <property>
        <name>hbase.wal.disruptor.batch</name>
        <value>true</value>
      </property>
      <property>
        <name>hbase.wal.provider</name>
        <value>multiwal</value>
      </property>
      <property>
        <name>hbase.wal.regiongrouping.numgroups</name>
        <value>16</value>
      </property>
      <property>
        <name>hbase.wal.storage.policy</name>
        <value>NONE</value>
      </property>
      <property>
        <name>hbase.zookeeper.property.tickTime</name>
        <value>6000</value>
      </property>
      <property>
        <name>hbase.zookeeper.session.timeout</name>
        <value>1200000</value>
      </property>
      <property>
        <name>hbase.zookeeper.znode.parent</name>
        <value>/hbase</value>
      </property>
      <property>
        <name>hbase.zookeeper.znode.rootserver</name>
        <value>root-region-server</value>
      </property>
      <property>
        <name>hbase.hstore.defaultengine.compactor.class</name>
        <value>com.splicemachine.compactions.SpliceDefaultCompactor</value>
      </property>
      <property>
        <name>hbase.hstore.defaultengine.compactionpolicy.class</name>
        <value>com.splicemachine.compactions.SpliceDefaultCompactionPolicy</value>
      </property>
      <property>
        <name>splice.debug.logStatementContext</name>
        <value>true</value>
      </property>
      <property>
        <name>splice.authentication</name>
        <value>NATIVE</value>
      </property>
      <property>
        <name>splice.independent.write.threads</name>
        <value>120</value>
      </property>
      <property>
        <name>splice.dependent.write.threads</name>
        <value>80</value>
      </property>
      <property>
        <name>splice.authentication.ldap.server</name>
        <value></value>
      </property>
      <property>
        <name>splice.authentication.ldap.searchAuthDN</name>
        <value></value>
      </property>
      <property>
        <name>splice.authentication.ldap.searchAuth.password</name>
        <value></value>
      </property>
      <property>
        <name>splice.authentication.ldap.searchBase</name>
        <value></value>
      </property>
      <property>
        <name>splice.authentication.ldap.searchFilter</name>
        <value></value>
      </property>
      <property>
        <name>splice.authentication.ldap.mapGroupAttr</name>
        <value>CLOUDADMIN=splice</value>
      </property>
      <property>
        <name>presto.s3.staging-directory</name>
        <value>/spark/tmp0,/spark/tmp1</value>
      </property>
      <property>
        <name>splice.olap_server.external</name>
        <value>true</value>
      </property>
      <property>
        <name>splice.olap_server.deployment.mode</name>
        <value>KUBERNETES</value>
      </property>
      <property>
        <name>presto.s3.socket-timeout</name>
        <value>120s</value>
      </property>
      <property>
        <name>hbase.master.balancer.stochastic.regionCountCost</name>
        <value>1500</value>
      </property>
      <property>
        <name>hbase.rowlock.wait.duration</name>
        <value>10</value>
      </property>
      <property>
        <name>hbase.zookeeper.quorum</name>
        <value>my-release-zookeeper-1.my-release-zookeeper-headless.default.svc.cluster.local:2181,my-release-zookeeper-2.my-release-zookeeper-headless.default.svc.cluster.local:2181,my-release-zookeeper-0.my-release-zookeeper-headless.default.svc.cluster.local:2181</value>
      </property>
      <property>
        <name>splice.authentication.native.algorithm</name>
        <value>SHA-512</value>
      </property>
      <property>
        <name>splice.client.numConnections</name>
        <value>1</value>
      </property>
      <property>
        <name>splice.client.write.maxDependentWrites</name>
        <value>60000</value>
      </property>
      <property>
        <name>splice.client.write.maxIndependentWrites</name>
        <value>60000</value>
      </property>
      <property>
        <name>splice.compression</name>
        <value>snappy</value>
      </property>
      <property>
        <name>splice.ignore.missing.transactions</name>
        <value>true</value>
      </property>
      <property>
        <name>splice.marshal.kryoPoolSize</name>
        <value>1100</value>
      </property>
      <property>
        <name>splice.olap.shuffle.partitions</name>
        <value>200</value>
      </property>
      <property>
        <name>splice.olap_server.clientWaitTime</name>
        <value>900000</value>
      </property>
      <property>
        <name>splice.optimizer.broadcastRegionRowThreshold</name>
        <value>1000000</value>
      </property>
      <property>
        <name>splice.ring.bufferSize</name>
        <value>131072</value>
      </property>
      <property>
        <name>splice.splitBlockSize</name>
        <value>67108864</value>
      </property>
      <property>
        <name>splice.timestamp_server.clientWaitTime</name>
        <value>120000</value>
      </property>
      <property>
        <name>splice.txn.activeTxns.cacheSize</name>
        <value>10240</value>
      </property>
      <property>
        <name>splice.txn.completedTxns.concurrency</name>
        <value>128</value>
      </property>
      <property>
        <name>splice.txn.concurrencyLevel</name>
        <value>4096</value>
      </property>
      <property>
        <name>splice.timestamp_server.port</name>
        <value>16012</value>
      </property>
      <property>
        <name>splice.olap_server.port</name>
        <value>16040</value>
      </property>
      <property>
        <name>splice.writer.maxThreads</name>
        <value>20</value>
      </property>
      <property>
        <name>hbase.master.hfilecleaner.plugins</name>
        <value>org.apache.hadoop.hbase.master.cleaner.TimeToLiveHFileCleaner</value>
      </property>
      <property>
        <name>hbase.unsafe.stream.capability.enforce</name>
        <value>false</value>
      </property>
      <property>
        <name>hbase.master.info.bindAddress</name>
        <value>my-release-hmaster-0.my-release-hmaster.default.svc.cluster.local</value>
      </property>
      <property>
        <name>splice.olap.log4j.configuration</name>
        <value>file:/etc/hbase/conf/log4j.properties</value>
      </property>
    </configuration>

  hbase-env.sh: |
    #!/usr/bin/env bash
    #
    # *
    # * Licensed to the Apache Software Foundation (ASF) under one
    # * or more contributor license agreements.  See the NOTICE file
    # * distributed with this work for additional information
    # * regarding copyright ownership.  The ASF licenses this file
    # * to you under the Apache License, Version 2.0 (the
    # * "License"); you may not use this file except in compliance
    # * with the License.  You may obtain a copy of the License at
    # *
    # *     http://www.apache.org/licenses/LICENSE-2.0
    # *
    # * Unless required by applicable law or agreed to in writing, software
    # * distributed under the License is distributed on an "AS IS" BASIS,
    # * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # * See the License for the specific language governing permissions and
    # * limitations under the License.
    # *
    # Set environment variables here.
    # This script sets variables multiple times over the course of starting an hbase process,
    # so try to keep things idempotent unless you want to take an even deeper look
    # into the startup scripts (bin/hbase, etc.)
    # The java implementation to use.  Java 1.8+ required.
    # export JAVA_HOME=/usr/java/jdk1.8.0/
    # Extra Java CLASSPATH elements.  Optional.
    export HBASE_CLASSPATH_PREFIX=/etc/hbase/conf:$(echo /usr/lib/splicemachine/lib/*.jar | tr ' ' ':'):$(echo /usr/lib/spark/jars/* | tr ' ' ':')
    export HBASE_CLASSPATH=$(echo /usr/lib/hadoop/lib/*.jar | tr ' ' ':'):$(echo /usr/lib/mlmanager/lib/* | tr ' ' ':'):/usr/lib/hadoop/hadoop-azure-datalake.jar:$(echo /usr/lib/hive/lib/*.jar | tr ' ' ':')
    export OLAP_CLASSPATH="/etc/hadoop/conf:/etc/hbase/conf:/etc/spark/conf:/usr/lib/splicemachine/lib/*:/usr/lib/spark/jars/*:/usr/lib/hbase/lib/*:/usr/lib/hadoop-yarn/lib/*:/usr/lib/hadoop/lib/*"
    # The maximum amount of heap to use. Default is left to JVM default.
    # export HBASE_HEAPSIZE=1G
    # Uncomment below if you intend to use off heap cache. For example, to allocate 8G of
    # offheap, set the value to "8G".
    # export HBASE_OFFHEAPSIZE=1G
    # Extra Java runtime options.
    # Below are what we set by default.  May only work with SUN JVM.
    # For more on why as well as other possible settings,
    # see http://hbase.apache.org/book.html#performance
    # DBAAS-3180 - We are going to use +UseG1GC for region, and +UseConcMarkSweepGC for master
    # Cannot specify on a global scale and then override later.
    # export HBASE_OPTS="$HBASE_OPTS -XX:+UseConcMarkSweepGC"
    # Uncomment one of the below three options to enable java garbage collection logging for the server-side processes.
    # This enables basic gc logging to the .out file.
    # export SERVER_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps"
    # This enables basic gc logging to its own file.
    # If FILE-PATH is not replaced, the log file(.gc) would still be generated in the HBASE_LOG_DIR .
    # export SERVER_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:<FILE-PATH>"
    # This enables basic GC logging to its own file with automatic log rolling. Only applies to jdk 1.6.0_34+ and 1.7.0_2+.
    # If FILE-PATH is not replaced, the log file(.gc) would still be generated in the HBASE_LOG_DIR .
    # export SERVER_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:<FILE-PATH> -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=1 -XX:GCLogFileSize=512M"
    # Uncomment one of the below three options to enable java garbage collection logging for the client processes.
    # This enables basic gc logging to the .out file.
    # export CLIENT_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps"
    # This enables basic gc logging to its own file.
    # If FILE-PATH is not replaced, the log file(.gc) would still be generated in the HBASE_LOG_DIR .
    # export CLIENT_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:<FILE-PATH>"
    # This enables basic GC logging to its own file with automatic log rolling. Only applies to jdk 1.6.0_34+ and 1.7.0_2+.
    # If FILE-PATH is not replaced, the log file(.gc) would still be generated in the HBASE_LOG_DIR .
    # export CLIENT_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:<FILE-PATH> -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=1 -XX:GCLogFileSize=512M"
    # See the package documentation for org.apache.hadoop.hbase.io.hfile for other configurations
    # needed setting up off-heap block caching.
    # FOR Splice Machine
    # build these out in a clear manner
    SPLICE_HBASE_REGIONSERVER_OPTS=""
    SPLICE_HBASE_REGIONSERVER_OPTS="$SPLICE_HBASE_REGIONSERVER_OPTS -Xms20g -Xmx20g -XX:+HeapDumpOnOutOfMemoryError -XX:MaxDirectMemorySize=2g -XX:+AlwaysPreTouch -XX:MaxNewSize=2g -XX:InitiatingHeapOccupancyPercent=60 -XX:+UseG1GC -XX:ParallelGCThreads=24 -XX:+ParallelRefProcEnabled -XX:MaxGCPauseMillis=5000"
    SPLICE_HBASE_REGIONSERVER_OPTS="$SPLICE_HBASE_REGIONSERVER_OPTS -Dderby.drda.logConnections=true"
    SPLICE_HBASE_REGIONSERVER_OPTS="$SPLICE_HBASE_REGIONSERVER_OPTS -Dderby.drda.timeSlice=0"
    SPLICE_HBASE_REGIONSERVER_OPTS="$SPLICE_HBASE_REGIONSERVER_OPTS -Dlog4j.debug=true"
    SPLICE_HBASE_MASTER_OPTS=""
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Xms4g -Xmx4g -XX:+HeapDumpOnOutOfMemoryError -XX:MaxDirectMemorySize=2g -XX:+AlwaysPreTouch -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dlog4j.debug=true"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.olap_server.external=true"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.olap_server.deployment.mode=KUBERNETES"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.kafka.bootstrapServers=my-release-kafka-headless.default.svc.cluster.local:9092"
    # DBAAS-1018: escape these quotes exactly once
    export EXTRA_QUOTE_OPTS="-Dsplice.spark.executor.extraJavaOptions=\"-Dlog4j.debug=true -Dlog4j.configuration=log4j.properties -XX:MaxDirectMemorySize=8g\""
    # Uncomment and adjust to enable JMX exporting
    # See jmxremote.password and jmxremote.access in $JRE_HOME/lib/management to configure remote password access.
    # More details at: http://java.sun.com/javase/6/docs/technotes/guides/management/agent.html
    # NOTE: HBase provides an alternative JMX implementation to fix the random ports issue, please see JMX
    # section in HBase Reference Guide for instructions.
    # export HBASE_THRIFT_OPTS="$HBASE_THRIFT_OPTS $HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10103"
    # export HBASE_ZOOKEEPER_OPTS="$HBASE_ZOOKEEPER_OPTS $HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10104"
    # export HBASE_REST_OPTS="$HBASE_REST_OPTS $HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10105"
    # File naming hosts on which HRegionServers will run.  $HBASE_HOME/conf/regionservers by default.
    # export HBASE_REGIONSERVERS=${HBASE_HOME}/conf/regionservers
    # Uncomment and adjust to keep all the Region Server pages mapped to be memory resident
    #HBASE_REGIONSERVER_MLOCK=true
    #HBASE_REGIONSERVER_UID="hbase"
    # File naming hosts on which backup HMaster will run.  $HBASE_HOME/conf/backup-masters by default.
    # export HBASE_BACKUP_MASTERS=${HBASE_HOME}/conf/backup-masters
    # Extra ssh options.  Empty by default.
    # export HBASE_SSH_OPTS="-o ConnectTimeout=1 -o SendEnv=HBASE_CONF_DIR"
    # Where log files are stored.  $HBASE_HOME/logs by default.
    export HBASE_LOG_DIR=/var/log/hbase
    # Enable remote JDWP debugging of major HBase processes. Meant for Core Developers
    # export HBASE_MASTER_OPTS="$HBASE_MASTER_OPTS -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8070"
    # export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8071"
    # export HBASE_THRIFT_OPTS="$HBASE_THRIFT_OPTS -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8072"
    # export HBASE_ZOOKEEPER_OPTS="$HBASE_ZOOKEEPER_OPTS -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8073"
    # A string representing this instance of hbase. $USER by default.
    export HBASE_IDENT_STRING=hbase
    # The scheduling priority for daemon processes.  See 'man nice'.
    # export HBASE_NICENESS=10
    # The directory where pid files are stored. /tmp by default.
    # export HBASE_PID_DIR=/var/hbase/pids
    # Seconds to sleep between slave commands.  Unset by default.  This
    # can be useful in large clusters, where, e.g., slave rsyncs can
    # otherwise arrive faster than the master can service them.
    # export HBASE_SLAVE_SLEEP=0.1
    # Tell HBase whether it should manage it's own instance of ZooKeeper or not.
    # export HBASE_MANAGES_ZK=true
    # The default log rolling policy is RFA, where the log file is rolled as per the size defined for the
    # RFA appender. Please refer to the log4j.properties file to see more details on this appender.
    # In case one needs to do log rolling on a date change, one should set the environment property
    # HBASE_ROOT_LOGGER to "<DESIRED_LOG LEVEL>,DRFA".
    # For example:
    # HBASE_ROOT_LOGGER=DEBUG,DRFA
    # The reason for changing default to RFA is to avoid the boundary case of filling out disk space as
    # DRFA doesn't put any cap on the log size. Please refer to HBase-5655 for more context.
    
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.enabled=true"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.submit.deployMode=client"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.app.name=my-release-olap-spark"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.master=k8s://https://kubernetes.default.svc.cluster.local:443"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.ui.port=4040"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.acls.enable=true"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.admin.acls=*"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.admin.acls.groups=*"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.logConf=true"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.ui.filters=org.apache.shiro.web.servlet.IniShiroFilter"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.driver.maxResultSize=2g"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.driver.cores=2"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.executor.instances=4"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.dynamicAllocation.enabled=false"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.dynamicAllocation.executorIdleTimeout=120"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.dynamicAllocation.cachedExecutorIdleTimeout=120"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.dynamicAllocation.minExecutors=1"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.dynamicAllocation.maxExecutors=4"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.io.compression.lz4.blockSize=32k"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kryo.referenceTracking=false"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kryo.registrator=com.splicemachine.derby.impl.SpliceSparkKryoRegistrator"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kryoserializer.buffer.max=512m"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kryoserializer.buffer=4m"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.locality.wait=0"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.memory.fraction=0.5"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.scheduler.mode=FAIR"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.serializer=org.apache.spark.serializer.KryoSerializer"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.shuffle.service.enabled=false"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.shuffle.io.maxRetries=30"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.shuffle.io.retryWait=10"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.sql.shuffle.partitions=1200"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.driver.extraLibraryPath=/usr/lib/hadoop/lib/native:/usr/lib/hbase/lib/native/Linux-amd64-64"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.driver.extraClassPath=/etc/hbase/conf:/usr/lib/mlmanager/lib/*"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.driver.extraJavaOptions=\"-Dlog4j.debug=true -Dlog4j.configuration=file:/etc/hbase/conf/log4j.properties\""
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.executor.extraLibraryPath=/usr/lib/hadoop/lib/native:/usr/lib/hbase/lib/native/Linux-amd64-64"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.executor.extraClassPath=/etc/spark/conf:/usr/lib/splicemachine/lib/*:/usr/lib/spark/jars/*:/usr/lib/hbase/lib/*:/usr/lib/hadoop/lib/*:/usr/lib/hadoop/hadoop-azure-datalake.jar:/usr/lib/hive/lib/*:/usr/lib/mlmanager/lib/*"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.ui.retainedJobs=150"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.ui.retainedStages=250"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.worker.ui.retainedExecutors=100"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.worker.ui.retainedDrivers=100"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.streaming.ui.retainedBatches=100"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.executor.cores=6"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.executor.memory=20480m"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.executor.memoryOverhead=7168m"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.network.timeout=120s"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.scheduler.minRegisteredResourcesRatio=0"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.scheduler.maxRegisteredResourcesWaitingTime=30s"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.executor.hearbeatInterval=10s"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dspark.compaction.reserved.slots=4"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.local.dir=/spark/tmp0,/spark/tmp1"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.files=/tmp/spark-config/hbase-site.xml,/tmp/hdfs-config/hdfs-site.xml,/tmp/hdfs-config/core-site.xml"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dlog4j.configuration=file:/etc/hbase/conf/log4j.properties"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.namespace=default"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.container.image=docker.io/splicemachine/sm_k8_spark-3.0.0:0.0.64"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.container.image.pullPolicy=IfNotPresent"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.authenticate.driver.serviceAccountName=spark"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.driverEnv.SPARK_CONF_DIR=/etc/spark/conf"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.driverEnv.HADOOP_CONF_DIR=/etc/hadoop/conf"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.driverEnv.HBASE_CONF_DIR=/etc/hbase/conf"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.executor.label.app=hbase"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.executor.label.release=my-release"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.executorEnv.RELEASE_NAME=my-release"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.executorEnv.FRAMEWORKID=splicedb-dev1"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.executorEnv.JAVASCOPE=true"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.executorEnv.ZOOKEEPER_QUORUM=my-release-zookeeper-1.my-release-zookeeper-headless.default.svc.cluster.local:2181,my-release-zookeeper-2.my-release-zookeeper-headless.default.svc.cluster.local:2181,my-release-zookeeper-0.my-release-zookeeper-headless.default.svc.cluster.local:2181"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.executorEnv.RELEASE_NAME=my-release"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.executorEnv.FRAMEWORKID=splicedb-dev1"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.executorEnv.JAVASCOPE=true"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.executorEnv.ZOOKEEPER_QUORUM=my-release-zookeeper-1.my-release-zookeeper-headless.default.svc.cluster.local:2181,my-release-zookeeper-2.my-release-zookeeper-headless.default.svc.cluster.local:2181,my-release-zookeeper-0.my-release-zookeeper-headless.default.svc.cluster.local:2181"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.executorEnv.TZ=UTC"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.executorEnv.TZ=UTC"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.executor.label.component=sparkexec"
    
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.namespace=default"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.local.dir.tmpfs=false"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.maxRemoteBlockSizeFetchToMem=134217728"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.executor.request.cores=6"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.kubernetes.executor.request.memory=27648m"
    SPLICE_HBASE_MASTER_OPTS="$SPLICE_HBASE_MASTER_OPTS -Dsplice.spark.io.compression.codec=snappy"
    export HBASE_JMX_BASE="-Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false"
    export HBASE_MASTER_OPTS="$HBASE_MASTER_OPTS $HBASE_JMX_BASE $SPLICE_HBASE_MASTER_OPTS -Dcom.sun.management.jmxremote.port=10101"
    export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS $HBASE_JMX_BASE $SPLICE_HBASE_REGIONSERVER_OPTS -Dcom.sun.management.jmxremote.port=10102"
    


  fairscheduler.xml: |
    <?xml version="1.0"?>
    <allocations>
        <pool name="import">
            <schedulingMode>FAIR</schedulingMode>
            <weight>10</weight>
            <minShare>0</minShare>
        </pool>
        <pool name="query">
            <schedulingMode>FAIR</schedulingMode>
            <weight>20</weight>
            <minShare>0</minShare>
        </pool>
        <pool name="admin">
            <schedulingMode>FAIR</schedulingMode>
            <weight>1</weight>
            <minShare>0</minShare>
        </pool>
        <pool name="compaction">
            <schedulingMode>FAIR</schedulingMode>
            <weight>15</weight>
            <minShare>2</minShare>
        </pool>
        <pool name="urgent">
            <schedulingMode>FAIR</schedulingMode>
            <weight>1000</weight>
            <minShare>0</minShare>
        </pool>
    </allocations>
    

  log4j.properties: |
    status = error
    name = PropertiesConfig
    property.filename = /var/log/hbase/hbase.log
    filters = threshold
    filter.threshold.type = ThresholdFilter
    filter.threshold.level = info
    log4j.rootLogger=INFO,RFA,RFAConsole
    log4j.logger.splice-derby=INFO, spliceDerby,spliceDerbyConsole
    log4j.additivity.splice-derby=false
    log4j.logger.org.mortbay.jetty=INFO, jettyLog,jettyLogConsole
    # Logging Threshold
    log4j.threshold=ALL
    #
    # Rolling File Appender
    #
    hbase.log.maxfilesize=500MB
    hbase.log.maxbackupindex=8
    log4j.appender.RFA=org.apache.log4j.RollingFileAppender
    log4j.appender.RFA.File=/var/log/hbase/hbase.log
    log4j.appender.RFA.MaxFileSize=${hbase.log.maxfilesize}
    log4j.appender.RFA.MaxBackupIndex=${hbase.log.maxbackupindex}
    log4j.appender.RFA.layout=org.apache.log4j.PatternLayout
    log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [{{TASK_NAME}}] [{{FRAMEWORK_NAME}}] [hbase] [%t] [%c{2}]: %m%n
    log4j.appender.RFAConsole=org.apache.log4j.ConsoleAppender
    log4j.appender.RFAConsole.target=System.err
    log4j.appender.RFAConsole.layout=org.apache.log4j.PatternLayout
    log4j.appender.RFAConsole.layout.ConversionPattern=%d{ISO8601} %-5p [{{TASK_NAME}}] [{{FRAMEWORK_NAME}}] [hbase] [%t] [%c{2}]: %m%n
    #
    # Security audit appender
    #
    hbase.security.log.file=SecurityAuth.audit
    hbase.security.log.maxfilesize=256MB
    hbase.security.log.maxbackupindex=5
    log4j.appender.RFAS=org.apache.log4j.RollingFileAppender
    log4j.appender.RFAS.File=/var/log/hbase/${hbase.security.log.file}
    log4j.appender.RFAS.MaxFileSize=${hbase.security.log.maxfilesize}
    log4j.appender.RFAS.MaxBackupIndex=${hbase.security.log.maxbackupindex}
    log4j.appender.RFAS.layout=org.apache.log4j.PatternLayout
    log4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %-5p [{{TASK_NAME}}] [{{FRAMEWORK_NAME}}] [hbase-security] [%t] [%c{2}]: %m%n
    log4j.appender.RFASConsole=org.apache.log4j.ConsoleAppender
    log4j.appender.RFASConsole.target=System.err
    log4j.appender.RFASConsole.layout=org.apache.log4j.PatternLayout
    log4j.appender.RFASConsole.layout.ConversionPattern=%d{ISO8601} %-5p [{{TASK_NAME}}] [{{FRAMEWORK_NAME}}] [hbase-security] [%t] [%c{2}]: %m%n
    log4j.category.SecurityLogger=${hbase.security.logger}
    log4j.additivity.SecurityLogger=false
    #
    # spliceDerby
    # Add spliceDerby to the rootlogger above if you want to use this
    #
    log4j.appender.spliceDerby=org.apache.log4j.RollingFileAppender
    log4j.appender.spliceDerby.File=/var/log/hbase/splice-derby.log
    log4j.appender.spliceDerby.MaxFileSize=${hbase.log.maxfilesize}
    log4j.appender.spliceDerby.MaxBackupIndex=${hbase.log.maxbackupindex}
    log4j.appender.spliceDerby.layout=org.apache.log4j.PatternLayout
    log4j.appender.spliceDerby.layout.ConversionPattern=%d{ISO8601} %-5p [{{TASK_NAME}}] [{{FRAMEWORK_NAME}}] [splicederby] [%t] [%c{2}]: %m%n
    log4j.appender.spliceDerbyConsole=org.apache.log4j.ConsoleAppender
    log4j.appender.spliceDerbyConsole.target=System.err
    log4j.appender.spliceDerbyConsole.layout=org.apache.log4j.PatternLayout
    log4j.appender.spliceDerbyConsole.layout.ConversionPattern=%d{ISO8601} %-5p [{{TASK_NAME}}] [{{FRAMEWORK_NAME}}] [splicederby] [%t] [%c{2}]: %m%n
    #
    # spliceDerby
    # Add spliceDerby to the rootlogger above if you want to use this
    #
    log4j.appender.jettyLog=org.apache.log4j.RollingFileAppender
    log4j.appender.jettyLog.File=/var/log/hbase/mortbay-jetty.log
    log4j.appender.jettyLog.MaxFileSize=${hbase.log.maxfilesize}
    log4j.appender.jettyLog.MaxBackupIndex=${hbase.log.maxbackupindex}
    log4j.appender.jettyLog.layout=org.apache.log4j.PatternLayout
    log4j.appender.jettyLog.layout.ConversionPattern=%d{ISO8601} %-5p [{{TASK_NAME}}] [{{FRAMEWORK_NAME}}] [jetty] [%t] [%c{2}]: %m%n
    log4j.appender.jettyLogConsole=org.apache.log4j.ConsoleAppender
    log4j.appender.jettyLogConsole.target=System.err
    log4j.appender.jettyLogConsole.layout=org.apache.log4j.PatternLayout
    log4j.appender.jettyLogConsole.layout.ConversionPattern=%d{ISO8601} %-5p [{{TASK_NAME}}] [{{FRAMEWORK_NAME}}] [jetty] [%t] [%c{2}]: %m%n
    # Custom Logging levels
    log4j.logger.org.apache.zookeeper=${hbase.log.level}
    log4j.logger.org.apache.hadoop.hbase=${hbase.log.level}
    log4j.logger.org.apache.hadoop.hbase.zookeeper.ZKUtil=${hbase.log.level}
    log4j.logger.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher=${hbase.log.level}
    # Splice Machine
    log4j.logger.org.apache.hadoop.hbase.wal=ERROR
    log4j.logger.org.apache.hadoop.hbase.master=INFO
    #log4j.logger.org.apache.hadoop.hbase.regionserver=TRACE
    log4j.logger.org.apache.hadoop.hbase.zookeeper.ZKTableStateManager=WARN
    log4j.logger.org.apache.hadoop.hbase.io.hfile.CacheConfig=WARN
    log4j.logger.org.apache.hadoop.hbase.MetaTableAccessor=WARN
    log4j.logger.org.apache.hadoop.hbase.coprocessor.CoprocessorHost=WARN
    log4j.logger.org.apache.spark.ContextCleaner=WARN
    log4j.logger.org.apache.spark.scheduler=WARN
    log4j.logger.org.apache.spark.storage=WARN
    log4j.logger.org.apache.spark.SparkContext=WARN
    log4j.logger.org.apache.spark.executor=WARN
    log4j.logger.org.apache.spark.rdd.NewHadoopRDD=WARN
    log4j.logger.com.splicemachine.stream=WARN
    log4j.logger.org.apache.hadoop.metrics2.impl.MetricsConfig=WARN
    log4j.logger.org.apache.hadoop.metrics2.impl.MetricsSinkAdapter=WARN
    log4j.logger.org.apache.hadoop.metrics2.impl.MetricsSystemImpl=WARN
    

  shiro.ini: |

    [main]
    # Custom Splice Machine Realm
    spliceRealm = com.splicemachine.shiro.SpliceDatabaseRealm
    securityManager.realm = $spliceRealm
    sessionManager = org.apache.shiro.web.session.mgt.DefaultWebSessionManager
    spliceRealm.databaseName = splicedb
    spliceRealm.serverName = my-release-hregion.default.svc.cluster.local
    spliceRealm.serverPort = 1527
    securityManager.sessionManager = $sessionManager
    # 86,400,000 milliseconds = 24 hour
    securityManager.sessionManager.globalSessionTimeout = 86400000
    [urls]
    /** = authcBasic
---
# Source: splice-helm/charts/hbase/templates/spark-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-spark-config
  labels:
    app: my-release-hbase
    chart: hbase-0.0.1
    release: my-release
    component: olap
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
data:
  hbase-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <!--
    /**
    *
    * Licensed to the Apache Software Foundation (ASF) under one
    * or more contributor license agreements.  See the NOTICE file
    * distributed with this work for additional information
    * regarding copyright ownership.  The ASF licenses this file
    * to you under the Apache License, Version 2.0 (the
    * "License"); you may not use this file except in compliance
    * with the License.  You may obtain a copy of the License at
    *
    *     http://www.apache.org/licenses/LICENSE-2.0
    *
    * Unless required by applicable law or agreed to in writing, software
    * distributed under the License is distributed on an "AS IS" BASIS,
    * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    * See the License for the specific language governing permissions and
    * limitations under the License.
    */
    -->
    <configuration>
      <property>
          <name>presto.s3.staging-directory</name>
          <value>/spark/tmp0,/spark/tmp1</value>
      </property>
      <property>
          <name>splice.olap_server.external</name>
          <value>true</value>
      </property>
      <property>
          <name>splice.olap_server.deployment.mode</name>
          <value>KUBERNETES</value>
      </property>
      <property>
          <name>presto.s3.socket-timeout</name>
          <value>120s</value>
      </property>
      <property>
          <name>hbase.balancer.period</name>
          <value>60000</value>
      </property>
      <property>
          <name>hbase.bulkload.staging.dir</name>
          <value>/tmp/splicedb-staging</value>
      </property>
      <property>
          <name>hbase.mapreduce.bulkload.max.hfiles.perRegion.perFamily</name>
          <value>1024</value>
      </property>
      <property>
          <name>hbase.client.ipc.pool.size</name>
          <value>10</value>
      </property>
      <property>
          <name>hbase.client.keyvalue.maxsize</name>
          <value>10485760</value>
      </property>
      <property>
          <name>hbase.client.max.perregion.tasks</name>
          <value>100</value>
      </property>
      <property>
          <name>hbase.client.pause</name>
          <value>100</value>
      </property>
      <property>
          <name>hbase.client.primaryCallTimeout.get</name>
          <value>10</value>
      </property>
      <property>
          <name>hbase.client.primaryCallTimeout.multiget</name>
          <value>10</value>
      </property>
      <property>
          <name>hbase.client.retries.number</name>
          <value>100</value>
      </property>
      <property>
          <name>hbase.client.scanner.caching</name>
          <value>1000</value>
      </property>
      <property>
          <name>hbase.client.scanner.timeout.period</name>
          <value>1200000</value>
      </property>
      <property>
          <name>hbase.client.write.buffer</name>
          <value>2097152</value>
      </property>
      <property>
          <name>hbase.cluster.distributed</name>
          <value>true</value>
      </property>
      <property>
          <name>dfs.client.read.shortcircuit.buffer.size</name>
          <value>131072</value>
      </property>
      <property>
          <name>hbase.hash.type</name>
          <value>murmur</value>
      </property>
      <property>
          <name>hfile.block.bloom.cacheonwrite</name>
          <value>true</value>
      </property>
      <property>
          <name>hfile.block.cache.size</name>
          <value>0.01</value>
      </property>
      <property>
          <name>hbase.hregion.majorcompactio</name>
          <value>604800000</value>
      </property>
      <property>
          <name>hbase.hregion.majorcompaction.jitter</name>
          <value>0.5</value>
      </property>
      <property>
          <name>hbase.hregion.max.filesize</name>
          <value>1073741824</value>
      </property>
      <property>
          <name>hbase.hregion.memstore.block.multiplier</name>
          <value>4</value>
      </property>
      <property>
          <name>hbase.hregion.memstore.flush.size</name>
          <value>134217728</value>
      </property>
      <property>
          <name>hbase.hregion.memstore.mslab.chunksize</name>
          <value>2097152</value>
      </property>
      <property>
          <name>hbase.hregion.memstore.mslab.enabled</name>
          <value>true</value>
      </property>
      <property>
          <name>hbase.hregion.memstore.mslab.max.allocation</name>
          <value>262144</value>
      </property>
      <property>
          <name>hbase.hregion.preclose.flush.size</name>
          <value>5242880</value>
      </property>
      <property>
          <name>hbase.hstore.blockingStoreFiles</name>
          <value>20</value>
      </property>
      <property>
          <name>hbase.hstore.blockingWaitTime</name>
          <value>90000</value>
      </property>
      <property>
          <name>hbase.hstore.compactionThreshold</name>
          <value>5</value>
      </property>
      <property>
          <name>hbase.hstore.compaction.max</name>
          <value>7</value>
      </property>
      <property>
          <name>hbase.hstore.compaction.max.size</name>
          <value>260046848</value>
      </property>
      <property>
          <name>hbase.hstore.compaction.min</name>
          <value>3</value>
      </property>
      <property>
          <name>hbase.hstore.compaction.min.size</name>
          <value>136314880</value>
      </property>
      <property>
          <name>hbase.hstore.defaultengine.compactionpolicy.class</name>
          <value>com.splicemachine.compactions.SpliceDefaultCompactionPolicy</value>
      </property>
      <property>
          <name>hbase.hstore.defaultengine.compactor.class</name>
          <value>com.splicemachine.compactions.SpliceDefaultCompactor</value>
      </property>
      <property>
          <name>hbase.htable.threads.max</name>
          <value>96</value>
      </property>
      <property>
          <name>io.storefile.bloom.error.rate</name>
          <value>0.005</value>
      </property>
      <property>
          <name>hbase.ipc.client.allowsInterrupt</name>
          <value>true</value>
      </property>
      <property>
          <name>hbase.ipc.server.read.threadpool.size</name>
          <value>10</value>
      </property>
      <property>
          <name>hbase.ipc.warn.response.size</name>
          <value>-1</value>
      </property>
      <property>
          <name>hbase.ipc.warn.response.time</name>
          <value>-1</value>
      </property>
      <property>
          <name>hbase.master.executor.closeregion.threads</name>
          <value>5</value>
      </property>
      <property>
          <name>hbase.master.executor.openregion.threads</name>
          <value>5</value>
      </property>
      <property>
          <name>hbase.master.executor.serverops.threads</name>
          <value>12</value>
      </property>
      <property>
          <name>hbase.master.handler.count</name>
          <value>25</value>
      </property>
      <property>
          <name>hbase.master.info.port</name>
          <value>16010</value>
      </property>
      <property>
          <name>hbase.master.loadbalance.bytable</name>
          <value>true</value>
      </property>
      <property>
          <name>hbase.master.logcleaner.ttl</name>
          <value>60000</value>
      </property>
      <property>
          <name>hbase.master.namespace.init.timeout</name>
          <value>2400000</value>
      </property>
      <property>
          <name>hbase.master.port</name>
          <value>16000</value>
      </property>
      <property>
          <name>hbase.mvcc.impl</name>
          <value>org.apache.hadoop.hbase.regionserver.SIMultiVersionConsistencyControl</value>
      </property>
      <property>
          <name>hbase.region.replica.replication.enabled</name>
          <value>false</value>
      </property>
      <property>
          <name>hbase.regions.slop</name>
          <value>0</value>
      </property>
      <property>
          <name>hbase.regionserver.executor.openregion.threads</name>
          <value>50</value>
      </property>
      <property>
          <name>hbase.regionserver.global.memstore.size</name>
          <value>0.25</value>
      </property>
      <property>
          <name>hbase.regionserver.global.memstore.size.lower.limit</name>
          <value>0.9</value>
      </property>
      <property>
          <name>hbase.regionserver.handler.count</name>
          <value>400</value>
      </property>
      <property>
          <name>hbase.regionserver.hlog.blocksize</name>
          <value>134217728</value>
      </property>
      <property>
          <name>hbase.regionserver.info.port</name>
          <value>16030</value>
      </property>
      <property>
          <name>hbase.regionserver.logroll.period</name>
          <value>3600000</value>
      </property>
      <property>
          <name>hbase.regionserver.maxlogs</name>
          <value>48</value>
      </property>
      <property>
          <name>hbase.regionserver.metahandler.count</name>
          <value>200</value>
      </property>
      <property>
          <name>hbase.regionserver.msginterval</name>
          <value>3000</value>
      </property>
      <property>
          <name>hbase.regionserver.nbreservationblocks</name>
          <value>4</value>
      </property>
      <property>
          <name>hbase.regionserver.optionallogflushinterval</name>
          <value>1000</value>
      </property>
      <property>
          <name>hbase.regionserver.debug.enable</name>
          <value>false</value>
      </property>
      <property>
          <name>hbase.regionserver.debug.port</name>
          <value>4000</value>
      </property>
      <property>
          <name>hbase.regionserver.port</name>
          <value>16020</value>
      </property>
      <property>
          <name>hbase.regionserver.regionSplitLimit</name>
          <value>2147483647</value>
      </property>
      <property>
          <name>hbaseregionserverThreadCompactionLarge</name>
          <value>1</value>
      </property>
      <property>
          <name>hbase.regionserver.thread.compaction.small</name>
          <value>4</value>
      </property>
      <property>
          <name>hbase.regionserver.wal.enablecompression</name>
          <value>true</value>
      </property>
      <property>
          <name>hbase.rootdir</name>
          <value>/splicedb/hbase</value>
      </property>
      <property>
          <name>hbase.fs.tmp.dir</name>
          <value>/tmp/splicedb-staging</value>
      </property>
      <property>
          <name>hbase.row.level.authorization</name>
          <value>false</value>
      </property>
      <property>
          <name>hbase.rpc.protection</name>
          <value>authentication</value>
      </property>
      <property>
          <name>hbase.rpc.timeout</name>
          <value>1200000</value>
      </property>
      <property>
          <name>hbase.security.authentication</name>
          <value>simple</value>
      </property>
      <property>
          <name>hbase.security.authorization</name>
          <value>false</value>
      </property>
      <property>
          <name>hbase.server.thread.wakefrequency</name>
          <value>10000</value>
      </property>
      <property>
          <name>hbase.snapshot.enabled</name>
          <value>true</value>
      </property>
      <property>
          <name>hbase.snapshot.master.timeoutMillis</name>
          <value>60000</value>
      </property>
      <property>
          <name>hbase.snapshot.master.timeout.millis</name>
          <value>60000</value>
      </property>
      <property>
          <name>hbase.snapshot.region.timeout</name>
          <value>60000</value>
      </property>
      <property>
          <name>hbase.splitlog.manager.timeout</name>
          <value>300000</value>
      </property>
      <property>
          <name>hbase.status.multicast.port</name>
          <value>16100</value>
      </property>
      <property>
          <name>hbase.superuser</name>
          <value>root</value>
      </property>
      <property>
          <name>hbase.wal.disruptor.batchhbase.wal.disruptor.batch</name>
          <value>true</value>
      </property>
      <property>
          <name>hbase.wal.provider</name>
          <value>multiwal</value>
      </property>
      <property>
          <name>hbase.wal.regiongrouping.numgroups</name>
          <value>16</value>
      </property>
      <property>
          <name>hbase.wal.storage.policy</name>
          <value>NONE</value>
      </property>
      <property>
          <name>hbase.zookeeper.property.tickTime</name>
          <value>6000</value>
      </property>
      <property>
          <name>zookeeper.session.timeout</name>
          <value>1200000</value>
      </property>
      <property>
          <name>zookeeper.znode.parent</name>
          <value>/hbase</value>
      </property>
      <property>
          <name>zookeeper.znode.rootserver</name>
          <value>root-region-server</value>
      </property>
      <property>
          <name>hbase.hstore.defaultengine.compactor.class</name>
          <value>com.splicemachine.compactions.SpliceDefaultCompactor</value>
      </property>
      <property>
          <name>hbase.hstore.defaultengine.compactionpolicy.class</name>
          <value>com.splicemachine.compactions.SpliceDefaultCompactionPolicy</value>
      </property>
      <property>
          <name>splice.debug.logStatementContext</name>
          <value>true</value>
      </property>
      <property>
          <name>splice.authentication</name>
          <value>NATIVE</value>
      </property>
      <property>
          <name>splice.authentication.ldap.server</name>
          <value></value>
      </property>
      <property>
          <name>splice.authentication.ldap.searchAuthDN</name>
          <value></value>
      </property>
      <property>
          <name>splice.authentication.ldap.searchAuth.password</name>
          <value></value>
      </property>
      <property>
          <name>splice.authentication.ldap.searchBase</name>
          <value></value>
      </property>
      <property>
          <name>splice.authentication.ldap.searchFilter</name>
          <value></value>
      </property>
      <property>
          <name>hbase.coprocessor.abortonerror</name>
          <value>true</value>
      </property>
      <property>
          <name>splicemachine.enterprise.key</name>
          <value></value>
      </property>
      <property>
          <name>hbase.coprocessor.master.classes</name>
          <value>com.splicemachine.hbase.SpliceMasterObserver</value>
      </property>
      <property>
          <name>hbase.coprocessor.region.classes</name>
          <value>org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint,com.splicemachine.hbase.MemstoreAwareObserver,com.splicemachine.derby.hbase.SpliceIndexObserver,com.splicemachine.derby.hbase.SpliceIndexEndpoint,com.splicemachine.hbase.RegionSizeEndpoint,com.splicemachine.si.data.hbase.coprocessor.TxnLifecycleEndpoint,com.splicemachine.si.data.hbase.coprocessor.SIObserver,com.splicemachine.hbase.BackupEndpointObserver</value>
      </property>
      <property>
          <name>hbase.coprocessor.regionserver.classes</name>
          <value>com.splicemachine.hbase.RegionServerLifecycleObserver</value>
      </property>
      <property>
          <name>hbase.master.balancer.stochastic.regionCountCost</name>
          <value>1500</value>
      </property>
      <property>
          <name>hbase.rowlock.wait.duration</name>
          <value>0</value>
      </property>
      <property>
          <name>hbase.zookeeper.quorum</name>
          <value>my-release-zookeeper-1.my-release-zookeeper-headless.default.svc.cluster.local:2181,my-release-zookeeper-2.my-release-zookeeper-headless.default.svc.cluster.local:2181,my-release-zookeeper-0.my-release-zookeeper-headless.default.svc.cluster.local:2181</value>
      </property>
      <property>
          <name>splice.authentication.native.algorithm</name>
          <value>SHA-512</value>
      </property>
      <property>
          <name>splice.client.numConnections</name>
          <value>1</value>
      </property>
      <property>
          <name>splice.client.write.maxDependentWrites</name>
          <value>60000</value>
      </property>
      <property>
          <name>splice.client.write.maxIndependentWrites</name>
          <value>60000</value>
      </property>
      <property>
          <name>splice.compression</name>
          <value>snappy</value>
      </property>
      <property>
          <name>splice.debug.logStatementContext</name>
          <value>true</value>
      </property>
      <property>
          <name>splice.ignore.missing.transactions</name>
          <value>true</value>
      </property>
      <property>
          <name>splice.marshal.kryoPoolSize</name>
          <value>1100</value>
      </property>
      <property>
          <name>splice.olap.shuffle.partitions</name>
          <value>200</value>
      </property>
      <property>
          <name>splice.olap_server.clientWaitTime</name>
          <value>900000</value>
      </property>
      <property>
          <name>splice.optimizer.broadcastRegionRowThreshold</name>
          <value>1000000</value>
      </property>
      <property>
          <name>splice.ring.bufferSize</name>
          <value>131072</value>
      </property>
      <property>
          <name>splice.splitBlockSize</name>
          <value>67108864</value>
      </property>
      <property>
          <name>splice.timestamp_server.clientWaitTime</name>
          <value>120000</value>
      </property>
      <property>
          <name>splice.txn.activeTxns.cacheSize</name>
          <value>10240</value>
      </property>
      <property>
          <name>splice.txn.completedTxns.concurrency</name>
          <value>128</value>
      </property>
      <property>
          <name>splice.txn.concurrencyLevel</name>
          <value>4096</value>
      </property>
      <property>
          <name>splice.timestamp_server.port</name>
          <value>16012</value>
      </property>
      <property>
          <name>splice.olap_server.port</name>
          <value>16040</value>
      </property>
      <property>
          <name>splice.writer.maxThreads</name>
          <value>20</value>
      </property>
      <property>
          <name>hbase.master.hfilecleaner.plugins</name>
          <value>org.apache.hadoop.hbase.master.cleaner.TimeToLiveHFileCleaner</value>
      </property>
      <property>
          <name>hbase.unsafe.stream.capability.enforce</name>
          <value>false</value>
      </property>
      <property>
          <name>hbase.master.info.bindAddress</name>
          <value>my-release-olap-0.my-release-olap.default.svc.cluster.local</value>
      </property>
      <property>
          <name>hbase.rowlock.wait.duration</name>
          <value>20000</value>
      </property>
    </configuration>
---
# Source: splice-helm/charts/jupyterhub/templates/hub/configmap.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: hub-config
  labels:
    component: hub
    app: jupyterhub
    release: my-release
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
    chart: jupyterhub-0.8.2
    heritage: Helm
data:
  values.yaml: |
    Chart:
      Name: jupyterhub
      Version: 0.8.2
    Release:
      Name: my-release
      Namespace: default
      Service: Helm
    auth:
      admin:
        access: true
      dummy: {}
      ldap:
        dn:
          search: {}
          user: {}
        user: {}
      state:
        enabled: true
      type: dummy
      whitelist: {}
    cull:
      concurrency: 10
      enabled: true
      every: 600
      maxAge: 0
      timeout: 3600
      users: false
    custom: {}
    debug:
      enabled: false
    global:
      addEnvironmentToDNSNames: true
      aws:
        accessId: ""
        accessKey: ""
        buckets: []
      azure:
        clientId: ""
        clientSecret: ""
        storageAccount: []
        tenantId: ""
      baseImage:
        pullPolicy: IfNotPresent
        pullSecrets: regcred
        registry: docker.io
        repository: splicemachine/sm_k8_base
        tag: 0.0.2
      certificateName: dev.splicemachine-dev.io
      cloudprovider: none
      dnsPrefix: splicedb
      dnsProvider: none
      dnsprefix: test
      environmentName: dev1
      frameworkId: splicedb-dev1
      gcp:
        serviceAccount: {}
      haproxyPrivate: false
      hdfs:
        image:
          pullPolicy: IfNotPresent
          pullSecrets: regcred
          registry: docker.io
          repository: splicemachine/sm_k8_hdfs-3.0.0:0.0.17
      image:
        pullPolicy: IfNotPresent
      journalnodeQuorumSize: 3
      jscpEnabled: true
      mlflow:
        port:
          bobby: 2375
          dash: 5003
          mlflow: 5001
      mlmanager:
        password: admin
        user: mlmanager
      nodeSelector:
        core: core
        db: db
        enabled: false
        meta: meta
      nodeSelectorComposed:
        nodeSelector: {}
      oauthProvider: auth0
      spark:
        image:
          pullPolicy: IfNotPresent
          pullSecrets: regcred
          registry: docker.io
          repository: splicemachine/sm_k8_spark-3.0.0:0.0.64
      splice:
        password: admin
        user: splice
      timezoneFlag: UTC
      tls:
        crt: REPLACE_TLSCRT
        enabled: false
        key: REPLACE_TLSKEY
      zookeeper:
        configuration:
          clientPort: 2181
          leaderElectionPort: 3888
          maximumClientConnections: 0
          maximumSessionTimeout: 120000
          serverPort: 2888
        quorumSize: 3
    hub:
      allowNamedServers: false
      annotations: {}
      baseUrl: /splicejupyter/
      concurrentSpawnLimit: 64
      consecutiveFailureLimit: 5
      db:
        pvc:
          accessModes:
          - ReadWriteOnce
          annotations: {}
          selector: {}
          storage: 1Gi
        type: sqlite-pvc
      deploymentStrategy:
        type: Recreate
      extraConfig:
        AuthConfig: |
          import sys
          sys.path.append('/srv')
          from Authentication import SpliceAuthenticator
          c.JupyterHub.authenticator_class = SpliceAuthenticator
          c.Authenticator.enable_auth_state = True
          c.Authenticator.admin_users = {'splice'}
        UserConfig: |
          c.JupyterHub.active_server_limit=10
      extraContainers: []
      extraVolumeMounts: []
      extraVolumes: []
      fsGid: 1000
      image:
        name: splicemachine/sm_k8_jupyterhub
        tag: 0.0.14
      imagePullPolicy: IfNotPresent
      imagePullSecret:
        enabled: false
      labels: {}
      networkPolicy:
        egress:
        - to:
          - ipBlock:
              cidr: 0.0.0.0/0
        enabled: false
      nodeSelector: {}
      pdb:
        enabled: true
        maxUnavailable: 1
        minAvailable: 1
      resources:
        requests:
          cpu: 200m
          memory: 512Mi
      service:
        annotations: {}
        ports: {}
        type: ClusterIP
      services: {}
      uid: 1000
    scheduling:
      corePods:
        nodeAffinity:
          matchNodePurpose: prefer
      podPriority:
        defaultPriority: 0
        enabled: false
        globalDefault: false
        userPlaceholderPriority: -10
      userPlaceholder:
        enabled: true
        replicas: 0
      userPods:
        nodeAffinity:
          matchNodePurpose: prefer
      userScheduler:
        enabled: false
        image:
          name: gcr.io/google_containers/kube-scheduler-amd64
          tag: v1.11.2
        logLevel: 4
        nodeSelector: {}
        pdb:
          enabled: true
          maxUnavailable: 1
          minAvailable: 1
        replicas: 1
        resources:
          requests:
            cpu: 50m
            memory: 256Mi
    singleuser:
      nodeSelector: {}
      extraTolerations: []
      extraNodeAffinity:
        required: []
        preferred: []
      extraPodAffinity:
        required: []
        preferred: []
      extraPodAntiAffinity:
        required: []
        preferred: []
      networkTools:
        image:
          name: jupyterhub/k8s-network-tools
          tag: 0.8.2
      cloudMetadata:
        enabled: false
        ip: 169.254.169.254
      networkPolicy:
        enabled: false
        egress:
          # Required egress is handled by other rules so it's safe to modify this
          - to:
              - ipBlock:
                  cidr: 0.0.0.0/0
                  except:
                    - 169.254.169.254/32
      events: true
      extraAnnotations: {}
      extraLabels:
        hub.jupyter.org/network-access-hub: 'true'
        dnsprefix: "splicedb"
      extraEnv:
        JUPYTER_NOTEBOOK_VERSION: 0.0.19-cloud
        JUPYTER_SPARK_NODE_SELECTOR: db
        SPARK_DOCKER_IMAGE: splicemachine/sm_k8_spark-3.0.0:0.0.64
        SPARK_EXECUTOR_COUNT: "2"
        SPARK_OPTS: >-
          --deploy-mode=client
          --master k8s://https://kubernetes.default.svc.cluster.local:443
          --conf spark.executor.instances=2
          --conf spark.dynamicAllocation.enabled=false
          --conf spark.kubernetes.container.image=splicemachine/sm_k8_spark-3.0.0:0.0.64
          --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark
        JDBC_HOST: my-release-hregion.default.svc.cluster.local
        SPLICE_REL_NAME: "my-release"
        SPLICE_NAMESPACE: "default"
        MLFLOW_URL:  http://my-release-mlflow-0.my-release-mlflow-headless.default.svc.cluster.local:5001
        TZ: UTC
        JUPYTER_CONFIG_DIR: /splice/beakerx/conf
      lifecycleHooks:
        postStart:
          exec:
            command:
              - "/bin/sh"
              - "-c"
              - |
                
                sed -i -e"s|JUPYTER_HOST_VAR|$(hostname -i)|g" $SPARK_HOME/conf/spark-defaults.conf;
                sed -i -e"s|JUPYTER_HOST_NAME_VAR|$(hostname)|g" $SPARK_HOME/conf/spark-defaults.conf;
                sed -i -e"s|SPLICE_REL_NAME_VAR|$SPLICE_REL_NAME|g" $SPARK_HOME/conf/spark-defaults.conf;
                sed -i -e"s|SPLICE_NAMESPACE_VAR|$SPLICE_NAMESPACE|g" $SPARK_HOME/conf/spark-defaults.conf;
                sed -i -e"s|SPARK_DOCKER_IMAGE_VAR|$SPARK_DOCKER_IMAGE|g" $SPARK_HOME/conf/spark-defaults.conf;
                sed -i -e"s|SPARK_EXECUTOR_COUNT_VAR|$SPARK_EXECUTOR_COUNT|g" $SPARK_HOME/conf/spark-defaults.conf;
                sed -i -e"s|JUPYTER_SPARK_NODE_SELECTOR_VAR|$JUPYTER_SPARK_NODE_SELECTOR|g" $SPARK_HOME/conf/spark-defaults.conf;
                sed -i -e"s|TIME_ZONE_VAR|$TZ|g" $SPARK_HOME/conf/spark-defaults.conf;
                cp /tmp/jupyter_beakerx.json $JUPYTER_CONFIG_DIR/beakerx.json;
                
                sed -i -e"s|JUPYTER_HOST_VAR|$(hostname -i)|g" $JUPYTER_CONFIG_DIR/beakerx.json;
                sed -i -e"s|JUPYTER_HOST_NAME_VAR|$(hostname)|g" $JUPYTER_CONFIG_DIR/beakerx.json;
                sed -i -e"s|SPLICE_REL_NAME_VAR|$SPLICE_REL_NAME|g" $JUPYTER_CONFIG_DIR/beakerx.json;
                sed -i -e"s|SPLICE_NAMESPACE_VAR|$SPLICE_NAMESPACE|g" $JUPYTER_CONFIG_DIR/beakerx.json;
                sed -i -e"s|SPARK_DOCKER_IMAGE_VAR|$SPARK_DOCKER_IMAGE|g" $JUPYTER_CONFIG_DIR/beakerx.json;
                sed -i -e"s|SPARK_EXECUTOR_COUNT_VAR|$SPARK_EXECUTOR_COUNT|g" $JUPYTER_CONFIG_DIR/beakerx.json;
                sed -i -e"s|TIME_ZONE_VAR|$TZ|g" $JUPYTER_CONFIG_DIR/beakerx.json;
                sed -i -e"s|JUPYTER_SPARK_NODE_SELECTOR_VAR|$JUPYTER_SPARK_NODE_SELECTOR|g" $JUPYTER_CONFIG_DIR/beakerx.json;
                cp /tmp/hbase-config/* /etc/hbase/conf/;
                cp /tmp/hdfs-config/* /etc/hadoop/conf/;
                /tmp/copy_notebooks.sh;
                /tmp/start_monitor.sh;
                cp /tmp/notebook.json /splice/beakerx/conf/nbconfig/notebook.json;
      initContainers: []
      extraContainers: []
      uid: 1000
      fsGid: 100
      serviceAccountName: spark
      storage:
        type: dynamic
        extraLabels: {}
        extraVolumes:
          - name: hbase-config
            configMap:
              name: my-release-hbase-config
          - name: hdfs-config
            configMap:
              name: my-release-hadoop-config
        extraVolumeMounts:
          - name: hbase-config
            mountPath: /tmp/hbase-config
          - name: hdfs-config
            mountPath: /tmp/hdfs-config
        static:
          pvcName:
          subPath: '{username}'
        capacity: 10Gi
        homeMountPath: /home/jovyan
        dynamic:
          storageClass:
          pvcNameTemplate: claim-{username}{servername}
          volumeNameTemplate: volume-{username}{servername}
          storageAccessModes: [ReadWriteOnce]
      image:
        name: splicemachine/sm_k8_jupyter_allspark-3.0.0:0.0.54
        pullPolicy: IfNotPresent
      imagePullSecret:
        enabled: false
        registry:
        username:
        email:
        password:
      startTimeout: 300
      cpu:
        limit:
        guarantee:
      memory:
        limit:
        guarantee: 1G
      extraResource:
        limits: {}
        guarantees: {}
      cmd: jupyterhub-singleuser
      defaultUrl:
---
# Source: splice-helm/charts/kafka/templates/kafka-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-kafka-config
  labels:
    app: my-release-kafka
    chart: kafka-0.0.1
    release: my-release
    component: kafka-broker
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
data:
  server.properties: |
    # Licensed to the Apache Software Foundation (ASF) under one or more
    # contributor license agreements.  See the NOTICE file distributed with
    # this work for additional information regarding copyright ownership.
    # The ASF licenses this file to You under the Apache License, Version 2.0
    # (the "License"); you may not use this file except in compliance with
    # the License.  You may obtain a copy of the License at
    #
    #    http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.

    # see kafka.server.KafkaConfig for additional details and defaults

    ############################# Server Basics #############################

    # The id of the broker. This must be set to a unique integer for each broker.
    broker.id=REPLACE_BROKER_ID

    ############################# Socket Server Settings #############################

    # The address the socket server listens on. It will get the value returned from
    # java.net.InetAddress.getCanonicalHostName() if not configured.
    #   FORMAT:
    #     listeners = listener_name://host_name:port
    #   EXAMPLE:
    #     listeners = PLAINTEXT://your.host.name:9092
    #listeners=PLAINTEXT://:9092
    listeners=PLAINTEXT://0.0.0.0:9092,EXTERNAL://0.0.0.0:19092

    # Hostname and port the broker will advertise to producers and consumers. If not set,
    # it uses the value for "listeners" if configured.  Otherwise, it will use the value
    # returned from java.net.InetAddress.getCanonicalHostName().
    #advertised.listeners=PLAINTEXT://your.host.name:9092
    #advertised.listeners=PLAINTEXT://splicedb-kafka-0.splicedb-kafka-headless.test.svc.cluster.local:9092,EXTERNAL://kafka-0-test.nonprod-eks-dev4.eks.splicemachine-dev.io:19092
    advertised.listeners=REPLACE_ADVT_LISTENERS

    # Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details
    #listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
    #listener.security.protocol.map=REPLACE_LISTENER_PROTOCOL
    listener.security.protocol.map=PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT


    # The number of threads that the server uses for receiving requests from the network and sending responses to the network
    num.network.threads=3

    # The number of threads that the server uses for processing requests, which may include disk I/O
    num.io.threads=8

    # The send buffer (SO_SNDBUF) used by the socket server
    socket.send.buffer.bytes=102400

    # The receive buffer (SO_RCVBUF) used by the socket server
    socket.receive.buffer.bytes=102400

    # The maximum size of a request that the socket server will accept (protection against OOM)
    socket.request.max.bytes=104857600


    ############################# Log Basics #############################

    # A comma separated list of directories under which to store log files
    log.dirs=/opt/kafka/data/kafka-logs

    # The default number of log partitions per topic. More partitions allow greater
    # parallelism for consumption, but this will also result in more files across
    # the brokers.
    num.partitions=1

    # The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
    # This value is recommended to be increased for installations with data dirs located in RAID array.
    num.recovery.threads.per.data.dir=1

    ############################# Internal Topic Settings  #############################
    # The replication factor for the group metadata internal topics "__consumer_offsets" and "__transaction_state"
    # For anything other than development testing, a value greater than 1 is recommended for to ensure availability such as 3.
    offsets.topic.replication.factor=1
    transaction.state.log.replication.factor=1
    transaction.state.log.min.isr=1

    ############################# Log Flush Policy #############################

    # Messages are immediately written to the filesystem but by default we only fsync() to sync
    # the OS cache lazily. The following configurations control the flush of data to disk.
    # There are a few important trade-offs here:
    #    1. Durability: Unflushed data may be lost if you are not using replication.
    #    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.
    #    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to excessive seeks.
    # The settings below allow one to configure the flush policy to flush data after a period of time or
    # every N messages (or both). This can be done globally and overridden on a per-topic basis.

    # The number of messages to accept before forcing a flush of data to disk
    #log.flush.interval.messages=10000

    # The maximum amount of time a message can sit in a log before we force a flush
    #log.flush.interval.ms=1000

    ############################# Log Retention Policy #############################

    # The following configurations control the disposal of log segments. The policy can
    # be set to delete segments after a period of time, or after a given size has accumulated.
    # A segment will be deleted whenever *either* of these criteria are met. Deletion always happens
    # from the end of the log.

    # The minimum age of a log file to be eligible for deletion due to age
    log.retention.hours=168

    # A size-based retention policy for logs. Segments are pruned from the log unless the remaining
    # segments drop below log.retention.bytes. Functions independently of log.retention.hours.
    #log.retention.bytes=1073741824

    # The maximum size of a log segment file. When this size is reached a new log segment will be created.
    log.segment.bytes=1073741824

    # The interval at which log segments are checked to see if they can be deleted according
    # to the retention policies
    log.retention.check.interval.ms=300000

    ############################# Zookeeper #############################

    # Zookeeper connection string (see zookeeper docs for details).
    # This is a comma separated host:port pairs, each corresponding to a zk
    # server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".
    # You can also append an optional chroot string to the urls to specify the
    # root directory for all kafka znodes.
    zookeeper.connect=my-release-zookeeper-1.my-release-zookeeper-headless.default.svc.cluster.local:2181,my-release-zookeeper-2.my-release-zookeeper-headless.default.svc.cluster.local:2181,my-release-zookeeper-0.my-release-zookeeper-headless.default.svc.cluster.local:2181

    # Timeout in ms for connecting to zookeeper
    zookeeper.connection.timeout.ms=6000

    ############################# Group Coordinator Settings #############################

    # The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.
    # The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms.
    # The default value for this is 3 seconds.
    # We override this to 0 here as it makes for a better out-of-the-box experience for development and testing.
    # However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup.
    group.initial.rebalance.delay.ms=0

  log4j.properties: |
    kafka.logs.dir=/var/log/kafka
    log4j.rootLogger=INFO, stdout, kafkaAppender

    log4j.appender.stdout=org.apache.log4j.ConsoleAppender
    log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
    log4j.appender.stdout.layout.ConversionPattern=%d{ISO8601} %-5p [{{TASK_NAME}}] [{{FRAMEWORK_NAME}}] [kafka] [%t] [%c{2}]: %m%n

    log4j.appender.kafkaAppender=org.apache.log4j.DailyRollingFileAppender
    log4j.appender.kafkaAppender.DatePattern='.'yyyy-MM-dd-HH
    log4j.appender.kafkaAppender.File=${kafka.logs.dir}/server.log
    log4j.appender.kafkaAppender.layout=org.apache.log4j.PatternLayout
    log4j.appender.kafkaAppender.layout.ConversionPattern=%d{ISO8601} %-5p [{{TASK_NAME}}] [{{FRAMEWORK_NAME}}] [kafka-server] [%t] [%c{2}]: %m%n

    log4j.appender.stateChangeAppender=org.apache.log4j.DailyRollingFileAppender
    log4j.appender.stateChangeAppender.DatePattern='.'yyyy-MM-dd-HH
    log4j.appender.stateChangeAppender.File=${kafka.logs.dir}/state-change.log
    log4j.appender.stateChangeAppender.layout=org.apache.log4j.PatternLayout
    log4j.appender.stateChangeAppender.layout.ConversionPattern=%d{ISO8601} %-5p [{{TASK_NAME}}] [{{FRAMEWORK_NAME}}] [kafka-state-change] [%t] [%c{2}]: %m%n

    log4j.appender.requestAppender=org.apache.log4j.DailyRollingFileAppender
    log4j.appender.requestAppender.DatePattern='.'yyyy-MM-dd-HH
    log4j.appender.requestAppender.File=${kafka.logs.dir}/kafka-request.log
    log4j.appender.requestAppender.layout=org.apache.log4j.PatternLayout
    log4j.appender.requestAppender.layout.ConversionPattern=%d{ISO8601} %-5p [{{TASK_NAME}}] [{{FRAMEWORK_NAME}}] [kafka-request] [%t] [%c{2}]: %m%n

    log4j.appender.cleanerAppender=org.apache.log4j.DailyRollingFileAppender
    log4j.appender.cleanerAppender.DatePattern='.'yyyy-MM-dd-HH
    log4j.appender.cleanerAppender.File=${kafka.logs.dir}/log-cleaner.log
    log4j.appender.cleanerAppender.layout=org.apache.log4j.PatternLayout
    log4j.appender.cleanerAppender.layout.ConversionPattern=%d{ISO8601} %-5p [{{TASK_NAME}}] [{{FRAMEWORK_NAME}}] [kafka-log-cleaner] [%t] [%c{2}]: %m%n

    log4j.appender.controllerAppender=org.apache.log4j.DailyRollingFileAppender
    log4j.appender.controllerAppender.DatePattern='.'yyyy-MM-dd-HH
    log4j.appender.controllerAppender.File=${kafka.logs.dir}/controller.log
    log4j.appender.controllerAppender.layout=org.apache.log4j.PatternLayout
    log4j.appender.controllerAppender.layout.ConversionPattern=%d{ISO8601} %-5p [{{TASK_NAME}}] [{{FRAMEWORK_NAME}}] [kafka-controller] [%t] [%c{2}]: %m%n

    log4j.appender.authorizerAppender=org.apache.log4j.DailyRollingFileAppender
    log4j.appender.authorizerAppender.DatePattern='.'yyyy-MM-dd-HH
    log4j.appender.authorizerAppender.File=${kafka.logs.dir}/kafka-authorizer.log
    log4j.appender.authorizerAppender.layout=org.apache.log4j.PatternLayout
    log4j.appender.authorizerAppender.layout.ConversionPattern=%d{ISO8601} %-5p [{{TASK_NAME}}] [{{FRAMEWORK_NAME}}] [kafka-authorizer] [%t] [%c{2}]: %m%n


    # Change the two lines below to adjust ZK client logging
    log4j.logger.org.I0Itec.zkclient.ZkClient=INFO
    log4j.logger.org.apache.zookeeper=INFO

    # Change the two lines below to adjust the general broker logging level (output to server.log and stdout)
    log4j.logger.kafka=INFO
    log4j.logger.org.apache.kafka=INFO

    # Change to DEBUG or TRACE to enable request logging
    log4j.logger.kafka.request.logger=WARN, requestAppender
    log4j.additivity.kafka.request.logger=false

    # Uncomment the lines below and change log4j.logger.kafka.network.RequestChannel$ to TRACE for additional output
    # related to the handling of requests
    #log4j.logger.kafka.network.Processor=TRACE, requestAppender
    #log4j.logger.kafka.server.KafkaApis=TRACE, requestAppender
    #log4j.additivity.kafka.server.KafkaApis=false
    log4j.logger.kafka.network.RequestChannel$=WARN, requestAppender
    log4j.additivity.kafka.network.RequestChannel$=false

    log4j.logger.kafka.controller=TRACE, controllerAppender
    log4j.additivity.kafka.controller=false

    log4j.logger.kafka.log.LogCleaner=INFO, cleanerAppender
    log4j.additivity.kafka.log.LogCleaner=false

    log4j.logger.state.change.logger=TRACE, stateChangeAppender
    log4j.additivity.state.change.logger=false

    # Access denials are logged at INFO level, change to DEBUG to also log allowed accesses
    log4j.logger.kafka.authorizer.logger=INFO, authorizerAppender
    log4j.additivity.kafka.authorizer.logger=false
---
# Source: splice-helm/charts/jupyterhub/templates/hub/pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: hub-db-dir
  labels:
    component: hub
    app: jupyterhub
    release: my-release
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
    chart: jupyterhub-0.8.2
    heritage: Helm
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: "1Gi"
---
# Source: splice-helm/charts/haproxy-controller/templates/clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels:
    app: haproxy-ingress
    component: controller
    release: my-release
  name: my-release-haproxy-controller-default
rules:
- apiGroups:
  - ""
  resources:
  - configmaps
  - endpoints
  - nodes
  - pods
  - services
  - namespaces
  - events
  - serviceaccounts
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - "extensions"
  resources:
  - ingresses
  - ingresses/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - secrets
  verbs:
  - get
  - list
  - watch
  - create
  - patch
  - update
- apiGroups:
  - extensions
  resources:
  - ingresses
  verbs:
  - get
  - list
  - watch
---
# Source: splice-helm/charts/nginx-ingress/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  labels:
    app: nginx-ingress
    chart: nginx-ingress-1.23.0-splice.9
    heritage: Helm
    release: my-release
  name: my-release-nginx-ingress
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - endpoints
      - nodes
      - pods
      - secrets
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - get
      - list
      - update
      - watch
  - apiGroups:
      - extensions
      - "networking.k8s.io" # k8s 1.14+
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - extensions
      - "networking.k8s.io" # k8s 1.14+
    resources:
      - ingresses/status
    verbs:
      - update
---
# Source: splice-helm/charts/haproxy-controller/templates/clusterrolebinding.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels:
    app: haproxy-ingress
    component: controller
    release: my-release
  name: my-release-haproxy-controller-default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: my-release-haproxy-controller-default
subjects:
- kind: ServiceAccount
  name: my-release-haproxy-controller
  namespace: default
---
# Source: splice-helm/charts/nginx-ingress/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  labels:
    app: nginx-ingress
    chart: nginx-ingress-1.23.0-splice.9
    heritage: Helm
    release: my-release
  name: my-release-nginx-ingress
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: my-release-nginx-ingress
subjects:
  - kind: ServiceAccount
    name: my-release-nginx-ingress
    namespace: default
---
# Source: splice-helm/charts/hbase/templates/olap-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  annotations:
  name: my-release-olap
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - pods/exec
  verbs:
  - '*'
- apiGroups:
  - monitoring.coreos.com
  resources:
  - servicemonitors
  verbs:
  - get
  - create
- apiGroups:
  - splicedbcluster.splicemachine.io
  resources:
  - '*'
  verbs:
  - '*'
---
# Source: splice-helm/charts/jupyterhub/templates/hub/rbac.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: hub
  labels:
    component: hub
    app: jupyterhub
    release: my-release
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
    chart: jupyterhub-0.8.2
    heritage: Helm
rules:
  - apiGroups: [""]       # "" indicates the core API group
    resources: ["pods", "persistentvolumeclaims"]
    verbs: ["get", "watch", "list", "create", "delete"]
  - apiGroups: [""]       # "" indicates the core API group
    resources: ["events"]
    verbs: ["get", "watch", "list"]
---
# Source: splice-helm/charts/nginx-ingress/templates/controller-role.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  labels:
    app: nginx-ingress
    chart: nginx-ingress-1.23.0-splice.9
    heritage: Helm
    release: my-release
  name: my-release-nginx-ingress
rules:
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - configmaps
      - pods
      - secrets
      - endpoints
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - get
      - list
      - update
      - watch
  - apiGroups:
      - extensions
      - "networking.k8s.io" # k8s 1.14+
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - extensions
      - "networking.k8s.io" # k8s 1.14+
    resources:
      - ingresses/status
    verbs:
      - update
  - apiGroups:
      - ""
    resources:
      - configmaps
    resourceNames:
      - ingress-controller-leader-nginx-core
    
    verbs:
      - get
      - update
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - create
  - apiGroups:
      - ""
    resources:
      - endpoints
    verbs:
      - create
      - get
      - update
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
# Source: splice-helm/charts/rbac-operator/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  creationTimestamp: null
  name: splicedb-operator
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - pods/exec
  - services
  - services/finalizers
  - endpoints
  - persistentvolumeclaims
  - events
  - configmaps
  - secrets
  - serviceaccounts
  - namespaces
  verbs:
  - '*'
- apiGroups:
  - apps
  resources:
  - deployments
  - daemonsets
  - replicasets
  - statefulsets
  verbs:
  - '*'
- apiGroups:
  - policy
  resources:
  - poddisruptionbudgets
  verbs:
  - '*'
- apiGroups:
  - monitoring.coreos.com
  resources:
  - servicemonitors
  verbs:
  - get
  - create
- apiGroups:
  - apps
  resourceNames:
  - splicedb-operator
  resources:
  - deployments/finalizers
  verbs:
  - update
- apiGroups:
  - apps
  resources:
  - replicasets
  verbs:
  - get
- apiGroups:
  - splicedbcluster.splicemachine.io
  resources:
  - '*'
  verbs:
  - '*'
- apiGroups:
  - "rbac.authorization.k8s.io"
  resources:
  - roles
  - rolebindings
  - clusterroles
  - clusterrolebindings
  - serviceaccounts
  verbs:
  - '*'
- apiGroups:
  - "extensions"
  resources:
  - ingresses
  verbs:
  - "*"
- apiGroups:
  - "batch"
  resources:
  - jobs
  - cronjobs
  verbs:
  - "*"
---
# Source: splice-helm/charts/rbac/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: spark-role
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["*"]
- apiGroups: [""]
  resources: ["services"]
  verbs: ["*"]
---
# Source: splice-helm/charts/hbase/templates/olap-rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  annotations:
  name: my-release-olap
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: my-release-olap
subjects:
- kind: ServiceAccount
  name: my-release-olap
---
# Source: splice-helm/charts/jupyterhub/templates/hub/rbac.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: hub
  labels:
    component: hub
    app: jupyterhub
    release: my-release
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
    chart: jupyterhub-0.8.2
    heritage: Helm
subjects:
  - kind: ServiceAccount
    name: hub
    namespace: default
roleRef:
  kind: Role
  name: hub
  apiGroup: rbac.authorization.k8s.io
---
# Source: splice-helm/charts/nginx-ingress/templates/controller-rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  labels:
    app: nginx-ingress
    chart: nginx-ingress-1.23.0-splice.9
    heritage: Helm
    release: my-release
  name: my-release-nginx-ingress
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: my-release-nginx-ingress
subjects:
  - kind: ServiceAccount
    name: my-release-nginx-ingress
    namespace: default
---
# Source: splice-helm/charts/rbac-operator/templates/role_binding.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: splicedb-operator
subjects:
- kind: ServiceAccount
  name: splicedb-operator
  namespace: splice-system
roleRef:
  kind: Role
  name: splicedb-operator
  apiGroup: rbac.authorization.k8s.io
---
# Source: splice-helm/charts/rbac/templates/cluster-role-binding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: default-cluster-role-binding
subjects:
- kind: ServiceAccount
  name: default
  namespace: default
roleRef:
  kind: Role
  name: splicedb-operator
  apiGroup: rbac.authorization.k8s.io
---
# Source: splice-helm/charts/rbac/templates/role-binding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: spark-role-binding
  namespace: default
subjects:
- kind: ServiceAccount
  name: spark
  namespace: default
roleRef:
  kind: Role
  name: spark-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: splice-helm/charts/hadoop/templates/dn-service.yaml
# A headless service to create DNS records
apiVersion: v1
kind: Service
metadata:
  name: my-release-hdfs-dn
  labels:
    app: hadoop
    chart: hadoop-0.0.1
    release: my-release
    component: hdfs-dn
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  ports:
  - name: dfs
    port: 9000
    protocol: TCP
  - name: webhdfs
    port: 50075
  clusterIP: None
  selector:
    app: hadoop
    release: my-release
    component: hdfs-dn
    dnsprefix: splicedb
---
# Source: splice-helm/charts/hadoop/templates/jn-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-hdfs-jn
  annotations:
  labels:
    app: hadoop
    chart: hadoop-0.0.1
    release: my-release
    component: hdfs-jn
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  ports:
  - port: 8485
    name: jn
  - port: 8480
    name: http
  clusterIP: None
  selector:
      app: hadoop
      release: my-release
      component: hdfs-jn
      dnsprefix: splicedb
---
# Source: splice-helm/charts/hadoop/templates/nn-service-headless.yaml
# A headless service to create DNS records
apiVersion: v1
kind: Service
metadata:
  name: my-release-hdfs-nn
  labels:
    app: hadoop
    chart: hadoop-0.0.1
    release: my-release
    component: hdfs-nn
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  ports:
  - name: dfs
    port: 8020
    protocol: TCP
  - name: webhdfs
    port: 50070
  clusterIP: None
  selector:
    app: hadoop
    release: my-release
    component: hdfs-nn
    dnsprefix: splicedb
---
# Source: splice-helm/charts/haproxy-controller/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp
    service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "240"
    service.beta.kubernetes.io/aws-load-balancer-type: nlb
  labels:
    app: haproxy-ingress
    component: controller
    release: my-release
    run: my-release-haproxy-controller
  name: my-release-haproxy-controller
spec:
  selector:
    run: my-release-haproxy-controller
  type: LoadBalancer
  ports:
  - name: jdbc
    port: 1527
    protocol: TCP
    targetPort: 1527
  - name: kafka
    port: 9092
    protocol: TCP
    targetPort: 9092
---
# Source: splice-helm/charts/hbase/templates/master-service-headless.yaml
# A headless service to create DNS records
apiVersion: v1
kind: Service
metadata:
  name: my-release-hmaster
  labels:
    app: hbase
    chart: hbase-0.0.1
    release: my-release
    component: hmaster
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  ports:
  - port: 8080
    name: restapi
  - port: 9090
    name: thriftapi
  - port: 16000
    name: master
  - port: 16010
    name: master-info
  - port: 4002
    name: jvm-debug
  clusterIP: None
  selector:
    app: hbase
    release: my-release
    component: hmaster
    dnsprefix: splicedb
---
# Source: splice-helm/charts/hbase/templates/olap-service-headless.yaml
# A headless service to create DNS records
apiVersion: v1
kind: Service
metadata:
  name: my-release-olap
  labels:
    app: hbase
    chart: hbase-0.0.1
    release: my-release
    component: olap
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  ports:
  - port: 16040
    name: olap
  clusterIP: None
  selector:
    app: hbase
    release: my-release
    component: olap
    dnsprefix: splicedb
---
# Source: splice-helm/charts/hbase/templates/region-service-headless.yaml
# A headless service to create DNS records
apiVersion: v1
kind: Service
metadata:
  name: my-release-hregion-hl
  labels:
    app: hbase
    chart: hbase-0.0.1
    release: my-release
    component: hregion
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  ports:
  - port: 16020
    name: rs
  - port: 16030
    name: rsinfo
  - port: 1527
    name: jdbc
  clusterIP: None
  selector:
    app: hbase
    release: my-release
    component: hregion
    dnsprefix: splicedb
---
# Source: splice-helm/charts/hbase/templates/region-service.yaml
# A headless service to create DNS records
apiVersion: v1
kind: Service
metadata:
  name: my-release-hregion
  labels:
    app: hbase
    chart: hbase-0.0.1
    release: my-release
    component: hregion
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  type: ClusterIP
  ports:
  - port: 1527
    name: jdbc
  selector:
    app: hbase
    release: my-release
    component: hregion
    dnsprefix: splicedb
---
# Source: splice-helm/charts/hbase/templates/spark-service-headless.yaml
# A headless service to create DNS records
apiVersion: v1
kind: Service
metadata:
  name: my-release-olap-spark-headless
  labels:
    app: hbase
    chart: hbase-0.0.1
    release: my-release
    component: olap
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  ports:
  - port: 4040
    name: olap-info
  selector:
    app: hbase
    release: my-release
    component: olap
    dnsprefix: splicedb
---
# Source: splice-helm/charts/jupyterhub/templates/hub/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: hub
  labels:
    component: hub
    app: jupyterhub
    release: my-release
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
    chart: jupyterhub-0.8.2
    heritage: Helm
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/path: /splicejupyter/hub/metrics
spec:
  type: ClusterIP
  selector:
    component: hub
    app: jupyterhub
    release: my-release
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
  ports:
    - protocol: TCP
      port: 8081
      targetPort: 8081
---
# Source: splice-helm/charts/jupyterhub/templates/proxy/proxy-api-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: proxy-api
  labels:
    component: proxy-api
    app: jupyterhub
    release: my-release
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
    chart: jupyterhub-0.8.2
    heritage: Helm
spec:
  selector:
    component: proxy
    app: jupyterhub
    release: my-release
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
  ports:
    - protocol: TCP
      port: 8001
      targetPort: 8001
---
# Source: splice-helm/charts/jupyterhub/templates/proxy/proxy-public-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: proxy-public
  labels:
    component: proxy-public
    app: jupyterhub
    release: my-release
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
    chart: jupyterhub-0.8.2
    heritage: Helm
spec:
  selector:
    # TODO: Refactor to utilize the helpers
    component: proxy
    release: my-release
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 8000
      # allow proxy.service.nodePort for http
  type: ClusterIP
---
# Source: splice-helm/charts/jvmprofiler/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-jvmprofiler
  labels:
    app: splice-jvmprofiler
    chart: jvmprofiler-0.0.1
    release: my-release
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 8686
    targetPort: "http"
    nodePort: null
  selector:
    app: splice-jvmprofiler
    release: my-release
    dnsprefix: splicedb
---
# Source: splice-helm/charts/kafka/templates/service-external.yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    external-dns.alpha.kubernetes.io/hostname: "kafka-0-splicedb-dev1.dev.splicemachine-dev.io"
  name: my-release-kafka-external-0
  labels:
    app: kafka-broker-lb
    dnsprefix: splicedb
    pod: "my-release-kafka-0"
    chart: kafka-0.0.1
    release: my-release
    component: controller
    frameworkId: splicedb-dev1
    brokerService: kafka-broker-0

spec:
  type: LoadBalancer
  ports:
    - name: external-broker
      port: 19092
      targetPort: 19092
      protocol: TCP
  selector:
    app: splice-kafka
    release: my-release
    dnsprefix: splicedb
    statefulset.kubernetes.io/pod-name: "my-release-kafka-0"
---
# Source: splice-helm/charts/kafka/templates/service-headless.yaml
# A headless service to create DNS records
# Do not change the name of this service
apiVersion: v1
kind: Service
metadata:
  name: my-release-kafka-headless
  labels:
    app: splice-kafka
    chart: kafka-0.0.1
    release: my-release
    component: kafka-broker
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  ports:
  - port: 9092
    name: broker
  clusterIP: None
  selector:
    app: splice-kafka
    release: my-release
    component: kafka-broker
    dnsprefix: splicedb
---
# Source: splice-helm/charts/kafka/templates/service.yaml
# A service to create DNS records
apiVersion: v1
kind: Service
metadata:
  name: my-release-kafka
  labels:
    app: splice-kafka
    chart: kafka-0.0.1
    release: my-release
    component: kafka-broker
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  type: ClusterIP
  ports:
  - port: 9092
    name: broker
  selector:
    app: splice-kafka
    release: my-release
    component: kafka-broker
    dnsprefix: splicedb
---
# Source: splice-helm/charts/mlmanager/templates/bobby-service-headless.yaml
# A service for the job tracker (bobby) user interface
apiVersion: v1
kind: Service
metadata:
  name: my-release-bobby-headless
  labels:
    app: splice-bobby
    chart: mlmanager-0.0.1
    release: my-release
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  ports:
  - port: 2375
    name: bobby
  clusterIP: None
  selector:
    app: splice-bobby
    release: my-release
    dnsprefix: splicedb
---
# Source: splice-helm/charts/mlmanager/templates/mlflow-service-headless.yaml
# A service for the mlflow api
apiVersion: v1
kind: Service
metadata:
  name: my-release-mlflow-headless
  labels:
    app: splice-mlflow
    chart: mlmanager-0.0.1
    release: my-release
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  ports:
  - port: 5003
    name: job-tracker
  - port: 5001
    name: mlflow
  clusterIP: None
  selector:
    app: splice-mlflow
    release: my-release
    dnsprefix: splicedb
---
# Source: splice-helm/charts/mlmanager/templates/mlflow-service.yaml
# A service for the mlflow user interface
apiVersion: v1
kind: Service
metadata:
  name: my-release-mlflow
  labels:
    app: splice-mlflow
    chart: mlmanager-0.0.1
    release: my-release
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  type: ClusterIP
  ports:
  - port: 5001
    name: mlflow
  - port: 5003
    name: job-tracker
  selector:
    app: splice-mlflow
    release: my-release
    dnsprefix: splicedb
---
# Source: splice-helm/charts/nginx-ingress/templates/controller-service.yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
  labels:
    app: nginx-ingress
    chart: nginx-ingress-1.23.0-splice.9
    component: "controller"
    heritage: Helm
    release: my-release
  name: my-release-nginx-ingress-controller
spec:
  sessionAffinity: None
  clusterIP: ""
  externalTrafficPolicy: "Local"
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
  selector:
    app: nginx-ingress
    component: "controller"
    release: my-release
  type: "LoadBalancer"
---
# Source: splice-helm/charts/nginx-ingress/templates/default-backend-service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: nginx-ingress
    chart: nginx-ingress-1.23.0-splice.9
    component: "default-backend"
    heritage: Helm
    release: my-release
  name: my-release-nginx-ingress-default-backend
spec:
  clusterIP: ""
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
  selector:
    app: nginx-ingress
    component: "default-backend"
    release: my-release
  type: "ClusterIP"
---
# Source: splice-helm/charts/zookeeper/templates/service-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-zookeeper-headless
  labels:
    app: splice-zookeeper
    chart: zookeeper-0.0.1
    release: my-release
    frameworkId: splicedb-dev1
spec:
  ports:
  - port: 2181
    name: client
  - port: 2888
    name: server
  - port: 3888
    name: leader-election
  clusterIP: None
  selector:
    app: splice-zookeeper
    release: my-release
---
# Source: splice-helm/charts/zookeeper/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-zookeeper
  labels:
    app: splice-zookeeper
    chart: zookeeper-0.0.1
    release: my-release
    frameworkId: splicedb-dev1
spec:
  ports:
  - port: 2181
    name: client
  selector:
    app: splice-zookeeper
    release: my-release
---
# Source: splice-helm/charts/haproxy-controller/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: haproxy-ingress
    component: controller
    release: my-release
    run: my-release-haproxy-controller
  name: my-release-haproxy-controller
spec:
  replicas: 1
  selector:
    matchLabels:
      app: haproxy-ingress
      component: controller
      release: my-release
      run: my-release-haproxy-controller
  template:
    metadata:
      labels:
        app: haproxy-ingress
        component: controller
        release: my-release
        run: my-release-haproxy-controller
    spec:
      serviceAccountName: my-release-haproxy-controller
      initContainers:
      - name: check-db-ready
        image: docker.io/splicemachine/sm_k8_base:0.0.2
        command:
          - "sh"
          - "-c"
          - "until /usr/bin/curl -kLs -X GET http://my-release-hregion-hl:16030/jmx?qry=com.splicemachine.version:type=DatabaseVersion | grep -q Release; do echo waiting for database; sleep 2; done;"
      containers:
      - name: haproxy-controller-controller
        image: "splicemachine/kubernetes-ingress:2.0.11_0.0.5"
        imagePullPolicy: IfNotPresent
        args:

          - --default-ssl-certificate=default/my-release-haproxy-controller-default-cert

          - --configmap=default/my-release-haproxy-controller
          - --configmap-tcp-services=default/my-release-haproxy-controller-tcp
          - --namespace-whitelist=default
        resources:
          requests:
            cpu: "500m"
            memory: "50Mi"
        livenessProbe:
          httpGet:
            path: /healthz
            port: 1042
        ports:
        - name: stat
          containerPort: 1024
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: TZ
          value: UTC
---
# Source: splice-helm/charts/jupyterhub/templates/hub/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hub
  labels:
    component: hub
    app: jupyterhub
    release: my-release
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
    chart: jupyterhub-0.8.2
    heritage: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      component: hub
      app: jupyterhub
      release: my-release
      frameworkId: splicedb-dev1
      dnsprefix: splicedb
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        component: hub
        app: jupyterhub
        release: my-release
        frameworkId: splicedb-dev1
        dnsprefix: splicedb
        hub.jupyter.org/network-access-proxy-api: "true"
        hub.jupyter.org/network-access-proxy-http: "true"
        hub.jupyter.org/network-access-singleuser: "true"
      annotations:
        # This lets us autorestart when the secret changes!
        checksum/config-map: d9e5cee8d2fee48a065e24c3d148cad22615c988a13c2512bd24790d7f58d039
        checksum/secret: dbfcdf2276dd33a7237c0b3810d1cfaa1e1f8e965e4f5defd90ecc0fc4c588d6
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: hub.jupyter.org/node-purpose
                    operator: In
                    values: [core]
      volumes:
        - name: config
          configMap:
            name: hub-config
        - name: secret
          secret:
            secretName: hub-secret
        - name: hub-db-dir
          persistentVolumeClaim:
            claimName: hub-db-dir
      serviceAccountName: hub
      securityContext:
        runAsUser: 1000
        fsGroup: 1000
      containers:
        - name: hub
          image: splicemachine/sm_k8_jupyterhub:0.0.14
          command:
            - jupyterhub
            - --config
            - /srv/jupyterhub_config.py
            - --upgrade-db
          volumeMounts:
            - mountPath: /etc/jupyterhub/config/
              name: config
            - mountPath: /etc/jupyterhub/secret/
              name: secret
            - mountPath: /srv/jupyterhub
              name: hub-db-dir
          resources:
            requests:
              cpu: 200m
              memory: 512Mi
          imagePullPolicy: IfNotPresent
          env:
            - name: PYTHONUNBUFFERED
              value: "1"
            - name: JDBC_HOST
              value: my-release-hregion.default.svc.cluster.local
            - name: HELM_RELEASE_NAME
              value: "my-release"
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: TZ
              value: UTC
            - name: CONFIGPROXY_AUTH_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hub-secret
                  key: proxy.token
            - name: JUPYTERHUB_CRYPT_KEY
              valueFrom:
                secretKeyRef:
                  name: hub-secret
                  key: auth.state.crypto-key
          ports:
            - containerPort: 8081
              name: hub
---
# Source: splice-helm/charts/jupyterhub/templates/proxy/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: proxy
  labels:
    component: proxy
    app: jupyterhub
    release: my-release
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
    chart: jupyterhub-0.8.2
    heritage: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      component: proxy
      app: jupyterhub
      release: my-release
      frameworkId: splicedb-dev1
      dnsprefix: splicedb
  template:
    metadata:
      labels:
        component: proxy
        app: jupyterhub
        release: my-release
        frameworkId: splicedb-dev1
        dnsprefix: splicedb
        hub.jupyter.org/network-access-hub: "true"
        hub.jupyter.org/network-access-singleuser: "true"
      annotations:
        # This lets us autorestart when the secret changes!
        checksum/hub-secret: 513f1a3b5f2a57c70ba0317ced867349d9898d74a4da200cf578a4df39b34083
        checksum/proxy-secret: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
    spec:
      terminationGracePeriodSeconds: 60
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: hub.jupyter.org/node-purpose
                    operator: In
                    values: [core]
      containers:
        - name: chp
          image: jupyterhub/configurable-http-proxy:3.0.0
          command:
            - configurable-http-proxy
            - --ip=0.0.0.0
            - --api-ip=0.0.0.0
            - --api-port=8001
            - --default-target=http://$(HUB_SERVICE_HOST):$(HUB_SERVICE_PORT)
            - --error-target=http://$(HUB_SERVICE_HOST):$(HUB_SERVICE_PORT)/hub/error
            - --port=8000
          resources:
            requests:
              cpu: 200m
              memory: 512Mi
          env:
            - name: CONFIGPROXY_AUTH_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hub-secret
                  key: proxy.token
            - name: TZ
              value: UTC
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8000
              name: proxy-public
            - containerPort: 8001
              name: api
---
# Source: splice-helm/charts/jvmprofiler/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-jvmprofiler
  labels:
    app: splice-jvmprofiler
    chart: jvmprofiler-0.0.1
    release: my-release
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: splice-jvmprofiler
      release: my-release
      dnsprefix: splicedb
  replicas: 1
  template:
    metadata:
      labels:
        app: splice-jvmprofiler
        chart: jvmprofiler-0.0.1
        release: my-release
        dnsprefix: splicedb
    spec:
      imagePullSecrets:
      - name: "regcred"
      containers:
      - name: jvmprofiler
        image: docker.io/splicemachine/sm_k8_base:0.0.2
        imagePullPolicy: "IfNotPresent"
        command: ["/opt/jdk/bin/java", "-jar", "/opt/tools/lib/jscpServer.jar", "8686", "jscp"]
        env:
          - name: JSCOPE_ZKSERVERS
            value: "my-release-zookeeper-1.my-release-zookeeper-headless.default.svc.cluster.local:2181,my-release-zookeeper-2.my-release-zookeeper-headless.default.svc.cluster.local:2181,my-release-zookeeper-0.my-release-zookeeper-headless.default.svc.cluster.local:2181"
          - name: TZ
            value: UTC
        ports:
        - name: http
          containerPort: 8686
        livenessProbe:
          httpGet:
            path: /
            port: http
          failureThreshold: 6
          initialDelaySeconds: 120
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /
            port: http
          failureThreshold: 6
          initialDelaySeconds: 30
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        resources:
          requests:
            cpu: "0.5"
            memory: 2G
      securityContext:
        runAsUser: 1001
        fsGroup: 1000
---
# Source: splice-helm/charts/nginx-ingress/templates/controller-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-ingress
    chart: nginx-ingress-1.23.0-splice.9
    component: "controller"
    heritage: Helm
    release: my-release
  name: my-release-nginx-ingress-controller
spec:
  selector:
    matchLabels:
      app: nginx-ingress
      release: my-release
  replicas: 1
  revisionHistoryLimit: 10
  strategy:
    {}
  minReadySeconds: 0
  template:
    metadata:
      labels:
        app: nginx-ingress
        component: "controller"
        release: my-release
    spec:
      dnsPolicy: ClusterFirst
      containers:
        - name: nginx-ingress-controller
          image: "quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.26.1"
          imagePullPolicy: "IfNotPresent"
          args:
            - /nginx-ingress-controller
            - --default-backend-service=default/my-release-nginx-ingress-default-backend
            - --election-id=ingress-controller-leader
            - --ingress-class=nginx-core
          
            - --configmap=default/my-release-nginx-ingress-controller
            - --default-ssl-certificate=default/REPLACE_CERTIFICATE
          securityContext:
            capabilities:
                drop:
                - ALL
                add:
                - NET_BIND_SERVICE
            runAsUser: 33
            allowPrivilegeEscalation: true
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          livenessProbe:
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 3
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
            - name: https
              containerPort: 443
              protocol: TCP
          readinessProbe:
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 3
          resources:
            {}
      hostNetwork: false
      serviceAccountName: my-release-nginx-ingress
      terminationGracePeriodSeconds: 60
---
# Source: splice-helm/charts/nginx-ingress/templates/default-backend-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-ingress
    chart: nginx-ingress-1.23.0-splice.9
    component: "default-backend"
    heritage: Helm
    release: my-release
  name: my-release-nginx-ingress-default-backend
spec:
  selector:
    matchLabels:
      app: nginx-ingress
      release: my-release
  replicas: 1
  revisionHistoryLimit: 10
  template:
    metadata:
      labels:
        app: nginx-ingress
        component: "default-backend"
        release: my-release
    spec:
      containers:
        - name: nginx-ingress-default-backend
          image: "k8s.gcr.io/defaultbackend-amd64:1.5"
          imagePullPolicy: "IfNotPresent"
          args:
          securityContext:
            runAsUser: 65534
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 5
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          resources:
            {}
      serviceAccountName: my-release-nginx-ingress-backend
      terminationGracePeriodSeconds: 60
---
# Source: splice-helm/charts/hadoop/templates/dn-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-hdfs-dn
  annotations:
  labels:
    app: hadoop
    chart: hadoop-0.0.1
    release: my-release
    component: hdfs-dn
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  selector:
    matchLabels:
      app: hadoop
      release: my-release
      component: hdfs-dn
      dnsprefix: splicedb
  serviceName: my-release-hdfs-dn
  podManagementPolicy: "Parallel"
  updateStrategy:
    type: RollingUpdate
  replicas: 3
  template:
    metadata:
      labels:
        app: hadoop
        release: my-release
        component: hdfs-dn
        dnsprefix: splicedb
        version: "v1"
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - topologyKey: "kubernetes.io/hostname"
            labelSelector:
              matchLabels:
                app:  hadoop
                release: "my-release"
                component: hdfs-dn
                dnsprefix: splicedb
      terminationGracePeriodSeconds: 0
      initContainers:
      - name: wait-nn
        image: docker.io/splicemachine/sm_k8_hdfs-3.0.0:0.0.17
        command:
           - "/bin/bash"
           - "/usr/lib/hadoop/bin/splice/wait-for-nn.sh"
           - "my-release-hdfs-nn"
           - "default.svc.cluster.local"
      containers:
      - name: hdfs-dn
        image: docker.io/splicemachine/sm_k8_hdfs-3.0.0:0.0.17
        imagePullPolicy: "IfNotPresent"
        env:
          - name: HADOOP_COMPONENT
            value: datanode
          - name: RELEASE_NAME
            value: my-release
          - name: FRAMEWORKID
            value: splicedb-dev1
          - name: SERVICE_NAME
            value: my-release-hdfs-dn
          - name: DOMAIN_NAME
            value: default.svc.cluster.local
          - name: DATANODE_DATA_DIR
            value: "/data-data"
          - name: NAMENODE_URL
            value: http://my-release-hdfs-nn:50070
          - name: JAVASCOPE
            value: "true"
          - name: ZOOKEEPER_QUORUM
            value: "my-release-zookeeper-1.my-release-zookeeper-headless.default.svc.cluster.local:2181,my-release-zookeeper-2.my-release-zookeeper-headless.default.svc.cluster.local:2181,my-release-zookeeper-0.my-release-zookeeper-headless.default.svc.cluster.local:2181"
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: HADOOP_OPTS
            value: "-Xms4294965096 -Xmx4294965096 -XX:MaxDirectMemorySize=1g"
          - name: TZ
            value: UTC
        command:
           - "/bin/bash"
           - "/usr/lib/hadoop/bin/splice/start-hadoop-component.sh"
        resources:
          requests:
            cpu: "2"
            memory: 6G
        lifecycle:
          postStart:
            exec:
              command:
                - "/bin/sh"
                - "-c"
                - |
                  /usr/lib/hadoop/bin/splice/create-hdfs-dirs.sh
        livenessProbe:
          exec:
            command:
              - "/bin/bash"
              - "/usr/lib/hadoop/bin/splice/dn-liveness.sh"
          failureThreshold: 16
          initialDelaySeconds: 180
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 120
        volumeMounts:
        - name: hadoop-config
          mountPath: /tmp/hadoop-config
        
        - name: dfs0
          mountPath: /data-data-0
        
      volumes:
      - name: hadoop-config
        configMap:
          name: my-release-hadoop-config
  volumeClaimTemplates:
  
  - metadata:
      name: dfs0
    spec:
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: .2T
      storageClassName:
---
# Source: splice-helm/charts/hadoop/templates/jn-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-hdfs-jn
  annotations:
  labels:
    app: hadoop
    chart: hadoop-0.0.1
    release: my-release
    component: hdfs-jn
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  selector:
    matchLabels:
      app: hadoop
      release: my-release
      component: hdfs-jn
      dnsprefix: splicedb
  serviceName: my-release-hdfs-jn
  podManagementPolicy: "Parallel"
  updateStrategy:
    type: RollingUpdate
  replicas: 3
  template:
    metadata:
      labels:
        app: hadoop
        release: my-release
        component: hdfs-jn
        dnsprefix: splicedb
        version: "v1"
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - topologyKey: "kubernetes.io/hostname"
            labelSelector:
              matchLabels:
                app:  hadoop
                release: "my-release"
                component: hdfs-jn
                dnsprefix: splicedb
      terminationGracePeriodSeconds: 0
      containers:
      - name: hdfs-jn
        image: docker.io/splicemachine/sm_k8_hdfs-3.0.0:0.0.17
        imagePullPolicy: "IfNotPresent"
        env:
          - name: HADOOP_COMPONENT
            value: journalnode
          - name: RELEASE_NAME
            value: my-release
          - name: FRAMEWORKID
            value: splicedb-dev1
          - name: JOURNAL_DATA_DIR
            value: "/journal-data"
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: HADOOP_OPTS
            value: "-Xms536870912 -Xmx536870912 -XX:MaxDirectMemorySize=1g"
          - name: TZ
            value: UTC
        command:
           - "/bin/bash"
           - "/usr/lib/hadoop/bin/splice/start-hadoop-component.sh"
        resources:
          requests:
            cpu: "1"
            memory: 1.5G
        livenessProbe:
          httpGet:
            path: /jmx
            port: 8480
          failureThreshold: 16
          initialDelaySeconds: 180
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 120
        volumeMounts:
        - name: hadoop-config
          mountPath: /tmp/hadoop-config
        - name: dfs
          mountPath: /journal-data
      volumes:
      - name: hadoop-config
        configMap:
          name: my-release-hadoop-config
  volumeClaimTemplates:
  - metadata:
      name: dfs
    spec:
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: 10Gi
---
# Source: splice-helm/charts/hadoop/templates/nn-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-hdfs-nn
  annotations:
  labels:
    app: hadoop
    chart: hadoop-0.0.1
    release: my-release
    component: hdfs-nn
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  selector:
    matchLabels:
      app: hadoop
      release: my-release
      component: hdfs-nn
      dnsprefix: splicedb
  serviceName: my-release-hdfs-nn
  podManagementPolicy: "Parallel"
  updateStrategy:
    type: RollingUpdate
  replicas: 2
  template:
    metadata:
      labels:
        app: hadoop
        release: my-release
        component: hdfs-nn
        dnsprefix: splicedb
        version: "v1"
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - topologyKey: "kubernetes.io/hostname"
            labelSelector:
              matchLabels:
                app:  hadoop
                release: "my-release"
                component: hdfs-nn
                dnsprefix: splicedb
      terminationGracePeriodSeconds: 0
      initContainers:
      - name: wait-journal
        image: docker.io/splicemachine/sm_k8_hdfs-3.0.0:0.0.17
        command:
           - "/bin/bash"
           - "/usr/lib/hadoop/bin/splice/wait-for-jn.sh"
           - "my-release-hdfs-jn"
           - "default.svc.cluster.local"
      - name: wait-zk
        image: docker.io/splicemachine/sm_k8_hdfs-3.0.0:0.0.17
        command:
           - "/bin/bash"
           - "/usr/lib/hadoop/bin/splice/wait-for-zk.sh"
           - "my-release-zookeeper"
           - "my-release-zookeeper-headless"
           - "default.svc.cluster.local"
      - name: hdfs-nn-init
        image: docker.io/splicemachine/sm_k8_hdfs-3.0.0:0.0.17
        imagePullPolicy: "IfNotPresent"
        env:
          - name: HADOOP_COMPONENT
            value: initnamenode
          - name: RELEASE_NAME
            value: my-release
          - name: FRAMEWORKID
            value: splicedb-dev1
          - name: NAMENODE_DATA_DIR
            value: /name-data
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
        command:
           - "/bin/bash"
           - "/usr/lib/hadoop/bin/splice/start-hadoop-component.sh"
        volumeMounts:
        - name: hadoop-config
          mountPath: /tmp/hadoop-config
        - name: dfs
          mountPath: /name-data
      - name: hdfs-nn-zk-init
        image: docker.io/splicemachine/sm_k8_hdfs-3.0.0:0.0.17
        imagePullPolicy: "IfNotPresent"
        env:
          - name: HADOOP_COMPONENT
            value: initzkfc
          - name: RELEASE_NAME
            value: my-release
          - name: FRAMEWORKID
            value: splicedb-dev1
          - name: NAMENODE_DATA_DIR
            value: /name-data
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
        command:
           - "/bin/bash"
           - "/usr/lib/hadoop/bin/splice/start-hadoop-component.sh"
        volumeMounts:
        - name: hadoop-config
          mountPath: /tmp/hadoop-config
        - name: dfs
          mountPath: /name-data
      containers:
      - name: hdfs-nn
        image: docker.io/splicemachine/sm_k8_hdfs-3.0.0:0.0.17
        imagePullPolicy: "IfNotPresent"
        env:
          - name: HADOOP_COMPONENT
            value: namenode
          - name: RELEASE_NAME
            value: my-release
          - name: FRAMEWORKID
            value: splicedb-dev1
          - name: SERVICE_NAME
            value: my-release-hdfs-nn
          - name: DOMAIN_NAME
            value: default.svc.cluster.local
          - name: NAMENODE_DATA_DIR
            value: /name-data
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: TZ
            value: UTC
        command:
           - "/bin/bash"
           - "/usr/lib/hadoop/bin/splice/start-hadoop-component.sh"
        resources:
          requests:
            cpu: "1"
            memory: 5G
        livenessProbe:
          exec:
            command:
              - "/bin/bash"
              - "/usr/lib/hadoop/bin/splice/nn-liveness.sh"
          failureThreshold: 16
          initialDelaySeconds: 180
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 120
        volumeMounts:
        - name: hadoop-config
          mountPath: /tmp/hadoop-config
        - name: dfs
          mountPath: /name-data
      - name: hdfs-nn-zkfc
        image: docker.io/splicemachine/sm_k8_hdfs-3.0.0:0.0.17
        imagePullPolicy: "IfNotPresent"
        env:
          - name: HADOOP_COMPONENT
            value: zkfc
          - name: NAMENODE_DATA_DIR
            value: /name-data
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: HADOOP_OPTS
            value: "-Xms4294965096 -Xmx4294965096 -XX:MaxDirectMemorySize=1g"
        command:
           - "/bin/bash"
           - "/usr/lib/hadoop/bin/splice/start-hadoop-component.sh"
        resources:
          requests:
            cpu: "1"
            memory: 5G
        volumeMounts:
        - name: hadoop-config
          mountPath: /tmp/hadoop-config
        - name: dfs
          mountPath: /name-data
      volumes:
      - name: hadoop-config
        configMap:
          name: my-release-hadoop-config
  volumeClaimTemplates:
  - metadata:
      name: dfs
    spec:
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: 10Gi
---
# Source: splice-helm/charts/hbase/templates/master-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-hmaster
  labels:
    app: hbase
    chart: hbase-0.0.1
    release: my-release
    component: hmaster
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  selector:
    matchLabels:
      app: hbase
      release: my-release
      component: hmaster
      dnsprefix: splicedb
  serviceName: my-release-hmaster
  podManagementPolicy: "OrderedReady"
  updateStrategy:
    type: RollingUpdate
  replicas: 2
  template:
    metadata:
      labels:
        app: hbase
        release: my-release
        component: hmaster
        dnsprefix: splicedb
        version: "v1"
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
                matchLabels:
                  app:  hbase
                  release: "my-release"
                  component: hmaster
                  dnsprefix: splicedb
            topologyKey: "kubernetes.io/hostname"
      terminationGracePeriodSeconds: 0
      initContainers:
      - name: wait-zk
        image: docker.io/splicemachine/sm_k8_hdfs-3.0.0:0.0.17
        command:
           - "/bin/bash"
           - "/usr/lib/hadoop/bin/splice/wait-for-zk.sh"
           - "my-release-zookeeper"
           - "my-release-zookeeper-headless"
           - "default.svc.cluster.local"
      - name: wait-nn
        image: docker.io/splicemachine/sm_k8_hdfs-3.0.0:0.0.17
        command:
           - "/bin/bash"
           - "/usr/lib/hadoop/bin/splice/wait-for-nn.sh"
           - "my-release-hdfs-nn"
           - "default.svc.cluster.local"
      containers:
      - name: hbase-master
        image: docker.io/splicemachine/sm_k8_hbase-3.0.0:0.0.63
        imagePullPolicy: IfNotPresent
        env:
          - name: HBASE_COMPONENT
            value: master
          - name: RELEASE_NAME
            value: my-release
          - name: FRAMEWORKID
            value: splicedb-dev1
          - name: SERVICE_NAME
            value: my-release-hmaster
          - name: DOMAIN_NAME
            value: default.svc.cluster.local
          - name: HBASE_DEBUG
            value: "false"
          - name: JAVASCOPE
            value: "true"
          - name: ZOOKEEPER_QUORUM
            value: "my-release-zookeeper-1.my-release-zookeeper-headless.default.svc.cluster.local:2181,my-release-zookeeper-2.my-release-zookeeper-headless.default.svc.cluster.local:2181,my-release-zookeeper-0.my-release-zookeeper-headless.default.svc.cluster.local:2181"
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: TZ
            value: UTC
        command:
        - "/bin/bash"
        - "/usr/lib/hbase/bin/splice/start-hbase-component.sh"
        lifecycle:
          preStop:
            exec:
              command: [
                "/bin/bash",
                "/usr/lib/hbase/bin/hbase",
                "stop",
                "master",
                "$HOSTNAME"
              ]
        startupProbe:
          exec:
            command:
              - sh
              - -c
              - "/usr/lib/hbase/bin/splice/hmaster-ready.sh"
          failureThreshold: 20
          initialDelaySeconds: 360
          periodSeconds: 60
          successThreshold: 1
          timeoutSeconds: 240
        livenessProbe:
          exec:
            command:
            - sh
            - -c
            - "/usr/lib/hbase/bin/splice/hmaster-liveness.sh"
          failureThreshold: 10
          initialDelaySeconds: 60
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 60
        resources:
          requests:
            cpu: "2"
            memory: 6G
        volumeMounts:
        - name: hbase-config
          mountPath: /tmp/hbase-config
        - name: spark-config
          mountPath: /tmp/spark-config
        - name: hdfs-config
          mountPath: /tmp/hdfs-config
        - name: spark1
          mountPath: /spark/tmp0
        - name: spark2
          mountPath: /spark/tmp1
        - name: logdir
          mountPath: /var/log/hbase
      - name: active-master
        image: docker.io/splicemachine/sm_k8_hbase-3.0.0:0.0.63
        imagePullPolicy: IfNotPresent
        env:
          - name: POD_NAMESPACE
            value: default
          - name: SERVICE_NAME
            value: my-release-hmaster
          - name: DOMAIN_NAME
            value: default.svc.cluster.local
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
        command:
          - "sh"
          - "-c"
          - "while true; do '/usr/lib/hbase/bin/splice/update-active-master.sh'; sleep 30; done;"
      securityContext:
        runAsUser: 1004
        fsGroup: 1000
      volumes:
      - name: hbase-config
        configMap:
          name: my-release-hbase-config
      - name: spark-config
        configMap:
          name: my-release-spark-config
      - name: hdfs-config
        configMap:
          name: my-release-hadoop-config
      - name: spark1
        emptyDir: {}
      - name: spark2
        emptyDir: {}
  volumeClaimTemplates:
  - metadata:
      name: logdir
    spec:
      accessModes:
              - ReadWriteOnce
      resources:
        requests:
          storage: 10Gi
      storageClassName:
---
# Source: splice-helm/charts/hbase/templates/olap-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-olap
  labels:
    app: hbase
    chart: hbase-0.0.1
    release: my-release
    component: olap
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  selector:
    matchLabels:
      app: hbase
      release: my-release
      component: olap
      dnsprefix: splicedb
  serviceName: my-release-olap
  podManagementPolicy: "OrderedReady"
  updateStrategy:
    type: RollingUpdate
  replicas: 1
  template:
    metadata:
      labels:
        app: hbase
        release: my-release
        component: olap
        dnsprefix: splicedb
        version: "v1"
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
                matchLabels:
                  app:  hbase
                  release: "my-release"
                  component: olap
                  dnsprefix: splicedb
            topologyKey: "kubernetes.io/hostname"
      terminationGracePeriodSeconds: 0
      initContainers:
      - name: wait-for-hregion
        image: docker.io/splicemachine/sm_k8_hbase-3.0.0:0.0.63
        imagePullPolicy: 
        command:
          - "sh"
          - "-c"
          - "until /usr/bin/curl -kLs -X GET http://my-release-hregion-hl.default.svc.cluster.local:16030/jmx?qry=com.splicemachine.version:type=DatabaseVersion | grep -q Release; do echo waiting for hregion; sleep 2; done;"
      containers:
      - name: hbase-olap
        image: docker.io/splicemachine/sm_k8_hbase-3.0.0:0.0.63
        imagePullPolicy: IfNotPresent
        env:
          - name: HBASE_COMPONENT
            value: olap
          - name: RELEASE_NAME
            value: my-release
          - name: FRAMEWORKID
            value: splicedb-dev1
          - name: SERVICE_NAME
            value: my-release-olap
          - name: DOMAIN_NAME
            value: default.svc.cluster.local
          - name: HBASE_DEBUG
            value: "false"
          - name: OLAP_PORT
            value: "16040"
          - name: OLAP_QUEUE_NAME
            value: "default"
          - name: OLAP_APPID
            value: "olap_default"
          - name: OLAP_SERVER_MODE
            value: "KUBERNETES"
          - name: JAVASCOPE
            value: "true"
          - name: ZOOKEEPER_QUORUM
            value: "my-release-zookeeper-1.my-release-zookeeper-headless.default.svc.cluster.local:2181,my-release-zookeeper-2.my-release-zookeeper-headless.default.svc.cluster.local:2181,my-release-zookeeper-0.my-release-zookeeper-headless.default.svc.cluster.local:2181"
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: TZ
            value: UTC
        command:
        - "/bin/bash"
        - "/usr/lib/hbase/bin/splice/start-hbase-component.sh"
        lifecycle:
          preStop:
            exec:
              command: [
                "/bin/bash",
                "/usr/lib/hbase/bin/hbase",
                "stop",
                "olap",
                "$HOSTNAME"
              ]
        startupProbe:
          exec:
            command:
              - sh
              - -c
              - "/usr/lib/hbase/bin/splice/olap-ready.sh"
          failureThreshold: 20
          initialDelaySeconds: 360
          periodSeconds: 60
          successThreshold: 1
          timeoutSeconds: 240
        livenessProbe:
          exec:
            command:
            - sh
            - -c
            - "/usr/lib/hbase/bin/splice/olap-ready.sh"
          failureThreshold: 10
          initialDelaySeconds: 60
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 60
        resources:
          requests:
            cpu: "2"
            memory: "6656"
        volumeMounts:
        - name: hbase-config
          mountPath: /tmp/hbase-config
        - name: spark-config
          mountPath: /tmp/spark-config
        - name: hdfs-config
          mountPath: /tmp/hdfs-config
        - name: spark1
          mountPath: /spark/tmp0
        - name: spark2
          mountPath: /spark/tmp1
        - name: logdir
          mountPath: /var/log/hbase
      securityContext:
        runAsUser: 1004
        fsGroup: 1000
      serviceAccount: my-release-olap
      serviceAccountName: my-release-olap
      volumes:
      - name: hbase-config
        configMap:
          name: my-release-olap-config
      - name: spark-config
        configMap:
          name: my-release-spark-config
      - name: hdfs-config
        configMap:
          name: my-release-hadoop-config
      - name: spark1
        emptyDir: {}
      - name: spark2
        emptyDir: {}
  volumeClaimTemplates:
  - metadata:
      name: logdir
    spec:
      accessModes:
              - ReadWriteOnce
      resources:
        requests:
          storage: 10Gi
      storageClassName:
---
# Source: splice-helm/charts/hbase/templates/region-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-hregion
  labels:
    app: hbase
    chart: hbase-0.0.1
    release: my-release
    component: hregion
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  selector:
    matchLabels:
      app: hbase
      release: my-release
      component: hregion
      dnsprefix: splicedb
  serviceName: my-release-hregion-hl
  podManagementPolicy: "Parallel"
  updateStrategy:
    type: RollingUpdate
  replicas: 3
  template:
    metadata:
      labels:
        app: hbase
        release: my-release
        component: hregion
        dnsprefix: splicedb
        version: "v1"
    spec:
      affinity:
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 45
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchLabels:
                  app:  hadoop
                  release: "my-release"
                  component: hdfs-dn
                  dnsprefix: splicedb
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - topologyKey: kubernetes.io/hostname
            labelSelector:
              matchLabels:
                app: hbase
                release: "my-release"
                component: hregion
                dnsprefix: splicedb
      terminationGracePeriodSeconds: 0
      initContainers:
      - name: wait-master
        image: docker.io/splicemachine/sm_k8_hbase-3.0.0:0.0.63
        imagePullPolicy: 
        command:
           - "/bin/bash"
           - "/usr/lib/hbase/bin/splice/wait-for-master.sh"
           - "my-release-hmaster"
           - "default.svc.cluster.local"
      containers:
      - name: hbase-rs
        image: docker.io/splicemachine/sm_k8_hbase-3.0.0:0.0.63
        imagePullPolicy: 
        env:
          - name: HBASE_COMPONENT
            value: region
          - name: RELEASE_NAME
            value: my-release
          - name: FRAMEWORKID
            value: splicedb-dev1
          - name: SERVICE_NAME
            value: my-release-hregion-hl
          - name: DOMAIN_NAME
            value: default.svc.cluster.local
          - name: HBASE_DEBUG
            value: "false"
          - name: JAVASCOPE
            value: "true"
          - name: ZOOKEEPER_QUORUM
            value: "my-release-zookeeper-1.my-release-zookeeper-headless.default.svc.cluster.local:2181,my-release-zookeeper-2.my-release-zookeeper-headless.default.svc.cluster.local:2181,my-release-zookeeper-0.my-release-zookeeper-headless.default.svc.cluster.local:2181"
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: TZ
            value: UTC
        command:
        - "/bin/bash"
        - "/usr/lib/hbase/bin/splice/start-hbase-component.sh"
        resources:
          requests:
            cpu: "4"
            memory: 24G
        lifecycle:
          preStop:
            exec:
              command: [
                "/bin/bash",
                "/usr/lib/hbase/bin/hbase",
                "stop",
                "regionserver",
                "$HOSTNAME"
              ]
        startupProbe:
          exec:
            command:
              - sh
              - -c
              - "/usr/lib/hbase/bin/splice/hregion-startup-check.sh"
          failureThreshold: 20
          initialDelaySeconds: 360
          periodSeconds: 60
          successThreshold: 1
          timeoutSeconds: 240
        livenessProbe:
          exec:
            command:
            - sh
            - -c
            - "/usr/lib/hbase/bin/splice/hregion-liveness.sh"
          failureThreshold: 10
          initialDelaySeconds: 60
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 60
        volumeMounts:
        - name: hbase-config
          mountPath: /tmp/hbase-config
        - name: spark-config
          mountPath: /tmp/spark-config
        - name: hdfs-config
          mountPath: /tmp/hdfs-config
        - name: logdir
          mountPath: /var/log/hbase
      securityContext:
        runAsUser: 1004
        fsGroup: 1000
      volumes:
      - name: hbase-config
        configMap:
          name: my-release-hbase-config
      - name: spark-config
        configMap:
          name: my-release-spark-config
      - name: hdfs-config
        configMap:
          name: my-release-hadoop-config
  volumeClaimTemplates:
  - metadata:
      name: logdir
    spec:
      accessModes:
              - ReadWriteOnce
      resources:
        requests:
          storage: 10Gi
      storageClassName:
---
# Source: splice-helm/charts/jupyterhub/templates/scheduling/user-placeholder/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: user-placeholder
  labels:
    component: user-placeholder
    app: jupyterhub
    release: my-release
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
    chart: jupyterhub-0.8.2
    heritage: Helm
spec:
  podManagementPolicy: Parallel
  replicas: 0
  selector:
    matchLabels:
      component: user-placeholder
      app: jupyterhub
      release: my-release
      frameworkId: splicedb-dev1
      dnsprefix: splicedb
  serviceName: "user-placeholder"
  template:
    metadata:
      labels:
        component: user-placeholder
        app: jupyterhub
        release: my-release
        frameworkId: splicedb-dev1
        dnsprefix: splicedb
    spec:
      tolerations:
        - key: hub.jupyter.org_dedicated
          operator: Equal
          value: user
          effect: NoSchedule
        - key: hub.jupyter.org/dedicated
          operator: Equal
          value: user
          effect: NoSchedule
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: hub.jupyter.org/node-purpose
                    operator: In
                    values: [user]
      terminationGracePeriodSeconds: 0
      automountServiceAccountToken: false
      containers:
        - name: pause
          image: gcr.io/google_containers/pause:3.0
          resources:
            requests:
              memory: 1G
---
# Source: splice-helm/charts/kafka/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-kafka
  labels:
    app: splice-kafka
    chart: kafka-0.0.1
    release: my-release
    component: kafka-broker
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  selector:
    matchLabels:
      app: splice-kafka
      release: my-release
      component: kafka-broker
      dnsprefix: splicedb
  serviceName: my-release-kafka-headless
  podManagementPolicy: "OrderedReady"
  updateStrategy:
    type: RollingUpdate
  replicas: 1
  template:
    metadata:
      labels:
        app: splice-kafka
        release: my-release
        component: kafka-broker
        dnsprefix: splicedb
        version: "v1"
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
                matchLabels:
                  app:  splice-kafka
                  release: "my-release"
                  component: kafka-broker
                  dnsprefix: splicedb
            topologyKey: "kubernetes.io/hostname"
      terminationGracePeriodSeconds: 0
      initContainers:
      - name: wait-zk
        image: docker.io/splicemachine/sm_k8_hdfs-3.0.0:0.0.17
        command:
           - "/bin/bash"
           - "/usr/lib/hadoop/bin/splice/wait-for-zk-connect.sh"
           - "my-release-zookeeper"
           - "my-release-zookeeper-headless"
           - "default.svc.cluster.local"
      containers:
      - name: splice-kafka-broker
        image: docker.io/splicemachine/sm_k8_kafka-3.0.0:0.0.5
        imagePullPolicy: Always
        ports:
        - containerPort: 9092
          name: kafka
        - containerPort: 31090
          name: external-0
        env:
          - name: RELEASE_NAME
            value: splicedb-dev1
          - name: SERVICE_NAME
            value: my-release-kafka
          - name: DOMAIN_NAME
            value: default.svc.cluster.local
          - name: POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: TZ
            value: UTC
        command:
        - "/bin/bash"
        - "/usr/lib/kafka/bin/splice/start-kafka.sh"
        - "PLAINTEXT://MY_POD_NAME_VAR.my-release-kafka-headless.default.svc.cluster.local:9092"
        - "EXTERNAL://kafka-broker-KAFKA_BROKER_ID_VAR-EXT_DOMAIN_VAR:19092"
        - "PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT"
        - "my-release-kafka"
        - "my-release-kafka-headless"
        - "default.svc.cluster.local"
        - "splicedb"
        - "REPLACE_INGRESS_DOMAIN"
        livenessProbe:
          exec:
            command:
            - sh
            - -c
            - "/usr/lib/kafka/bin/splice/check-running.sh"
            -
          failureThreshold: 10
          initialDelaySeconds: 360
          periodSeconds: 60
          successThreshold: 1
          timeoutSeconds: 240
        volumeMounts:
        - name: kafka-config
          mountPath: /tmp/kafka-config
        - name: datadir
          mountPath: "/opt/kafka/data"
      volumes:
      - name: kafka-config
        configMap:
          name: my-release-kafka-config
  volumeClaimTemplates:
  - metadata:
      name: datadir
    spec:
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: 1Gi
      storageClassName:
---
# Source: splice-helm/charts/mlmanager/templates/bobby-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-bobby
  labels:
    app: splice-bobby
    chart: mlmanager-0.0.1
    release: my-release
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  selector:
    matchLabels:
      app: splice-bobby
      release: my-release
      dnsprefix: splicedb
  serviceName: my-release-bobby
  replicas: 0
  template:
    metadata:
      labels:
        app: splice-bobby
        release: my-release
        dnsprefix: splicedb
    spec:
      initContainers:
      - name: check-db-ready
        image: docker.io/splicemachine/sm_k8_base:0.0.2
        command:
          - "sh"
          - "-c"
          - "until /usr/bin/curl -kLs -X GET http://my-release-hregion-hl:16030/jmx?qry=com.splicemachine.version:type=DatabaseVersion | grep -q Release; do echo waiting for database; sleep 2; done;"
      containers:
      - name: bobby
        image: docker.io/splicemachine/sm_k8_bobby:0.1.22
        imagePullPolicy: IfNotPresent
        env:
          - name: DB_USER
            value: mlmanager
          - name: DB_HOST
            value: my-release-hregion.default.svc.cluster.local
          - name: DB_PASSWORD
            value: admin
          - name: FRAMEWORK_NAME
            value: splicedb-dev1
          - name: SAGEMAKER_ROLE
            value: REPLACE_SAGE_MAKER_ROLE
          - name: MLFLOW_URL
            value: http://my-release-mlflow-0.my-release-mlflow-headless.default.svc.cluster.local:5001
          - name: AWS_ACCESS_KEY_ID
            value: 
          - name: AWS_SECRET_ACCESS_KEY
            value: 
          - name: MODE
            value: production
          - name: ENVIRONMENT
            value: none
          - name: TZ
            value: UTC
        command:
        - "/bin/bash"
        - "/opt/bobby/scripts/entrypoint.sh"
        securityContext:
          privileged: true
        volumeMounts:
        - name: docker-graph-storage
          mountPath: /var/lib/docker
        resources:
          requests:
            cpu: "0.5"
            memory: 2G
      volumes:
        - name: docker-graph-storage
          emptyDir: {}
---
# Source: splice-helm/charts/mlmanager/templates/mlflow-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-mlflow
  labels:
    app: splice-mlflow
    chart: mlmanager-0.0.1
    release: my-release
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  selector:
    matchLabels:
      app: splice-mlflow
      release: my-release
      dnsprefix: splicedb
  serviceName: my-release-mlflow-headless
  replicas: 1
  template:
    metadata:
      labels:
        app: splice-mlflow
        release: my-release
        dnsprefix: splicedb
    spec:
      initContainers:
      - name: check-db-ready
        image: docker.io/splicemachine/sm_k8_base:0.0.2
        command:
          - "sh"
          - "-c"
          - "until /usr/bin/curl -kLs -X GET http://my-release-hregion-hl:16030/jmx?qry=com.splicemachine.version:type=DatabaseVersion | grep -q Release; do echo waiting for database; sleep 2; done;"
      containers:
      - name: mlflow
        image: docker.io/splicemachine/sm_k8_mlflow:0.1.22
        imagePullPolicy: IfNotPresent
        env:
          - name: DB_HOST
            value: my-release-hregion.default.svc.cluster.local
          - name: DB_USER
            value: mlmanager
          - name: DB_PASSWORD
            value: admin
          - name: FRAMEWORK_NAME
            value: splicedb-dev1
          - name: MODE
            value: production
          - name: BOBBY_URL
            value: http://my-release-bobby-headless
          - name: ENVIRONMENT
            value: none
          - name: TZ
            value: UTC
        command:
        - "/bin/bash"
        - "/opt/mlflow/scripts/entrypoint.sh"
        resources:
          requests:
            cpu: "0.5"
            memory: 2G
---
# Source: splice-helm/charts/zookeeper/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-zookeeper
  labels:
    app: splice-zookeeper
    chart: zookeeper-0.0.1
    release: my-release
    frameworkId: splicedb-dev1
spec:
  selector:
    matchLabels:
      app: splice-zookeeper
      release: my-release
      dnsprefix: splicedb
  serviceName: my-release-zookeeper-headless
  replicas: 3
  podManagementPolicy: "Parallel"
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: splice-zookeeper
        chart: zookeeper-0.0.1
        release: my-release
        dnsprefix: splicedb
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - topologyKey: "kubernetes.io/hostname"
            labelSelector:
              matchLabels:
                app:  splice-zookeeper
                release: "my-release"
                dnsprefix: splicedb
      imagePullSecrets:
      - name: "regcred"
      containers:
      - name: zookeeper
        imagePullPolicy: "IfNotPresent"
        image: docker.io/splicemachine/sm_k8_zookeeper:0.0.4
        env:
          - name: RELEASE_NAME
            value: my-release
          - name: FRAMEWORKID
            value: splicedb-dev1
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: TZ
            value: "UTC"
        ports:
        - containerPort: 2181
          name: client
        - containerPort: 2888
          name: server
        - containerPort: 3888
          name: leader-election
        command:
        - sh
        - -c
        - "start-zookeeper \
          --servers=3 \
          --data_dir=/var/lib/zookeeper/data \
          --data_log_dir=/var/lib/zookeeper/data/log \
          --conf_dir=/opt/zookeeper/conf \
          --client_port=2181 \
          --election_port=3888 \
          --server_port=2888 \
          --tick_time=2000 \
          --init_limit=30 \
          --sync_limit=15 \
          --heap=512M \
          --max_client_cnxns=0 \
          --snap_retain_count=3 \
          --purge_interval=12 \
          --max_session_timeout=120000 \
          --min_session_timeout=4000 \
          --cnx_timeout=30 \
          --log_level=INFO"

        readinessProbe:
          exec:
            command:
            - sh
            - -c
            - "zookeeper-ready 2181"
          failureThreshold: 6
          initialDelaySeconds: 30
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        livenessProbe:
          exec:
            command:
            - sh
            - -c
            - "zookeeper-ready 2181"
          failureThreshold: 6
          initialDelaySeconds: 120
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        resources:
          requests:
            cpu: "0.5"
            memory: 1Gi
        volumeMounts:
        - name: datadir
          mountPath: /var/lib/zookeeper
      securityContext:
        runAsUser: 1002
        fsGroup: 1000
  volumeClaimTemplates:
  - metadata:
      name: datadir
    spec:
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: 1Gi
---
# Source: splice-helm/charts/hbase/templates/postinstall-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: my-release-postinstall
  labels:
    app: hbase
    chart: hbase-0.0.1
    release: my-release
    component: postinstall
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  template:
    spec:
      initContainers:
      - name: check-db-ready
        image: docker.io/splicemachine/sm_k8_base:0.0.2
        command:
          - "sh"
          - "-c"
          - "until /usr/bin/curl -kLs -X GET http://my-release-hregion-hl.default.svc.cluster.local:16030/jmx?qry=com.splicemachine.version:type=DatabaseVersion | grep -q Release; do echo waiting for database; sleep 2; done;"
      containers:
      - name: postinstall
        image: docker.io/splicemachine/sm_k8_hbase-3.0.0:0.0.63
        imagePullPolicy: 
        env:
          - name: DB_HOST
            value: my-release-hregion.default.svc.cluster.local
          - name: DB_PASSWORD
            value: admin
          - name: DB_ML_PASSWORD
            value: admin
        command:
           - "/bin/bash"
           - "/usr/lib/hbase/bin/splice/postinstall.sh"
      restartPolicy: Never
---
# Source: splice-helm/charts/hbase/templates/dbbackup-job.yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: my-release-dbbackup
  labels:
    app: hbase
    chart: hbase-0.0.1
    release: my-release
    component: backup
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
spec:
  replicas: 0
  schedule: "30 21 * * 0"
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure
          containers:
          - name: dbbackup
            image: docker.io/splicemachine/sm_k8_hbase-3.0.0:0.0.63
            imagePullPolicy: 
            env:
              - name: DB_HOST
                value: my-release-hregion.default.svc.cluster.local
              - name: DB_PASSWORD
                value: admin
              - name: DB_PORT
                value: "1527"
              - name: ENABLERETENTION
                value: "true"
              - name: BACKUPWINDOW
                value: "5"
              - name: BACKUPTYPE
                value: full
              - name: ENTERPRISECODE
                value: 
              - name: DESTINATIONPATH
                value: s3://bucket/backuptest/
              - name: DEBUG
                value: "false"
            command:
               - "/bin/bash"
               - "/usr/lib/hbase/bin/splice/run-backup.sh"
---
# Source: splice-helm/charts/hadoop/templates/ingress.yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: hdfs-ingress
  annotations:
    kubernetes.io/ingress.class: "nginx-core"
    kubernetes.io/tls-acme: "true"
spec:
  rules:
    - host: "splicedb-dev1admin-hdfs.dev.splicemachine-dev.io"
      http:
        paths:
          - path: /
            backend:
              serviceName: my-release-hdfs-nn
              servicePort: 50070
---
# Source: splice-helm/charts/hbase/templates/master-ingress.yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: hbase-ingress
  annotations:
    kubernetes.io/ingress.class: "nginx-core"
    kubernetes.io/tls-acme: "true"
spec:
  rules:
    - host: "splicedb-dev1admin-hbase.dev.splicemachine-dev.io"
      http:
        paths:
          - path: /
            backend:
              serviceName: my-release-hmaster
              servicePort: 16010
---
# Source: splice-helm/charts/hbase/templates/olap-ingress.yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: olap-ingress
  annotations:
    kubernetes.io/ingress.class: "nginx-core"
    kubernetes.io/tls-acme: "true"
spec:
  rules:
    - host: "splicedb-dev1spark.dev.splicemachine-dev.io"
      http:
        paths:
          - path: /
            backend:
              serviceName: my-release-olap
              servicePort: 16040
---
# Source: splice-helm/charts/hbase/templates/region-ingress.yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: splicedb-ingress
  annotations:
    kubernetes.io/ingress.class: default-splice-nginx
    kubernetes.io/tls-acme: "true"
spec:
  rules:
    - host: "splicedb-dev1.dev.splicemachine-dev.io"
      http:
        paths:
          - path: /splicedb
            backend:
              serviceName: my-release-hregion
              servicePort: 1527
---
# Source: splice-helm/charts/hbase/templates/spark-ingress.yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: spark-ingress
  annotations:
    kubernetes.io/ingress.class: "nginx-core"
    kubernetes.io/tls-acme: "true"
spec:
  rules:
    - host: "splicedb-dev1spark.dev.splicemachine-dev.io"
      http:
        paths:
          - path: /
            backend:
              serviceName: my-release-olap-spark-headless
              servicePort: 4040
---
# Source: splice-helm/charts/jupyterhub/templates/ingress.yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: jupyterhub
  labels:
    component: ingress
    app: jupyterhub
    release: my-release
    frameworkId: splicedb-dev1
    dnsprefix: splicedb
    chart: jupyterhub-0.8.2
    heritage: Helm
  annotations:
    kubernetes.io/ingress.class: "nginx-core"
    kubernetes.io/tls-acme: "true"
    nginx.ingress.kubernetes.io/proxy-body-size: "0"
spec:
  rules:
    - host: "splicedb-dev1.dev.splicemachine-dev.io"
      http:
        paths:
          - path: /splicejupyter/
            backend:
              serviceName: proxy-public
              servicePort: 80
---
# Source: splice-helm/charts/jvmprofiler/templates/ingress.yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: jvmprofiler-ingress
  annotations:
    kubernetes.io/ingress.class: "nginx-core"
    kubernetes.io/tls-acme: "true"
    nginx.ingress.kubernetes.io/configuration-snippet: "rewrite ^(/jvmprofiler)$ $1/ redirect;\n"
    nginx.ingress.kubernetes.io/rewrite-target: "/$1"
spec:
  rules:
    - host: "splicedb-dev1admin.dev.splicemachine-dev.io"
      http:
        paths:
          - path: /jvmprofiler/?(.*)
            backend:
              serviceName: my-release-jvmprofiler
              servicePort: 8686
---
# Source: splice-helm/charts/mlmanager/templates/ingress.yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: mlflow-ingress
  annotations:
    kubernetes.io/ingress.class: "nginx-core"
    kubernetes.io/tls-acme: "true"
    nginx.ingress.kubernetes.io/configuration-snippet: "rewrite ^(/mlflow)$ $1/ redirect;\n"
    nginx.ingress.kubernetes.io/rewrite-target: "/$1"
spec:
  rules:
    - host: "splicedb-dev1.dev.splicemachine-dev.io"
      http:
        paths:
          - path: /mlflow/?(.*)
            backend:
              serviceName: my-release-mlflow
              servicePort: 5001
---
# Source: splice-helm/charts/mlmanager/templates/jobtracker-ingress.yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: jobtracker-ingress
  annotations:
    kubernetes.io/ingress.class: "nginx-core"
    kubernetes.io/tls-acme: "true"
spec:
  rules:
    - host: "splicedb-dev1-jobtracker.dev.splicemachine-dev.io"
      http:
        paths:
          - path: /
            backend:
              serviceName: my-release-mlflow
              servicePort: 5003
