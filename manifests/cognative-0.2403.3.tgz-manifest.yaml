---
# Source: cognative/charts/clickhouse/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-clickhouse
  labels:
    helm.sh/chart: clickhouse-0.2403.1
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
---
# Source: cognative/charts/cluster/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-cluster-agent
  namespace: default
  labels:
    helm.sh/chart: cluster-0.84.0
    app.kubernetes.io/name: cluster-agent
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "0.96.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: cognative/charts/collector/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-collector
  namespace: default
  labels:
    helm.sh/chart: collector-0.84.0
    app.kubernetes.io/name: collector
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "0.96.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: cognative/charts/grafana/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: false
metadata:
  labels:
    helm.sh/chart: grafana-7.3.7
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "10.4.0"
    app.kubernetes.io/managed-by: Helm
  name: my-release-grafana
  namespace: default
---
# Source: cognative/charts/node/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-node
  namespace: default
  labels:
    helm.sh/chart: node-0.84.0
    app.kubernetes.io/name: node
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "0.96.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: cognative/charts/clickhouse/templates/secret-config.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-release-clickhouse-configd
  labels:
    helm.sh/chart: clickhouse-0.2403.1
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
data:
  "docker_related_config.xml": PGNsaWNraG91c2U+CiAgPGxpc3Rlbl9ob3N0Pjo6PC9saXN0ZW5faG9zdD4KICA8bGlzdGVuX2hvc3Q+MC4wLjAuMDwvbGlzdGVuX2hvc3Q+CiAgPGxpc3Rlbl90cnk+MTwvbGlzdGVuX3RyeT4KCiAgPGxvZ2dlcj4KICAgIDxjb25zb2xlPjE8L2NvbnNvbGU+CiAgPC9sb2dnZXI+CjwvY2xpY2tob3VzZT4K
---
# Source: cognative/charts/clickhouse/templates/secret-env.yaml
# TODO: look at moving some of this into XML or YAML. See link below.
#       https://github.com/ClickHouse/ClickHouse/blob/master/programs/server/config.yaml.example
apiVersion: v1
kind: Secret
metadata:
  name: my-release-clickhouse-env
  labels:
    helm.sh/chart: clickhouse-0.2403.1
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
data:
  CLICKHOUSE_DB: "ZGVmYXVsdA=="
  CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: "MQ=="
  CLICKHOUSE_USER: "Y2xpY2tob3VzZQ=="
  CLICKHOUSE_PASSWORD: "Y2xpY2tob3VzZQ=="
---
# Source: cognative/charts/grafana/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-release-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-7.3.7
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "10.4.0"
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  
  admin-user: "YWRtaW4="
  admin-password: "YWRtaW4="
  ldap-toml: ""
---
# Source: cognative/charts/cluster/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-cluster-agent
  namespace: default
  labels:
    helm.sh/chart: cluster-0.84.0
    app.kubernetes.io/name: cluster-agent
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "0.96.0"
    app.kubernetes.io/managed-by: Helm
    
data:
  relay: |
    exporters:
      debug: {}
      logging: {}
      otlp/2:
        endpoint: 'my-release-collector:4317'
        tls:
          insecure: true
    extensions:
      health_check:
        endpoint: ${env:MY_POD_IP}:13133
    processors:
      batch:
        send_batch_size: 10000
        timeout: 200ms
      memory_limiter:
        check_interval: 3s
        limit_mib: 1500
        spike_limit_mib: 500
      resourcedetection/system:
        detectors:
        - env
        - system
        - gcp
        - eks
        override: false
        timeout: 2s
    receivers:
      jaeger:
        protocols:
          grpc:
            endpoint: ${env:MY_POD_IP}:14250
          thrift_compact:
            endpoint: ${env:MY_POD_IP}:6831
          thrift_http:
            endpoint: ${env:MY_POD_IP}:14268
      k8s_cluster:
        allocatable_types_to_report:
        - cpu
        - memory
        - storage
        - ephemeral-storage
        collection_interval: 10s
        node_conditions_to_report:
        - Ready
        - MemoryPressure
        - DiskPressure
        - NetworkUnavailable
      k8s_events:
        auth_type: serviceAccount
      k8sobjects:
        objects:
        - exclude_watch_type:
          - DELETED
          group: events.k8s.io
          mode: watch
          name: events
      otlp:
        protocols:
          grpc:
            endpoint: ${env:MY_POD_IP}:4317
          http:
            endpoint: ${env:MY_POD_IP}:4318
      prometheus:
        config:
          scrape_configs:
          - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
            job_name: kubernetes/cadvisor
            kubernetes_sd_configs:
            - role: node
            relabel_configs:
            - replacement: kubernetes.default.svc.cluster.local:443
              target_label: __address__
            - regex: (.+)
              replacement: /api/v1/nodes/$${1}/proxy/metrics/cadvisor
              source_labels:
              - __meta_kubernetes_node_name
              target_label: __metrics_path__
            scheme: https
            tls_config:
              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              insecure_skip_verify: false
              server_name: kubernetes
          - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
            job_name: kubernetes/kubelet
            kubernetes_sd_configs:
            - role: node
            relabel_configs:
            - replacement: kubernetes.default.svc.cluster.local:443
              target_label: __address__
            - regex: (.+)
              replacement: /api/v1/nodes/$${1}/proxy/metrics
              source_labels:
              - __meta_kubernetes_node_name
              target_label: __metrics_path__
            scheme: https
            tls_config:
              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              insecure_skip_verify: false
              server_name: kubernetes
          - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
            job_name: kubernetes/apiservers
            kubernetes_sd_configs:
            - namespaces:
                names:
                - default
              role: endpoints
            relabel_configs:
            - action: keep
              regex: kubernetes;https
              source_labels:
              - __meta_kubernetes_service_name
              - __meta_kubernetes_endpoint_port_name
            - action: replace
              source_labels:
              - __meta_kubernetes_namespace
              target_label: Namespace
            - action: replace
              source_labels:
              - __meta_kubernetes_service_name
              target_label: Service
            scheme: https
            tls_config:
              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              insecure_skip_verify: true
      zipkin:
        endpoint: ${env:MY_POD_IP}:9411
    service:
      extensions:
      - health_check
      pipelines:
        logs:
          exporters:
          - otlp/2
          processors:
          - memory_limiter
          - resourcedetection/system
          - batch
          receivers:
          - k8s_events
          - k8sobjects
        metrics:
          exporters:
          - otlp/2
          processors:
          - memory_limiter
          - resourcedetection/system
          - batch
          receivers:
          - k8s_cluster
          - prometheus
        traces:
          exporters:
          - debug
          processors:
          - memory_limiter
          - batch
          receivers:
          - otlp
          - jaeger
          - zipkin
      telemetry:
        metrics:
          address: ${env:MY_POD_IP}:8888
---
# Source: cognative/charts/collector/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-collector
  namespace: default
  labels:
    helm.sh/chart: collector-0.84.0
    app.kubernetes.io/name: collector
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "0.96.0"
    app.kubernetes.io/managed-by: Helm
    
data:
  relay: |
    exporters:
      clickhouse:
        database: otel
        endpoint: tcp://my-release-clickhouse:9000?dial_timeout=10s&compress=lz4
        logs_table_name: otel_logs
        metrics_table_name: otel_metrics
        password: clickhouse
        retry_on_failure:
          enabled: true
          initial_interval: 5s
          max_elapsed_time: 300s
          max_interval: 30s
        timeout: 5s
        traces_table_name: otel_traces
        ttl: 72h
        username: clickhouse
      debug: {}
      logging: {}
    extensions:
      health_check:
        endpoint: ${env:MY_POD_IP}:13133
    processors:
      batch:
        send_batch_size: 100000
        timeout: 5s
      memory_limiter:
        check_interval: 5s
        limit_percentage: 80
        spike_limit_percentage: 25
    receivers:
      jaeger:
        protocols:
          grpc:
            endpoint: ${env:MY_POD_IP}:14250
          thrift_compact:
            endpoint: ${env:MY_POD_IP}:6831
          thrift_http:
            endpoint: ${env:MY_POD_IP}:14268
      otlp:
        protocols:
          grpc:
            endpoint: ${env:MY_POD_IP}:4317
          http:
            endpoint: ${env:MY_POD_IP}:4318
      prometheus:
        config:
          scrape_configs:
          - job_name: opentelemetry-collector
            scrape_interval: 10s
            static_configs:
            - targets:
              - ${env:MY_POD_IP}:8888
      zipkin:
        endpoint: ${env:MY_POD_IP}:9411
    service:
      extensions:
      - health_check
      pipelines:
        logs:
          exporters:
          - clickhouse
          processors:
          - memory_limiter
          - batch
          receivers:
          - otlp
        metrics:
          exporters:
          - clickhouse
          processors:
          - memory_limiter
          - batch
          receivers:
          - otlp
          - prometheus
        traces:
          exporters:
          - clickhouse
          processors:
          - memory_limiter
          - batch
          receivers:
          - otlp
          - jaeger
          - zipkin
      telemetry:
        metrics:
          address: ${env:MY_POD_IP}:8888
---
# Source: cognative/charts/grafana/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-7.3.7
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "10.4.0"
    app.kubernetes.io/managed-by: Helm
data:
  
  plugins: grafana-clickhouse-datasource
  grafana.ini: |
    [analytics]
    check_for_updates = true
    [grafana_net]
    url = https://grafana.net
    [log]
    mode = console
    [paths]
    data = /var/lib/grafana/
    logs = /var/log/grafana
    plugins = /var/lib/grafana/plugins
    provisioning = /etc/grafana/provisioning
    [server]
    domain = ''
  clickhouse.yaml: |
    apiVersion: 1
    datasources:
    - jsonData:
        defaultDatabase: default
        host: 'my-release-clickhouse'
        logs:
          defaultDatabase: otel
          defaultTable: otel_logs
          otelEnabled: true
          otelVersion: latest
        port: 9000
        protocol: native
        secure: false
        traces:
          defaultDatabase: otel
          defaultTable: otel_traces
          durationUnit: milliseconds
          otelEnabled: true
          otelVersion: latest
        username: clickhouse
      name: ClickHouse
      secureJsonData:
        password: clickhouse
      type: grafana-clickhouse-datasource
---
# Source: cognative/charts/node/templates/configmap-agent.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-node-agent
  namespace: default
  labels:
    helm.sh/chart: node-0.84.0
    app.kubernetes.io/name: node
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "0.96.0"
    app.kubernetes.io/managed-by: Helm
    
data:
  relay: |
    exporters:
      debug: {}
      logging: {}
      otlp/2:
        endpoint: 'my-release-collector:4317'
        tls:
          insecure: true
    extensions:
      health_check:
        endpoint: ${env:MY_POD_IP}:13133
    processors:
      batch:
        send_batch_size: 10000
        timeout: 200ms
      k8sattributes:
        auth_type: serviceAccount
        extract:
          metadata:
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.deployment.name
          - k8s.container.name
          - k8s.namespace.name
          - k8s.node.name
          - k8s.pod.start_time
        filter:
          node_from_env_var: K8S_NODE_NAME
        passthrough: true
        pod_association:
        - sources:
          - from: resource_attribute
            name: k8s.pod.ip
        - sources:
          - from: resource_attribute
            name: k8s.pod.uid
        - sources:
          - from: connection
      memory_limiter:
        check_interval: 3s
        limit_mib: 1500
        spike_limit_mib: 500
      resourcedetection/system:
        detectors:
        - env
        - system
        - gcp
        - eks
        override: false
        timeout: 2s
    receivers:
      filelog:
        exclude:
        - /var/log/pods/default_my-release-node*_*/node/*.log
        include:
        - /var/log/pods/*/*/*.log
        include_file_name: false
        include_file_path: true
        operators:
        - id: get-format
          routes:
          - expr: body matches "^\\{"
            output: parser-docker
          - expr: body matches "^[^ Z]+ "
            output: parser-crio
          - expr: body matches "^[^ Z]+Z"
            output: parser-containerd
          type: router
        - id: parser-crio
          regex: ^(?P<time>[^ Z]+) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$
          timestamp:
            layout: 2006-01-02T15:04:05.999999999Z07:00
            layout_type: gotime
            parse_from: attributes.time
          type: regex_parser
        - combine_field: attributes.log
          combine_with: ""
          id: crio-recombine
          is_last_entry: attributes.logtag == 'F'
          max_log_size: 102400
          output: extract_metadata_from_filepath
          source_identifier: attributes["log.file.path"]
          type: recombine
        - id: parser-containerd
          regex: ^(?P<time>[^ ^Z]+Z) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$
          timestamp:
            layout: '%Y-%m-%dT%H:%M:%S.%LZ'
            parse_from: attributes.time
          type: regex_parser
        - combine_field: attributes.log
          combine_with: ""
          id: containerd-recombine
          is_last_entry: attributes.logtag == 'F'
          max_log_size: 102400
          output: extract_metadata_from_filepath
          source_identifier: attributes["log.file.path"]
          type: recombine
        - id: parser-docker
          output: extract_metadata_from_filepath
          timestamp:
            layout: '%Y-%m-%dT%H:%M:%S.%LZ'
            parse_from: attributes.time
          type: json_parser
        - id: extract_metadata_from_filepath
          parse_from: attributes["log.file.path"]
          regex: ^.*\/(?P<namespace>[^_]+)_(?P<pod_name>[^_]+)_(?P<uid>[a-f0-9\-]+)\/(?P<container_name>[^\._]+)\/(?P<restart_count>\d+)\.log$
          type: regex_parser
        - from: attributes.stream
          to: attributes["log.iostream"]
          type: move
        - from: attributes.container_name
          to: resource["k8s.container.name"]
          type: move
        - from: attributes.namespace
          to: resource["k8s.namespace.name"]
          type: move
        - from: attributes.pod_name
          to: resource["k8s.pod.name"]
          type: move
        - from: attributes.restart_count
          to: resource["k8s.container.restart_count"]
          type: move
        - from: attributes.uid
          to: resource["k8s.pod.uid"]
          type: move
        - from: attributes.log
          to: body
          type: move
        retry_on_failure:
          enabled: true
        start_at: end
      hostmetrics:
        collection_interval: 10s
        root_path: /hostfs
        scrapers:
          cpu: null
          disk: null
          filesystem:
            exclude_fs_types:
              fs_types:
              - autofs
              - binfmt_misc
              - bpf
              - cgroup2
              - configfs
              - debugfs
              - devpts
              - devtmpfs
              - fusectl
              - hugetlbfs
              - iso9660
              - mqueue
              - nsfs
              - overlay
              - proc
              - procfs
              - pstore
              - rpc_pipefs
              - securityfs
              - selinuxfs
              - squashfs
              - sysfs
              - tracefs
              match_type: strict
            exclude_mount_points:
              match_type: regexp
              mount_points:
              - /dev/*
              - /proc/*
              - /sys/*
              - /run/k3s/containerd/*
              - /var/lib/docker/*
              - /var/lib/kubelet/*
              - /snap/*
          load: null
          memory: null
          network: null
      jaeger:
        protocols:
          grpc:
            endpoint: ${env:MY_POD_IP}:14250
          thrift_compact:
            endpoint: ${env:MY_POD_IP}:6831
          thrift_http:
            endpoint: ${env:MY_POD_IP}:14268
      kubeletstats:
        auth_type: serviceAccount
        collection_interval: 10s
        endpoint: https://${env:K8S_NODE_NAME}:10250
        extra_metadata_labels:
        - container.id
        insecure_skip_verify: true
        metric_groups:
        - container
        - pod
        - volume
        - node
      otlp:
        protocols:
          grpc:
            endpoint: ${env:MY_POD_IP}:4317
          http:
            endpoint: ${env:MY_POD_IP}:4318
      prometheus:
        config:
          scrape_configs:
          - job_name: opentelemetry-collector
            scrape_interval: 10s
            static_configs:
            - targets:
              - ${env:MY_POD_IP}:8888
      zipkin:
        endpoint: ${env:MY_POD_IP}:9411
    service:
      extensions:
      - health_check
      pipelines:
        logs:
          exporters:
          - otlp/2
          processors:
          - memory_limiter
          - resourcedetection/system
          - k8sattributes
          - batch
          receivers:
          - filelog
        metrics:
          exporters:
          - otlp/2
          processors:
          - memory_limiter
          - resourcedetection/system
          - k8sattributes
          - batch
          receivers:
          - hostmetrics
          - kubeletstats
        traces:
          exporters:
          - debug
          processors:
          - k8sattributes
          - memory_limiter
          - batch
          receivers:
          - otlp
          - jaeger
          - zipkin
      telemetry:
        metrics:
          address: ${env:MY_POD_IP}:8888
---
# Source: cognative/charts/cluster/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: my-release-cluster-agent
  labels:
    helm.sh/chart: cluster-0.84.0
    app.kubernetes.io/name: cluster-agent
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "0.96.0"
    app.kubernetes.io/managed-by: Helm
    
rules:
  - apiGroups:
    - '*'
    resources:
    - '*'
    verbs:
    - '*'
  - nonResourceURLs:
    - '*'
    verbs:
    - '*'
  - apiGroups: [""]
    resources: ["events", "namespaces", "namespaces/status", "nodes", "nodes/spec", "pods", "pods/status", "replicationcontrollers", "replicationcontrollers/status", "resourcequotas", "services" ]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["apps"]
    resources: ["daemonsets", "deployments", "replicasets", "statefulsets"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["extensions"]
    resources: ["daemonsets", "deployments", "replicasets"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["batch"]
    resources: ["jobs", "cronjobs"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["autoscaling"]
    resources: ["horizontalpodautoscalers"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["events.k8s.io"]
    resources: ["events"]
    verbs: ["watch", "list"]
---
# Source: cognative/charts/grafana/templates/clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels:
    helm.sh/chart: grafana-7.3.7
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "10.4.0"
    app.kubernetes.io/managed-by: Helm
  name: my-release-grafana-clusterrole
rules: []
---
# Source: cognative/charts/node/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: my-release-node
  labels:
    helm.sh/chart: node-0.84.0
    app.kubernetes.io/name: node
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "0.96.0"
    app.kubernetes.io/managed-by: Helm
    
rules:
  - apiGroups: [""]
    resources: ["pods", "namespaces"]
    verbs: ["get", "watch", "list"]
  - apiGroups: ["apps"]
    resources: ["replicasets"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["extensions"]
    resources: ["replicasets"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["nodes/stats"]
    verbs: ["get", "watch", "list"]
---
# Source: cognative/charts/cluster/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: my-release-cluster-agent
  labels:
    helm.sh/chart: cluster-0.84.0
    app.kubernetes.io/name: cluster-agent
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "0.96.0"
    app.kubernetes.io/managed-by: Helm
    
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: my-release-cluster-agent
subjects:
- kind: ServiceAccount
  name: my-release-cluster-agent
  namespace: default
---
# Source: cognative/charts/grafana/templates/clusterrolebinding.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: my-release-grafana-clusterrolebinding
  labels:
    helm.sh/chart: grafana-7.3.7
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "10.4.0"
    app.kubernetes.io/managed-by: Helm
subjects:
  - kind: ServiceAccount
    name: my-release-grafana
    namespace: default
roleRef:
  kind: ClusterRole
  name: my-release-grafana-clusterrole
  apiGroup: rbac.authorization.k8s.io
---
# Source: cognative/charts/node/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: my-release-node
  labels:
    helm.sh/chart: node-0.84.0
    app.kubernetes.io/name: node
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "0.96.0"
    app.kubernetes.io/managed-by: Helm
    
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: my-release-node
subjects:
- kind: ServiceAccount
  name: my-release-node
  namespace: default
---
# Source: cognative/charts/grafana/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: my-release-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-7.3.7
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "10.4.0"
    app.kubernetes.io/managed-by: Helm
rules: []
---
# Source: cognative/charts/grafana/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: my-release-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-7.3.7
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "10.4.0"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: my-release-grafana
subjects:
- kind: ServiceAccount
  name: my-release-grafana
  namespace: default
---
# Source: cognative/charts/clickhouse/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-clickhouse
  labels:
    helm.sh/chart: clickhouse-0.2403.1
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8123
      protocol: TCP
      targetPort: http
    - name: clickhouse
      protocol: TCP
      port: 9000
      targetPort: clickhouse
  selector:
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/instance: my-release
---
# Source: cognative/charts/cluster/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-cluster-agent
  namespace: default
  labels:
    helm.sh/chart: cluster-0.84.0
    app.kubernetes.io/name: cluster-agent
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "0.96.0"
    app.kubernetes.io/managed-by: Helm
    
    component: standalone-collector
spec:
  type: ClusterIP
  ports:
    
    - name: jaeger-compact
      port: 6831
      targetPort: 6831
      protocol: UDP
    - name: jaeger-grpc
      port: 14250
      targetPort: 14250
      protocol: TCP
    - name: jaeger-thrift
      port: 14268
      targetPort: 14268
      protocol: TCP
    - name: otlp
      port: 4317
      targetPort: 4317
      protocol: TCP
      appProtocol: grpc
    - name: otlp-http
      port: 4318
      targetPort: 4318
      protocol: TCP
    - name: zipkin
      port: 9411
      targetPort: 9411
      protocol: TCP
  selector:
    app.kubernetes.io/name: cluster-agent
    app.kubernetes.io/instance: my-release
    component: standalone-collector
  internalTrafficPolicy: Cluster
---
# Source: cognative/charts/collector/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-collector
  namespace: default
  labels:
    helm.sh/chart: collector-0.84.0
    app.kubernetes.io/name: collector
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "0.96.0"
    app.kubernetes.io/managed-by: Helm
    
    component: standalone-collector
spec:
  type: ClusterIP
  ports:
    
    - name: jaeger-compact
      port: 6831
      targetPort: 6831
      protocol: UDP
    - name: jaeger-grpc
      port: 14250
      targetPort: 14250
      protocol: TCP
    - name: jaeger-thrift
      port: 14268
      targetPort: 14268
      protocol: TCP
    - name: otlp
      port: 4317
      targetPort: 4317
      protocol: TCP
      appProtocol: grpc
    - name: otlp-http
      port: 4318
      targetPort: 4318
      protocol: TCP
    - name: zipkin
      port: 9411
      targetPort: 9411
      protocol: TCP
  selector:
    app.kubernetes.io/name: collector
    app.kubernetes.io/instance: my-release
    component: standalone-collector
  internalTrafficPolicy: Cluster
---
# Source: cognative/charts/grafana/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-7.3.7
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "10.4.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: service
      port: 80
      protocol: TCP
      targetPort: 3000
  selector:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: my-release
---
# Source: cognative/charts/node/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: my-release-node-agent
  namespace: default
  labels:
    helm.sh/chart: node-0.84.0
    app.kubernetes.io/name: node
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "0.96.0"
    app.kubernetes.io/managed-by: Helm
    
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: node
      app.kubernetes.io/instance: my-release
      component: agent-collector
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: f98975b69c39d91159cb7af1052e913b6a179fd4f1e9e537cfaacecd85aff642
        
      labels:
        app.kubernetes.io/name: node
        app.kubernetes.io/instance: my-release
        component: agent-collector
        
    spec:
      
      serviceAccountName: my-release-node
      securityContext:
        {}
      containers:
        - name: node
          command:
            - /otelcol-contrib
            - --config=/conf/relay.yaml
          securityContext:
            {}
          image: "otel/opentelemetry-collector-contrib:0.96.0"
          imagePullPolicy: IfNotPresent
          ports:
            
            - name: jaeger-compact
              containerPort: 6831
              protocol: UDP
              hostPort: 6831
            - name: jaeger-grpc
              containerPort: 14250
              protocol: TCP
              hostPort: 14250
            - name: jaeger-thrift
              containerPort: 14268
              protocol: TCP
              hostPort: 14268
            - name: otlp
              containerPort: 4317
              protocol: TCP
              hostPort: 4317
            - name: otlp-http
              containerPort: 4318
              protocol: TCP
              hostPort: 4318
            - name: zipkin
              containerPort: 9411
              protocol: TCP
              hostPort: 9411
          env:
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
            - name: K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          livenessProbe:
            httpGet:
              path: /
              port: 13133
          readinessProbe:
            httpGet:
              path: /
              port: 13133
          volumeMounts:
            - mountPath: /conf
              name: node-configmap
            - name: varlogpods
              mountPath: /var/log/pods
              readOnly: true
            - name: varlibdockercontainers
              mountPath: /var/lib/docker/containers
              readOnly: true
            - name: hostfs
              mountPath: /hostfs
              readOnly: true
              mountPropagation: HostToContainer
      volumes:
        - name: node-configmap
          configMap:
            name: my-release-node-agent
            items:
              - key: relay
                path: relay.yaml
        - name: varlogpods
          hostPath:
            path: /var/log/pods
        - name: varlibdockercontainers
          hostPath:
            path: /var/lib/docker/containers
        - name: hostfs
          hostPath:
            path: /
      hostNetwork: false
---
# Source: cognative/charts/cluster/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-cluster-agent
  namespace: default
  labels:
    helm.sh/chart: cluster-0.84.0
    app.kubernetes.io/name: cluster-agent
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "0.96.0"
    app.kubernetes.io/managed-by: Helm
    
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: cluster-agent
      app.kubernetes.io/instance: my-release
      component: standalone-collector
  strategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: 5e5afdec467115dc799c5db3ab7f6f0514e6df5383663f5e521ce9def96ba3c6
        
      labels:
        app.kubernetes.io/name: cluster-agent
        app.kubernetes.io/instance: my-release
        component: standalone-collector
        
    spec:
      
      serviceAccountName: my-release-cluster-agent
      securityContext:
        {}
      containers:
        - name: cluster
          command:
            - /otelcol-contrib
            - --config=/conf/relay.yaml
          securityContext:
            {}
          image: "otel/opentelemetry-collector-contrib:0.96.0"
          imagePullPolicy: IfNotPresent
          ports:
            
            - name: jaeger-compact
              containerPort: 6831
              protocol: UDP
            - name: jaeger-grpc
              containerPort: 14250
              protocol: TCP
            - name: jaeger-thrift
              containerPort: 14268
              protocol: TCP
            - name: otlp
              containerPort: 4317
              protocol: TCP
            - name: otlp-http
              containerPort: 4318
              protocol: TCP
            - name: zipkin
              containerPort: 9411
              protocol: TCP
          env:
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
          livenessProbe:
            httpGet:
              path: /
              port: 13133
          readinessProbe:
            httpGet:
              path: /
              port: 13133
          volumeMounts:
            - mountPath: /conf
              name: cluster-configmap
      volumes:
        - name: cluster-configmap
          configMap:
            name: my-release-cluster-agent
            items:
              - key: relay
                path: relay.yaml
      hostNetwork: false
---
# Source: cognative/charts/collector/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-collector
  namespace: default
  labels:
    helm.sh/chart: collector-0.84.0
    app.kubernetes.io/name: collector
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "0.96.0"
    app.kubernetes.io/managed-by: Helm
    
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: collector
      app.kubernetes.io/instance: my-release
      component: standalone-collector
  strategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: cbedbdc791704c8a09975a190e6e3572dba66280a23565b0cbbc8ed8314b0224
        
      labels:
        app.kubernetes.io/name: collector
        app.kubernetes.io/instance: my-release
        component: standalone-collector
        
    spec:
      
      serviceAccountName: my-release-collector
      securityContext:
        {}
      containers:
        - name: collector
          command:
            - /otelcol-contrib
            - --config=/conf/relay.yaml
          securityContext:
            {}
          image: "otel/opentelemetry-collector-contrib:0.96.0"
          imagePullPolicy: IfNotPresent
          ports:
            
            - name: jaeger-compact
              containerPort: 6831
              protocol: UDP
            - name: jaeger-grpc
              containerPort: 14250
              protocol: TCP
            - name: jaeger-thrift
              containerPort: 14268
              protocol: TCP
            - name: otlp
              containerPort: 4317
              protocol: TCP
            - name: otlp-http
              containerPort: 4318
              protocol: TCP
            - name: zipkin
              containerPort: 9411
              protocol: TCP
          env:
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
          livenessProbe:
            httpGet:
              path: /
              port: 13133
          readinessProbe:
            httpGet:
              path: /
              port: 13133
          volumeMounts:
            - mountPath: /conf
              name: collector-configmap
      volumes:
        - name: collector-configmap
          configMap:
            name: my-release-collector
            items:
              - key: relay
                path: relay.yaml
      hostNetwork: false
---
# Source: cognative/charts/grafana/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-7.3.7
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "10.4.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: grafana
      app.kubernetes.io/instance: my-release
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: grafana
        app.kubernetes.io/instance: my-release
      annotations:
        checksum/config: 191756d8a3a46f71031a5ba366ef1882f79e7e4f8e1d4a6e35d9f3e18ab4eace
        checksum/sc-dashboard-provider-config: 593c0a8778b83f11fe80ccb21dfb20bc46705e2be3178df1dc4c89d164c8cd9c
        checksum/secret: bed677784356b2af7fb0d87455db21f077853059b594101a4f6532bfbd962a7f
        kubectl.kubernetes.io/default-container: grafana
    spec:
      
      serviceAccountName: my-release-grafana
      automountServiceAccountToken: true
      securityContext:
        fsGroup: 472
        runAsGroup: 472
        runAsNonRoot: true
        runAsUser: 472
      enableServiceLinks: true
      containers:
        - name: grafana
          image: "docker.io/grafana/grafana:10.4.0"
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - name: config
              mountPath: "/etc/grafana/grafana.ini"
              subPath: grafana.ini
            - name: storage
              mountPath: "/var/lib/grafana"
            - name: config
              mountPath: "/etc/grafana/provisioning/datasources/clickhouse.yaml"
              subPath: "clickhouse.yaml"
          ports:
            - name: grafana
              containerPort: 3000
              protocol: TCP
            - name: gossip-tcp
              containerPort: 9094
              protocol: TCP
            - name: gossip-udp
              containerPort: 9094
              protocol: UDP
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: GF_SECURITY_ADMIN_USER
              valueFrom:
                secretKeyRef:
                  name: my-release-grafana
                  key: admin-user
            - name: GF_SECURITY_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-grafana
                  key: admin-password
            - name: GF_INSTALL_PLUGINS
              valueFrom:
                configMapKeyRef:
                  name: my-release-grafana
                  key: plugins
            - name: GF_PATHS_DATA
              value: /var/lib/grafana/
            - name: GF_PATHS_LOGS
              value: /var/log/grafana
            - name: GF_PATHS_PLUGINS
              value: /var/lib/grafana/plugins
            - name: GF_PATHS_PROVISIONING
              value: /etc/grafana/provisioning
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
            initialDelaySeconds: 60
            timeoutSeconds: 30
          readinessProbe:
            httpGet:
              path: /api/health
              port: 3000
      volumes:
        - name: config
          configMap:
            name: my-release-grafana
        - name: storage
          emptyDir: {}
---
# Source: cognative/charts/clickhouse/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-clickhouse
  labels:
    helm.sh/chart: clickhouse-0.2403.1
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 10Gi
  selector:
    matchLabels:
      app.kubernetes.io/name: clickhouse
      app.kubernetes.io/instance: my-release
  template:
    metadata:
      labels:
        app.kubernetes.io/name: clickhouse
        app.kubernetes.io/instance: my-release
    spec:
      serviceAccountName: my-release-clickhouse
      securityContext:
        {}
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: data
        - name: configd
          secret:
            secretName: my-release-clickhouse-configd
      containers:
        - name: clickhouse
          securityContext:
            {}
          image: "clickhouse/clickhouse-server:24.2"
          imagePullPolicy: IfNotPresent
          envFrom:
            - secretRef:
                name: my-release-clickhouse-env
          ports:
            - name: http
              containerPort: 8123
              protocol: TCP
            - name: clickhouse
              containerPort: 9000
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /ping
              port: http
          readinessProbe:
            httpGet:
              path: /ping
              port: http
          resources:
            {}
          volumeMounts:
            - mountPath: /var/lib/clickhouse
              name: data
            - mountPath: /etc/clickhouse-server/config.d
              name: configd
              readOnly: true
---
# Source: cognative/charts/grafana/templates/tests/test-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    helm.sh/chart: grafana-7.3.7
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "10.4.0"
    app.kubernetes.io/managed-by: Helm
  name: my-release-grafana-test
  namespace: default
  annotations:
    "helm.sh/hook": test-success
    "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"
---
# Source: cognative/charts/grafana/templates/tests/test-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-grafana-test
  namespace: default
  annotations:
    "helm.sh/hook": test-success
    "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"
  labels:
    helm.sh/chart: grafana-7.3.7
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "10.4.0"
    app.kubernetes.io/managed-by: Helm
data:
  run.sh: |-
    @test "Test Health" {
      url="http://my-release-grafana/api/health"

      code=$(wget --server-response --spider --timeout 90 --tries 10 ${url} 2>&1 | awk '/^  HTTP/{print $2}')
      [ "$code" == "200" ]
    }
---
# Source: cognative/charts/grafana/templates/tests/test.yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-release-grafana-test
  labels:
    helm.sh/chart: grafana-7.3.7
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "10.4.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test-success
    "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"
  namespace: default
spec:
  serviceAccountName: my-release-grafana-test
  containers:
    - name: my-release-test
      image: "docker.io/bats/bats:v1.4.1"
      imagePullPolicy: "IfNotPresent"
      command: ["/opt/bats/bin/bats", "-t", "/tests/run.sh"]
      volumeMounts:
        - mountPath: /tests
          name: tests
          readOnly: true
  volumes:
    - name: tests
      configMap:
        name: my-release-grafana-test
  restartPolicy: Never
