---
# Source: hdm/charts/elasticsearch/templates/poddisruptionbudget.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: "hdm-deployement-elasticsearch-master-pdb"
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: "hdm-deployement-elasticsearch-master"
---
# Source: hdm/charts/mysql/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-mysql
  namespace: default
  labels:
    app.kubernetes.io/name: mysql
    helm.sh/chart: mysql-8.9.1
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
  annotations:
secrets:
  - name: my-release-mysql
---
# Source: hdm/charts/mysql/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-release-mysql
  namespace: default
  labels:
    app.kubernetes.io/name: mysql
    helm.sh/chart: mysql-8.9.1
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  mysql-root-password: "cm9vdHBhc3N3b3Jk"
  mysql-password: "cGFzc3dvcmQ="
---
# Source: hdm/templates/conf.yaml
apiVersion: v1
data:
  conf-ldap.json: IiIK
kind: Secret
metadata:
  name: hdm-conf-ldap
---
# Source: hdm/templates/conf.yaml
apiVersion: v1
data:
  conf-db.json: IiIK
  create_tables.sql: IiIK
kind: Secret
metadata:
  name: hdm-conf-db
---
# Source: hdm/templates/conf.yaml
apiVersion: v1
data:
  conf-appli.json: IiIK
kind: Secret
metadata:
  name: hdm-conf-appli
---
# Source: hdm/templates/conf.yaml
apiVersion: v1
data:
  msmtprc: IiIK
kind: Secret
metadata:
  name: hdm-conf-mail
---
# Source: hdm/charts/mysql/templates/primary/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-mysql
  namespace: default
  labels:
    app.kubernetes.io/name: mysql
    helm.sh/chart: mysql-8.9.1
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: primary
data:
  my.cnf: |-
    
    [mysqld]
    default_authentication_plugin=mysql_native_password
    skip-name-resolve
    explicit_defaults_for_timestamp
    basedir=/opt/bitnami/mysql
    plugin_dir=/opt/bitnami/mysql/lib/plugin
    port=3306
    socket=/opt/bitnami/mysql/tmp/mysql.sock
    datadir=/bitnami/mysql/data
    tmpdir=/opt/bitnami/mysql/tmp
    max_allowed_packet=16M
    bind-address=0.0.0.0
    pid-file=/opt/bitnami/mysql/tmp/mysqld.pid
    log-error=/opt/bitnami/mysql/logs/mysqld.log
    character-set-server=UTF8
    collation-server=utf8_general_ci
    slow_query_log=0
    slow_query_log_file=/opt/bitnami/mysql/logs/mysqld.log
    long_query_time=10.0
    
    [client]
    port=3306
    socket=/opt/bitnami/mysql/tmp/mysql.sock
    default-character-set=UTF8
    plugin_dir=/opt/bitnami/mysql/lib/plugin
    
    [manager]
    port=3306
    socket=/opt/bitnami/mysql/tmp/mysql.sock
    pid-file=/opt/bitnami/mysql/tmp/mysqld.pid
---
# Source: hdm/templates/conf.yaml
apiVersion: v1
data:
  version.json: "{\n\t\"version\": \" 2.3.0 \"\n}"
kind: ConfigMap
metadata:
  name: "my-release-version"
---
# Source: hdm/charts/elasticsearch/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: hdm-deployement-elasticsearch-master
  labels:
    heritage: "Helm"
    release: "my-release"
    chart: "elasticsearch"
    app: "hdm-deployement-elasticsearch-master"
  annotations:
    {}
spec:
  type: ClusterIP
  selector:
    release: "my-release"
    chart: "elasticsearch"
    app: "hdm-deployement-elasticsearch-master"
  ports:
  - name: http
    protocol: TCP
    port: 9200
  - name: transport
    protocol: TCP
    port: 9300
---
# Source: hdm/charts/elasticsearch/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: hdm-deployement-elasticsearch-master-headless
  labels:
    heritage: "Helm"
    release: "my-release"
    chart: "elasticsearch"
    app: "hdm-deployement-elasticsearch-master"
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  clusterIP: None # This is needed for statefulset hostnames like elasticsearch-0 to resolve
  # Create endpoints also if the related pod isn't ready
  publishNotReadyAddresses: true
  selector:
    app: "hdm-deployement-elasticsearch-master"
  ports:
  - name: http
    port: 9200
  - name: transport
    port: 9300
---
# Source: hdm/charts/kibana/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-kibana
  labels: 
    app: kibana
    release: "my-release"
    heritage: Helm
spec:
  type: ClusterIP
  ports:
    - port: 5601
      protocol: TCP
      name: http
      targetPort: 5601
  selector:
    app: kibana
    release: "my-release"
---
# Source: hdm/charts/mysql/templates/primary/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-mysql-headless
  namespace: default
  labels:
    app.kubernetes.io/name: mysql
    helm.sh/chart: mysql-8.9.1
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: primary
  annotations:
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: mysql
      port: 3306
      targetPort: mysql
  selector: 
    app.kubernetes.io/name: mysql
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: primary
---
# Source: hdm/charts/mysql/templates/primary/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-mysql
  namespace: default
  labels:
    app.kubernetes.io/name: mysql
    helm.sh/chart: mysql-8.9.1
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: primary
  annotations:
spec:
  type: ClusterIP
  ports:
    - name: mysql
      port: 3306
      protocol: TCP
      targetPort: mysql
      nodePort: null
  selector: 
    app.kubernetes.io/name: mysql
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: primary
---
# Source: hdm/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: "my-release-hdm"
spec:
  type: ClusterIP
  ports:
    - name: 80tcp
      port: 80
      protocol: TCP
      targetPort: 80
  selector:
    app: "my-release-hdm"
    release: my-release
---
# Source: hdm/charts/kibana/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-kibana
  labels: 
    app: kibana
    release: "my-release"
    heritage: Helm
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: kibana
      release: "my-release"
  template:
    metadata:
      labels:
        app: kibana
        release: "my-release"
      annotations:
        
    spec:
      automountServiceAccountToken: true
      securityContext:
        fsGroup: 1000
      volumes:
      containers:
      - name: kibana
        securityContext:
          capabilities:
            drop:
            - ALL
          runAsNonRoot: true
          runAsUser: 1000
        image: "docker.elastic.co/kibana/kibana:7.16.1"
        imagePullPolicy: "IfNotPresent"
        env:
          - name: ELASTICSEARCH_HOSTS
            value: "http://hdm-deployement-elasticsearch-master:9200"
          - name: SERVER_HOST
            value: "0.0.0.0"
          - name: NODE_OPTIONS
            value: --max-old-space-size=1800
        readinessProbe:
          failureThreshold: 3
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 3
          timeoutSeconds: 5
          exec:
            command:
              - sh
              - -c
              - |
                #!/usr/bin/env bash -e

                # Disable nss cache to avoid filling dentry cache when calling curl
                # This is required with Kibana Docker using nss < 3.52
                export NSS_SDB_USE_CACHE=no

                http () {
                    local path="${1}"
                    set -- -XGET -s --fail -L

                    if [ -n "${ELASTICSEARCH_USERNAME}" ] && [ -n "${ELASTICSEARCH_PASSWORD}" ]; then
                      set -- "$@" -u "${ELASTICSEARCH_USERNAME}:${ELASTICSEARCH_PASSWORD}"
                    fi

                    STATUS=$(curl --output /dev/null --write-out "%{http_code}" -k "$@" "http://localhost:5601${path}")
                    if [[ "${STATUS}" -eq 200 ]]; then
                      exit 0
                    fi

                    echo "Error: Got HTTP code ${STATUS} but expected a 200"
                    exit 1
                }

                http "/app/kibana"
        ports:
        - containerPort: 5601
        resources:
          limits:
            cpu: 1000m
            memory: 2Gi
          requests:
            cpu: 1000m
            memory: 2Gi
        volumeMounts:
---
# Source: hdm/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: "my-release-hdm"
  labels:
    app: "my-release-hdm"
    chart: "hdm-2.4.5"
    heritage: Helm
    release: my-release
    app.kubernetes.io/name: hdm
    helm.sh/chart: hdm-2.4.5
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "2.3.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  selector:
    matchLabels:
      app: "my-release-hdm"
      release: my-release
  template:
    metadata:
      labels:
        app: "my-release-hdm"
        release: my-release
      annotations:
        rollme: "5rIRy"
    spec:
      containers:
      - name: hdm
        image: "ghcr.io/curie-data-factory/hdm:2.3.0"
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
          name: "80tcp"
          protocol: TCP
        env:
        - name: APACHE_LOG_DIR
          value: /var/www/html/
        - name: APACHE_RUN_DIR
          value: /etc/apache2
        - name: APACHE_RUN_GROUP
          value: www-data
        - name: APACHE_RUN_USER
          value: www-data
        volumeMounts:
        - mountPath: /var/www/html/conf/appli/
          name: hdm-conf-appli
        - mountPath: /var/www/html/conf/db/
          name: hdm-conf-db
        - mountPath: /var/www/html/conf/ldap/
          name: hdm-conf-ldap
        - mountPath: /var/www/html/conf/mail/
          name: hdm-conf-mail
        - mountPath: /var/www/html/version/
          name: hdm-version
      volumes:
      - name: hdm-conf-appli
        secret:
          defaultMode: 0777
          items:
          - key: conf-appli.json
            path: ./conf-appli.json
          secretName: hdm-conf-appli
      - name: hdm-conf-db
        secret:
          defaultMode: 0777
          items:
          - key: conf-db.json
            path: ./conf-db.json
          - key: create_tables.sql
            path: ./create_tables.sql
          optional: false
          secretName: hdm-conf-db
      - name: hdm-conf-ldap
        secret:
          defaultMode: 0777
          items:
          - key: conf-ldap.json
            path: ./conf-ldap.json
          secretName: hdm-conf-ldap
      - name: hdm-conf-mail
        secret:
          defaultMode: 0777
          items:
          - key: msmtprc
            path: ./msmtprc
          secretName: hdm-conf-mail
      - name: hdm-version
        configMap:
          defaultMode: 0777
          items:
          - key: version.json
            path: ./version.json
          name: "my-release-version"
---
# Source: hdm/charts/elasticsearch/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: hdm-deployement-elasticsearch-master
  labels:
    heritage: "Helm"
    release: "my-release"
    chart: "elasticsearch"
    app: "hdm-deployement-elasticsearch-master"
  annotations:
    esMajorVersion: "7"
spec:
  serviceName: hdm-deployement-elasticsearch-master-headless
  selector:
    matchLabels:
      app: "hdm-deployement-elasticsearch-master"
  replicas: 3
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      name: "hdm-deployement-elasticsearch-master"
      labels:
        release: "my-release"
        chart: "elasticsearch"
        app: "hdm-deployement-elasticsearch-master"
      annotations:
        
    spec:
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
      automountServiceAccountToken: true
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - "hdm-deployement-elasticsearch-master"
            topologyKey: kubernetes.io/hostname
      terminationGracePeriodSeconds: 120
      volumes:
      enableServiceLinks: true
      initContainers:
      - name: configure-sysctl
        securityContext:
          runAsUser: 0
          privileged: true
        image: "docker.elastic.co/elasticsearch/elasticsearch:7.16.1"
        imagePullPolicy: "IfNotPresent"
        command: ["sysctl", "-w", "vm.max_map_count=262144"]
        resources:
          {}

      containers:
      - name: "elasticsearch"
        securityContext:
          capabilities:
            drop:
            - ALL
          runAsNonRoot: true
          runAsUser: 1000
        image: "docker.elastic.co/elasticsearch/elasticsearch:7.16.1"
        imagePullPolicy: "IfNotPresent"
        readinessProbe:
          exec:
            command:
              - bash
              - -c
              - |
                set -e
                # If the node is starting up wait for the cluster to be ready (request params: "wait_for_status=green&timeout=1s" )
                # Once it has started only check that the node itself is responding
                START_FILE=/tmp/.es_start_file

                # Disable nss cache to avoid filling dentry cache when calling curl
                # This is required with Elasticsearch Docker using nss < 3.52
                export NSS_SDB_USE_CACHE=no

                http () {
                  local path="${1}"
                  local args="${2}"
                  set -- -XGET -s

                  if [ "$args" != "" ]; then
                    set -- "$@" $args
                  fi

                  if [ -n "${ELASTIC_USERNAME}" ] && [ -n "${ELASTIC_PASSWORD}" ]; then
                    set -- "$@" -u "${ELASTIC_USERNAME}:${ELASTIC_PASSWORD}"
                  fi

                  curl --output /dev/null -k "$@" "http://127.0.0.1:9200${path}"
                }

                if [ -f "${START_FILE}" ]; then
                  echo 'Elasticsearch is already running, lets check the node is healthy'
                  HTTP_CODE=$(http "/" "-w %{http_code}")
                  RC=$?
                  if [[ ${RC} -ne 0 ]]; then
                    echo "curl --output /dev/null -k -XGET -s -w '%{http_code}' \${BASIC_AUTH} http://127.0.0.1:9200/ failed with RC ${RC}"
                    exit ${RC}
                  fi
                  # ready if HTTP code 200, 503 is tolerable if ES version is 6.x
                  if [[ ${HTTP_CODE} == "200" ]]; then
                    exit 0
                  elif [[ ${HTTP_CODE} == "503" && "7" == "6" ]]; then
                    exit 0
                  else
                    echo "curl --output /dev/null -k -XGET -s -w '%{http_code}' \${BASIC_AUTH} http://127.0.0.1:9200/ failed with HTTP code ${HTTP_CODE}"
                    exit 1
                  fi

                else
                  echo 'Waiting for elasticsearch cluster to become ready (request params: "wait_for_status=green&timeout=1s" )'
                  if http "/_cluster/health?wait_for_status=green&timeout=1s" "--fail" ; then
                    touch ${START_FILE}
                    exit 0
                  else
                    echo 'Cluster is not yet ready (request params: "wait_for_status=green&timeout=1s" )'
                    exit 1
                  fi
                fi
          failureThreshold: 3
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 3
          timeoutSeconds: 5
        ports:
        - name: http
          containerPort: 9200
        - name: transport
          containerPort: 9300
        resources:
          limits:
            cpu: 1000m
            memory: 2Gi
          requests:
            cpu: 1000m
            memory: 2Gi
        env:
          - name: node.name
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: cluster.initial_master_nodes
            value: "hdm-deployement-elasticsearch-master-0,hdm-deployement-elasticsearch-master-1,hdm-deployement-elasticsearch-master-2,"
          - name: discovery.seed_hosts
            value: "hdm-deployement-elasticsearch-master-headless"
          - name: cluster.name
            value: "hdm-deployement-elasticsearch"
          - name: network.host
            value: "0.0.0.0"
          - name: cluster.deprecation_indexing.enabled
            value: "false"
          - name: node.data
            value: "true"
          - name: node.ingest
            value: "true"
          - name: node.master
            value: "true"
          - name: node.ml
            value: "true"
          - name: node.remote_cluster_client
            value: "true"
        volumeMounts:
---
# Source: hdm/charts/mysql/templates/primary/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-mysql
  namespace: default
  labels:
    app.kubernetes.io/name: mysql
    helm.sh/chart: mysql-8.9.1
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: primary
spec:
  replicas: 1
  selector:
    matchLabels: 
      app.kubernetes.io/name: mysql
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: primary
  serviceName: my-release-mysql
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/configuration: 5d92abe232387374305152165afc6c534c2f68864f813ed7cc7f73c31b3b8470
      labels:
        app.kubernetes.io/name: mysql
        helm.sh/chart: mysql-8.9.1
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: primary
    spec:
      
      serviceAccountName: my-release-mysql
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: mysql
                    app.kubernetes.io/instance: my-release
                    app.kubernetes.io/component: primary
                namespaces:
                  - "default"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      containers:
        - name: mysql
          image: docker.io/bitnami/mysql:8.0.28-debian-10-r73
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MYSQL_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-mysql
                  key: mysql-root-password
            - name: MYSQL_USER
              value: "hdm"
            - name: MYSQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-mysql
                  key: mysql-password
            - name: MYSQL_DATABASE
              value: "hdm"
          ports:
            - name: mysql
              containerPort: 3306
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
            exec:
              command:
                - /bin/bash
                - -ec
                - |
                  password_aux="${MYSQL_ROOT_PASSWORD:-}"
                  if [[ -f "${MYSQL_ROOT_PASSWORD_FILE:-}" ]]; then
                      password_aux=$(cat "$MYSQL_ROOT_PASSWORD_FILE")
                  fi
                  mysqladmin status -uroot -p"${password_aux}"
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
            exec:
              command:
                - /bin/bash
                - -ec
                - |
                  password_aux="${MYSQL_ROOT_PASSWORD:-}"
                  if [[ -f "${MYSQL_ROOT_PASSWORD_FILE:-}" ]]; then
                      password_aux=$(cat "$MYSQL_ROOT_PASSWORD_FILE")
                  fi
                  mysqladmin status -uroot -p"${password_aux}"
          startupProbe:
            failureThreshold: 10
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
            exec:
              command:
                - /bin/bash
                - -ec
                - |
                  password_aux="${MYSQL_ROOT_PASSWORD:-}"
                  if [[ -f "${MYSQL_ROOT_PASSWORD_FILE:-}" ]]; then
                      password_aux=$(cat "$MYSQL_ROOT_PASSWORD_FILE")
                  fi
                  mysqladmin status -uroot -p"${password_aux}"
          resources: 
            limits: {}
            requests: {}
          volumeMounts:
            - name: data
              mountPath: /bitnami/mysql
            - name: config
              mountPath: /opt/bitnami/mysql/conf/my.cnf
              subPath: my.cnf
      volumes:
        - name: config
          configMap:
            name: my-release-mysql
  volumeClaimTemplates:
    - metadata:
        name: data
        labels: 
          app.kubernetes.io/name: mysql
          app.kubernetes.io/instance: my-release
          app.kubernetes.io/component: primary
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "20Gi"
---
# Source: hdm/charts/elasticsearch/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: hdm-deployement-elasticsearch-master
  labels:
    app: elasticsearch
    release: my-release
    heritage: Helm
spec:
  ingressClassName: "nginx"
  tls:
    - hosts:
      - hdm-elastic.company.com
      secretName: hdm-elastic
  rules:
  - host: hdm-elastic.company.com
    http:
      paths:
      - path: /
        pathType: ImplementationSpecific
        backend:
          service:
            name: hdm-deployement-elasticsearch-master
            port:
              number: 9200
---
# Source: hdm/charts/kibana/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-release-kibana
  labels: 
    app: kibana
    release: "my-release"
    heritage: Helm
spec:
  ingressClassName: "nginx"
  tls:
    - hosts:
      - hdm-kibana.company.com
      secretName: hdm-kibana
  rules:
  - host: hdm-kibana.company.com
    http:
      paths:
      - path: /
        pathType: ImplementationSpecific
        backend:
          service:
            name: my-release-kibana
            port: 
              number: 5601
---
# Source: hdm/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: hdm
  labels:
    app: hdm
    chart: "hdm-2.4.5"
    heritage: Helm
    release: my-release
spec:
  tls:
    - hosts:
      - hdm.company.com
      secretName: hdm
  rules:
  - host: "hdm.company.com"
    http:
      paths:
        - path: /
          pathType: Prefix
          backend:
            service:
              name: my-release-hdm
              port:
                name: 80tcp
---
# Source: hdm/charts/elasticsearch/templates/test/test-elasticsearch-health.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "my-release-thyiy-test"
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-delete-policy": hook-succeeded
spec:
  securityContext:
    fsGroup: 1000
    runAsUser: 1000
  containers:
  - name: "my-release-pfamu-test"
    image: "docker.elastic.co/elasticsearch/elasticsearch:7.16.1"
    imagePullPolicy: "IfNotPresent"
    command:
      - "sh"
      - "-c"
      - |
        #!/usr/bin/env bash -e
        curl -XGET --fail 'hdm-deployement-elasticsearch-master:9200/_cluster/health?wait_for_status=green&timeout=1s'
  restartPolicy: Never
