---
# Source: knative-serving/templates/net/kourier.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: kourier-system
  labels:
    networking.knative.dev/ingress-provider: kourier
    app.kubernetes.io/component: net-kourier
    serving.knative.dev/release: "1.6.0"
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: knative-serving/templates/core/controller/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: controller
  namespace: default
  labels:
    serving.knative.dev/release: "1.6.0"
---
# Source: knative-serving/templates/net/kourier.yaml
# Copyright 2020 The Knative Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: v1
kind: ServiceAccount
metadata:
  name: net-kourier
  namespace: default
  labels:
    networking.knative.dev/ingress-provider: kourier
    app.kubernetes.io/component: net-kourier
    serving.knative.dev/release: "1.6.0"
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: knative-serving/templates/core/domain-mapping/webhook.yaml
apiVersion: v1
kind: Secret
metadata:
  name: domainmapping-webhook-certs
  namespace: default
  labels:
    app.kubernetes.io/component: domain-mapping
    serving.knative.dev/release: "1.6.0"
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: knative-serving/templates/core/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  # Do not drop -ctrl-ca suffix as control-protocol requires it.
  # https://github.com/knative-sandbox/control-protocol/blob/main/pkg/certificates/reconciler/controller.go
  name: serving-certs-ctrl-ca
  namespace: default
---
# Source: knative-serving/templates/core/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: knative-serving-certs
  namespace: default
  labels:
    serving-certs-ctrl: "data-plane"
    networking.internal.knative.dev/certificate-uid: "serving-certs"
# The data is populated when internal-encryption is enabled.
---
# Source: knative-serving/templates/core/webhook/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: webhook-certs
  namespace: knative-serving
  labels:
    serving.knative.dev/release: "1.6.0"
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
# The data is populated at install time.
---
# Source: knative-serving/templates/core/configmaps.yaml
# Copyright 2018 The Knative Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: v1
kind: ConfigMap
metadata:
  name: config-autoscaler
  namespace: default
  labels:
    app.kubernetes.io/component: autoscaler
    serving.knative.dev/release: "1.6.0"
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
data:
  _example: |
    ################################
    #                              #
    #    EXAMPLE CONFIGURATION     #
    #                              #
    ################################

    # This block is not actually functional configuration,
    # but serves to illustrate the available configuration
    # options and document them in a way that is accessible
    # to users that `kubectl edit` this config map.
    #
    # These sample configuration options may be copied out of
    # this example block and unindented to be in the data block
    # to actually change the configuration.

    # The Revision ContainerConcurrency field specifies the maximum number
    # of requests the Container can handle at once. Container concurrency
    # target percentage is how much of that maximum to use in a stable
    # state. E.g. if a Revision specifies ContainerConcurrency of 10, then
    # the Autoscaler will try to maintain 7 concurrent connections per pod
    # on average.
    # Note: this limit will be applied to container concurrency set at every
    # level (ConfigMap, Revision Spec or Annotation).
    # For legacy and backwards compatibility reasons, this value also accepts
    # fractional values in (0, 1] interval (i.e. 0.7 â‡’ 70%).
    # Thus minimal percentage value must be greater than 1.0, or it will be
    # treated as a fraction.
    # NOTE: that this value does not affect actual number of concurrent requests
    #       the user container may receive, but only the average number of requests
    #       that the revision pods will receive.
    container-concurrency-target-percentage: "70"

    # The container concurrency target default is what the Autoscaler will
    # try to maintain when concurrency is used as the scaling metric for the
    # Revision and the Revision specifies unlimited concurrency.
    # When revision explicitly specifies container concurrency, that value
    # will be used as a scaling target for autoscaler.
    # When specifying unlimited concurrency, the autoscaler will
    # horizontally scale the application based on this target concurrency.
    # This is what we call "soft limit" in the documentation, i.e. it only
    # affects number of pods and does not affect the number of requests
    # individual pod processes.
    # The value must be a positive number such that the value multiplied
    # by container-concurrency-target-percentage is greater than 0.01.
    # NOTE: that this value will be adjusted by application of
    #       container-concurrency-target-percentage, i.e. by default
    #       the system will target on average 70 concurrent requests
    #       per revision pod.
    # NOTE: Only one metric can be used for autoscaling a Revision.
    container-concurrency-target-default: "100"

    # The requests per second (RPS) target default is what the Autoscaler will
    # try to maintain when RPS is used as the scaling metric for a Revision and
    # the Revision specifies unlimited RPS. Even when specifying unlimited RPS,
    # the autoscaler will horizontally scale the application based on this
    # target RPS.
    # Must be greater than 1.0.
    # NOTE: Only one metric can be used for autoscaling a Revision.
    requests-per-second-target-default: "200"

    # The target burst capacity specifies the size of burst in concurrent
    # requests that the system operator expects the system will receive.
    # Autoscaler will try to protect the system from queueing by introducing
    # Activator in the request path if the current spare capacity of the
    # service is less than this setting.
    # If this setting is 0, then Activator will be in the request path only
    # when the revision is scaled to 0.
    # If this setting is > 0 and container-concurrency-target-percentage is
    # 100% or 1.0, then activator will always be in the request path.
    # -1 denotes unlimited target-burst-capacity and activator will always
    # be in the request path.
    # Other negative values are invalid.
    target-burst-capacity: "211"

    # When operating in a stable mode, the autoscaler operates on the
    # average concurrency over the stable window.
    # Stable window must be in whole seconds.
    stable-window: "60s"

    # When observed average concurrency during the panic window reaches
    # panic-threshold-percentage the target concurrency, the autoscaler
    # enters panic mode. When operating in panic mode, the autoscaler
    # scales on the average concurrency over the panic window which is
    # panic-window-percentage of the stable-window.
    # Must be in the [1, 100] range.
    # When computing the panic window it will be rounded to the closest
    # whole second, at least 1s.
    panic-window-percentage: "10.0"

    # The percentage of the container concurrency target at which to
    # enter panic mode when reached within the panic window.
    panic-threshold-percentage: "200.0"

    # Max scale up rate limits the rate at which the autoscaler will
    # increase pod count. It is the maximum ratio of desired pods versus
    # observed pods.
    # Cannot be less or equal to 1.
    # I.e with value of 2.0 the number of pods can at most go N to 2N
    # over single Autoscaler period (2s), but at least N to
    # N+1, if Autoscaler needs to scale up.
    max-scale-up-rate: "1000.0"

    # Max scale down rate limits the rate at which the autoscaler will
    # decrease pod count. It is the maximum ratio of observed pods versus
    # desired pods.
    # Cannot be less or equal to 1.
    # I.e. with value of 2.0 the number of pods can at most go N to N/2
    # over single Autoscaler evaluation period (2s), but at
    # least N to N-1, if Autoscaler needs to scale down.
    max-scale-down-rate: "2.0"

    # Scale to zero feature flag.
    enable-scale-to-zero: "true"

    # Scale to zero grace period is the time an inactive revision is left
    # running before it is scaled to zero (must be positive, but recommended
    # at least a few seconds if running with mesh networking).
    # This is the upper limit and is provided not to enforce timeout after
    # the revision stopped receiving requests for stable window, but to
    # ensure network reprogramming to put activator in the path has completed.
    # If the system determines that a shorter period is satisfactory,
    # then the system will only wait that amount of time before scaling to 0.
    # NOTE: this period might actually be 0, if activator has been
    # in the request path sufficiently long.
    # If there is necessity for the last pod to linger longer use
    # scale-to-zero-pod-retention-period flag.
    scale-to-zero-grace-period: "30s"

    # Scale to zero pod retention period defines the minimum amount
    # of time the last pod will remain after Autoscaler has decided to
    # scale to zero.
    # This flag is for the situations where the pod startup is very expensive
    # and the traffic is bursty (requiring smaller windows for fast action),
    # but patchy.
    # The larger of this flag and `scale-to-zero-grace-period` will effectively
    # determine how the last pod will hang around.
    scale-to-zero-pod-retention-period: "0s"

    # pod-autoscaler-class specifies the default pod autoscaler class
    # that should be used if none is specified. If omitted,
    # the Knative Pod Autoscaler (KPA) is used by default.
    pod-autoscaler-class: "kpa.autoscaling.knative.dev"

    # The capacity of a single activator task.
    # The `unit` is one concurrent request proxied by the activator.
    # activator-capacity must be at least 1.
    # This value is used for computation of the Activator subset size.
    # See the algorithm here: http://bit.ly/38XiCZ3.
    # TODO(vagababov): tune after actual benchmarking.
    activator-capacity: "100.0"

    # initial-scale is the cluster-wide default value for the initial target
    # scale of a revision after creation, unless overridden by the
    # "autoscaling.knative.dev/initialScale" annotation.
    # This value must be greater than 0 unless allow-zero-initial-scale is true.
    initial-scale: "1"

    # allow-zero-initial-scale controls whether either the cluster-wide initial-scale flag,
    # or the "autoscaling.knative.dev/initialScale" annotation, can be set to 0.
    allow-zero-initial-scale: "false"

    # min-scale is the cluster-wide default value for the min scale of a revision,
    # unless overridden by the "autoscaling.knative.dev/minScale" annotation.
    min-scale: "0"

    # max-scale is the cluster-wide default value for the max scale of a revision,
    # unless overridden by the "autoscaling.knative.dev/maxScale" annotation.
    # If set to 0, the revision has no maximum scale.
    max-scale: "0"

    # scale-down-delay is the amount of time that must pass at reduced
    # concurrency before a scale down decision is applied. This can be useful,
    # for example, to maintain replica count and avoid a cold start penalty if
    # more requests come in within the scale down delay period.
    # The default, 0s, imposes no delay at all.
    scale-down-delay: "0s"

    # max-scale-limit sets the maximum permitted value for the max scale of a revision.
    # When this is set to a positive value, a revision with a maxScale above that value
    # (including a maxScale of "0" = unlimited) is disallowed.
    # A value of zero (the default) allows any limit, including unlimited.
    max-scale-limit: "0"
---
# Source: knative-serving/templates/core/configmaps.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-defaults
  namespace: default
  labels:
    serving.knative.dev/release: "1.6.0"
    app.kubernetes.io/component: controller
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
data:
  _example: |
    ################################
    #                              #
    #    EXAMPLE CONFIGURATION     #
    #                              #
    ################################

    # This block is not actually functional configuration,
    # but serves to illustrate the available configuration
    # options and document them in a way that is accessible
    # to users that `kubectl edit` this config map.
    #
    # These sample configuration options may be copied out of
    # this example block and unindented to be in the data block
    # to actually change the configuration.

    # revision-timeout-seconds contains the default number of
    # seconds to use for the revision's per-request timeout, if
    # none is specified.
    revision-timeout-seconds: "300"  # 5 minutes

    # max-revision-timeout-seconds contains the maximum number of
    # seconds that can be used for revision-timeout-seconds.
    # This value must be greater than or equal to revision-timeout-seconds.
    # If omitted, the system default is used (600 seconds).
    #
    # If this value is increased, the activator's terminationGraceTimeSeconds
    # should also be increased to prevent in-flight requests being disrupted.
    max-revision-timeout-seconds: "600"  # 10 minutes

    # revision-cpu-request contains the cpu allocation to assign
    # to revisions by default.  If omitted, no value is specified
    # and the system default is used.
    # Below is an example of setting revision-cpu-request.
    # By default, it is not set by Knative.
    revision-cpu-request: "400m"  # 0.4 of a CPU (aka 400 milli-CPU)

    # revision-memory-request contains the memory allocation to assign
    # to revisions by default.  If omitted, no value is specified
    # and the system default is used.
    # Below is an example of setting revision-memory-request.
    # By default, it is not set by Knative.
    revision-memory-request: "100M"  # 100 megabytes of memory

    # revision-ephemeral-storage-request contains the ephemeral storage
    # allocation to assign to revisions by default.  If omitted, no value is
    # specified and the system default is used.
    revision-ephemeral-storage-request: "500M"  # 500 megabytes of storage

    # revision-cpu-limit contains the cpu allocation to limit
    # revisions to by default.  If omitted, no value is specified
    # and the system default is used.
    # Below is an example of setting revision-cpu-limit.
    # By default, it is not set by Knative.
    revision-cpu-limit: "1000m"  # 1 CPU (aka 1000 milli-CPU)

    # revision-memory-limit contains the memory allocation to limit
    # revisions to by default.  If omitted, no value is specified
    # and the system default is used.
    # Below is an example of setting revision-memory-limit.
    # By default, it is not set by Knative.
    revision-memory-limit: "200M"  # 200 megabytes of memory

    # revision-ephemeral-storage-limit contains the ephemeral storage
    # allocation to limit revisions to by default.  If omitted, no value is
    # specified and the system default is used.
    revision-ephemeral-storage-limit: "750M"  # 750 megabytes of storage

    # container-name-template contains a template for the default
    # container name, if none is specified.  This field supports
    # Go templating and is supplied with the ObjectMeta of the
    # enclosing Service or Configuration, so values such as
    #  are also valid.
    container-name-template: "user-container"

    # init-container-name-template contains a template for the default
    # init container name, if none is specified.  This field supports
    # Go templating and is supplied with the ObjectMeta of the
    # enclosing Service or Configuration, so values such as
    #  are also valid.
    init-container-name-template: "init-container"

    # container-concurrency specifies the maximum number
    # of requests the Container can handle at once, and requests
    # above this threshold are queued.  Setting a value of zero
    # disables this throttling and lets through as many requests as
    # the pod receives.
    container-concurrency: "0"

    # The container concurrency max limit is an operator setting ensuring that
    # the individual revisions cannot have arbitrary large concurrency
    # values, or autoscaling targets. `container-concurrency` default setting
    # must be at or below this value.
    #
    # Must be greater than 1.
    #
    # Note: even with this set, a user can choose a containerConcurrency
    # of 0 (i.e. unbounded) unless allow-container-concurrency-zero is
    # set to "false".
    container-concurrency-max-limit: "1000"

    # allow-container-concurrency-zero controls whether users can
    # specify 0 (i.e. unbounded) for containerConcurrency.
    allow-container-concurrency-zero: "true"

    # enable-service-links specifies the default value used for the
    # enableServiceLinks field of the PodSpec, when it is omitted by the user.
    # See: https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/#accessing-the-service
    #
    # This is a tri-state flag with possible values of (true|false|default).
    #
    # In environments with large number of services it is suggested
    # to set this value to `false`.
    # See https://github.com/knative/serving/issues/8498.
    enable-service-links: "false"
---
# Source: knative-serving/templates/core/configmaps.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-deployment
  namespace: default
  labels:
    serving.knative.dev/release: "1.6.0"
    app.kubernetes.io/component: controller
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
data:
  # This is the Go import path for the binary that is containerized
  # and substituted here.
  # TODO: switch to 'queue-sidecar-image' after 0.27
  queueSidecarImage: gcr.io/knative-releases/knative.dev/serving/cmd/queue@sha256:813ea20d55b5063596cf967d1c63f51b9e34f883653957157b9b5341dfad0001
  _example: |-
    ################################
    #                              #
    #    EXAMPLE CONFIGURATION     #
    #                              #
    ################################

    # This block is not actually functional configuration,
    # but serves to illustrate the available configuration
    # options and document them in a way that is accessible
    # to users that `kubectl edit` this config map.
    #
    # These sample configuration options may be copied out of
    # this example block and unindented to be in the data block
    # to actually change the configuration.

    # List of repositories for which tag to digest resolving should be skipped
    registries-skipping-tag-resolving: "kind.local,ko.local,dev.local"

    # Maximum time allowed for an image's digests to be resolved.
    digest-resolution-timeout: "10s"

    # Duration we wait for the deployment to be ready before considering it failed.
    progress-deadline: "600s"

    # Sets the queue proxy's CPU request.
    # If omitted, a default value (currently "25m"), is used.
    queue-sidecar-cpu-request: "25m"

    # Sets the queue proxy's CPU limit.
    # If omitted, no value is specified and the system default is used.
    queue-sidecar-cpu-limit: "1000m"

    # Sets the queue proxy's memory request.
    # If omitted, no value is specified and the system default is used.
    queue-sidecar-memory-request: "400Mi"

    # Sets the queue proxy's memory limit.
    # If omitted, no value is specified and the system default is used.
    queue-sidecar-memory-limit: "800Mi"

    # Sets the queue proxy's ephemeral storage request.
    # If omitted, no value is specified and the system default is used.
    queue-sidecar-ephemeral-storage-request: "512Mi"

    # Sets the queue proxy's ephemeral storage limit.
    # If omitted, no value is specified and the system default is used.
    queue-sidecar-ephemeral-storage-limit: "1024Mi"

    # The freezer service endpoint that queue-proxy calls when its traffic drops to zero or
    # scales up from zero.
    #
    # Freezer service is available at: https://github.com/knative-sandbox/container-freezer
    # or users may write their own service.
    #
    # The value will need to include both the host and the port that will be accessed.
    # For the host, $HOST_IP can be passed, and the appropriate host IP value will be swapped
    # in at runtime, which will enable the freezer daemonset to be reachable via the node IP.
    #
    # As an example:
    #     concurrency-state-endpoint: "http://$HOST_IP:9696"
    #
    # If not set, queue proxy takes no action (this is the default behavior).
    #
    # When enabled, a serviceAccountToken will be mounted to queue-proxy using
    # a projected volume. This requires the Service Account Token Volume Projection feature
    # to be enabled. For details, see this link:
    # https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection
    #
    # NOTE THAT THIS IS AN EXPERIMENTAL / ALPHA FEATURE
    concurrency-state-endpoint: ""
---
# Source: knative-serving/templates/core/configmaps.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-domain
  namespace: default
  labels:
    serving.knative.dev/release: "1.6.0"
    app.kubernetes.io/component: controller
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
data:
  openai.harix.cloudminds.com: |
    # Routes having domain suffix of 'svc.cluster.local' will not be exposed
    # through Ingress. You can define your own label selector to assign that
    # domain suffix to your Route here, or you can set the label
    #    "serving.knative.dev/visibility=cluster-local"
    # to achieve the same effect.  This shows how to make routes having
    # the label app=secret only exposed to the local cluster.
---
# Source: knative-serving/templates/core/configmaps.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-features
  namespace: default
  labels:
    serving.knative.dev/release: "1.6.0"
    app.kubernetes.io/component: controller
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
data:
  _example: |
    ################################
    #                              #
    #    EXAMPLE CONFIGURATION     #
    #                              #
    ################################

    # This block is not actually functional configuration,
    # but serves to illustrate the available configuration
    # options and document them in a way that is accessible
    # to users that `kubectl edit` this config map.
    #
    # These sample configuration options may be copied out of
    # this example block and unindented to be in the data block
    # to actually change the configuration.

    # Indicates whether multi container support is enabled
    #
    # WARNING: Cannot safely be disabled once enabled.
    # See: https://knative.dev/docs/serving/feature-flags/#multi-containers
    multi-container: "enabled"

    # Indicates whether Kubernetes affinity support is enabled
    #
    # WARNING: Cannot safely be disabled once enabled.
    # See: https://knative.dev/docs/serving/feature-flags/#kubernetes-node-affinity
    kubernetes.podspec-affinity: "disabled"

    # Indicates whether Kubernetes topologySpreadConstraints support is enabled
    #
    # WARNING: Cannot safely be disabled once enabled.
    # See: https://knative.dev/docs/serving/feature-flags/#kubernetes-topology-spread-constraints
    kubernetes.podspec-topologyspreadconstraints: "disabled"

    # Indicates whether Kubernetes hostAliases support is enabled
    #
    # WARNING: Cannot safely be disabled once enabled.
    # See: https://knative.dev/docs/serving/feature-flags/#kubernetes-host-aliases
    kubernetes.podspec-hostaliases: "disabled"

    # Indicates whether Kubernetes nodeSelector support is enabled
    #
    # WARNING: Cannot safely be disabled once enabled.
    # See: https://knative.dev/docs/serving/feature-flags/#kubernetes-node-selector
    kubernetes.podspec-nodeselector: "disabled"

    # Indicates whether Kubernetes tolerations support is enabled
    #
    # WARNING: Cannot safely be disabled once enabled
    # See: https://knative.dev/docs/serving/feature-flags/#kubernetes-toleration
    kubernetes.podspec-tolerations: "disabled"

    # Indicates whether Kubernetes FieldRef support is enabled
    #
    # WARNING: Cannot safely be disabled once enabled.
    # See: https://knative.dev/docs/serving/feature-flags/#kubernetes-fieldref
    kubernetes.podspec-fieldref: "disabled"

    # Indicates whether Kubernetes RuntimeClassName support is enabled
    #
    # WARNING: Cannot safely be disabled once enabled.
    # See: https://knative.dev/docs/serving/feature-flags/#kubernetes-runtime-class
    kubernetes.podspec-runtimeclassname: "disabled"

    # Indicates whether Kubernetes DNSPolicy support is enabled
    #
    # WARNING: Cannot safely be disabled once enabled.
    # See: https://knative.dev/docs/serving/feature-flags/#kubernetes-dnspolicy
    kubernetes.podspec-dnspolicy: "disabled"

    # Indicates whether Kubernetes DNSConfig support is enabled
    #
    # WARNING: Cannot safely be disabled once enabled.
    # See: https://knative.dev/docs/serving/feature-flags/#kubernetes-dnsconfig
    kubernetes.podspec-dnsconfig: "disabled"

    # This feature allows end-users to set a subset of fields on the Pod's SecurityContext
    #
    # When set to "enabled" or "allowed" it allows the following
    # PodSecurityContext properties:
    # - FSGroup
    # - RunAsGroup
    # - RunAsNonRoot
    # - SupplementalGroups
    # - RunAsUser
    #
    # This feature flag should be used with caution as the PodSecurityContext
    # properties may have a side-effect on non-user sidecar containers that come
    # from Knative or your service mesh
    #
    # WARNING: Cannot safely be disabled once enabled.
    # See: https://knative.dev/docs/serving/feature-flags/#kubernetes-security-context
    kubernetes.podspec-securitycontext: "disabled"

    # Indicates whether Kubernetes PriorityClassName support is enabled
    #
    # WARNING: Cannot safely be disabled once enabled.
    # See: https://knative.dev/docs/serving/feature-flags/#kubernetes-priority-class-name
    kubernetes.podspec-priorityclassname: "disabled"

    # Indicates whether Kubernetes SchedulerName support is enabled
    #
    # WARNING: Cannot safely be disabled once enabled.
    # See: https://knative.dev/docs/serving/feature-flags/#kubernetes-scheduler-name
    kubernetes.podspec-schedulername: "disabled"

    # This feature flag allows end-users to add a subset of capabilities on the Pod's SecurityContext.
    #
    # When set to "enabled" or "allowed" it allows capabilities to be added to the container.
    # For a list of possible capabilities, see https://man7.org/linux/man-pages/man7/capabilities.7.html
    kubernetes.containerspec-addcapabilities: "disabled"

    # This feature validates PodSpecs from the validating webhook
    # against the K8s API Server.
    #
    # When "enabled", the server will always run the extra validation.
    # When "allowed", the server will not run the dry-run validation by default.
    #   However, clients may enable the behavior on an individual Service by
    #   attaching the following metadata annotation: "features.knative.dev/podspec-dryrun":"enabled".
    # See: https://knative.dev/docs/serving/feature-flags/#kubernetes-dry-run
    kubernetes.podspec-dryrun: "allowed"

    # Controls whether tag header based routing feature are enabled or not.
    # 1. Enabled: enabling tag header based routing
    # 2. Disabled: disabling tag header based routing
    # See: https://knative.dev/docs/serving/feature-flags/#tag-header-based-routing
    tag-header-based-routing: "disabled"

    # Controls whether http2 auto-detection should be enabled or not.
    # 1. Enabled: http2 connection will be attempted via upgrade.
    # 2. Disabled: http2 connection will only be attempted when port name is set to "h2c".
    autodetect-http2: "disabled"

    # Controls whether volume support for EmptyDir is enabled or not.
    # 1. Enabled: enabling EmptyDir volume support
    # 2. Disabled: disabling EmptyDir volume support
    kubernetes.podspec-volumes-emptydir: "disabled"

    # Controls whether init containers support is enabled or not.
    # 1. Enabled: enabling init containers support
    # 2. Disabled: disabling init containers support
    kubernetes.podspec-init-containers: "disabled"

    # Controls whether persistent volume claim support is enabled or not.
    # 1. Enabled: enabling persistent volume claim support
    # 2. Disabled: disabling persistent volume claim support
    kubernetes.podspec-persistent-volume-claim: "disabled"

    # Controls whether write access for persistent volumes is enabled or not.
    # 1. Enabled: enabling write access for persistent volumes
    # 2. Disabled: disabling write access for persistent volumes
    kubernetes.podspec-persistent-volume-write: "disabled"
  autodetect-http2: disabled
  kubernetes.containerspec-addcapabilities: disabled
  kubernetes.podspec-affinity: disabled
  kubernetes.podspec-dnsconfig: disabled
  kubernetes.podspec-dnspolicy: disabled
  kubernetes.podspec-dryrun: allowed
  kubernetes.podspec-fieldref: disabled
  kubernetes.podspec-hostaliases: disabled
  kubernetes.podspec-init-containers: disabled
  kubernetes.podspec-nodeselector: enabled
  kubernetes.podspec-persistent-volume-claim: enabled
  kubernetes.podspec-persistent-volume-write: enabled
  kubernetes.podspec-priorityclassname: disabled
  kubernetes.podspec-runtimeclassname: disabled
  kubernetes.podspec-schedulername: disabled
  kubernetes.podspec-securitycontext: disabled
  kubernetes.podspec-tolerations: disabled
  kubernetes.podspec-topologyspreadconstraints: disabled
  kubernetes.podspec-volumes-emptydir: disabled
  multi-container: enabled
  tag-header-based-routing: disabled
---
# Source: knative-serving/templates/core/configmaps.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-gc
  namespace: default
  labels:
    serving.knative.dev/release: "1.6.0"
    app.kubernetes.io/component: controller
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
data:
  _example: |
    ################################
    #                              #
    #    EXAMPLE CONFIGURATION     #
    #                              #
    ################################

    # This block is not actually functional configuration,
    # but serves to illustrate the available configuration
    # options and document them in a way that is accessible
    # to users that `kubectl edit` this config map.
    #
    # These sample configuration options may be copied out of
    # this example block and unindented to be in the data block
    # to actually change the configuration.

    # ---------------------------------------
    # Garbage Collector Settings
    # ---------------------------------------
    #
    # Active
    #   * Revisions which are referenced by a Route are considered active.
    #   * Individual revisions may be marked with the annotation
    #      "serving.knative.dev/no-gc":"true" to be permanently considered active.
    #   * Active revisions are not considered for GC.
    # Retention
    #   * Revisions are retained if they are any of the following:
    #       1. Active
    #       2. Were created within "retain-since-create-time"
    #       3. Were last referenced by a route within
    #           "retain-since-last-active-time"
    #       4. There are fewer than "min-non-active-revisions"
    #     If none of these conditions are met, or if the count of revisions exceed
    #      "max-non-active-revisions", they will be deleted by GC.
    #     The special value "disabled" may be used to turn off these limits.
    #
    # Example config to immediately collect any inactive revision:
    #    min-non-active-revisions: "0"
    #    max-non-active-revisions: "0"
    #    retain-since-create-time: "disabled"
    #    retain-since-last-active-time: "disabled"
    #
    # Example config to always keep around the last ten non-active revisions:
    #     retain-since-create-time: "disabled"
    #     retain-since-last-active-time: "disabled"
    #     max-non-active-revisions: "10"
    #
    # Example config to disable all garbage collection:
    #     retain-since-create-time: "disabled"
    #     retain-since-last-active-time: "disabled"
    #     max-non-active-revisions: "disabled"
    #
    # Example config to keep recently deployed or active revisions,
    # always maintain the last two in case of rollback, and prevent
    # burst activity from exploding the count of old revisions:
    #      retain-since-create-time: "48h"
    #      retain-since-last-active-time: "15h"
    #      min-non-active-revisions: "2"
    #      max-non-active-revisions: "1000"

    # Duration since creation before considering a revision for GC or "disabled".
    retain-since-create-time: "48h"

    # Duration since active before considering a revision for GC or "disabled".
    retain-since-last-active-time: "15h"

    # Minimum number of non-active revisions to retain.
    min-non-active-revisions: "20"

    # Maximum number of non-active revisions to retain
    # or "disabled" to disable any maximum limit.
    max-non-active-revisions: "1000"
---
# Source: knative-serving/templates/core/configmaps.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-leader-election
  namespace: default
  labels:
    serving.knative.dev/release: "1.6.0"
    app.kubernetes.io/component: controller
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
data:
  _example: |
    ################################
    #                              #
    #    EXAMPLE CONFIGURATION     #
    #                              #
    ################################

    # This block is not actually functional configuration,
    # but serves to illustrate the available configuration
    # options and document them in a way that is accessible
    # to users that `kubectl edit` this config map.
    #
    # These sample configuration options may be copied out of
    # this example block and unindented to be in the data block
    # to actually change the configuration.

    # lease-duration is how long non-leaders will wait to try to acquire the
    # lock; 15 seconds is the value used by core kubernetes controllers.
    lease-duration: "60s"

    # renew-deadline is how long a leader will try to renew the lease before
    # giving up; 10 seconds is the value used by core kubernetes controllers.
    renew-deadline: "40s"

    # retry-period is how long the leader election client waits between tries of
    # actions; 2 seconds is the value used by core kubernetes controllers.
    retry-period: "10s"

    # buckets is the number of buckets used to partition key space of each
    # Reconciler. If this number is M and the replica number of the controller
    # is N, the N replicas will compete for the M buckets. The owner of a
    # bucket will take care of the reconciling for the keys partitioned into
    # that bucket.
    buckets: "1"
---
# Source: knative-serving/templates/core/configmaps.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-logging
  namespace: knative-serving
  labels:
    serving.knative.dev/release: "1.6.0"
    app.kubernetes.io/component: logging
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
data:
  _example: |
    ################################
    #                              #
    #    EXAMPLE CONFIGURATION     #
    #                              #
    ################################

    # This block is not actually functional configuration,
    # but serves to illustrate the available configuration
    # options and document them in a way that is accessible
    # to users that `kubectl edit` this config map.
    #
    # These sample configuration options may be copied out of
    # this example block and unindented to be in the data block
    # to actually change the configuration.

    # Common configuration for all Knative codebase
    zap-logger-config: |
      {
        "level": "info",
        "development": false,
        "outputPaths": ["stdout"],
        "errorOutputPaths": ["stderr"],
        "encoding": "json",
        "encoderConfig": {
          "timeKey": "timestamp",
          "levelKey": "severity",
          "nameKey": "logger",
          "callerKey": "caller",
          "messageKey": "message",
          "stacktraceKey": "stacktrace",
          "lineEnding": "",
          "levelEncoder": "",
          "timeEncoder": "iso8601",
          "durationEncoder": "",
          "callerEncoder": ""
        }
      }

    # Log level overrides
    # For all components except the queue proxy,
    # changes are picked up immediately.
    # For queue proxy, changes require recreation of the pods.
    loglevel.controller: "info"
    loglevel.autoscaler: "info"
    loglevel.queueproxy: "info"
    loglevel.webhook: "info"
    loglevel.activator: "info"
    loglevel.hpaautoscaler: "info"
    loglevel.net-certmanager-controller: "info"
    loglevel.net-istio-controller: "info"
    loglevel.net-contour-controller: "info"
---
# Source: knative-serving/templates/core/configmaps.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-network
  namespace: default
  labels:
    serving.knative.dev/release: "1.6.0"
    app.kubernetes.io/component: networking
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
data:
  _example: |
    ################################
    #                              #
    #    EXAMPLE CONFIGURATION     #
    #                              #
    ################################

    # This block is not actually functional configuration,
    # but serves to illustrate the available configuration
    # options and document them in a way that is accessible
    # to users that `kubectl edit` this config map.
    #
    # These sample configuration options may be copied out of
    # this example block and unindented to be in the data block
    # to actually change the configuration.

    # ingress-class specifies the default ingress class
    # to use when not dictated by Route annotation.
    #
    # If not specified, will use the Istio ingress.
    #
    # Note that changing the Ingress class of an existing Route
    # will result in undefined behavior.  Therefore it is best to only
    # update this value during the setup of Knative, to avoid getting
    # undefined behavior.
    ingress-class: "istio.ingress.networking.knative.dev"

    # certificate-class specifies the default Certificate class
    # to use when not dictated by Route annotation.
    #
    # If not specified, will use the Cert-Manager Certificate.
    #
    # Note that changing the Certificate class of an existing Route
    # will result in undefined behavior.  Therefore it is best to only
    # update this value during the setup of Knative, to avoid getting
    # undefined behavior.
    certificate-class: "cert-manager.certificate.networking.knative.dev"

    # namespace-wildcard-cert-selector specifies a LabelSelector which
    # determines which namespaces should have a wildcard certificate
    # provisioned.
    #
    # Use an empty value to disable the feature (this is the default):
    #   namespace-wildcard-cert-selector: ""
    #
    # Use an empty object to enable for all namespaces
    #   namespace-wildcard-cert-selector: {}
    #
    # Useful labels include the "kubernetes.io/metadata.name" label to
    # avoid provisioning a certifcate for the "kube-system" namespaces.
    # Use the following selector to match pre-1.0 behavior of using
    # "networking.knative.dev/disableWildcardCert" to exclude namespaces:
    #
    # matchExpressions:
    # - key: "networking.knative.dev/disableWildcardCert"
    #   operator: "NotIn"
    #   values: ["true"]
    namespace-wildcard-cert-selector: ""

    # domain-template specifies the golang text template string to use
    # when constructing the Knative service's DNS name. The default
    # value is "..".
    #
    # Valid variables defined in the template include Name, Namespace, Domain,
    # Labels, and Annotations. Name will be the result of the tagTemplate
    # below, if a tag is specified for the route.
    #
    # Changing this value might be necessary when the extra levels in
    # the domain name generated is problematic for wildcard certificates
    # that only support a single level of domain name added to the
    # certificate's domain. In those cases you might consider using a value
    # of "-.", or removing the Namespace
    # entirely from the template. When choosing a new value be thoughtful
    # of the potential for conflicts - for example, when users choose to use
    # characters such as `-` in their service, or namespace, names.
    #  or  can be used for any customization in the
    # go template if needed.
    # We strongly recommend keeping namespace part of the template to avoid
    # domain name clashes:
    domain-template: ".."

    # tagTemplate specifies the golang text template string to use
    # when constructing the DNS name for "tags" within the traffic blocks
    # of Routes and Configuration.  This is used in conjunction with the
    # domainTemplate above to determine the full URL for the tag.
    tag-template: "-"

    # Controls whether TLS certificates are automatically provisioned and
    # installed in the Knative ingress to terminate external TLS connection.
    # 1. Enabled: enabling auto-TLS feature.
    # 2. Disabled: disabling auto-TLS feature.
    auto-tls: "Disabled"

    # Controls the behavior of the HTTP endpoint for the Knative ingress.
    # It requires autoTLS to be enabled.
    # 1. Enabled: The Knative ingress will be able to serve HTTP connection.
    # 2. Redirected: The Knative ingress will send a 301 redirect for all
    # http connections, asking the clients to use HTTPS.
    #
    # "Disabled" option is deprecated.
    http-protocol: "Enabled"

    # rollout-duration contains the minimal duration in seconds over which the
    # Configuration traffic targets are rolled out to the newest revision.
    rollout-duration: "0"

    # autocreate-cluster-domain-claims controls whether ClusterDomainClaims should
    # be automatically created (and deleted) as needed when DomainMappings are
    # reconciled.
    #
    # If this is "false" (the default), the cluster administrator is
    # responsible for creating ClusterDomainClaims and delegating them to
    # namespaces via their spec.Namespace field. This setting should be used in
    # multitenant environments which need to control which namespace can use a
    # particular domain name in a domain mapping.
    #
    # If this is "true", users are able to associate arbitrary names with their
    # services via the DomainMapping feature.
    autocreate-cluster-domain-claims: "false"

    # If true, networking plugins can add additional information to deployed
    # applications to make their pods directly accessible via their IPs even if mesh is
    # enabled and thus direct-addressability is usually not possible.
    # Consumers like Knative Serving can use this setting to adjust their behavior
    # accordingly, i.e. to drop fallback solutions for non-pod-addressable systems.
    #
    # NOTE: This flag is in an alpha state and is mostly here to enable internal testing
    #       for now. Use with caution.
    enable-mesh-pod-addressability: "false"

    # mesh-compatibility-mode indicates whether consumers of network plugins
    # should directly contact Pod IPs (most efficient), or should use the
    # Cluster IP (less efficient, needed when mesh is enabled unless
    # `enable-mesh-pod-addressability`, above, is set).
    # Permitted values are:
    #  - "auto" (default): automatically determine which mesh mode to use by trying Pod IP and falling back to Cluster IP as needed.
    #  - "enabled": always use Cluster IP and do not attempt to use Pod IPs.
    #  - "disabled": always use Pod IPs and do not fall back to Cluster IP on failure.
    mesh-compatibility-mode: "auto"

    # Defines the scheme used for external URLs if autoTLS is not enabled.
    # This can be used for making Knative report all URLs as "HTTPS" for example, if you're
    # fronting Knative with an external loadbalancer that deals with TLS termination and
    # Knative doesn't know about that otherwise.
    default-external-scheme: "http"

    # internal-encryption indicates whether internal traffic is encrypted or not.
    # If this is "true", the following traffic are encrypted:
    #  - ingress to activator
    #  - ingress to queue-proxy
    #  - activator to queue-proxy
    #
    # NOTE: This flag is in an alpha state and is mostly here to enable internal testing
    #       for now. Use with caution.
    internal-encryption: "false"
  ingress-class: kourier.ingress.networking.knative.dev
  internal-encryption: "false"
---
# Source: knative-serving/templates/core/configmaps.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-observability
  namespace: default
  labels:
    serving.knative.dev/release: "1.6.0"
    app.kubernetes.io/component: observability
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
data:
  _example: |
    ################################
    #                              #
    #    EXAMPLE CONFIGURATION     #
    #                              #
    ################################

    # This block is not actually functional configuration,
    # but serves to illustrate the available configuration
    # options and document them in a way that is accessible
    # to users that `kubectl edit` this config map.
    #
    # These sample configuration options may be copied out of
    # this example block and unindented to be in the data block
    # to actually change the configuration.

    # logging.enable-var-log-collection defaults to false.
    # The fluentd daemon set will be set up to collect /var/log if
    # this flag is true.
    logging.enable-var-log-collection: "false"

    # logging.revision-url-template provides a template to use for producing the
    # logging URL that is injected into the status of each Revision.
    logging.revision-url-template: "http://logging.example.com/?revisionUID=${REVISION_UID}"

    # If non-empty, this enables queue proxy writing user request logs to stdout, excluding probe
    # requests.
    # NB: after 0.18 release logging.enable-request-log must be explicitly set to true
    # in order for request logging to be enabled.
    #
    # The value determines the shape of the request logs and it must be a valid go text/template.
    # It is important to keep this as a single line. Multiple lines are parsed as separate entities
    # by most collection agents and will split the request logs into multiple records.
    #
    # The following fields and functions are available to the template:
    #
    # Request: An http.Request (see https://golang.org/pkg/net/http/#Request)
    # representing an HTTP request received by the server.
    #
    # Response:
    # struct {
    #   Code    int       // HTTP status code (see https://www.iana.org/assignments/http-status-codes/http-status-codes.xhtml)
    #   Size    int       // An int representing the size of the response.
    #   Latency float64   // A float64 representing the latency of the response in seconds.
    # }
    #
    # Revision:
    # struct {
    #   Name          string  // Knative revision name
    #   Namespace     string  // Knative revision namespace
    #   Service       string  // Knative service name
    #   Configuration string  // Knative configuration name
    #   PodName       string  // Name of the pod hosting the revision
    #   PodIP         string  // IP of the pod hosting the revision
    # }
    #
    logging.request-log-template: {"httpRequest": {"requestMethod": "{{.Request.Method}}", "requestUrl": "{{js .Request.RequestURI}}", "requestSize": "{{.Request.ContentLength}}", "status": {{.Response.Code}}, "responseSize": "{{.Response.Size}}", "userAgent": "{{js .Request.UserAgent}}", "remoteIp": "{{js .Request.RemoteAddr}}", "serverIp": "{{.Revision.PodIP}}", "referer": "{{js .Request.Referer}}", "latency": "{{.Response.Latency}}s", "protocol": "{{.Request.Proto}}"}, "traceId": "{{index .Request.Header "X-B3-Traceid"}}"}

    # If true, the request logging will be enabled.
    # NB: up to and including Knative version 0.18 if logging.request-log-template is non-empty, this value
    # will be ignored.
    logging.enable-request-log: "false"

    # If true, this enables queue proxy writing request logs for probe requests to stdout.
    # It uses the same template for user requests, i.e. logging.request-log-template.
    logging.enable-probe-request-log: "false"

    # metrics.backend-destination field specifies the system metrics destination.
    # It supports either prometheus (the default) or opencensus.
    metrics.backend-destination: prometheus

    # metrics.request-metrics-backend-destination specifies the request metrics
    # destination. It enables queue proxy to send request metrics.
    # Currently supported values: prometheus (the default), opencensus.
    metrics.request-metrics-backend-destination: prometheus

    # profiling.enable indicates whether it is allowed to retrieve runtime profiling data from
    # the pods via an HTTP server in the format expected by the pprof visualization tool. When
    # enabled, the Knative Serving pods expose the profiling data on an alternate HTTP port 8008.
    # The HTTP context root for profiling is then /debug/pprof/.
    profiling.enable: "false"
---
# Source: knative-serving/templates/core/configmaps.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-tracing
  namespace: default
  labels:
    serving.knative.dev/release: "1.6.0"
    app.kubernetes.io/component: tracing
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
data:
  _example: |
    ################################
    #                              #
    #    EXAMPLE CONFIGURATION     #
    #                              #
    ################################

    # This block is not actually functional configuration,
    # but serves to illustrate the available configuration
    # options and document them in a way that is accessible
    # to users that `kubectl edit` this config map.
    #
    # These sample configuration options may be copied out of
    # this example block and unindented to be in the data block
    # to actually change the configuration.
    #
    # This may be "zipkin" or "none" (default)
    backend: "none"

    # URL to zipkin collector where traces are sent.
    # This must be specified when backend is "zipkin"
    zipkin-endpoint: "http://zipkin.istio-system.svc.cluster.local:9411/api/v2/spans"

    # Enable zipkin debug mode. This allows all spans to be sent to the server
    # bypassing sampling.
    debug: "false"

    # Percentage (0-1) of requests to trace
    sample-rate: "0.1"
---
# Source: knative-serving/templates/net/kourier.yaml
# Copyright 2020 The Knative Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: v1
kind: ConfigMap
metadata:
  name: kourier-bootstrap
  namespace: kourier-system
  labels:
    networking.knative.dev/ingress-provider: kourier
    app.kubernetes.io/component: net-kourier
    serving.knative.dev/release: "1.6.0"
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
data:
  envoy-bootstrap.yaml: |
    dynamic_resources:
      ads_config:
        transport_api_version: V3
        api_type: GRPC
        rate_limit_settings: {}
        grpc_services:
        - envoy_grpc: {cluster_name: xds_cluster}
      cds_config:
        resource_api_version: V3
        ads: {}
      lds_config:
        resource_api_version: V3
        ads: {}
    node:
      cluster: kourier-knative
      id: 3scale-kourier-gateway
    static_resources:
      listeners:
        - name: stats_listener
          address:
            socket_address:
              address: 0.0.0.0
              port_value: 9000
          filter_chains:
            - filters:
                - name: envoy.filters.network.http_connection_manager
                  typed_config:
                    "@type": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager
                    stat_prefix: stats_server
                    http_filters:
                      - name: envoy.filters.http.router
                    route_config:
                      virtual_hosts:
                        - name: admin_interface
                          domains:
                            - "*"
                          routes:
                            - match:
                                safe_regex:
                                  google_re2: {}
                                  regex: '/(certs|stats(/prometheus)?|server_info|clusters|listeners|ready)?'
                                headers:
                                  - name: ':method'
                                    exact_match: GET
                              route:
                                cluster: service_stats
      clusters:
        - name: service_stats
          connect_timeout: 0.250s
          type: static
          load_assignment:
            cluster_name: service_stats
            endpoints:
              lb_endpoints:
                endpoint:
                  address:
                    pipe:
                      path: /tmp/envoy.admin
        - name: xds_cluster
          connect_timeout: 1s
          type: strict_dns
          load_assignment:
            cluster_name: xds_cluster
            endpoints:
              lb_endpoints:
                endpoint:
                  address:
                    socket_address:
                      address: "net-kourier-controller.knative-serving"
                      port_value: 18000
          http2_protocol_options: {}
          type: STRICT_DNS
    admin:
      access_log_path: "/dev/stdout"
      address:
        pipe:
          path: /tmp/envoy.admin
---
# Source: knative-serving/templates/net/kourier.yaml
# Copyright 2021 The Knative Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: v1
kind: ConfigMap
metadata:
  name: config-kourier
  namespace: default
  labels:
    networking.knative.dev/ingress-provider: kourier
    app.kubernetes.io/component: net-kourier
    serving.knative.dev/release: "1.6.0"
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
data:
  _example: |
    ################################
    #                              #
    #    EXAMPLE CONFIGURATION     #
    #                              #
    ################################

    # This block is not actually functional configuration,
    # but serves to illustrate the available configuration
    # options and document them in a way that is accessible
    # to users that `kubectl edit` this config map.
    #
    # These sample configuration options may be copied out of
    # this example block and unindented to be in the data block
    # to actually change the configuration.

    # Specifies whether requests reaching the Kourier gateway
    # in the context of services should be logged. Readiness
    # probes etc. must be configured via the bootstrap config.
    enable-service-access-logging: "true"

    # Specifies whether to use proxy-protocol in order to safely
    # transport connection information such as a client's address
    # across multiple layers of TCP proxies.
    # NOTE THAT THIS IS AN EXPERIMENTAL / ALPHA FEATURE
    enable-proxy-protocol: "false"

    # The server certificates to serve the internal TLS traffic for Kourier Gateway.
    # It is specified by the secret name in controller namespace, which has
    # the "tls.crt" and "tls.key" data field.
    # Use an empty value to disable the feature (default).
    #
    # NOTE: This flag is in an alpha state and is mostly here to enable internal testing
    #       for now. Use with caution.
    cluster-cert-secret: ""

    # Specifies the amount of time that Kourier waits for the incoming requests.
    stream-idle-timeout: "300s"

    # Control the desired level of incoming traffic isolation.
    #
    # When set to an empty value (default), all incoming traffic flows through
    # a shared ingress and listeners.
    #
    # When set to "port", incoming traffic is isolated by using different
    # listener ports.
    #
    # NOTE: This flag is in an alpha state.
    traffic-isolation: ""
---
# Source: knative-serving/templates/core/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  # Named like this to avoid clashing with eventing's existing `addressable-resolver` role
  # (which should be identical, but isn't guaranteed to be installed alongside serving).
  name: knative-serving-aggregated-addressable-resolver
  labels:
    serving.knative.dev/release: "1.6.0"
aggregationRule:
  clusterRoleSelectors:
    - matchLabels:
        duck.knative.dev/addressable: "true"
---
# Source: knative-serving/templates/core/clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: knative-serving-addressable-resolver
  labels:
    serving.knative.dev/release: "1.6.0"
    duck.knative.dev/addressable: "true"
rules:
- apiGroups:
  - serving.knative.dev
  resources:
  - routes
  - routes/status
  - services
  - services/status
  verbs:
  - get
  - list
  - watch
---
# Source: knative-serving/templates/core/clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: knative-serving-namespaced-edit
  labels:
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
    serving.knative.dev/release: "1.6.0"
rules:
- apiGroups: ["serving.knative.dev"]
  resources: ["*"]
  verbs: ["create", "update", "patch", "delete"]
- apiGroups: ["networking.internal.knative.dev", "autoscaling.internal.knative.dev",
    "caching.internal.knative.dev"]
  resources: ["*"]
  verbs: ["get", "list", "watch"]
---
# Source: knative-serving/templates/core/clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: knative-serving-namespaced-admin
  labels:
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
    serving.knative.dev/release: "1.6.0"
rules:
- apiGroups: ["serving.knative.dev"]
  resources: ["*"]
  verbs: ["*"]
- apiGroups: ["networking.internal.knative.dev", "autoscaling.internal.knative.dev",
    "caching.internal.knative.dev"]
  resources: ["*"]
  verbs: ["get", "list", "watch"]
---
# Source: knative-serving/templates/core/clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: knative-serving-namespaced-view
  labels:
    rbac.authorization.k8s.io/aggregate-to-view: "true"
    serving.knative.dev/release: "1.6.0"
rules:
- apiGroups: ["serving.knative.dev", "networking.internal.knative.dev", "autoscaling.internal.knative.dev",
    "caching.internal.knative.dev"]
  resources: ["*"]
  verbs: ["get", "list", "watch"]
---
# Source: knative-serving/templates/core/clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: knative-serving-podspecable-binding
  labels:
    serving.knative.dev/release: "1.6.0"
    # Labeled to facilitate aggregated cluster roles that act on PodSpecables.
    duck.knative.dev/podspecable: "true"
# Do not use this role directly. These rules will be added to the "podspecable-binder" role.
rules:
- apiGroups:
  - serving.knative.dev
  resources:
  - configurations
  - services
  verbs:
  - list
  - watch
  - patch
---
# Source: knative-serving/templates/core/controller/clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: knative-serving-admin
  labels:
    serving.knative.dev/release: "1.6.0"
aggregationRule:
  clusterRoleSelectors:
  - matchLabels:
      serving.knative.dev/controller: "true"
rules: [] # Rules are automatically filled in by the controller manager.
---
# Source: knative-serving/templates/core/controller/clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: knative-serving-core
  labels:
    serving.knative.dev/release: "1.6.0"
    serving.knative.dev/controller: "true"
rules:
- apiGroups: [""]
  resources: ["pods", "namespaces", "secrets", "configmaps", "endpoints", "services",
    "events", "serviceaccounts"]
  verbs: ["get", "list", "create", "update", "delete", "patch", "watch"]
- apiGroups: [""]
  resources: ["endpoints/restricted"] # Permission for RestrictedEndpointsAdmission
  verbs: ["create"]
- apiGroups: ["apps"]
  resources: ["deployments", "deployments/finalizers"] # finalizers are needed for the owner reference of the webhook
  verbs: ["get", "list", "create", "update", "delete", "patch", "watch"]
- apiGroups: ["admissionregistration.k8s.io"]
  resources: ["mutatingwebhookconfigurations", "validatingwebhookconfigurations"]
  verbs: ["get", "list", "create", "update", "delete", "patch", "watch"]
- apiGroups: ["apiextensions.k8s.io"]
  resources: ["customresourcedefinitions", "customresourcedefinitions/status"]
  verbs: ["get", "list", "create", "update", "delete", "patch", "watch"]
- apiGroups: ["autoscaling"]
  resources: ["horizontalpodautoscalers"]
  verbs: ["get", "list", "create", "update", "delete", "patch", "watch"]
- apiGroups: ["coordination.k8s.io"]
  resources: ["leases"]
  verbs: ["get", "list", "create", "update", "delete", "patch", "watch"]
- apiGroups: ["serving.knative.dev", "autoscaling.internal.knative.dev", "networking.internal.knative.dev"]
  resources: ["*", "*/status", "*/finalizers"]
  verbs: ["get", "list", "create", "update", "delete", "deletecollection", "patch",
    "watch"]
- apiGroups: ["caching.internal.knative.dev"]
  resources: ["images"]
  verbs: ["get", "list", "create", "update", "delete", "patch", "watch"]
---
# Source: knative-serving/templates/net/kourier.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: net-kourier
  namespace: default
  labels:
    networking.knative.dev/ingress-provider: kourier
    app.kubernetes.io/component: net-kourier
    serving.knative.dev/release: "1.6.0"
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "update", "patch"]
  - apiGroups: [""]
    resources: ["pods", "endpoints", "services", "secrets"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["namespaces"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["coordination.k8s.io"]
    resources: ["leases"]
    verbs: ["get", "list", "create", "update", "delete", "patch", "watch"]
  - apiGroups: ["networking.internal.knative.dev"]
    resources: ["ingresses"]
    verbs: ["get", "list", "watch", "patch"]
  - apiGroups: ["networking.internal.knative.dev"]
    resources: ["ingresses/status"]
    verbs: ["update"]
---
# Source: knative-serving/templates/core/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: knative-serving-controller-addressable-resolver
  labels:
   serving.knative.dev/release: "1.6.0"
subjects:
  - kind: ServiceAccount
    name: controller
    namespace: knative-serving
roleRef:
  kind: ClusterRole
  name: knative-serving-aggregated-addressable-resolver
  apiGroup: rbac.authorization.k8s.io
---
# Source: knative-serving/templates/core/controller/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: knative-serving-controller-admin
  labels:
    serving.knative.dev/release: "1.6.0"
subjects:
- kind: ServiceAccount
  name: controller
  namespace: knative-serving
roleRef:
  kind: ClusterRole
  name: knative-serving-admin
  apiGroup: rbac.authorization.k8s.io
---
# Source: knative-serving/templates/net/kourier.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: net-kourier
  labels:
    networking.knative.dev/ingress-provider: kourier
    app.kubernetes.io/component: net-kourier
    serving.knative.dev/release: "1.6.0"
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: net-kourier
subjects:
  - kind: ServiceAccount
    name: net-kourier
    namespace: default
---
# Source: knative-serving/templates/core/activator/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: activator-service
  namespace: default
  labels:
    app: activator
    app.kubernetes.io/component: activator
    serving.knative.dev/release: "1.6.0"
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    app: activator
  ports:
    # Define metrics and profiling for them to be accessible within service meshes.
    - name: http-metrics
      port: 9090
      targetPort: 9090
    - name: http-profiling
      port: 8008
      targetPort: 8008
    - name: http
      port: 80
      targetPort: 8012
    - name: http2
      port: 81
      targetPort: 8013
    - name: https
      port: 443
      targetPort: 8112
  type: ClusterIP
---
# Source: knative-serving/templates/core/autoscaler/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: autoscaler
    app.kubernetes.io/component: autoscaler
    serving.knative.dev/release: "1.6.0"
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
  name: autoscaler
  namespace: default
spec:
  ports:
  - # Define metrics and profiling for them to be accessible within service meshes.
    name: http-metrics
    port: 9090
    targetPort: 9090
  - name: http-profiling
    port: 8008
    targetPort: 8008
  - name: http
    port: 8080
    targetPort: 8080
  selector:
    app: autoscaler
---
# Source: knative-serving/templates/core/controller/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: controller
    app.kubernetes.io/component: controller
    serving.knative.dev/release: "1.6.0"
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
  name: controller
  namespace: default
spec:
  ports:
  - # Define metrics and profiling for them to be accessible within service meshes.
    name: http-metrics
    port: 9090
    targetPort: 9090
  - name: http-profiling
    port: 8008
    targetPort: 8008
  selector:
    app: controller
---
# Source: knative-serving/templates/core/domain-mapping/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    role: domainmapping-webhook
    app.kubernetes.io/component: domain-mapping
    serving.knative.dev/release: "1.6.0"
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
  name: domainmapping-webhook
  namespace: default
spec:
  ports:
    # Define metrics and profiling for them to be accessible within service meshes.
    - name: http-metrics
      port: 9090
      targetPort: 9090
    - name: http-profiling
      port: 8008
      targetPort: 8008
    - name: https-webhook
      port: 443
      targetPort: 8443
  selector:
    role: domainmapping-webhook
---
# Source: knative-serving/templates/core/webhook/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    role: webhook
    serving.knative.dev/release: "1.6.0"
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
  name: webhook
  namespace: default
spec:
  ports:
  - # Define metrics and profiling for them to be accessible within service meshes.
    name: http-metrics
    port: 9090
    targetPort: 9090
  - name: http-profiling
    port: 8008
    targetPort: 8008
  - name: https-webhook
    port: 443
    targetPort: 8443
  selector:
    role: webhook
---
# Source: knative-serving/templates/net/kourier.yaml
apiVersion: v1
kind: Service
metadata:
  name: net-kourier-controller
  namespace: default
  labels:
    networking.knative.dev/ingress-provider: kourier
    app.kubernetes.io/component: net-kourier
    serving.knative.dev/release: "1.6.0"
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  ports:
    - name: grpc-xds
      port: 18000
      protocol: TCP
      targetPort: 18000
  selector:
    app: net-kourier-controller
  type: ClusterIP
---
# Source: knative-serving/templates/net/kourier.yaml
apiVersion: v1
kind: Service
metadata:
  name: kourier
  namespace: kourier-system
  labels:
    networking.knative.dev/ingress-provider: kourier
    app.kubernetes.io/component: net-kourier
    serving.knative.dev/release: "1.6.0"
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  ports:
    - name: http2
      port: 80
      protocol: TCP
      targetPort: 8080
    - name: https
      port: 443
      protocol: TCP
      targetPort: 8443
  selector:
    app: 3scale-kourier-gateway
  type: LoadBalancer
---
# Source: knative-serving/templates/net/kourier.yaml
apiVersion: v1
kind: Service
metadata:
  name: kourier-internal
  namespace: kourier-system
  labels:
    networking.knative.dev/ingress-provider: kourier
    app.kubernetes.io/component: net-kourier
    serving.knative.dev/release: "1.6.0"
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  ports:
    - name: http2
      port: 80
      protocol: TCP
      targetPort: 8081
    - name: https
      port: 443
      protocol: TCP
      targetPort: 8444
  selector:
    app: 3scale-kourier-gateway
  type: ClusterIP
---
# Source: knative-serving/templates/core/activator/deployment.yaml
# Copyright 2018 The Knative Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: activator
  namespace: default
  labels:
    serving.knative.dev/release: "1.6.0"
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app: activator
      role: activator
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "false"
      labels:
        app: activator
        role: activator
        serving.knative.dev/release: "1.6.0"
    spec:
      serviceAccountName: controller
      containers:
      - name: activator
        # This is the Go import path for the binary that is containerized
        # and substituted here.
        image: gcr.io/knative-releases/knative.dev/serving/cmd/activator@sha256:08315309da4b219ec74bb2017f569a98a7cfecee5e1285b03dfddc2410feb7d7
        # The numbers are based on performance test results from
        # https://github.com/knative/serving/issues/1625#issuecomment-511930023
        resources:
          {}
        env:
          - name: GOGC
            value: "500"
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: SYSTEM_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: CONFIG_LOGGING_NAME
            value: config-logging
          - name: CONFIG_OBSERVABILITY_NAME
            value: config-observability
          # TODO(https://github.com/knative/pkg/pull/953): Remove stackdriver specific config
          - name: METRICS_DOMAIN
            value: knative.dev/internal/serving
          - name: KUBERNETES_MIN_VERSION
            value: v1.21.14
        securityContext:
          allowPrivilegeEscalation: false
        ports:
        - name: metrics
          containerPort: 9090
        - name: profiling
          containerPort: 8008
        - name: http1
          containerPort: 8012
        - name: h2c
          containerPort: 8013
        readinessProbe: &probe
          httpGet:
            port: 8012
            httpHeaders:
            - name: k-kubelet-probe
              value: "activator"
          failureThreshold: 12
        livenessProbe: *probe
      # The activator (often) sits on the dataplane, and may proxy long (e.g.
      # streaming, websockets) requests.  We give a long grace period for the
      # activator to "lame duck" and drain outstanding requests before we
      # forcibly terminate the pod (and outstanding connections).  This value
      # should be at least as large as the upper bound on the Revision's
      # timeoutSeconds property to avoid servicing events disrupting
      # connections.
      terminationGracePeriodSeconds: 300
---
# Source: knative-serving/templates/core/autoscaler/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: autoscaler
  namespace: default
  labels:
    serving.knative.dev/release: "1.6.0"
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: autoscaler
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "false"
      labels:
        app: autoscaler
        serving.knative.dev/release: "1.6.0"
    spec:
      serviceAccountName: controller
      containers:
      - name: autoscaler
        # This is the Go import path for the binary that is containerized
        # and substituted here.
        image: gcr.io/knative-releases/knative.dev/serving/cmd/autoscaler@sha256:105bdd14ecaabad79d9bbcb8359bf2c317bd72382f80a7c4a335adfea53844f2
        resources:
          {}
        env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: SYSTEM_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: CONFIG_LOGGING_NAME
            value: config-logging
          - name: CONFIG_OBSERVABILITY_NAME
            value: config-observability
          # TODO(https://github.com/knative/pkg/pull/953): Remove stackdriver specific config
          - name: METRICS_DOMAIN
            value: knative.dev/serving
          - name: KUBERNETES_MIN_VERSION
            value: v1.21.14
        securityContext:
          allowPrivilegeEscalation: false
        ports:
        - name: metrics
          containerPort: 9090
        - name: profiling
          containerPort: 8008
        - name: websocket
          containerPort: 8080
        readinessProbe: &probe
          httpGet:
            port: 8080
            httpHeaders:
            - name: k-kubelet-probe
              value: "autoscaler"
        livenessProbe: *probe
---
# Source: knative-serving/templates/core/controller/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: controller
  namespace: default
  labels:
    app.kubernetes.io/component: controller
    serving.knative.dev/release: "1.6.0"
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app: controller
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: controller
        serving.knative.dev/release: "1.6.0"
    spec:
      serviceAccountName: controller
      containers:
      - name: controller
        # This is the Go import path for the binary that is containerized
        # and substituted here.
        image: gcr.io/knative-releases/knative.dev/serving/cmd/controller@sha256:bac158dfb0c73d13ed42266ba287f1a86192c0ba581e23fbe012d30a1c34837c
        resources:
          {}
        env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: SYSTEM_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: CONFIG_LOGGING_NAME
            value: config-logging
          - name: CONFIG_OBSERVABILITY_NAME
            value: config-observability
          # TODO(https://github.com/knative/pkg/pull/953): Remove stackdriver specific config
          - name: METRICS_DOMAIN
            value: knative.dev/internal/serving
          - name: KUBERNETES_MIN_VERSION
            value: v1.21.14
        securityContext:
          allowPrivilegeEscalation: false
        ports:
        - name: metrics
          containerPort: 9090
        - name: profiling
          containerPort: 8008
---
# Source: knative-serving/templates/core/domain-mapping/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: domain-mapping
  namespace: default
  labels:
    app.kubernetes.io/component: domain-mapping
    serving.knative.dev/release: "1.6.0"
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app: domain-mapping
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: domain-mapping
        app.kubernetes.io/component: domain-mapping
        serving.knative.dev/release: "1.6.0"
    spec:
      # To avoid node becoming SPOF, spread our replicas to different nodes.
      serviceAccountName: controller
      containers:
        - name: domain-mapping
          # This is the Go import path for the binary that is containerized
          # and substituted here.
          image: gcr.io/knative-releases/knative.dev/serving/cmd/domain-mapping@sha256:e384a295069b9e10e509fc3986cce4fe7be4ff5c73413d1c2234a813b1f4f99b
          env:
            - name: SYSTEM_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: CONFIG_LOGGING_NAME
              value: config-logging
            - name: CONFIG_OBSERVABILITY_NAME
              value: config-observability
            # TODO(https://github.com/knative/pkg/pull/953): Remove stackdriver specific config
            - name: METRICS_DOMAIN
              value: knative.dev/serving
            - name: KUBERNETES_MIN_VERSION
              value: v1.21.14
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            capabilities:
              drop:
                - all
          ports:
            - name: metrics
              containerPort: 9090
            - name: profiling
              containerPort: 8008
---
# Source: knative-serving/templates/core/domain-mapping/webhook.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: domainmapping-webhook
  namespace: default
  labels:
    app.kubernetes.io/component: domain-mapping
    serving.knative.dev/release: "1.6.0"
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app: domainmapping-webhook
      role: domainmapping-webhook
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "false"
      labels:
        app: domainmapping-webhook
        role: domainmapping-webhook
        app.kubernetes.io/component: domain-mapping
        serving.knative.dev/release: "1.6.0"
    spec:
      # To avoid node becoming SPOF, spread our replicas to different nodes.
      serviceAccountName: controller
      containers:
        - name: domainmapping-webhook
          # This is the Go import path for the binary that is containerized
          # and substituted here.
          image: gcr.io/knative-releases/knative.dev/serving/cmd/domain-mapping-webhook@sha256:15f1ce7f35b4765cc3b1c073423ab8d8bf2c8c2630eea3995c610f520fb68ca0
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
            limits:
              cpu: 500m
              memory: 500Mi
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: SYSTEM_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: CONFIG_LOGGING_NAME
              value: config-logging
            - name: CONFIG_OBSERVABILITY_NAME
              value: config-observability
            - name: WEBHOOK_PORT
              value: "8443"
            # TODO(https://github.com/knative/pkg/pull/953): Remove stackdriver specific config
            - name: METRICS_DOMAIN
              value: knative.dev/serving
            - name: KUBERNETES_MIN_VERSION
              value: v1.21.14
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            capabilities:
              drop:
                - all
          ports:
            - name: metrics
              containerPort: 9090
            - name: profiling
              containerPort: 8008
            - name: https-webhook
              containerPort: 8443
          readinessProbe:
            periodSeconds: 1
            httpGet:
              scheme: HTTPS
              port: 8443
              httpHeaders:
                - name: k-kubelet-probe
                  value: "webhook"
          livenessProbe:
            periodSeconds: 1
            httpGet:
              scheme: HTTPS
              port: 8443
              httpHeaders:
                - name: k-kubelet-probe
                  value: "webhook"
            failureThreshold: 6
            initialDelaySeconds: 20
      # Our webhook should gracefully terminate by lame ducking first, set this to a sufficiently
      # high value that we respect whatever value it has configured for the lame duck grace period.
      terminationGracePeriodSeconds: 300
---
# Source: knative-serving/templates/core/webhook/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webhook
  namespace: default
  labels:
    serving.knative.dev/release: "1.6.0"
spec:
  selector:
    matchLabels:
      app: webhook
      role: webhook
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "false"
      labels:
        app: webhook
        role: webhook
        serving.knative.dev/release: "1.6.0"
    spec:
      serviceAccountName: controller
      containers:
      - name: webhook
        # This is the Go import path for the binary that is containerized
        # and substituted here.
        image: gcr.io/knative-releases/knative.dev/serving/cmd/webhook@sha256:1282a399cbb94f3b9de4f199239b39e795b87108efe7d8ba0380147160a97abb
        resources:
          {}
        env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: SYSTEM_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: CONFIG_LOGGING_NAME
            value: config-logging
          - name: CONFIG_OBSERVABILITY_NAME
            value: config-observability
          - name: WEBHOOK_NAME
            value: webhook
          - name: WEBHOOK_PORT
            value: "8443"
          # TODO(https://github.com/knative/pkg/pull/953): Remove stackdriver specific config
          - name: METRICS_DOMAIN
            value: knative.dev/internal/serving
          - name: KUBERNETES_MIN_VERSION
            value: v1.21.14        
        securityContext:
          allowPrivilegeEscalation: false
        ports:
        - name: metrics
          containerPort: 9090
        - name: profiling
          containerPort: 8008
        - name: https-webhook
          containerPort: 8443
        readinessProbe: &probe
          periodSeconds: 1
          httpGet:
            scheme: HTTPS
            port: 8443
            httpHeaders:
            - name: k-kubelet-probe
              value: "webhook"
        livenessProbe: *probe
---
# Source: knative-serving/templates/net/kourier.yaml
# Copyright 2020 The Knative Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: net-kourier-controller
  namespace: default
  labels:
    networking.knative.dev/ingress-provider: kourier
    app.kubernetes.io/component: net-kourier
    serving.knative.dev/release: "1.6.0"
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: net-kourier-controller
  template:
    metadata:
      labels:
        app: net-kourier-controller
    spec:
      containers:
        - image: gcr.io/knative-releases/knative.dev/net-kourier/cmd/kourier@sha256:197fbb71d1f115673a62843dd8d23a751a72d81d66e2fe8aa9d8d91452521d22
          name: controller
          env:
            - name: CERTS_SECRET_NAMESPACE
              value: ""
            - name: CERTS_SECRET_NAME
              value: ""
            - name: SYSTEM_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: METRICS_DOMAIN
              value: "knative.dev/samples"
            - name: KOURIER_GATEWAY_NAMESPACE
              value: "kourier-system"
            - name: ENABLE_SECRET_INFORMER_FILTERING_BY_CERT_UID
              value: "false"
            - name: KUBERNETES_MIN_VERSION
              value: "v1.21.14"
          ports:
            - name: http2-xds
              containerPort: 18000
              protocol: TCP
          readinessProbe:
            exec:
              command: ["/ko-app/kourier", "-probe-addr=:18000"]
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            capabilities:
              drop:
                - all
      restartPolicy: Always
      serviceAccountName: net-kourier
---
# Source: knative-serving/templates/net/kourier.yaml
# Copyright 2020 The Knative Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: 3scale-kourier-gateway
  namespace: kourier-system
  labels:
    networking.knative.dev/ingress-provider: kourier
    app.kubernetes.io/component: net-kourier
    serving.knative.dev/release: "1.6.0"
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 1
  selector:
    matchLabels:
      app: 3scale-kourier-gateway
  template:
    metadata:
      labels:
        app: 3scale-kourier-gateway
      annotations:
        # v0.26 supports envoy v3 API, so
        # adding this label to restart pod.
        networking.knative.dev/poke: "v0.26"
    spec:
      containers:
        - args:
            - --base-id 1
            - -c /tmp/config/envoy-bootstrap.yaml
            - --log-level info
          command:
            - /usr/local/bin/envoy
          image: harbor.cloudminds.com/library/envoy:v1.20-latest
          name: kourier-gateway
          ports:
            - name: http2-external
              containerPort: 8080
              protocol: TCP
            - name: http2-internal
              containerPort: 8081
              protocol: TCP
            - name: https-external
              containerPort: 8443
              protocol: TCP
            - name: http-probe
              containerPort: 8090
              protocol: TCP
            - name: https-probe
              containerPort: 9443
              protocol: TCP
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: false
            runAsNonRoot: false
            capabilities:
              drop:
                - all
          volumeMounts:
            - name: config-volume
              mountPath: /tmp/config
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh", "-c", "curl -X POST --unix /tmp/envoy.admin http://localhost/healthcheck/fail; sleep 15"]
          readinessProbe:
            httpGet:
              httpHeaders:
                - name: Host
                  value: internalkourier
              path: /ready
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
      volumes:
        - name: config-volume
          configMap:
            name: kourier-bootstrap
      restartPolicy: Always
---
# Source: knative-serving/templates/core/activator/hpa.yaml
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: activator
  namespace: default
  labels:
    serving.knative.dev/release: "1.6.0"
    helm.sh/chart: knative-serving-1.0.1
    app.kubernetes.io/name: knative-serving
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  minReplicas: 1
  maxReplicas: 20
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: activator
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        # Percentage of the requested CPU
        averageUtilization: 100
---
# Source: knative-serving/templates/net/kourier.yaml
# Copyright 2020 The Knative Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
---
# Source: knative-serving/templates/core/image.yaml
# Copyright 2018 The Knative Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: caching.internal.knative.dev/v1alpha1
kind: Image
metadata:
  name: queue-proxy
  namespace: default
  labels:
    serving.knative.dev/release: "1.6.0"
spec:
  # This is the Go import path for the binary that is containerized
  # and substituted here.
  image: gcr.io/knative-releases/knative.dev/serving/cmd/queue@sha256:813ea20d55b5063596cf967d1c63f51b9e34f883653957157b9b5341dfad0001
---
# Source: knative-serving/templates/core/webhook/configuration.yaml
apiVersion: admissionregistration.k8s.io/v1
kind: MutatingWebhookConfiguration
metadata:
  name: webhook.serving.knative.dev
  labels:
    serving.knative.dev/release: "1.6.0"
webhooks:
- admissionReviewVersions: ["v1", "v1beta1"]
  clientConfig:
    service:
      name: webhook
      namespace: default
  failurePolicy: Fail
  sideEffects: None
  name: webhook.serving.knative.dev
  timeoutSeconds: 10
---
# Source: knative-serving/templates/core/webhook/configuration.yaml
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  name: config.webhook.serving.knative.dev
  labels:
    serving.knative.dev/release: "1.6.0"
webhooks:
- admissionReviewVersions: ["v1", "v1beta1"]
  clientConfig:
    service:
      name: webhook
      namespace: default
  failurePolicy: Fail
  sideEffects: None
  name: config.webhook.serving.knative.dev
  namespaceSelector:
    matchExpressions:
    - key: serving.knative.dev/release
      operator: Exists
  timeoutSeconds: 10
---
# Source: knative-serving/templates/core/webhook/configuration.yaml
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  name: validation.webhook.serving.knative.dev
  labels:
    serving.knative.dev/release: "1.6.0"
webhooks:
- admissionReviewVersions: ["v1", "v1beta1"]
  clientConfig:
    service:
      name: webhook
      namespace: default
  failurePolicy: Fail
  sideEffects: None
  name: validation.webhook.serving.knative.dev
  timeoutSeconds: 10
